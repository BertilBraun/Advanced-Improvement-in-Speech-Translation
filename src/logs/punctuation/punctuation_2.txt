2024-01-30 22:29:54 | INFO | train_inner | epoch 100:    458 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=64557.4, ups=16.58, wpb=3894.8, bsz=227.7, num_updates=134000, lr=8.63868e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=8607
2024-01-30 22:30:00 | INFO | train_inner | epoch 100:    558 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65863.7, ups=16.62, wpb=3962.5, bsz=221.4, num_updates=134100, lr=8.63546e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=8613
2024-01-30 22:30:06 | INFO | train_inner | epoch 100:    658 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=66414.7, ups=16.8, wpb=3952.3, bsz=199.5, num_updates=134200, lr=8.63224e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=8619
2024-01-30 22:30:12 | INFO | train_inner | epoch 100:    758 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=64343, ups=16.68, wpb=3857.1, bsz=214.3, num_updates=134300, lr=8.62903e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=8625
2024-01-30 22:30:18 | INFO | train_inner | epoch 100:    858 / 1349 loss=1.794, nll_loss=0.195, ppl=1.15, wps=64938.9, ups=16.61, wpb=3909.1, bsz=215.8, num_updates=134400, lr=8.62582e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.8, wall=8631
2024-01-30 22:30:24 | INFO | train_inner | epoch 100:    958 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=64947.9, ups=16.7, wpb=3888.3, bsz=199.8, num_updates=134500, lr=8.62261e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.7, wall=8637
2024-01-30 22:30:30 | INFO | train_inner | epoch 100:   1058 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=66348.6, ups=16.74, wpb=3962.9, bsz=201.7, num_updates=134600, lr=8.61941e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.9, wall=8643
2024-01-30 22:30:36 | INFO | train_inner | epoch 100:   1158 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65705.3, ups=16.79, wpb=3914.3, bsz=204.4, num_updates=134700, lr=8.61621e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=8649
2024-01-30 22:30:42 | INFO | train_inner | epoch 100:   1258 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=64917.1, ups=16.63, wpb=3903.1, bsz=229, num_updates=134800, lr=8.61301e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=8655
2024-01-30 22:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:30:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:30:49 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 2.157 | nll_loss 0.415 | ppl 1.33 | wps 170974 | wpb 3275.2 | bsz 176.5 | num_updates 134891 | best_loss 2.063
2024-01-30 22:30:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 134891 updates
2024-01-30 22:30:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint100.pt
2024-01-30 22:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint100.pt
2024-01-30 22:30:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint100.pt (epoch 100 @ 134891 updates, score 2.157) (writing took 2.282351272064261 seconds)
2024-01-30 22:30:52 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2024-01-30 22:30:52 | INFO | train | epoch 100 | loss 1.795 | nll_loss 0.197 | ppl 1.15 | wps 62148 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 134891 | lr 8.61011e-05 | gnorm 0.5 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 8665
2024-01-30 22:30:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:30:52 | INFO | fairseq_cli.train | done training in 8659.8 seconds
Training complete.
Training complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 4504 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.en: 15529 sents, 225049 tokens, 0.00355% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.de: 15529 sents, 256398 tokens, 0.0215% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
Traceback (most recent call last):
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 176, in <module>
    main()
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 149, in main
    args.inputs = last_n_checkpoints(
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 94, in last_n_checkpoints
    raise Exception(
Exception: ('Found {} checkpoint files but need at least {}', 1, 5)
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/train_punctuation_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 4504 types
INFO:fairseq.tasks.translation:[de] dictionary: 6520 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Traceback (most recent call last):
  File "/home/kit/stud/uxude/miniconda3/envs/nmt/bin/fairseq-generate", line 8, in <module>
    sys.exit(cli_main())
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 413, in cli_main
    main(args)
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 50, in main
    return _main(cfg, sys.stdout)
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 96, in _main
    models, saved_cfg = checkpoint_utils.load_model_ensemble(
  File "/home/kit/stud/uxude/fairseq/fairseq/checkpoint_utils.py", line 392, in load_model_ensemble
    ensemble, args, _task = load_model_ensemble_and_task(
  File "/home/kit/stud/uxude/fairseq/fairseq/checkpoint_utils.py", line 448, in load_model_ensemble_and_task
    raise IOError("Model file not found: {}".format(filename))
OSError: Model file not found: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/train_punctuation_covost/hyp.txt and /home/kit/stud/uxude/predictions/train_punctuation_covost/ref.txt
Sample predictions:
Sample:
Reference:
Sample:
Reference:
WER:
BLEU:
sacreBLEU: System and reference streams have different lengths.
Translation complete.

============================= JOB FEEDBACK =============================

NodeName=uc2n512
Job ID: 23092674
Cluster: uc2
User/Group: uxude/stud
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 2
CPU Utilized: 03:13:16
CPU Efficiency: 63.44% of 05:04:40 core-walltime
Job Wall-clock time: 02:32:20
Memory Utilized: 2.10 GB
Memory Efficiency: 1.07% of 195.31 GB
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[2024-01-31 00:07:47] [INFO] [Dataset::Prepare Datasets]: Preparing CoVoST...
Loading CoVoST train: 100%|██████████| 289430/289430 [00:02<00:00, 120153.81it/s]
Loading CoVoST dev: 100%|██████████| 15531/15531 [00:00<00:00, 207186.57it/s]
Loading CoVoST test: 100%|██████████| 15531/15531 [00:00<00:00, 203326.47it/s]
cp: cannot stat '/pfs/work7/workspace/scratch/uxude-ASR/spm.asr.model': No such file or directory
[2024-01-31 00:43:14] [INFO] [Dataset::Prepare Datasets]: Creating ASR configs for CoVoST...
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Encodings for train already exist, skipping encoding creation...
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Encodings for dev already exist, skipping encoding creation...
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Encodings for test already exist, skipping encoding creation...
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Fetching manifest from train...
Manifest train:  76%|███████▌  | 2187/2895 [1:48:51<39:32,  3.35s/it][2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411967!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411975!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411976!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411984!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411985!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411986!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411987!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411989!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411990!
Manifest train: 100%|██████████| 2895/2895 [2:26:33<00:00,  3.04s/it]
[2024-01-31 03:09:48] [INFO] [ASR::Config]: Saving manifest for train...
[2024-01-31 03:09:53] [INFO] [ASR::Config]: Fetching manifest from dev...
Manifest dev: 100%|██████████| 156/156 [09:03<00:00,  3.48s/it]
[2024-01-31 03:18:56] [INFO] [ASR::Config]: Saving manifest for dev...
[2024-01-31 03:18:56] [INFO] [ASR::Config]: Fetching manifest from test...
Manifest test: 100%|██████████| 156/156 [08:56<00:00,  3.44s/it]
[2024-01-31 03:27:53] [INFO] [ASR::Config]: Saving manifest for test...
[2024-01-31 03:27:53] [INFO] [ASR::Config]: Generating config yaml...
[2024-01-31 03:27:53] [INFO] [ASR::Config]: Generated config yaml...
[2024-01-31 03:27:53] [INFO] [Dataset::Prepare Datasets]: Preparing MT CoVoST...
Loading CoVoST train: 100%|██████████| 289430/289430 [00:04<00:00, 70736.46it/s]
100%|██████████| 2895/2895 [00:04<00:00, 585.59it/s]
Processing dataset: 100%|██████████| 289430/289430 [00:02<00:00, 116091.05it/s]
Loading CoVoST dev: 100%|██████████| 15531/15531 [00:00<00:00, 208987.89it/s]
100%|██████████| 156/156 [00:00<00:00, 905.20it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 117229.71it/s]
Loading CoVoST test: 100%|██████████| 15531/15531 [00:00<00:00, 24182.41it/s]
100%|██████████| 156/156 [00:00<00:00, 899.83it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 131370.69it/s]
[2024-01-31 04:40:24] [INFO] [MT::Config]: Writing MT Dataset train to disk...
Writing to files: 289024it [00:00, 854394.71it/s]
[2024-01-31 04:40:25] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:40:26] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:40:26] [INFO] [MT::Config]: Encoding split train...
[2024-01-31 04:40:26] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/train.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train.en already exists.
[2024-01-31 04:40:26] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/train.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train.de already exists.
[2024-01-31 04:40:26] [INFO] [MT::Config]: Writing MT Dataset dev to disk...
Writing to files: 15520it [00:00, 1526023.82it/s]
[2024-01-31 04:40:27] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding split dev...
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/dev.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev.en already exists.
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/dev.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev.de already exists.
[2024-01-31 04:40:27] [INFO] [MT::Config]: Writing MT Dataset test to disk...
Writing to files: 15516it [00:00, 1665357.00it/s]
[2024-01-31 04:40:27] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding split test...
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/test.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.en already exists.
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/test.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.de already exists.
[2024-01-31 04:40:27] [INFO] [Dataset::Prepare Datasets]: Preparing MT CoVoST with punctuation...
Loading CoVoST train: 100%|██████████| 289430/289430 [00:01<00:00, 151489.17it/s]
100%|██████████| 2895/2895 [00:05<00:00, 534.20it/s]
Processing dataset: 100%|██████████| 289430/289430 [00:02<00:00, 127714.36it/s]
Loading CoVoST dev: 100%|██████████| 15531/15531 [00:00<00:00, 210243.14it/s]
100%|██████████| 156/156 [00:00<00:00, 607.99it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 126207.47it/s]
Loading CoVoST test: 100%|██████████| 15531/15531 [00:00<00:00, 204601.80it/s]
100%|██████████| 156/156 [00:00<00:00, 624.70it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 141260.85it/s]
[2024-01-31 04:41:15] [INFO] [MT::Config]: Writing MT Dataset train to disk...
Writing to files: 289365it [00:00, 1232080.98it/s]
[2024-01-31 04:41:16] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding split train...
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/train.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train.en already exists.
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/train.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train.de already exists.
[2024-01-31 04:41:16] [INFO] [MT::Config]: Writing MT Dataset dev to disk...
Writing to files: 15530it [00:00, 1798088.14it/s]
[2024-01-31 04:41:16] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding split dev...
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/dev.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev.en already exists.
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/dev.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev.de already exists.
[2024-01-31 04:41:16] [INFO] [MT::Config]: Writing MT Dataset test to disk...
Writing to files: 15529it [00:00, 1737074.54it/s]
[2024-01-31 04:41:16] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding split test...
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/test.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.en already exists.
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/test.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.de already exists.
[2024-01-31 04:41:16] [INFO] [Dataset::Prepare Datasets]: Done!
Training the Punctuation model...
Binarized data already exists. Skipping binarization.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models
Training time: 6 hours
2024-01-31 04:43:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=50000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-01-31 04:43:38 | INFO | fairseq.tasks.translation | [en] dictionary: 4504 types
2024-01-31 04:43:38 | INFO | fairseq.tasks.translation | [de] dictionary: 6520 types
2024-01-31 04:43:41 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4504, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6520, bias=False)
  )
)
2024-01-31 04:43:41 | INFO | fairseq_cli.train | task: TranslationTask
2024-01-31 04:43:41 | INFO | fairseq_cli.train | model: TransformerModel
2024-01-31 04:43:41 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-31 04:43:41 | INFO | fairseq_cli.train | num. shared model params: 49,782,784 (num. trained: 49,782,784)
2024-01-31 04:43:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-31 04:43:41 | INFO | fairseq.data.data_utils | loaded 15,530 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/valid.en-de.en
2024-01-31 04:43:41 | INFO | fairseq.data.data_utils | loaded 15,530 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/valid.en-de.de
2024-01-31 04:43:41 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset valid en-de 15530 examples
2024-01-31 04:43:41 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-31 04:43:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 04:43:41 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-01-31 04:43:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 04:43:41 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-31 04:43:41 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-01-31 04:43:41 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt
2024-01-31 04:43:45 | INFO | fairseq.trainer | Loaded checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt (epoch 101 @ 134891 updates)
2024-01-31 04:43:45 | INFO | fairseq.trainer | loading train data for epoch 101
2024-01-31 04:43:45 | INFO | fairseq.data.data_utils | loaded 289,365 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/train.en-de.en
2024-01-31 04:43:45 | INFO | fairseq.data.data_utils | loaded 289,365 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/train.en-de.de
2024-01-31 04:43:45 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset train en-de 289365 examples
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 101
2024-01-31 04:43:46 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-31 04:43:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:43:48 | INFO | fairseq.trainer | begin training epoch 101
2024-01-31 04:43:48 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-01-31 04:43:56 | INFO | train_inner | epoch 101:      9 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=51619.2, ups=13.21, wpb=3854.1, bsz=268.4, num_updates=134900, lr=8.60982e-05, gnorm=0.619, loss_scale=8, train_wall=8, gb_free=29.8, wall=15
2024-01-31 04:44:03 | INFO | train_inner | epoch 101:    109 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=59933.9, ups=15.25, wpb=3929.6, bsz=213.7, num_updates=135000, lr=8.60663e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.7, wall=22
2024-01-31 04:44:09 | INFO | train_inner | epoch 101:    209 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=60319.4, ups=15.38, wpb=3922.5, bsz=200.5, num_updates=135100, lr=8.60344e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.8, wall=28
2024-01-31 04:44:16 | INFO | train_inner | epoch 101:    309 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59392.6, ups=15.27, wpb=3889.2, bsz=212, num_updates=135200, lr=8.60026e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.8, wall=35
2024-01-31 04:44:22 | INFO | train_inner | epoch 101:    409 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59695.6, ups=15.33, wpb=3894.1, bsz=227.8, num_updates=135300, lr=8.59708e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.8, wall=41
2024-01-31 04:44:29 | INFO | train_inner | epoch 101:    509 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60429.4, ups=15.25, wpb=3962.4, bsz=217.4, num_updates=135400, lr=8.59391e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=48
2024-01-31 04:44:36 | INFO | train_inner | epoch 101:    609 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59935.5, ups=15.28, wpb=3922.1, bsz=215.3, num_updates=135500, lr=8.59074e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.7, wall=54
2024-01-31 04:44:42 | INFO | train_inner | epoch 101:    709 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=59708.8, ups=15.31, wpb=3900.9, bsz=210.1, num_updates=135600, lr=8.58757e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.9, wall=61
2024-01-31 04:44:49 | INFO | train_inner | epoch 101:    809 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=59884.3, ups=15.29, wpb=3915.8, bsz=212.9, num_updates=135700, lr=8.5844e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.8, wall=68
2024-01-31 04:44:55 | INFO | train_inner | epoch 101:    909 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=59966.2, ups=15.25, wpb=3932.1, bsz=208.6, num_updates=135800, lr=8.58124e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.7, wall=74
2024-01-31 04:45:02 | INFO | train_inner | epoch 101:   1009 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=59888.1, ups=15.17, wpb=3947.6, bsz=220.9, num_updates=135900, lr=8.57808e-05, gnorm=0.553, loss_scale=8, train_wall=6, gb_free=29.8, wall=81
2024-01-31 04:45:08 | INFO | train_inner | epoch 101:   1109 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=59500.8, ups=15.29, wpb=3890.3, bsz=213.5, num_updates=136000, lr=8.57493e-05, gnorm=0.574, loss_scale=8, train_wall=6, gb_free=29.8, wall=87
2024-01-31 04:45:15 | INFO | train_inner | epoch 101:   1209 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60351.9, ups=15.25, wpb=3956.3, bsz=218.8, num_updates=136100, lr=8.57178e-05, gnorm=0.476, loss_scale=8, train_wall=6, gb_free=29.8, wall=94
2024-01-31 04:45:21 | INFO | train_inner | epoch 101:   1309 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=60539.6, ups=15.39, wpb=3933.9, bsz=206.2, num_updates=136200, lr=8.56863e-05, gnorm=0.735, loss_scale=8, train_wall=6, gb_free=29.8, wall=100
2024-01-31 04:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:45:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:45:26 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 2.155 | nll_loss 0.413 | ppl 1.33 | wps 128233 | wpb 3275.2 | bsz 176.5 | num_updates 136240 | best_loss 2.063
2024-01-31 04:45:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 136240 updates
2024-01-31 04:45:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint101.pt
2024-01-31 04:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint101.pt
2024-01-31 04:45:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint101.pt (epoch 101 @ 136240 updates, score 2.155) (writing took 2.3735130779678 seconds)
2024-01-31 04:45:29 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2024-01-31 04:45:29 | INFO | train | epoch 101 | loss 1.795 | nll_loss 0.197 | ppl 1.15 | wps 56912.6 | ups 14.51 | wpb 3921.2 | bsz 214.5 | num_updates 136240 | lr 8.56737e-05 | gnorm 0.523 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 107
2024-01-31 04:45:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:45:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:45:29 | INFO | fairseq.trainer | begin training epoch 102
2024-01-31 04:45:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:45:33 | INFO | train_inner | epoch 102:     60 / 1349 loss=1.791, nll_loss=0.192, ppl=1.14, wps=34561.3, ups=8.83, wpb=3912.2, bsz=222, num_updates=136300, lr=8.56549e-05, gnorm=0.446, loss_scale=8, train_wall=6, gb_free=29.8, wall=112
2024-01-31 04:45:39 | INFO | train_inner | epoch 102:    160 / 1349 loss=1.792, nll_loss=0.193, ppl=1.14, wps=60569.3, ups=15.3, wpb=3959.8, bsz=223.8, num_updates=136400, lr=8.56235e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=118
2024-01-31 04:45:46 | INFO | train_inner | epoch 102:    260 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59598.4, ups=15.17, wpb=3927.6, bsz=223.3, num_updates=136500, lr=8.55921e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=125
2024-01-31 04:45:52 | INFO | train_inner | epoch 102:    360 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=60402.4, ups=15.32, wpb=3942.7, bsz=217.7, num_updates=136600, lr=8.55608e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=131
2024-01-31 04:45:59 | INFO | train_inner | epoch 102:    460 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60384, ups=15.36, wpb=3931, bsz=201.8, num_updates=136700, lr=8.55295e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.7, wall=138
2024-01-31 04:46:05 | INFO | train_inner | epoch 102:    560 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=60035.5, ups=15.34, wpb=3913.2, bsz=214.6, num_updates=136800, lr=8.54982e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=144
2024-01-31 04:46:12 | INFO | train_inner | epoch 102:    660 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60131.8, ups=15.3, wpb=3930.7, bsz=223.3, num_updates=136900, lr=8.5467e-05, gnorm=0.439, loss_scale=8, train_wall=6, gb_free=29.8, wall=151
2024-01-31 04:46:18 | INFO | train_inner | epoch 102:    760 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=59499.2, ups=15.32, wpb=3883.9, bsz=211.8, num_updates=137000, lr=8.54358e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.7, wall=157
2024-01-31 04:46:25 | INFO | train_inner | epoch 102:    860 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60519.4, ups=15.24, wpb=3970.2, bsz=223.8, num_updates=137100, lr=8.54046e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.8, wall=164
2024-01-31 04:46:32 | INFO | train_inner | epoch 102:    960 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=58913.1, ups=15.32, wpb=3845.5, bsz=208.6, num_updates=137200, lr=8.53735e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=170
2024-01-31 04:46:38 | INFO | train_inner | epoch 102:   1060 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=59617.8, ups=15.21, wpb=3918.8, bsz=212, num_updates=137300, lr=8.53424e-05, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.7, wall=177
2024-01-31 04:46:45 | INFO | train_inner | epoch 102:   1160 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60205.9, ups=15.36, wpb=3918.9, bsz=214.5, num_updates=137400, lr=8.53113e-05, gnorm=0.596, loss_scale=8, train_wall=6, gb_free=29.9, wall=183
2024-01-31 04:46:51 | INFO | train_inner | epoch 102:   1260 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=59995.7, ups=15.47, wpb=3877.7, bsz=196.2, num_updates=137500, lr=8.52803e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=190
2024-01-31 04:46:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:46:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:46:59 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 2.156 | nll_loss 0.413 | ppl 1.33 | wps 129537 | wpb 3275.2 | bsz 176.5 | num_updates 137589 | best_loss 2.063
2024-01-31 04:46:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 137589 updates
2024-01-31 04:46:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint102.pt
2024-01-31 04:47:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint102.pt
2024-01-31 04:47:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint102.pt (epoch 102 @ 137589 updates, score 2.156) (writing took 2.2675092520657927 seconds)
2024-01-31 04:47:01 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2024-01-31 04:47:01 | INFO | train | epoch 102 | loss 1.795 | nll_loss 0.196 | ppl 1.15 | wps 56998.9 | ups 14.54 | wpb 3921.2 | bsz 214.5 | num_updates 137589 | lr 8.52527e-05 | gnorm 0.505 | loss_scale 8 | train_wall 85 | gb_free 29.8 | wall 200
2024-01-31 04:47:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:47:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:47:01 | INFO | fairseq.trainer | begin training epoch 103
2024-01-31 04:47:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:47:02 | INFO | train_inner | epoch 103:     11 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=35335.9, ups=8.94, wpb=3954.7, bsz=214.9, num_updates=137600, lr=8.52493e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.7, wall=201
2024-01-31 04:47:09 | INFO | train_inner | epoch 103:    111 / 1349 loss=1.79, nll_loss=0.191, ppl=1.14, wps=60404.3, ups=15.33, wpb=3940.4, bsz=224.6, num_updates=137700, lr=8.52183e-05, gnorm=0.43, loss_scale=8, train_wall=6, gb_free=29.8, wall=208
2024-01-31 04:47:15 | INFO | train_inner | epoch 103:    211 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=60000.2, ups=15.22, wpb=3941.5, bsz=224.4, num_updates=137800, lr=8.51874e-05, gnorm=0.435, loss_scale=8, train_wall=6, gb_free=29.8, wall=214
2024-01-31 04:47:22 | INFO | train_inner | epoch 103:    311 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60096.7, ups=15.33, wpb=3921, bsz=207.7, num_updates=137900, lr=8.51565e-05, gnorm=0.533, loss_scale=8, train_wall=6, gb_free=30, wall=221
2024-01-31 04:47:28 | INFO | train_inner | epoch 103:    411 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59063.2, ups=15.28, wpb=3865, bsz=205.8, num_updates=138000, lr=8.51257e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=227
2024-01-31 04:47:35 | INFO | train_inner | epoch 103:    511 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59509, ups=15.25, wpb=3903.2, bsz=216.8, num_updates=138100, lr=8.50948e-05, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.9, wall=234
2024-01-31 04:47:42 | INFO | train_inner | epoch 103:    611 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60818.4, ups=15.27, wpb=3981.8, bsz=219.6, num_updates=138200, lr=8.5064e-05, gnorm=0.445, loss_scale=8, train_wall=6, gb_free=29.7, wall=240
2024-01-31 04:47:48 | INFO | train_inner | epoch 103:    711 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60632.9, ups=15.32, wpb=3956.8, bsz=214.3, num_updates=138300, lr=8.50333e-05, gnorm=0.454, loss_scale=8, train_wall=6, gb_free=29.8, wall=247
2024-01-31 04:47:55 | INFO | train_inner | epoch 103:    811 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=59598.6, ups=15.35, wpb=3883.1, bsz=211.6, num_updates=138400, lr=8.50026e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.9, wall=253
2024-01-31 04:48:01 | INFO | train_inner | epoch 103:    911 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60118.9, ups=15.2, wpb=3955.9, bsz=211.5, num_updates=138500, lr=8.49719e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=260
2024-01-31 04:48:08 | INFO | train_inner | epoch 103:   1011 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=59964.9, ups=15.31, wpb=3915.6, bsz=209.2, num_updates=138600, lr=8.49412e-05, gnorm=0.538, loss_scale=8, train_wall=6, gb_free=29.8, wall=267
2024-01-31 04:48:14 | INFO | train_inner | epoch 103:   1111 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=59385.2, ups=15.27, wpb=3890.1, bsz=208.3, num_updates=138700, lr=8.49106e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=273
2024-01-31 04:48:21 | INFO | train_inner | epoch 103:   1211 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=60363.8, ups=15.36, wpb=3928.7, bsz=217.3, num_updates=138800, lr=8.488e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=280
2024-01-31 04:48:27 | INFO | train_inner | epoch 103:   1311 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=59707, ups=15.27, wpb=3909.5, bsz=220.6, num_updates=138900, lr=8.48494e-05, gnorm=0.558, loss_scale=8, train_wall=6, gb_free=29.8, wall=286
2024-01-31 04:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:48:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:48:32 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 2.158 | nll_loss 0.417 | ppl 1.34 | wps 129713 | wpb 3275.2 | bsz 176.5 | num_updates 138938 | best_loss 2.063
2024-01-31 04:48:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 138938 updates
2024-01-31 04:48:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint103.pt
2024-01-31 04:48:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint103.pt
2024-01-31 04:48:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint103.pt (epoch 103 @ 138938 updates, score 2.158) (writing took 2.248017951962538 seconds)
2024-01-31 04:48:34 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2024-01-31 04:48:34 | INFO | train | epoch 103 | loss 1.794 | nll_loss 0.196 | ppl 1.15 | wps 57002.1 | ups 14.54 | wpb 3921.2 | bsz 214.5 | num_updates 138938 | lr 8.48378e-05 | gnorm 0.494 | loss_scale 8 | train_wall 85 | gb_free 29.8 | wall 293
2024-01-31 04:48:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:48:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:48:34 | INFO | fairseq.trainer | begin training epoch 104
2024-01-31 04:48:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:48:38 | INFO | train_inner | epoch 104:     62 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=35435.4, ups=9.03, wpb=3924.8, bsz=213.8, num_updates=139000, lr=8.48189e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.7, wall=297
2024-01-31 04:48:45 | INFO | train_inner | epoch 104:    162 / 1349 loss=1.789, nll_loss=0.19, ppl=1.14, wps=60449.4, ups=15.32, wpb=3946.4, bsz=222.4, num_updates=139100, lr=8.47884e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.8, wall=304
2024-01-31 04:48:51 | INFO | train_inner | epoch 104:    262 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=59564.6, ups=15.4, wpb=3867.6, bsz=206.9, num_updates=139200, lr=8.47579e-05, gnorm=0.657, loss_scale=8, train_wall=6, gb_free=29.7, wall=310
2024-01-31 04:48:58 | INFO | train_inner | epoch 104:    362 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=59949.5, ups=15.24, wpb=3934.2, bsz=221, num_updates=139300, lr=8.47275e-05, gnorm=0.432, loss_scale=8, train_wall=6, gb_free=29.8, wall=317
2024-01-31 04:49:04 | INFO | train_inner | epoch 104:    462 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60581.9, ups=15.31, wpb=3957.9, bsz=225.8, num_updates=139400, lr=8.46971e-05, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.8, wall=323
2024-01-31 04:49:11 | INFO | train_inner | epoch 104:    562 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=60611.5, ups=15.56, wpb=3895.9, bsz=202.4, num_updates=139500, lr=8.46668e-05, gnorm=0.583, loss_scale=8, train_wall=6, gb_free=29.8, wall=330
2024-01-31 04:49:17 | INFO | train_inner | epoch 104:    662 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60464.2, ups=15.32, wpb=3947.3, bsz=219.4, num_updates=139600, lr=8.46364e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=336
2024-01-31 04:49:24 | INFO | train_inner | epoch 104:    762 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60094.2, ups=15.38, wpb=3907.9, bsz=217.5, num_updates=139700, lr=8.46061e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.8, wall=343
2024-01-31 04:49:30 | INFO | train_inner | epoch 104:    862 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=61154.8, ups=15.49, wpb=3948.1, bsz=218.2, num_updates=139800, lr=8.45759e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=349
2024-01-31 04:49:37 | INFO | train_inner | epoch 104:    962 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60076.4, ups=15.46, wpb=3886.1, bsz=208.6, num_updates=139900, lr=8.45456e-05, gnorm=0.451, loss_scale=8, train_wall=6, gb_free=29.8, wall=356
2024-01-31 04:49:43 | INFO | train_inner | epoch 104:   1062 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=61149.8, ups=15.48, wpb=3950.7, bsz=207.7, num_updates=140000, lr=8.45154e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.7, wall=362
2024-01-31 04:49:50 | INFO | train_inner | epoch 104:   1162 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=60204, ups=15.43, wpb=3902.6, bsz=210.6, num_updates=140100, lr=8.44853e-05, gnorm=0.6, loss_scale=8, train_wall=6, gb_free=29.7, wall=369
2024-01-31 04:49:56 | INFO | train_inner | epoch 104:   1262 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60563.8, ups=15.45, wpb=3919.8, bsz=218.5, num_updates=140200, lr=8.44551e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=375
2024-01-31 04:50:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:50:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:50:04 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 2.157 | nll_loss 0.415 | ppl 1.33 | wps 129862 | wpb 3275.2 | bsz 176.5 | num_updates 140287 | best_loss 2.063
2024-01-31 04:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 140287 updates
2024-01-31 04:50:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint104.pt
2024-01-31 04:50:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint104.pt
2024-01-31 04:50:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint104.pt (epoch 104 @ 140287 updates, score 2.157) (writing took 2.195768535020761 seconds)
2024-01-31 04:50:06 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2024-01-31 04:50:06 | INFO | train | epoch 104 | loss 1.794 | nll_loss 0.195 | ppl 1.15 | wps 57427.2 | ups 14.65 | wpb 3921.2 | bsz 214.5 | num_updates 140287 | lr 8.44289e-05 | gnorm 0.508 | loss_scale 8 | train_wall 85 | gb_free 30 | wall 385
2024-01-31 04:50:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:50:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:50:06 | INFO | fairseq.trainer | begin training epoch 105
2024-01-31 04:50:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:50:07 | INFO | train_inner | epoch 105:     13 / 1349 loss=1.798, nll_loss=0.201, ppl=1.15, wps=35341.3, ups=9.1, wpb=3885.5, bsz=204.4, num_updates=140300, lr=8.4425e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=386
2024-01-31 04:50:14 | INFO | train_inner | epoch 105:    113 / 1349 loss=1.794, nll_loss=0.195, ppl=1.14, wps=59966.5, ups=15.43, wpb=3885.2, bsz=225.8, num_updates=140400, lr=8.43949e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.8, wall=393
2024-01-31 04:50:20 | INFO | train_inner | epoch 105:    213 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60772.2, ups=15.42, wpb=3942.2, bsz=215.8, num_updates=140500, lr=8.43649e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.8, wall=399
2024-01-31 04:50:27 | INFO | train_inner | epoch 105:    313 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60388.7, ups=15.43, wpb=3913.3, bsz=215.7, num_updates=140600, lr=8.43349e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=30.1, wall=406
2024-01-31 04:50:33 | INFO | train_inner | epoch 105:    413 / 1349 loss=1.794, nll_loss=0.195, ppl=1.15, wps=59398.3, ups=15.49, wpb=3834.7, bsz=204.6, num_updates=140700, lr=8.43049e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.9, wall=412
2024-01-31 04:50:40 | INFO | train_inner | epoch 105:    513 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60606.1, ups=15.56, wpb=3895.7, bsz=209.7, num_updates=140800, lr=8.4275e-05, gnorm=0.565, loss_scale=8, train_wall=6, gb_free=29.8, wall=419
2024-01-31 04:50:46 | INFO | train_inner | epoch 105:    613 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=61032, ups=15.48, wpb=3942.9, bsz=202.7, num_updates=140900, lr=8.42451e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.9, wall=425
2024-01-31 04:50:53 | INFO | train_inner | epoch 105:    713 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60394, ups=15.34, wpb=3937.3, bsz=211.7, num_updates=141000, lr=8.42152e-05, gnorm=0.436, loss_scale=8, train_wall=6, gb_free=29.8, wall=431
2024-01-31 04:50:59 | INFO | train_inner | epoch 105:    813 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60375, ups=15.43, wpb=3912.1, bsz=223.5, num_updates=141100, lr=8.41853e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=438
2024-01-31 04:51:06 | INFO | train_inner | epoch 105:    913 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=61220.7, ups=15.39, wpb=3978.9, bsz=219.5, num_updates=141200, lr=8.41555e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=444
2024-01-31 04:51:12 | INFO | train_inner | epoch 105:   1013 / 1349 loss=1.794, nll_loss=0.197, ppl=1.15, wps=60870.4, ups=15.37, wpb=3959.4, bsz=214.5, num_updates=141300, lr=8.41257e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.7, wall=451
2024-01-31 04:51:19 | INFO | train_inner | epoch 105:   1113 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60200, ups=15.44, wpb=3899.2, bsz=217.1, num_updates=141400, lr=8.4096e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=457
2024-01-31 04:51:25 | INFO | train_inner | epoch 105:   1213 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60151.7, ups=15.34, wpb=3920.8, bsz=226.7, num_updates=141500, lr=8.40663e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=464
2024-01-31 04:51:31 | INFO | train_inner | epoch 105:   1313 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=61244.5, ups=15.58, wpb=3930.3, bsz=197.4, num_updates=141600, lr=8.40366e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.7, wall=470
2024-01-31 04:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:51:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:51:36 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 2.157 | nll_loss 0.415 | ppl 1.33 | wps 131871 | wpb 3275.2 | bsz 176.5 | num_updates 141636 | best_loss 2.063
2024-01-31 04:51:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 141636 updates
2024-01-31 04:51:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint105.pt
2024-01-31 04:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint105.pt
2024-01-31 04:51:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint105.pt (epoch 105 @ 141636 updates, score 2.157) (writing took 2.2683006370207295 seconds)
2024-01-31 04:51:38 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2024-01-31 04:51:38 | INFO | train | epoch 105 | loss 1.793 | nll_loss 0.195 | ppl 1.14 | wps 57484.4 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 141636 | lr 8.40259e-05 | gnorm 0.505 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 477
2024-01-31 04:51:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:51:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:51:38 | INFO | fairseq.trainer | begin training epoch 106
2024-01-31 04:51:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:51:43 | INFO | train_inner | epoch 106:     64 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=35907.7, ups=9.01, wpb=3985.4, bsz=211.5, num_updates=141700, lr=8.40069e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.8, wall=481
2024-01-31 04:51:49 | INFO | train_inner | epoch 106:    164 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60043, ups=15.5, wpb=3873.7, bsz=213.9, num_updates=141800, lr=8.39773e-05, gnorm=0.56, loss_scale=8, train_wall=6, gb_free=29.7, wall=488
2024-01-31 04:51:56 | INFO | train_inner | epoch 106:    264 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=60672.6, ups=15.39, wpb=3943.5, bsz=214.2, num_updates=141900, lr=8.39477e-05, gnorm=0.6, loss_scale=8, train_wall=6, gb_free=29.8, wall=494
2024-01-31 04:52:02 | INFO | train_inner | epoch 106:    364 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60469, ups=15.35, wpb=3939.1, bsz=226.2, num_updates=142000, lr=8.39181e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=501
2024-01-31 04:52:09 | INFO | train_inner | epoch 106:    464 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=61183.9, ups=15.42, wpb=3968.6, bsz=224, num_updates=142100, lr=8.38886e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=507
2024-01-31 04:52:15 | INFO | train_inner | epoch 106:    564 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=61173.8, ups=15.42, wpb=3967, bsz=219.4, num_updates=142200, lr=8.38591e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=514
2024-01-31 04:52:21 | INFO | train_inner | epoch 106:    664 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60726.9, ups=15.48, wpb=3923.9, bsz=213, num_updates=142300, lr=8.38296e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=520
2024-01-31 04:52:28 | INFO | train_inner | epoch 106:    764 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=59841, ups=15.53, wpb=3853.8, bsz=205.4, num_updates=142400, lr=8.38002e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.7, wall=527
2024-01-31 04:52:34 | INFO | train_inner | epoch 106:    864 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=59193.9, ups=15.51, wpb=3815.3, bsz=205, num_updates=142500, lr=8.37708e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=533
2024-01-31 04:52:41 | INFO | train_inner | epoch 106:    964 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60702.3, ups=15.39, wpb=3943.6, bsz=212.2, num_updates=142600, lr=8.37414e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=540
2024-01-31 04:52:47 | INFO | train_inner | epoch 106:   1064 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60453.6, ups=15.37, wpb=3932, bsz=225.5, num_updates=142700, lr=8.37121e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=546
2024-01-31 04:52:54 | INFO | train_inner | epoch 106:   1164 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60863, ups=15.58, wpb=3907.6, bsz=199.2, num_updates=142800, lr=8.36827e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=553
2024-01-31 04:53:00 | INFO | train_inner | epoch 106:   1264 / 1349 loss=1.796, nll_loss=0.199, ppl=1.15, wps=59840.1, ups=15.32, wpb=3907.2, bsz=235.4, num_updates=142900, lr=8.36535e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=559
2024-01-31 04:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:53:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:53:08 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 2.16 | nll_loss 0.419 | ppl 1.34 | wps 131411 | wpb 3275.2 | bsz 176.5 | num_updates 142985 | best_loss 2.063
2024-01-31 04:53:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 142985 updates
2024-01-31 04:53:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint106.pt
2024-01-31 04:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint106.pt
2024-01-31 04:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint106.pt (epoch 106 @ 142985 updates, score 2.16) (writing took 2.243905122973956 seconds)
2024-01-31 04:53:10 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2024-01-31 04:53:10 | INFO | train | epoch 106 | loss 1.793 | nll_loss 0.195 | ppl 1.14 | wps 57487.2 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 142985 | lr 8.36286e-05 | gnorm 0.519 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 569
2024-01-31 04:53:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:53:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:53:10 | INFO | fairseq.trainer | begin training epoch 107
2024-01-31 04:53:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:53:11 | INFO | train_inner | epoch 107:     15 / 1349 loss=1.795, nll_loss=0.198, ppl=1.15, wps=35657.9, ups=9, wpb=3960.4, bsz=207.8, num_updates=143000, lr=8.36242e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.7, wall=570
2024-01-31 04:53:18 | INFO | train_inner | epoch 107:    115 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=61019, ups=15.5, wpb=3936.2, bsz=196.3, num_updates=143100, lr=8.3595e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.9, wall=577
2024-01-31 04:53:24 | INFO | train_inner | epoch 107:    215 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60864.4, ups=15.45, wpb=3938.2, bsz=205.6, num_updates=143200, lr=8.35658e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=583
2024-01-31 04:53:31 | INFO | train_inner | epoch 107:    315 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60276.1, ups=15.37, wpb=3922.5, bsz=221.8, num_updates=143300, lr=8.35366e-05, gnorm=0.578, loss_scale=8, train_wall=6, gb_free=29.7, wall=590
2024-01-31 04:53:37 | INFO | train_inner | epoch 107:    415 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=59922.8, ups=15.43, wpb=3883.6, bsz=212.8, num_updates=143400, lr=8.35075e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.8, wall=596
2024-01-31 04:53:44 | INFO | train_inner | epoch 107:    515 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60264.4, ups=15.42, wpb=3907.7, bsz=214.2, num_updates=143500, lr=8.34784e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=603
2024-01-31 04:53:50 | INFO | train_inner | epoch 107:    615 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60496.9, ups=15.49, wpb=3905.2, bsz=217, num_updates=143600, lr=8.34493e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=609
2024-01-31 04:53:57 | INFO | train_inner | epoch 107:    715 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=60752.7, ups=15.53, wpb=3912.8, bsz=201.3, num_updates=143700, lr=8.34203e-05, gnorm=0.545, loss_scale=8, train_wall=6, gb_free=29.7, wall=616
2024-01-31 04:54:03 | INFO | train_inner | epoch 107:    815 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60407.7, ups=15.38, wpb=3928.7, bsz=231.8, num_updates=143800, lr=8.33913e-05, gnorm=0.405, loss_scale=8, train_wall=6, gb_free=29.7, wall=622
2024-01-31 04:54:10 | INFO | train_inner | epoch 107:    915 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=59941.3, ups=15.41, wpb=3890.4, bsz=229.4, num_updates=143900, lr=8.33623e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=629
2024-01-31 04:54:16 | INFO | train_inner | epoch 107:   1015 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60866.1, ups=15.41, wpb=3948.5, bsz=209, num_updates=144000, lr=8.33333e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=635
2024-01-31 04:54:23 | INFO | train_inner | epoch 107:   1115 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=60291.5, ups=15.36, wpb=3925.6, bsz=214.1, num_updates=144100, lr=8.33044e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.7, wall=642
2024-01-31 04:54:29 | INFO | train_inner | epoch 107:   1215 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60384.5, ups=15.41, wpb=3918.1, bsz=221.1, num_updates=144200, lr=8.32755e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=648
2024-01-31 04:54:36 | INFO | train_inner | epoch 107:   1315 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60863.1, ups=15.43, wpb=3944, bsz=207, num_updates=144300, lr=8.32467e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=655
2024-01-31 04:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:54:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:54:40 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 2.16 | nll_loss 0.418 | ppl 1.34 | wps 132610 | wpb 3275.2 | bsz 176.5 | num_updates 144334 | best_loss 2.063
2024-01-31 04:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 144334 updates
2024-01-31 04:54:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint107.pt
2024-01-31 04:54:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint107.pt
2024-01-31 04:54:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint107.pt (epoch 107 @ 144334 updates, score 2.16) (writing took 2.2578000200446695 seconds)
2024-01-31 04:54:42 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2024-01-31 04:54:42 | INFO | train | epoch 107 | loss 1.792 | nll_loss 0.194 | ppl 1.14 | wps 57461.2 | ups 14.65 | wpb 3921.2 | bsz 214.5 | num_updates 144334 | lr 8.32369e-05 | gnorm 0.5 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 661
2024-01-31 04:54:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:54:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:54:42 | INFO | fairseq.trainer | begin training epoch 108
2024-01-31 04:54:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:54:47 | INFO | train_inner | epoch 108:     66 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=35011.4, ups=9.02, wpb=3879.8, bsz=216.4, num_updates=144400, lr=8.32178e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.7, wall=666
2024-01-31 04:54:53 | INFO | train_inner | epoch 108:    166 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60925, ups=15.5, wpb=3930.5, bsz=214.6, num_updates=144500, lr=8.3189e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=672
2024-01-31 04:55:00 | INFO | train_inner | epoch 108:    266 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60791.2, ups=15.41, wpb=3944, bsz=205, num_updates=144600, lr=8.31603e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=679
2024-01-31 04:55:06 | INFO | train_inner | epoch 108:    366 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=60634.9, ups=15.55, wpb=3899.6, bsz=202.1, num_updates=144700, lr=8.31315e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=685
2024-01-31 04:55:13 | INFO | train_inner | epoch 108:    466 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60861, ups=15.46, wpb=3937.6, bsz=205.4, num_updates=144800, lr=8.31028e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.8, wall=692
2024-01-31 04:55:19 | INFO | train_inner | epoch 108:    566 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=59852, ups=15.42, wpb=3882.6, bsz=213.5, num_updates=144900, lr=8.30741e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.7, wall=698
2024-01-31 04:55:26 | INFO | train_inner | epoch 108:    666 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60929.8, ups=15.45, wpb=3943.1, bsz=217.9, num_updates=145000, lr=8.30455e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=704
2024-01-31 04:55:32 | INFO | train_inner | epoch 108:    766 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60308.3, ups=15.36, wpb=3925.3, bsz=208.5, num_updates=145100, lr=8.30169e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.8, wall=711
2024-01-31 04:55:39 | INFO | train_inner | epoch 108:    866 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60708.7, ups=15.44, wpb=3932.1, bsz=228.7, num_updates=145200, lr=8.29883e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.7, wall=717
2024-01-31 04:55:45 | INFO | train_inner | epoch 108:    966 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60705.8, ups=15.44, wpb=3931.2, bsz=209.9, num_updates=145300, lr=8.29597e-05, gnorm=0.575, loss_scale=8, train_wall=6, gb_free=29.8, wall=724
2024-01-31 04:55:51 | INFO | train_inner | epoch 108:   1066 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60641, ups=15.49, wpb=3915.5, bsz=211.3, num_updates=145400, lr=8.29312e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=730
2024-01-31 04:55:58 | INFO | train_inner | epoch 108:   1166 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60807.1, ups=15.48, wpb=3928.8, bsz=208.1, num_updates=145500, lr=8.29027e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=737
2024-01-31 04:56:04 | INFO | train_inner | epoch 108:   1266 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59690, ups=15.27, wpb=3909.5, bsz=231.7, num_updates=145600, lr=8.28742e-05, gnorm=0.444, loss_scale=8, train_wall=6, gb_free=29.8, wall=743
2024-01-31 04:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:56:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:56:12 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 2.166 | nll_loss 0.427 | ppl 1.34 | wps 131561 | wpb 3275.2 | bsz 176.5 | num_updates 145683 | best_loss 2.063
2024-01-31 04:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 145683 updates
2024-01-31 04:56:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint108.pt
2024-01-31 04:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint108.pt
2024-01-31 04:56:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint108.pt (epoch 108 @ 145683 updates, score 2.166) (writing took 2.2045433369930834 seconds)
2024-01-31 04:56:14 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2024-01-31 04:56:14 | INFO | train | epoch 108 | loss 1.792 | nll_loss 0.194 | ppl 1.14 | wps 57495.6 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 145683 | lr 8.28506e-05 | gnorm 0.506 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 753
2024-01-31 04:56:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:56:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:56:14 | INFO | fairseq.trainer | begin training epoch 109
2024-01-31 04:56:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:56:16 | INFO | train_inner | epoch 109:     17 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=35607.2, ups=8.99, wpb=3959.3, bsz=235.4, num_updates=145700, lr=8.28457e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.8, wall=755
2024-01-31 04:56:22 | INFO | train_inner | epoch 109:    117 / 1349 loss=1.789, nll_loss=0.19, ppl=1.14, wps=60763.8, ups=15.45, wpb=3933.4, bsz=213.9, num_updates=145800, lr=8.28173e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.8, wall=761
2024-01-31 04:56:29 | INFO | train_inner | epoch 109:    217 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60266.4, ups=15.45, wpb=3901.9, bsz=211.9, num_updates=145900, lr=8.27889e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=767
2024-01-31 04:56:35 | INFO | train_inner | epoch 109:    317 / 1349 loss=1.793, nll_loss=0.195, ppl=1.15, wps=60562.8, ups=15.56, wpb=3891.4, bsz=214.4, num_updates=146000, lr=8.27606e-05, gnorm=0.459, loss_scale=8, train_wall=6, gb_free=29.7, wall=774
2024-01-31 04:56:41 | INFO | train_inner | epoch 109:    417 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=61190.2, ups=15.47, wpb=3955.9, bsz=213, num_updates=146100, lr=8.27323e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.7, wall=780
2024-01-31 04:56:48 | INFO | train_inner | epoch 109:    517 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60904.5, ups=15.36, wpb=3965.7, bsz=215.2, num_updates=146200, lr=8.2704e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.8, wall=787
2024-01-31 04:56:54 | INFO | train_inner | epoch 109:    617 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60336.5, ups=15.47, wpb=3900.4, bsz=208.6, num_updates=146300, lr=8.26757e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=793
2024-01-31 04:57:01 | INFO | train_inner | epoch 109:    717 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=61132, ups=15.41, wpb=3966, bsz=218.4, num_updates=146400, lr=8.26475e-05, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.7, wall=800
2024-01-31 04:57:07 | INFO | train_inner | epoch 109:    817 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60070.5, ups=15.31, wpb=3924, bsz=238.1, num_updates=146500, lr=8.26192e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.7, wall=806
2024-01-31 04:57:14 | INFO | train_inner | epoch 109:    917 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=59556.8, ups=15.43, wpb=3861, bsz=215.3, num_updates=146600, lr=8.25911e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=813
2024-01-31 04:57:20 | INFO | train_inner | epoch 109:   1017 / 1349 loss=1.795, nll_loss=0.198, ppl=1.15, wps=60635.1, ups=15.59, wpb=3889.6, bsz=201.9, num_updates=146700, lr=8.25629e-05, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.8, wall=819
2024-01-31 04:57:27 | INFO | train_inner | epoch 109:   1117 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60658.3, ups=15.5, wpb=3912.5, bsz=201.8, num_updates=146800, lr=8.25348e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=826
2024-01-31 04:57:33 | INFO | train_inner | epoch 109:   1217 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60143.5, ups=15.42, wpb=3899.3, bsz=223.9, num_updates=146900, lr=8.25067e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=832
2024-01-31 04:57:40 | INFO | train_inner | epoch 109:   1317 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60614.9, ups=15.41, wpb=3933.8, bsz=213.8, num_updates=147000, lr=8.24786e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.7, wall=839
2024-01-31 04:57:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:57:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:57:44 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 2.164 | nll_loss 0.422 | ppl 1.34 | wps 132034 | wpb 3275.2 | bsz 176.5 | num_updates 147032 | best_loss 2.063
2024-01-31 04:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 147032 updates
2024-01-31 04:57:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint109.pt
2024-01-31 04:57:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint109.pt
2024-01-31 04:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint109.pt (epoch 109 @ 147032 updates, score 2.164) (writing took 2.2898396340897307 seconds)
2024-01-31 04:57:46 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2024-01-31 04:57:46 | INFO | train | epoch 109 | loss 1.792 | nll_loss 0.194 | ppl 1.14 | wps 57496.4 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 147032 | lr 8.24696e-05 | gnorm 0.503 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 845
2024-01-31 04:57:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:57:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:57:46 | INFO | fairseq.trainer | begin training epoch 110
2024-01-31 04:57:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:57:51 | INFO | train_inner | epoch 110:     68 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=35141.8, ups=9, wpb=3902.7, bsz=206.6, num_updates=147100, lr=8.24506e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=850
2024-01-31 04:57:57 | INFO | train_inner | epoch 110:    168 / 1349 loss=1.789, nll_loss=0.19, ppl=1.14, wps=61125.6, ups=15.39, wpb=3972.6, bsz=210.9, num_updates=147200, lr=8.24226e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=856
2024-01-31 04:58:04 | INFO | train_inner | epoch 110:    268 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61009.7, ups=15.42, wpb=3957.1, bsz=211.1, num_updates=147300, lr=8.23946e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.7, wall=863
2024-01-31 04:58:10 | INFO | train_inner | epoch 110:    368 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60600.2, ups=15.45, wpb=3921.7, bsz=207.2, num_updates=147400, lr=8.23666e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.7, wall=869
2024-01-31 04:58:17 | INFO | train_inner | epoch 110:    468 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60192.4, ups=15.33, wpb=3926, bsz=228.3, num_updates=147500, lr=8.23387e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.7, wall=876
2024-01-31 04:58:23 | INFO | train_inner | epoch 110:    568 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60566.1, ups=15.49, wpb=3910.7, bsz=201.8, num_updates=147600, lr=8.23108e-05, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.8, wall=882
2024-01-31 04:58:30 | INFO | train_inner | epoch 110:    668 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60172.7, ups=15.45, wpb=3895.1, bsz=222.9, num_updates=147700, lr=8.22829e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=889
2024-01-31 04:58:36 | INFO | train_inner | epoch 110:    768 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60980.8, ups=15.39, wpb=3962.8, bsz=221, num_updates=147800, lr=8.22551e-05, gnorm=0.449, loss_scale=8, train_wall=6, gb_free=29.8, wall=895
2024-01-31 04:58:43 | INFO | train_inner | epoch 110:    868 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60352.7, ups=15.43, wpb=3910.9, bsz=222.4, num_updates=147900, lr=8.22273e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=902
2024-01-31 04:58:49 | INFO | train_inner | epoch 110:    968 / 1349 loss=1.796, nll_loss=0.199, ppl=1.15, wps=60919.4, ups=15.55, wpb=3917.3, bsz=205.6, num_updates=148000, lr=8.21995e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=908
2024-01-31 04:58:56 | INFO | train_inner | epoch 110:   1068 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60726.4, ups=15.46, wpb=3928.2, bsz=214.3, num_updates=148100, lr=8.21717e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=915
2024-01-31 04:59:02 | INFO | train_inner | epoch 110:   1168 / 1349 loss=1.794, nll_loss=0.197, ppl=1.15, wps=60220.1, ups=15.46, wpb=3895.3, bsz=218.3, num_updates=148200, lr=8.2144e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=921
2024-01-31 04:59:09 | INFO | train_inner | epoch 110:   1268 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60417.9, ups=15.47, wpb=3905.7, bsz=214.7, num_updates=148300, lr=8.21163e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.8, wall=927
2024-01-31 04:59:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:59:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:59:16 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 2.163 | nll_loss 0.421 | ppl 1.34 | wps 133019 | wpb 3275.2 | bsz 176.5 | num_updates 148381 | best_loss 2.063
2024-01-31 04:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 148381 updates
2024-01-31 04:59:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint110.pt
2024-01-31 04:59:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint110.pt
2024-01-31 04:59:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint110.pt (epoch 110 @ 148381 updates, score 2.163) (writing took 2.3174784049624577 seconds)
2024-01-31 04:59:18 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2024-01-31 04:59:18 | INFO | train | epoch 110 | loss 1.792 | nll_loss 0.194 | ppl 1.14 | wps 57481.8 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 148381 | lr 8.20939e-05 | gnorm 0.504 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 937
2024-01-31 04:59:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:59:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 04:59:18 | INFO | fairseq.trainer | begin training epoch 111
2024-01-31 04:59:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:59:20 | INFO | train_inner | epoch 111:     19 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=35175.6, ups=8.93, wpb=3937.7, bsz=226.3, num_updates=148400, lr=8.20886e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=939
2024-01-31 04:59:26 | INFO | train_inner | epoch 111:    119 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=59364.9, ups=15.18, wpb=3911.5, bsz=213.7, num_updates=148500, lr=8.2061e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=945
2024-01-31 04:59:33 | INFO | train_inner | epoch 111:    219 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60921.7, ups=15.36, wpb=3965.8, bsz=215.9, num_updates=148600, lr=8.20334e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.8, wall=952
2024-01-31 04:59:39 | INFO | train_inner | epoch 111:    319 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60825.9, ups=15.54, wpb=3915.4, bsz=218.2, num_updates=148700, lr=8.20058e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=958
2024-01-31 04:59:46 | INFO | train_inner | epoch 111:    419 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60874.6, ups=15.48, wpb=3932.3, bsz=207.3, num_updates=148800, lr=8.19782e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=965
2024-01-31 04:59:52 | INFO | train_inner | epoch 111:    519 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60995.9, ups=15.39, wpb=3964.6, bsz=217.6, num_updates=148900, lr=8.19507e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=971
2024-01-31 04:59:59 | INFO | train_inner | epoch 111:    619 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60231.6, ups=15.48, wpb=3891.8, bsz=202.3, num_updates=149000, lr=8.19232e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=978
2024-01-31 05:00:05 | INFO | train_inner | epoch 111:    719 / 1349 loss=1.795, nll_loss=0.198, ppl=1.15, wps=60119.9, ups=15.48, wpb=3884, bsz=208, num_updates=149100, lr=8.18957e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=984
2024-01-31 05:00:12 | INFO | train_inner | epoch 111:    819 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60713.9, ups=15.39, wpb=3945.4, bsz=223.7, num_updates=149200, lr=8.18683e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.7, wall=991
2024-01-31 05:00:18 | INFO | train_inner | epoch 111:    919 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60663.6, ups=15.47, wpb=3920.1, bsz=218.4, num_updates=149300, lr=8.18408e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.7, wall=997
2024-01-31 05:00:25 | INFO | train_inner | epoch 111:   1019 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60059.3, ups=15.55, wpb=3862.4, bsz=201, num_updates=149400, lr=8.18134e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=30, wall=1004
2024-01-31 05:00:31 | INFO | train_inner | epoch 111:   1119 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=60210.4, ups=15.57, wpb=3867.9, bsz=209.4, num_updates=149500, lr=8.17861e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=1010
2024-01-31 05:00:38 | INFO | train_inner | epoch 111:   1219 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60636.7, ups=15.39, wpb=3939.2, bsz=217.3, num_updates=149600, lr=8.17587e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.8, wall=1016
2024-01-31 05:00:44 | INFO | train_inner | epoch 111:   1319 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60691, ups=15.42, wpb=3935.8, bsz=225.9, num_updates=149700, lr=8.17314e-05, gnorm=0.452, loss_scale=8, train_wall=6, gb_free=29.8, wall=1023
2024-01-31 05:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:00:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:00:48 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 2.163 | nll_loss 0.423 | ppl 1.34 | wps 132946 | wpb 3275.2 | bsz 176.5 | num_updates 149730 | best_loss 2.063
2024-01-31 05:00:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 149730 updates
2024-01-31 05:00:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint111.pt
2024-01-31 05:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint111.pt
2024-01-31 05:00:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint111.pt (epoch 111 @ 149730 updates, score 2.163) (writing took 2.2750210649101064 seconds)
2024-01-31 05:00:50 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2024-01-31 05:00:50 | INFO | train | epoch 111 | loss 1.791 | nll_loss 0.193 | ppl 1.14 | wps 57457.8 | ups 14.65 | wpb 3921.2 | bsz 214.5 | num_updates 149730 | lr 8.17232e-05 | gnorm 0.49 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1029
2024-01-31 05:00:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:00:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:00:51 | INFO | fairseq.trainer | begin training epoch 112
2024-01-31 05:00:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:00:55 | INFO | train_inner | epoch 112:     70 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=35635.4, ups=8.99, wpb=3962.1, bsz=209.8, num_updates=149800, lr=8.17041e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.7, wall=1034
2024-01-31 05:01:02 | INFO | train_inner | epoch 112:    170 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=59931.4, ups=15.29, wpb=3919.9, bsz=237, num_updates=149900, lr=8.16769e-05, gnorm=0.421, loss_scale=8, train_wall=6, gb_free=29.8, wall=1041
2024-01-31 05:01:08 | INFO | train_inner | epoch 112:    270 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60047.8, ups=15.46, wpb=3884.9, bsz=215.6, num_updates=150000, lr=8.16497e-05, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.8, wall=1047
2024-01-31 05:01:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:01:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:01:10 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 2.161 | nll_loss 0.421 | ppl 1.34 | wps 131894 | wpb 3275.2 | bsz 176.5 | num_updates 150000 | best_loss 2.063
2024-01-31 05:01:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 150000 updates
2024-01-31 05:01:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_112_150000.pt
2024-01-31 05:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_112_150000.pt
2024-01-31 05:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_112_150000.pt (epoch 112 @ 150000 updates, score 2.161) (writing took 2.3069458780810237 seconds)
2024-01-31 05:01:19 | INFO | train_inner | epoch 112:    370 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=35725.9, ups=9.05, wpb=3946.2, bsz=218.6, num_updates=150100, lr=8.16225e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=1058
2024-01-31 05:01:26 | INFO | train_inner | epoch 112:    470 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=59604.4, ups=15.5, wpb=3846.2, bsz=218, num_updates=150200, lr=8.15953e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=1065
2024-01-31 05:01:32 | INFO | train_inner | epoch 112:    570 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60262.9, ups=15.51, wpb=3884.4, bsz=200.9, num_updates=150300, lr=8.15681e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=1071
2024-01-31 05:01:39 | INFO | train_inner | epoch 112:    670 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60932.6, ups=15.49, wpb=3934.3, bsz=207.5, num_updates=150400, lr=8.1541e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=1077
2024-01-31 05:01:45 | INFO | train_inner | epoch 112:    770 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=61515.8, ups=15.48, wpb=3974.4, bsz=210, num_updates=150500, lr=8.15139e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.7, wall=1084
2024-01-31 05:01:51 | INFO | train_inner | epoch 112:    870 / 1349 loss=1.791, nll_loss=0.192, ppl=1.14, wps=60492.8, ups=15.49, wpb=3906.5, bsz=210.4, num_updates=150600, lr=8.14868e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.7, wall=1090
2024-01-31 05:01:58 | INFO | train_inner | epoch 112:    970 / 1349 loss=1.793, nll_loss=0.196, ppl=1.15, wps=60479.4, ups=15.4, wpb=3928.3, bsz=221, num_updates=150700, lr=8.14598e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=1097
2024-01-31 05:02:04 | INFO | train_inner | epoch 112:   1070 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=61144.4, ups=15.43, wpb=3961.9, bsz=218.2, num_updates=150800, lr=8.14328e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=1103
2024-01-31 05:02:11 | INFO | train_inner | epoch 112:   1170 / 1349 loss=1.793, nll_loss=0.196, ppl=1.15, wps=61674.1, ups=15.51, wpb=3975.3, bsz=207.5, num_updates=150900, lr=8.14058e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=1110
2024-01-31 05:02:17 | INFO | train_inner | epoch 112:   1270 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60849.1, ups=15.47, wpb=3932.1, bsz=224.5, num_updates=151000, lr=8.13788e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.8, wall=1116
2024-01-31 05:02:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:02:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:02:25 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 2.165 | nll_loss 0.424 | ppl 1.34 | wps 132750 | wpb 3275.2 | bsz 176.5 | num_updates 151079 | best_loss 2.063
2024-01-31 05:02:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 151079 updates
2024-01-31 05:02:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint112.pt
2024-01-31 05:02:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint112.pt
2024-01-31 05:02:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint112.pt (epoch 112 @ 151079 updates, score 2.165) (writing took 2.336601604009047 seconds)
2024-01-31 05:02:27 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2024-01-31 05:02:27 | INFO | train | epoch 112 | loss 1.791 | nll_loss 0.193 | ppl 1.14 | wps 54808.4 | ups 13.98 | wpb 3921.2 | bsz 214.5 | num_updates 151079 | lr 8.13576e-05 | gnorm 0.507 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 1126
2024-01-31 05:02:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:02:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:02:27 | INFO | fairseq.trainer | begin training epoch 113
2024-01-31 05:02:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:02:28 | INFO | train_inner | epoch 113:     21 / 1349 loss=1.796, nll_loss=0.199, ppl=1.15, wps=34750.3, ups=8.99, wpb=3866.5, bsz=201.8, num_updates=151100, lr=8.13519e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=1127
2024-01-31 05:02:35 | INFO | train_inner | epoch 113:    121 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=61117.1, ups=15.42, wpb=3964.4, bsz=218.8, num_updates=151200, lr=8.1325e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.7, wall=1134
2024-01-31 05:02:41 | INFO | train_inner | epoch 113:    221 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=61330.6, ups=15.46, wpb=3968.2, bsz=211.8, num_updates=151300, lr=8.12981e-05, gnorm=0.452, loss_scale=16, train_wall=6, gb_free=29.9, wall=1140
2024-01-31 05:02:48 | INFO | train_inner | epoch 113:    321 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60082.4, ups=15.46, wpb=3887.2, bsz=219.2, num_updates=151400, lr=8.12713e-05, gnorm=0.448, loss_scale=16, train_wall=6, gb_free=29.9, wall=1147
2024-01-31 05:02:54 | INFO | train_inner | epoch 113:    421 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60753.6, ups=15.48, wpb=3925.3, bsz=212.4, num_updates=151500, lr=8.12444e-05, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.7, wall=1153
2024-01-31 05:03:01 | INFO | train_inner | epoch 113:    521 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60325, ups=15.51, wpb=3889.2, bsz=214.5, num_updates=151600, lr=8.12176e-05, gnorm=0.527, loss_scale=16, train_wall=6, gb_free=29.8, wall=1160
2024-01-31 05:03:07 | INFO | train_inner | epoch 113:    621 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60751.6, ups=15.52, wpb=3914.3, bsz=206.4, num_updates=151700, lr=8.11909e-05, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.7, wall=1166
2024-01-31 05:03:14 | INFO | train_inner | epoch 113:    721 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=59741.3, ups=15.49, wpb=3857.6, bsz=216.6, num_updates=151800, lr=8.11641e-05, gnorm=0.552, loss_scale=16, train_wall=6, gb_free=29.7, wall=1173
2024-01-31 05:03:20 | INFO | train_inner | epoch 113:    821 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60391.8, ups=15.38, wpb=3927.6, bsz=217.8, num_updates=151900, lr=8.11374e-05, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.8, wall=1179
2024-01-31 05:03:27 | INFO | train_inner | epoch 113:    921 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=60834.8, ups=15.51, wpb=3922.3, bsz=216.4, num_updates=152000, lr=8.11107e-05, gnorm=0.444, loss_scale=16, train_wall=6, gb_free=29.8, wall=1186
2024-01-31 05:03:33 | INFO | train_inner | epoch 113:   1021 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60197, ups=15.33, wpb=3927.1, bsz=223.7, num_updates=152100, lr=8.1084e-05, gnorm=0.549, loss_scale=16, train_wall=6, gb_free=29.8, wall=1192
2024-01-31 05:03:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 05:03:40 | INFO | train_inner | epoch 113:   1122 / 1349 loss=1.795, nll_loss=0.198, ppl=1.15, wps=59347.2, ups=15.37, wpb=3861.7, bsz=209.9, num_updates=152200, lr=8.10574e-05, gnorm=0.593, loss_scale=8, train_wall=6, gb_free=29.8, wall=1199
2024-01-31 05:03:46 | INFO | train_inner | epoch 113:   1222 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=61492.2, ups=15.45, wpb=3981, bsz=211.9, num_updates=152300, lr=8.10308e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=1205
2024-01-31 05:03:53 | INFO | train_inner | epoch 113:   1322 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60765.8, ups=15.47, wpb=3927.9, bsz=209.9, num_updates=152400, lr=8.10042e-05, gnorm=0.545, loss_scale=8, train_wall=6, gb_free=29.8, wall=1212
2024-01-31 05:03:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:03:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:03:57 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 2.167 | nll_loss 0.425 | ppl 1.34 | wps 132305 | wpb 3275.2 | bsz 176.5 | num_updates 152427 | best_loss 2.063
2024-01-31 05:03:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 152427 updates
2024-01-31 05:03:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint113.pt
2024-01-31 05:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint113.pt
2024-01-31 05:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint113.pt (epoch 113 @ 152427 updates, score 2.167) (writing took 2.2750116309616715 seconds)
2024-01-31 05:03:59 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2024-01-31 05:03:59 | INFO | train | epoch 113 | loss 1.791 | nll_loss 0.193 | ppl 1.14 | wps 57503.4 | ups 14.66 | wpb 3921.1 | bsz 214.5 | num_updates 152427 | lr 8.0997e-05 | gnorm 0.512 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1218
2024-01-31 05:03:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:03:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:03:59 | INFO | fairseq.trainer | begin training epoch 114
2024-01-31 05:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:04:04 | INFO | train_inner | epoch 114:     73 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=35552.2, ups=9, wpb=3952, bsz=214.1, num_updates=152500, lr=8.09776e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=1223
2024-01-31 05:04:10 | INFO | train_inner | epoch 114:    173 / 1349 loss=1.792, nll_loss=0.193, ppl=1.14, wps=60734, ups=15.57, wpb=3899.9, bsz=202.2, num_updates=152600, lr=8.09511e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=1229
2024-01-31 05:04:17 | INFO | train_inner | epoch 114:    273 / 1349 loss=1.792, nll_loss=0.193, ppl=1.14, wps=60448.8, ups=15.5, wpb=3899, bsz=204.8, num_updates=152700, lr=8.09246e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.7, wall=1236
2024-01-31 05:04:23 | INFO | train_inner | epoch 114:    373 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=61072.6, ups=15.41, wpb=3962.2, bsz=216.4, num_updates=152800, lr=8.08981e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=1242
2024-01-31 05:04:30 | INFO | train_inner | epoch 114:    473 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=60262.8, ups=15.42, wpb=3908.2, bsz=217.6, num_updates=152900, lr=8.08716e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=1248
2024-01-31 05:04:36 | INFO | train_inner | epoch 114:    573 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=59686.6, ups=15.36, wpb=3885.7, bsz=211.8, num_updates=153000, lr=8.08452e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.7, wall=1255
2024-01-31 05:04:43 | INFO | train_inner | epoch 114:    673 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60652.7, ups=15.47, wpb=3921.3, bsz=212.6, num_updates=153100, lr=8.08188e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.7, wall=1261
2024-01-31 05:04:49 | INFO | train_inner | epoch 114:    773 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60883.8, ups=15.36, wpb=3962.7, bsz=215.2, num_updates=153200, lr=8.07924e-05, gnorm=0.54, loss_scale=8, train_wall=6, gb_free=29.8, wall=1268
2024-01-31 05:04:56 | INFO | train_inner | epoch 114:    873 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61426.6, ups=15.44, wpb=3977.2, bsz=219.4, num_updates=153300, lr=8.07661e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.7, wall=1274
2024-01-31 05:05:02 | INFO | train_inner | epoch 114:    973 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=60024, ups=15.61, wpb=3845, bsz=186.8, num_updates=153400, lr=8.07397e-05, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.7, wall=1281
2024-01-31 05:05:08 | INFO | train_inner | epoch 114:   1073 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=59426.5, ups=15.42, wpb=3853.2, bsz=228.9, num_updates=153500, lr=8.07134e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.7, wall=1287
2024-01-31 05:05:15 | INFO | train_inner | epoch 114:   1173 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=61053.9, ups=15.46, wpb=3950, bsz=229.4, num_updates=153600, lr=8.06872e-05, gnorm=0.451, loss_scale=8, train_wall=6, gb_free=29.8, wall=1294
2024-01-31 05:05:21 | INFO | train_inner | epoch 114:   1273 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=61628.9, ups=15.44, wpb=3990.9, bsz=225.8, num_updates=153700, lr=8.06609e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=1300
2024-01-31 05:05:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:05:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:05:28 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 2.166 | nll_loss 0.425 | ppl 1.34 | wps 131283 | wpb 3275.2 | bsz 176.5 | num_updates 153776 | best_loss 2.063
2024-01-31 05:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 153776 updates
2024-01-31 05:05:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint114.pt
2024-01-31 05:05:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint114.pt
2024-01-31 05:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint114.pt (epoch 114 @ 153776 updates, score 2.166) (writing took 2.3477632580325007 seconds)
2024-01-31 05:05:31 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2024-01-31 05:05:31 | INFO | train | epoch 114 | loss 1.79 | nll_loss 0.193 | ppl 1.14 | wps 57492.6 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 153776 | lr 8.0641e-05 | gnorm 0.512 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1310
2024-01-31 05:05:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:05:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:05:31 | INFO | fairseq.trainer | begin training epoch 115
2024-01-31 05:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:05:33 | INFO | train_inner | epoch 115:     24 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=34851.4, ups=8.92, wpb=3906.2, bsz=216.6, num_updates=153800, lr=8.06347e-05, gnorm=0.441, loss_scale=8, train_wall=6, gb_free=29.8, wall=1311
2024-01-31 05:05:39 | INFO | train_inner | epoch 115:    124 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60786.9, ups=15.38, wpb=3952.1, bsz=230.8, num_updates=153900, lr=8.06085e-05, gnorm=0.453, loss_scale=8, train_wall=6, gb_free=29.7, wall=1318
2024-01-31 05:05:46 | INFO | train_inner | epoch 115:    224 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60646, ups=15.39, wpb=3940.4, bsz=219.7, num_updates=154000, lr=8.05823e-05, gnorm=0.553, loss_scale=8, train_wall=6, gb_free=29.7, wall=1324
2024-01-31 05:05:52 | INFO | train_inner | epoch 115:    324 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=60673.7, ups=15.51, wpb=3911.6, bsz=203, num_updates=154100, lr=8.05561e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=1331
2024-01-31 05:05:58 | INFO | train_inner | epoch 115:    424 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=61140.9, ups=15.52, wpb=3938.8, bsz=203.4, num_updates=154200, lr=8.053e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=1337
2024-01-31 05:06:05 | INFO | train_inner | epoch 115:    524 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60468.4, ups=15.5, wpb=3901.9, bsz=206.4, num_updates=154300, lr=8.05039e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.7, wall=1344
2024-01-31 05:06:11 | INFO | train_inner | epoch 115:    624 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60408.7, ups=15.45, wpb=3910.1, bsz=211.7, num_updates=154400, lr=8.04778e-05, gnorm=0.533, loss_scale=8, train_wall=6, gb_free=29.8, wall=1350
2024-01-31 05:06:18 | INFO | train_inner | epoch 115:    724 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60741.9, ups=15.44, wpb=3934.3, bsz=218, num_updates=154500, lr=8.04518e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=1357
2024-01-31 05:06:24 | INFO | train_inner | epoch 115:    824 / 1349 loss=1.793, nll_loss=0.196, ppl=1.15, wps=60386.4, ups=15.46, wpb=3905.2, bsz=202.7, num_updates=154600, lr=8.04258e-05, gnorm=0.562, loss_scale=8, train_wall=6, gb_free=29.8, wall=1363
2024-01-31 05:06:31 | INFO | train_inner | epoch 115:    924 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60181, ups=15.4, wpb=3907.9, bsz=227.6, num_updates=154700, lr=8.03998e-05, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.8, wall=1370
2024-01-31 05:06:37 | INFO | train_inner | epoch 115:   1024 / 1349 loss=1.793, nll_loss=0.196, ppl=1.15, wps=61118.9, ups=15.59, wpb=3920.1, bsz=214.4, num_updates=154800, lr=8.03738e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.9, wall=1376
2024-01-31 05:06:44 | INFO | train_inner | epoch 115:   1124 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61138.4, ups=15.41, wpb=3967.4, bsz=218.1, num_updates=154900, lr=8.03479e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=1383
2024-01-31 05:06:50 | INFO | train_inner | epoch 115:   1224 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=60168.8, ups=15.46, wpb=3892.6, bsz=208.7, num_updates=155000, lr=8.03219e-05, gnorm=0.551, loss_scale=8, train_wall=6, gb_free=29.8, wall=1389
2024-01-31 05:06:57 | INFO | train_inner | epoch 115:   1324 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60005.7, ups=15.45, wpb=3883.3, bsz=225.4, num_updates=155100, lr=8.0296e-05, gnorm=0.589, loss_scale=8, train_wall=6, gb_free=29.7, wall=1396
2024-01-31 05:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:06:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:07:00 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 2.165 | nll_loss 0.424 | ppl 1.34 | wps 133188 | wpb 3275.2 | bsz 176.5 | num_updates 155125 | best_loss 2.063
2024-01-31 05:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 155125 updates
2024-01-31 05:07:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint115.pt
2024-01-31 05:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint115.pt
2024-01-31 05:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint115.pt (epoch 115 @ 155125 updates, score 2.165) (writing took 2.3329852760070935 seconds)
2024-01-31 05:07:03 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2024-01-31 05:07:03 | INFO | train | epoch 115 | loss 1.79 | nll_loss 0.193 | ppl 1.14 | wps 57496.8 | ups 14.66 | wpb 3921.2 | bsz 214.5 | num_updates 155125 | lr 8.02896e-05 | gnorm 0.513 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1402
2024-01-31 05:07:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:07:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:07:03 | INFO | fairseq.trainer | begin training epoch 116
2024-01-31 05:07:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:07:08 | INFO | train_inner | epoch 116:     75 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=35127, ups=8.93, wpb=3931.4, bsz=221.9, num_updates=155200, lr=8.02702e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=1407
2024-01-31 05:07:14 | INFO | train_inner | epoch 116:    175 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60634.6, ups=15.43, wpb=3930, bsz=215, num_updates=155300, lr=8.02443e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.7, wall=1413
2024-01-31 05:07:21 | INFO | train_inner | epoch 116:    275 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=59653, ups=15.51, wpb=3845.3, bsz=218.7, num_updates=155400, lr=8.02185e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=1420
2024-01-31 05:07:27 | INFO | train_inner | epoch 116:    375 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=59965.4, ups=15.41, wpb=3891, bsz=211.8, num_updates=155500, lr=8.01927e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.7, wall=1426
2024-01-31 05:07:34 | INFO | train_inner | epoch 116:    475 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60865.8, ups=15.48, wpb=3932.1, bsz=209.2, num_updates=155600, lr=8.01669e-05, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.8, wall=1433
2024-01-31 05:07:40 | INFO | train_inner | epoch 116:    575 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60596.8, ups=15.32, wpb=3956.2, bsz=218.5, num_updates=155700, lr=8.01412e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.7, wall=1439
2024-01-31 05:07:47 | INFO | train_inner | epoch 116:    675 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60593.3, ups=15.51, wpb=3906.6, bsz=209.5, num_updates=155800, lr=8.01154e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=1446
2024-01-31 05:07:53 | INFO | train_inner | epoch 116:    775 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61099.9, ups=15.48, wpb=3946.9, bsz=216.6, num_updates=155900, lr=8.00898e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.7, wall=1452
2024-01-31 05:08:00 | INFO | train_inner | epoch 116:    875 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=60947.1, ups=15.48, wpb=3936.8, bsz=205.8, num_updates=156000, lr=8.00641e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.7, wall=1459
2024-01-31 05:08:06 | INFO | train_inner | epoch 116:    975 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60765.2, ups=15.43, wpb=3938.7, bsz=209, num_updates=156100, lr=8.00384e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.7, wall=1465
2024-01-31 05:08:13 | INFO | train_inner | epoch 116:   1075 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=60627, ups=15.47, wpb=3919.7, bsz=213, num_updates=156200, lr=8.00128e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=30, wall=1472
2024-01-31 05:08:19 | INFO | train_inner | epoch 116:   1175 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=61253.2, ups=15.45, wpb=3963.7, bsz=222.2, num_updates=156300, lr=7.99872e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=1478
2024-01-31 05:08:26 | INFO | train_inner | epoch 116:   1275 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=59537.9, ups=15.31, wpb=3890.1, bsz=226.2, num_updates=156400, lr=7.99616e-05, gnorm=0.589, loss_scale=8, train_wall=6, gb_free=29.8, wall=1485
2024-01-31 05:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:08:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:08:33 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 2.168 | nll_loss 0.426 | ppl 1.34 | wps 125955 | wpb 3275.2 | bsz 176.5 | num_updates 156474 | best_loss 2.063
2024-01-31 05:08:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 156474 updates
2024-01-31 05:08:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint116.pt
2024-01-31 05:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint116.pt
2024-01-31 05:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint116.pt (epoch 116 @ 156474 updates, score 2.168) (writing took 2.2677738579222932 seconds)
2024-01-31 05:08:35 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2024-01-31 05:08:35 | INFO | train | epoch 116 | loss 1.79 | nll_loss 0.192 | ppl 1.14 | wps 57429.9 | ups 14.65 | wpb 3921.2 | bsz 214.5 | num_updates 156474 | lr 7.99427e-05 | gnorm 0.517 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1494
2024-01-31 05:08:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:08:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:08:35 | INFO | fairseq.trainer | begin training epoch 117
2024-01-31 05:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:08:37 | INFO | train_inner | epoch 117:     26 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=34807.5, ups=8.9, wpb=3909, bsz=209.4, num_updates=156500, lr=7.99361e-05, gnorm=0.468, loss_scale=8, train_wall=6, gb_free=29.8, wall=1496
2024-01-31 05:08:43 | INFO | train_inner | epoch 117:    126 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60570.4, ups=15.41, wpb=3930.9, bsz=226.5, num_updates=156600, lr=7.99106e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=1502
2024-01-31 05:08:50 | INFO | train_inner | epoch 117:    226 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60897.9, ups=15.51, wpb=3926, bsz=212.7, num_updates=156700, lr=7.9885e-05, gnorm=0.586, loss_scale=8, train_wall=6, gb_free=29.8, wall=1509
2024-01-31 05:08:56 | INFO | train_inner | epoch 117:    326 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60600.1, ups=15.46, wpb=3919.9, bsz=206.1, num_updates=156800, lr=7.98596e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=1515
2024-01-31 05:09:03 | INFO | train_inner | epoch 117:    426 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60426.6, ups=15.31, wpb=3946.1, bsz=218.1, num_updates=156900, lr=7.98341e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=1522
2024-01-31 05:09:09 | INFO | train_inner | epoch 117:    526 / 1349 loss=1.794, nll_loss=0.197, ppl=1.15, wps=61068.1, ups=15.45, wpb=3951.6, bsz=209, num_updates=157000, lr=7.98087e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.7, wall=1528
2024-01-31 05:09:16 | INFO | train_inner | epoch 117:    626 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61010.7, ups=15.46, wpb=3945.6, bsz=211.7, num_updates=157100, lr=7.97833e-05, gnorm=0.582, loss_scale=8, train_wall=6, gb_free=29.7, wall=1535
2024-01-31 05:09:22 | INFO | train_inner | epoch 117:    726 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60661.2, ups=15.5, wpb=3913.6, bsz=215.2, num_updates=157200, lr=7.97579e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.7, wall=1541
2024-01-31 05:09:29 | INFO | train_inner | epoch 117:    826 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=59421.6, ups=15.49, wpb=3835.5, bsz=208.2, num_updates=157300, lr=7.97325e-05, gnorm=0.579, loss_scale=8, train_wall=6, gb_free=29.8, wall=1548
2024-01-31 05:09:35 | INFO | train_inner | epoch 117:    926 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=60704.2, ups=15.46, wpb=3927.6, bsz=205.6, num_updates=157400, lr=7.97072e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=1554
2024-01-31 05:09:42 | INFO | train_inner | epoch 117:   1026 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60919.5, ups=15.45, wpb=3943.8, bsz=218.8, num_updates=157500, lr=7.96819e-05, gnorm=0.446, loss_scale=8, train_wall=6, gb_free=29.8, wall=1560
2024-01-31 05:09:48 | INFO | train_inner | epoch 117:   1126 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60335, ups=15.42, wpb=3912.5, bsz=221, num_updates=157600, lr=7.96566e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=1567
2024-01-31 05:09:55 | INFO | train_inner | epoch 117:   1226 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=61173.2, ups=15.44, wpb=3962.8, bsz=221.6, num_updates=157700, lr=7.96314e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.9, wall=1573
2024-01-31 05:10:01 | INFO | train_inner | epoch 117:   1326 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=60373.9, ups=15.54, wpb=3885.7, bsz=207.8, num_updates=157800, lr=7.96061e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.7, wall=1580
2024-01-31 05:10:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:10:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:10:05 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 2.171 | nll_loss 0.429 | ppl 1.35 | wps 132130 | wpb 3275.2 | bsz 176.5 | num_updates 157823 | best_loss 2.063
2024-01-31 05:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 157823 updates
2024-01-31 05:10:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint117.pt
2024-01-31 05:10:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint117.pt
2024-01-31 05:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint117.pt (epoch 117 @ 157823 updates, score 2.171) (writing took 2.241965673980303 seconds)
2024-01-31 05:10:07 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2024-01-31 05:10:07 | INFO | train | epoch 117 | loss 1.79 | nll_loss 0.192 | ppl 1.14 | wps 57528.3 | ups 14.67 | wpb 3921.2 | bsz 214.5 | num_updates 157823 | lr 7.96003e-05 | gnorm 0.522 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 1586
2024-01-31 05:10:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:10:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:10:07 | INFO | fairseq.trainer | begin training epoch 118
2024-01-31 05:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:10:12 | INFO | train_inner | epoch 118:     77 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=35000.4, ups=9.03, wpb=3874.4, bsz=209.1, num_updates=157900, lr=7.95809e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.7, wall=1591
2024-01-31 05:10:18 | INFO | train_inner | epoch 118:    177 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=61322.4, ups=15.54, wpb=3945.5, bsz=210.8, num_updates=158000, lr=7.95557e-05, gnorm=0.576, loss_scale=8, train_wall=6, gb_free=29.8, wall=1597
2024-01-31 05:10:25 | INFO | train_inner | epoch 118:    277 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60822.3, ups=15.48, wpb=3928.6, bsz=212.1, num_updates=158100, lr=7.95306e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=1604
2024-01-31 05:10:31 | INFO | train_inner | epoch 118:    377 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60759.9, ups=15.51, wpb=3917.6, bsz=215.2, num_updates=158200, lr=7.95054e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.9, wall=1610
2024-01-31 05:10:38 | INFO | train_inner | epoch 118:    477 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60493.8, ups=15.45, wpb=3915.9, bsz=208.7, num_updates=158300, lr=7.94803e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=1617
2024-01-31 05:10:44 | INFO | train_inner | epoch 118:    577 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=59519.1, ups=15.35, wpb=3877.4, bsz=211.8, num_updates=158400, lr=7.94552e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=1623
2024-01-31 05:10:51 | INFO | train_inner | epoch 118:    677 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60676.2, ups=15.41, wpb=3938.6, bsz=216.1, num_updates=158500, lr=7.94301e-05, gnorm=0.467, loss_scale=8, train_wall=6, gb_free=29.7, wall=1630
2024-01-31 05:10:57 | INFO | train_inner | epoch 118:    777 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=59451.8, ups=15.47, wpb=3842.6, bsz=214.4, num_updates=158600, lr=7.94051e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=1636
2024-01-31 05:11:04 | INFO | train_inner | epoch 118:    877 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60789.7, ups=15.44, wpb=3937.2, bsz=207.8, num_updates=158700, lr=7.93801e-05, gnorm=0.554, loss_scale=8, train_wall=6, gb_free=29.8, wall=1643
2024-01-31 05:11:10 | INFO | train_inner | epoch 118:    977 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61412.9, ups=15.45, wpb=3973.9, bsz=219.2, num_updates=158800, lr=7.93551e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=1649
2024-01-31 05:11:17 | INFO | train_inner | epoch 118:   1077 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61079.8, ups=15.43, wpb=3959.3, bsz=213.5, num_updates=158900, lr=7.93301e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.8, wall=1656
2024-01-31 05:11:23 | INFO | train_inner | epoch 118:   1177 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60300, ups=15.37, wpb=3922.4, bsz=226.1, num_updates=159000, lr=7.93052e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.8, wall=1662
2024-01-31 05:11:30 | INFO | train_inner | epoch 118:   1277 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61054.5, ups=15.44, wpb=3955.5, bsz=225.3, num_updates=159100, lr=7.92802e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.9, wall=1669
2024-01-31 05:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:11:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:11:37 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 2.165 | nll_loss 0.425 | ppl 1.34 | wps 132788 | wpb 3275.2 | bsz 176.5 | num_updates 159172 | best_loss 2.063
2024-01-31 05:11:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 159172 updates
2024-01-31 05:11:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint118.pt
2024-01-31 05:11:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint118.pt
2024-01-31 05:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint118.pt (epoch 118 @ 159172 updates, score 2.165) (writing took 2.2639799989992753 seconds)
2024-01-31 05:11:39 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2024-01-31 05:11:39 | INFO | train | epoch 118 | loss 1.789 | nll_loss 0.192 | ppl 1.14 | wps 57523.5 | ups 14.67 | wpb 3921.2 | bsz 214.5 | num_updates 159172 | lr 7.92623e-05 | gnorm 0.524 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 1678
2024-01-31 05:11:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:11:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:11:39 | INFO | fairseq.trainer | begin training epoch 119
2024-01-31 05:11:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:11:41 | INFO | train_inner | epoch 119:     28 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=34850.6, ups=9.01, wpb=3867.5, bsz=216.6, num_updates=159200, lr=7.92553e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.7, wall=1680
2024-01-31 05:11:47 | INFO | train_inner | epoch 119:    128 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60424.3, ups=15.5, wpb=3899.2, bsz=210.2, num_updates=159300, lr=7.92304e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.9, wall=1686
2024-01-31 05:11:54 | INFO | train_inner | epoch 119:    228 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60774.9, ups=15.45, wpb=3934, bsz=207.6, num_updates=159400, lr=7.92056e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=1693
2024-01-31 05:12:00 | INFO | train_inner | epoch 119:    328 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60879.8, ups=15.58, wpb=3906.9, bsz=204.2, num_updates=159500, lr=7.91808e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=1699
2024-01-31 05:12:07 | INFO | train_inner | epoch 119:    428 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60718, ups=15.57, wpb=3900.3, bsz=206.4, num_updates=159600, lr=7.91559e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.7, wall=1706
2024-01-31 05:12:13 | INFO | train_inner | epoch 119:    528 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60565.9, ups=15.5, wpb=3906.9, bsz=206.6, num_updates=159700, lr=7.91312e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=1712
2024-01-31 05:12:20 | INFO | train_inner | epoch 119:    628 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60721.8, ups=15.47, wpb=3924, bsz=217, num_updates=159800, lr=7.91064e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=1718
2024-01-31 05:12:26 | INFO | train_inner | epoch 119:    728 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60336.9, ups=15.43, wpb=3910.2, bsz=222.6, num_updates=159900, lr=7.90817e-05, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.7, wall=1725
2024-01-31 05:12:32 | INFO | train_inner | epoch 119:    828 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60056.1, ups=15.4, wpb=3900.4, bsz=215.2, num_updates=160000, lr=7.90569e-05, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.7, wall=1731
2024-01-31 05:12:39 | INFO | train_inner | epoch 119:    928 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=61217.2, ups=15.4, wpb=3975.3, bsz=224.6, num_updates=160100, lr=7.90322e-05, gnorm=0.435, loss_scale=8, train_wall=6, gb_free=29.7, wall=1738
2024-01-31 05:12:45 | INFO | train_inner | epoch 119:   1028 / 1349 loss=1.793, nll_loss=0.195, ppl=1.15, wps=60811, ups=15.62, wpb=3892.5, bsz=211.5, num_updates=160200, lr=7.90076e-05, gnorm=0.598, loss_scale=8, train_wall=6, gb_free=29.8, wall=1744
2024-01-31 05:12:52 | INFO | train_inner | epoch 119:   1128 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61242.1, ups=15.46, wpb=3961.8, bsz=213.4, num_updates=160300, lr=7.89829e-05, gnorm=0.474, loss_scale=8, train_wall=6, gb_free=29.7, wall=1751
2024-01-31 05:12:58 | INFO | train_inner | epoch 119:   1228 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60859.2, ups=15.34, wpb=3967.3, bsz=229, num_updates=160400, lr=7.89583e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.8, wall=1757
2024-01-31 05:13:05 | INFO | train_inner | epoch 119:   1328 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60652.9, ups=15.42, wpb=3933.9, bsz=217.4, num_updates=160500, lr=7.89337e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.7, wall=1764
2024-01-31 05:13:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:13:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:13:08 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 2.169 | nll_loss 0.428 | ppl 1.35 | wps 132977 | wpb 3275.2 | bsz 176.5 | num_updates 160521 | best_loss 2.063
2024-01-31 05:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 160521 updates
2024-01-31 05:13:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint119.pt
2024-01-31 05:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint119.pt
2024-01-31 05:13:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint119.pt (epoch 119 @ 160521 updates, score 2.169) (writing took 2.253672480932437 seconds)
2024-01-31 05:13:11 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2024-01-31 05:13:11 | INFO | train | epoch 119 | loss 1.789 | nll_loss 0.192 | ppl 1.14 | wps 57618.1 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 160521 | lr 7.89285e-05 | gnorm 0.516 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1770
2024-01-31 05:13:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:13:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:13:11 | INFO | fairseq.trainer | begin training epoch 120
2024-01-31 05:13:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:13:16 | INFO | train_inner | epoch 120:     79 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=35373.3, ups=9.03, wpb=3915.7, bsz=215, num_updates=160600, lr=7.89091e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.7, wall=1775
2024-01-31 05:13:22 | INFO | train_inner | epoch 120:    179 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60756.8, ups=15.45, wpb=3932.2, bsz=211.6, num_updates=160700, lr=7.88846e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=1781
2024-01-31 05:13:29 | INFO | train_inner | epoch 120:    279 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60454.5, ups=15.52, wpb=3896.4, bsz=203.3, num_updates=160800, lr=7.886e-05, gnorm=0.569, loss_scale=8, train_wall=6, gb_free=29.7, wall=1788
2024-01-31 05:13:35 | INFO | train_inner | epoch 120:    379 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60687.5, ups=15.45, wpb=3927, bsz=215.8, num_updates=160900, lr=7.88355e-05, gnorm=0.568, loss_scale=8, train_wall=6, gb_free=29.8, wall=1794
2024-01-31 05:13:42 | INFO | train_inner | epoch 120:    479 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=61025.8, ups=15.45, wpb=3949.4, bsz=213.3, num_updates=161000, lr=7.8811e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=1801
2024-01-31 05:13:48 | INFO | train_inner | epoch 120:    579 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60336.4, ups=15.35, wpb=3931.6, bsz=214.6, num_updates=161100, lr=7.87866e-05, gnorm=0.449, loss_scale=8, train_wall=6, gb_free=29.7, wall=1807
2024-01-31 05:13:55 | INFO | train_inner | epoch 120:    679 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60572.1, ups=15.45, wpb=3920.5, bsz=212.1, num_updates=161200, lr=7.87621e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=1814
2024-01-31 05:14:01 | INFO | train_inner | epoch 120:    779 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60629.1, ups=15.5, wpb=3910.6, bsz=200.6, num_updates=161300, lr=7.87377e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.8, wall=1820
2024-01-31 05:14:08 | INFO | train_inner | epoch 120:    879 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60850.3, ups=15.43, wpb=3943.2, bsz=222.2, num_updates=161400, lr=7.87133e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=1827
2024-01-31 05:14:14 | INFO | train_inner | epoch 120:    979 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60669.2, ups=15.47, wpb=3920.6, bsz=212, num_updates=161500, lr=7.86889e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=1833
2024-01-31 05:14:21 | INFO | train_inner | epoch 120:   1079 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=59789.7, ups=15.56, wpb=3842.2, bsz=223.2, num_updates=161600, lr=7.86646e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=30, wall=1840
2024-01-31 05:14:27 | INFO | train_inner | epoch 120:   1179 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60962.8, ups=15.5, wpb=3934.3, bsz=212.2, num_updates=161700, lr=7.86403e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.8, wall=1846
2024-01-31 05:14:34 | INFO | train_inner | epoch 120:   1279 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60376.9, ups=15.36, wpb=3930.7, bsz=216.5, num_updates=161800, lr=7.8616e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=1852
2024-01-31 05:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:14:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:14:40 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 2.17 | nll_loss 0.429 | ppl 1.35 | wps 132961 | wpb 3275.2 | bsz 176.5 | num_updates 161870 | best_loss 2.063
2024-01-31 05:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 161870 updates
2024-01-31 05:14:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint120.pt
2024-01-31 05:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint120.pt
2024-01-31 05:14:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint120.pt (epoch 120 @ 161870 updates, score 2.17) (writing took 2.2783394259167835 seconds)
2024-01-31 05:14:43 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2024-01-31 05:14:43 | INFO | train | epoch 120 | loss 1.789 | nll_loss 0.192 | ppl 1.14 | wps 57553.6 | ups 14.68 | wpb 3921.2 | bsz 214.5 | num_updates 161870 | lr 7.8599e-05 | gnorm 0.515 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 1862
2024-01-31 05:14:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:14:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:14:43 | INFO | fairseq.trainer | begin training epoch 121
2024-01-31 05:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:14:45 | INFO | train_inner | epoch 121:     30 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=35210.5, ups=8.99, wpb=3917.5, bsz=233, num_updates=161900, lr=7.85917e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.7, wall=1864
2024-01-31 05:14:51 | INFO | train_inner | epoch 121:    130 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=60465.1, ups=15.43, wpb=3917.4, bsz=225, num_updates=162000, lr=7.85674e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=1870
2024-01-31 05:14:58 | INFO | train_inner | epoch 121:    230 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60244.8, ups=15.52, wpb=3883, bsz=215.8, num_updates=162100, lr=7.85432e-05, gnorm=0.558, loss_scale=8, train_wall=6, gb_free=29.8, wall=1877
2024-01-31 05:15:04 | INFO | train_inner | epoch 121:    330 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60735.5, ups=15.4, wpb=3942.8, bsz=223.8, num_updates=162200, lr=7.8519e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=1883
2024-01-31 05:15:11 | INFO | train_inner | epoch 121:    430 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60952.1, ups=15.55, wpb=3920.9, bsz=208.6, num_updates=162300, lr=7.84948e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=1889
2024-01-31 05:15:17 | INFO | train_inner | epoch 121:    530 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61098.7, ups=15.45, wpb=3954.9, bsz=221.1, num_updates=162400, lr=7.84706e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.8, wall=1896
2024-01-31 05:15:24 | INFO | train_inner | epoch 121:    630 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60751.4, ups=15.37, wpb=3952.4, bsz=219.7, num_updates=162500, lr=7.84465e-05, gnorm=0.454, loss_scale=8, train_wall=6, gb_free=29.7, wall=1902
2024-01-31 05:15:30 | INFO | train_inner | epoch 121:    730 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=61285.9, ups=15.56, wpb=3939.4, bsz=203.9, num_updates=162600, lr=7.84223e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=1909
2024-01-31 05:15:36 | INFO | train_inner | epoch 121:    830 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61123.7, ups=15.43, wpb=3960.6, bsz=218.6, num_updates=162700, lr=7.83982e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=1915
2024-01-31 05:15:43 | INFO | train_inner | epoch 121:    930 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=59734.3, ups=15.42, wpb=3874.3, bsz=217.8, num_updates=162800, lr=7.83741e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.8, wall=1922
2024-01-31 05:15:49 | INFO | train_inner | epoch 121:   1030 / 1349 loss=1.792, nll_loss=0.194, ppl=1.14, wps=60029.5, ups=15.52, wpb=3868.4, bsz=203.3, num_updates=162900, lr=7.83501e-05, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.7, wall=1928
2024-01-31 05:15:56 | INFO | train_inner | epoch 121:   1130 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=61361.8, ups=15.45, wpb=3971.5, bsz=221.9, num_updates=163000, lr=7.8326e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.8, wall=1935
2024-01-31 05:16:02 | INFO | train_inner | epoch 121:   1230 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60587, ups=15.59, wpb=3886.1, bsz=200.9, num_updates=163100, lr=7.8302e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=1941
2024-01-31 05:16:09 | INFO | train_inner | epoch 121:   1330 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60839, ups=15.53, wpb=3917.5, bsz=207.1, num_updates=163200, lr=7.8278e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.7, wall=1948
2024-01-31 05:16:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:16:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:16:12 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 2.173 | nll_loss 0.432 | ppl 1.35 | wps 131726 | wpb 3275.2 | bsz 176.5 | num_updates 163219 | best_loss 2.063
2024-01-31 05:16:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 163219 updates
2024-01-31 05:16:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint121.pt
2024-01-31 05:16:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint121.pt
2024-01-31 05:16:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint121.pt (epoch 121 @ 163219 updates, score 2.173) (writing took 2.2919619689928368 seconds)
2024-01-31 05:16:14 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2024-01-31 05:16:14 | INFO | train | epoch 121 | loss 1.788 | nll_loss 0.191 | ppl 1.14 | wps 57592.7 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 163219 | lr 7.82735e-05 | gnorm 0.5 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 1953
2024-01-31 05:16:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:16:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:16:15 | INFO | fairseq.trainer | begin training epoch 122
2024-01-31 05:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:16:20 | INFO | train_inner | epoch 122:     81 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=35149.2, ups=8.96, wpb=3923.6, bsz=223, num_updates=163300, lr=7.82541e-05, gnorm=0.454, loss_scale=8, train_wall=6, gb_free=29.8, wall=1959
2024-01-31 05:16:26 | INFO | train_inner | epoch 122:    181 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60844.4, ups=15.49, wpb=3929.1, bsz=217, num_updates=163400, lr=7.82301e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=1965
2024-01-31 05:16:33 | INFO | train_inner | epoch 122:    281 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60602.7, ups=15.4, wpb=3935, bsz=219.2, num_updates=163500, lr=7.82062e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.7, wall=1972
2024-01-31 05:16:39 | INFO | train_inner | epoch 122:    381 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61170.6, ups=15.49, wpb=3947.9, bsz=213.4, num_updates=163600, lr=7.81823e-05, gnorm=0.46, loss_scale=8, train_wall=6, gb_free=29.8, wall=1978
2024-01-31 05:16:46 | INFO | train_inner | epoch 122:    481 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60636.5, ups=15.45, wpb=3925.6, bsz=217.2, num_updates=163700, lr=7.81584e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.7, wall=1985
2024-01-31 05:16:52 | INFO | train_inner | epoch 122:    581 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60906, ups=15.38, wpb=3960.1, bsz=231.3, num_updates=163800, lr=7.81345e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=1991
2024-01-31 05:16:59 | INFO | train_inner | epoch 122:    681 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60547.1, ups=15.45, wpb=3917.7, bsz=217.4, num_updates=163900, lr=7.81107e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=1998
2024-01-31 05:17:05 | INFO | train_inner | epoch 122:    781 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61268.8, ups=15.54, wpb=3942.2, bsz=193.8, num_updates=164000, lr=7.80869e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=2004
2024-01-31 05:17:12 | INFO | train_inner | epoch 122:    881 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60765.2, ups=15.51, wpb=3918.3, bsz=213.1, num_updates=164100, lr=7.80631e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=2010
2024-01-31 05:17:18 | INFO | train_inner | epoch 122:    981 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60004.8, ups=15.47, wpb=3879.4, bsz=219, num_updates=164200, lr=7.80393e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=2017
2024-01-31 05:17:24 | INFO | train_inner | epoch 122:   1081 / 1349 loss=1.793, nll_loss=0.196, ppl=1.15, wps=60683.9, ups=15.53, wpb=3908.2, bsz=196.2, num_updates=164300, lr=7.80156e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.7, wall=2023
2024-01-31 05:17:31 | INFO | train_inner | epoch 122:   1181 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60782.1, ups=15.55, wpb=3908.9, bsz=206.5, num_updates=164400, lr=7.79918e-05, gnorm=0.551, loss_scale=8, train_wall=6, gb_free=29.7, wall=2030
2024-01-31 05:17:37 | INFO | train_inner | epoch 122:   1281 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60050.8, ups=15.39, wpb=3902.1, bsz=223.4, num_updates=164500, lr=7.79681e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=2036
2024-01-31 05:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:17:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:17:44 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 2.172 | nll_loss 0.432 | ppl 1.35 | wps 132205 | wpb 3275.2 | bsz 176.5 | num_updates 164568 | best_loss 2.063
2024-01-31 05:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 164568 updates
2024-01-31 05:17:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint122.pt
2024-01-31 05:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint122.pt
2024-01-31 05:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint122.pt (epoch 122 @ 164568 updates, score 2.172) (writing took 2.2662424870068207 seconds)
2024-01-31 05:17:46 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2024-01-31 05:17:46 | INFO | train | epoch 122 | loss 1.789 | nll_loss 0.191 | ppl 1.14 | wps 57607.4 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 164568 | lr 7.7952e-05 | gnorm 0.51 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 2045
2024-01-31 05:17:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:17:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:17:46 | INFO | fairseq.trainer | begin training epoch 123
2024-01-31 05:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:17:48 | INFO | train_inner | epoch 123:     32 / 1349 loss=1.794, nll_loss=0.197, ppl=1.15, wps=35260.1, ups=9.05, wpb=3897.3, bsz=206.5, num_updates=164600, lr=7.79444e-05, gnorm=0.593, loss_scale=8, train_wall=6, gb_free=29.8, wall=2047
2024-01-31 05:17:55 | INFO | train_inner | epoch 123:    132 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61252.3, ups=15.56, wpb=3937.1, bsz=211.8, num_updates=164700, lr=7.79208e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.8, wall=2054
2024-01-31 05:18:01 | INFO | train_inner | epoch 123:    232 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60179.3, ups=15.45, wpb=3895.2, bsz=199.9, num_updates=164800, lr=7.78971e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.7, wall=2060
2024-01-31 05:18:08 | INFO | train_inner | epoch 123:    332 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60715, ups=15.38, wpb=3947.8, bsz=223.2, num_updates=164900, lr=7.78735e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.7, wall=2067
2024-01-31 05:18:14 | INFO | train_inner | epoch 123:    432 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60908.6, ups=15.38, wpb=3959.8, bsz=230.2, num_updates=165000, lr=7.78499e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=2073
2024-01-31 05:18:21 | INFO | train_inner | epoch 123:    532 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60267.4, ups=15.57, wpb=3871, bsz=220.5, num_updates=165100, lr=7.78263e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.8, wall=2080
2024-01-31 05:18:27 | INFO | train_inner | epoch 123:    632 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=61335.1, ups=15.59, wpb=3935.3, bsz=199.6, num_updates=165200, lr=7.78028e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=2086
2024-01-31 05:18:34 | INFO | train_inner | epoch 123:    732 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61056.4, ups=15.32, wpb=3986.4, bsz=222.2, num_updates=165300, lr=7.77792e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=2093
2024-01-31 05:18:40 | INFO | train_inner | epoch 123:    832 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60517.9, ups=15.45, wpb=3918.2, bsz=217.4, num_updates=165400, lr=7.77557e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=2099
2024-01-31 05:18:47 | INFO | train_inner | epoch 123:    932 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60402.1, ups=15.46, wpb=3906.9, bsz=219.5, num_updates=165500, lr=7.77322e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.8, wall=2106
2024-01-31 05:18:53 | INFO | train_inner | epoch 123:   1032 / 1349 loss=1.791, nll_loss=0.193, ppl=1.14, wps=60466.4, ups=15.45, wpb=3913, bsz=217.6, num_updates=165600, lr=7.77087e-05, gnorm=0.554, loss_scale=8, train_wall=6, gb_free=29.7, wall=2112
2024-01-31 05:19:00 | INFO | train_inner | epoch 123:   1132 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60085.2, ups=15.43, wpb=3893.3, bsz=209.2, num_updates=165700, lr=7.76853e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=2119
2024-01-31 05:19:06 | INFO | train_inner | epoch 123:   1232 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60724, ups=15.46, wpb=3926.8, bsz=216.2, num_updates=165800, lr=7.76619e-05, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=29.7, wall=2125
2024-01-31 05:19:13 | INFO | train_inner | epoch 123:   1332 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60427, ups=15.54, wpb=3887.3, bsz=207, num_updates=165900, lr=7.76384e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=2131
2024-01-31 05:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:19:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:19:16 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 2.17 | nll_loss 0.429 | ppl 1.35 | wps 132652 | wpb 3275.2 | bsz 176.5 | num_updates 165917 | best_loss 2.063
2024-01-31 05:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 165917 updates
2024-01-31 05:19:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint123.pt
2024-01-31 05:19:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint123.pt
2024-01-31 05:19:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint123.pt (epoch 123 @ 165917 updates, score 2.17) (writing took 2.2434791730483994 seconds)
2024-01-31 05:19:18 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2024-01-31 05:19:18 | INFO | train | epoch 123 | loss 1.788 | nll_loss 0.191 | ppl 1.14 | wps 57608.9 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 165917 | lr 7.76345e-05 | gnorm 0.517 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 2137
2024-01-31 05:19:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:19:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:19:18 | INFO | fairseq.trainer | begin training epoch 124
2024-01-31 05:19:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:19:24 | INFO | train_inner | epoch 124:     83 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=35266.5, ups=9.01, wpb=3913.9, bsz=225, num_updates=166000, lr=7.76151e-05, gnorm=0.58, loss_scale=8, train_wall=6, gb_free=29.7, wall=2143
2024-01-31 05:19:30 | INFO | train_inner | epoch 124:    183 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60791.2, ups=15.47, wpb=3930.4, bsz=210.8, num_updates=166100, lr=7.75917e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=2149
2024-01-31 05:19:37 | INFO | train_inner | epoch 124:    283 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60521.2, ups=15.46, wpb=3913.5, bsz=211.1, num_updates=166200, lr=7.75683e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=2155
2024-01-31 05:19:43 | INFO | train_inner | epoch 124:    383 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60718.4, ups=15.45, wpb=3930.7, bsz=213.8, num_updates=166300, lr=7.7545e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.7, wall=2162
2024-01-31 05:19:49 | INFO | train_inner | epoch 124:    483 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=61057.6, ups=15.6, wpb=3913.5, bsz=194.9, num_updates=166400, lr=7.75217e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=2168
2024-01-31 05:19:56 | INFO | train_inner | epoch 124:    583 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60853.6, ups=15.55, wpb=3913.9, bsz=211, num_updates=166500, lr=7.74984e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=2175
2024-01-31 05:20:02 | INFO | train_inner | epoch 124:    683 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60946.8, ups=15.48, wpb=3936.3, bsz=211.6, num_updates=166600, lr=7.74752e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=2181
2024-01-31 05:20:09 | INFO | train_inner | epoch 124:    783 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60903.1, ups=15.37, wpb=3961.5, bsz=239.8, num_updates=166700, lr=7.74519e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.7, wall=2188
2024-01-31 05:20:15 | INFO | train_inner | epoch 124:    883 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60112.7, ups=15.39, wpb=3906.7, bsz=221.8, num_updates=166800, lr=7.74287e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.8, wall=2194
2024-01-31 05:20:22 | INFO | train_inner | epoch 124:    983 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60953.3, ups=15.44, wpb=3948.9, bsz=220.6, num_updates=166900, lr=7.74055e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.7, wall=2201
2024-01-31 05:20:28 | INFO | train_inner | epoch 124:   1083 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60445.3, ups=15.46, wpb=3910.3, bsz=222.4, num_updates=167000, lr=7.73823e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.7, wall=2207
2024-01-31 05:20:35 | INFO | train_inner | epoch 124:   1183 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60703.6, ups=15.45, wpb=3929.4, bsz=211.9, num_updates=167100, lr=7.73592e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.9, wall=2214
2024-01-31 05:20:41 | INFO | train_inner | epoch 124:   1283 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60903.1, ups=15.49, wpb=3930.8, bsz=212.9, num_updates=167200, lr=7.7336e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.8, wall=2220
2024-01-31 05:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:20:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:20:48 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 2.169 | nll_loss 0.428 | ppl 1.35 | wps 132262 | wpb 3275.2 | bsz 176.5 | num_updates 167266 | best_loss 2.063
2024-01-31 05:20:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 167266 updates
2024-01-31 05:20:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint124.pt
2024-01-31 05:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint124.pt
2024-01-31 05:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint124.pt (epoch 124 @ 167266 updates, score 2.169) (writing took 2.2667767819948494 seconds)
2024-01-31 05:20:50 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2024-01-31 05:20:50 | INFO | train | epoch 124 | loss 1.788 | nll_loss 0.191 | ppl 1.14 | wps 57592.9 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 167266 | lr 7.73208e-05 | gnorm 0.514 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 2229
2024-01-31 05:20:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:20:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:20:50 | INFO | fairseq.trainer | begin training epoch 125
2024-01-31 05:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:20:52 | INFO | train_inner | epoch 125:     34 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=34633.1, ups=9.01, wpb=3844.1, bsz=201.3, num_updates=167300, lr=7.73129e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=2231
2024-01-31 05:20:59 | INFO | train_inner | epoch 125:    134 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61148.8, ups=15.36, wpb=3982.3, bsz=233.3, num_updates=167400, lr=7.72898e-05, gnorm=0.427, loss_scale=8, train_wall=6, gb_free=29.8, wall=2238
2024-01-31 05:21:05 | INFO | train_inner | epoch 125:    234 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=61238, ups=15.46, wpb=3961.7, bsz=219.1, num_updates=167500, lr=7.72667e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=2244
2024-01-31 05:21:12 | INFO | train_inner | epoch 125:    334 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61125.4, ups=15.52, wpb=3938.9, bsz=212.1, num_updates=167600, lr=7.72437e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=2251
2024-01-31 05:21:18 | INFO | train_inner | epoch 125:    434 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60830.7, ups=15.49, wpb=3928.1, bsz=216.2, num_updates=167700, lr=7.72207e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=2257
2024-01-31 05:21:25 | INFO | train_inner | epoch 125:    534 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=59807.6, ups=15.42, wpb=3878.4, bsz=210.5, num_updates=167800, lr=7.71976e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.8, wall=2264
2024-01-31 05:21:31 | INFO | train_inner | epoch 125:    634 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60240.1, ups=15.44, wpb=3900.5, bsz=206.7, num_updates=167900, lr=7.71746e-05, gnorm=0.583, loss_scale=8, train_wall=6, gb_free=29.9, wall=2270
2024-01-31 05:21:38 | INFO | train_inner | epoch 125:    734 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61381, ups=15.53, wpb=3953.1, bsz=212.1, num_updates=168000, lr=7.71517e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.9, wall=2277
2024-01-31 05:21:44 | INFO | train_inner | epoch 125:    834 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60604, ups=15.47, wpb=3916.7, bsz=208.1, num_updates=168100, lr=7.71287e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.9, wall=2283
2024-01-31 05:21:51 | INFO | train_inner | epoch 125:    934 / 1349 loss=1.792, nll_loss=0.195, ppl=1.14, wps=60182.5, ups=15.51, wpb=3879.5, bsz=204.3, num_updates=168200, lr=7.71058e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=2289
2024-01-31 05:21:57 | INFO | train_inner | epoch 125:   1034 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=59494.2, ups=15.43, wpb=3856.1, bsz=219.4, num_updates=168300, lr=7.70829e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=2296
2024-01-31 05:22:03 | INFO | train_inner | epoch 125:   1134 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=61307.8, ups=15.47, wpb=3964.3, bsz=208.1, num_updates=168400, lr=7.706e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=2302
2024-01-31 05:22:10 | INFO | train_inner | epoch 125:   1234 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60921, ups=15.54, wpb=3921.2, bsz=207.8, num_updates=168500, lr=7.70371e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=2309
2024-01-31 05:22:16 | INFO | train_inner | epoch 125:   1334 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60695.5, ups=15.51, wpb=3912.9, bsz=227.9, num_updates=168600, lr=7.70143e-05, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=2315
2024-01-31 05:22:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:22:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:22:19 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 2.167 | nll_loss 0.426 | ppl 1.34 | wps 133070 | wpb 3275.2 | bsz 176.5 | num_updates 168615 | best_loss 2.063
2024-01-31 05:22:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 168615 updates
2024-01-31 05:22:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint125.pt
2024-01-31 05:22:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint125.pt
2024-01-31 05:22:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint125.pt (epoch 125 @ 168615 updates, score 2.167) (writing took 2.2483534299535677 seconds)
2024-01-31 05:22:22 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2024-01-31 05:22:22 | INFO | train | epoch 125 | loss 1.788 | nll_loss 0.19 | ppl 1.14 | wps 57597 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 168615 | lr 7.70108e-05 | gnorm 0.506 | loss_scale 16 | train_wall 84 | gb_free 29.7 | wall 2321
2024-01-31 05:22:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:22:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:22:22 | INFO | fairseq.trainer | begin training epoch 126
2024-01-31 05:22:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:22:27 | INFO | train_inner | epoch 126:     85 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=35476.6, ups=9, wpb=3940.2, bsz=221.8, num_updates=168700, lr=7.69914e-05, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=2326
2024-01-31 05:22:34 | INFO | train_inner | epoch 126:    185 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=60231.6, ups=15.37, wpb=3918.3, bsz=234.9, num_updates=168800, lr=7.69686e-05, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=2333
2024-01-31 05:22:40 | INFO | train_inner | epoch 126:    285 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61021.6, ups=15.61, wpb=3909.2, bsz=208.2, num_updates=168900, lr=7.69458e-05, gnorm=0.557, loss_scale=16, train_wall=6, gb_free=30, wall=2339
2024-01-31 05:22:47 | INFO | train_inner | epoch 126:    385 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60491.7, ups=15.61, wpb=3874.9, bsz=196.7, num_updates=169000, lr=7.69231e-05, gnorm=0.544, loss_scale=16, train_wall=6, gb_free=29.8, wall=2346
2024-01-31 05:22:53 | INFO | train_inner | epoch 126:    485 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60673.2, ups=15.46, wpb=3923.9, bsz=208.2, num_updates=169100, lr=7.69003e-05, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=2352
2024-01-31 05:23:00 | INFO | train_inner | epoch 126:    585 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61076.6, ups=15.42, wpb=3959.7, bsz=213.2, num_updates=169200, lr=7.68776e-05, gnorm=0.555, loss_scale=16, train_wall=6, gb_free=29.7, wall=2359
2024-01-31 05:23:06 | INFO | train_inner | epoch 126:    685 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60189.2, ups=15.44, wpb=3899.4, bsz=221.8, num_updates=169300, lr=7.68549e-05, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.7, wall=2365
2024-01-31 05:23:13 | INFO | train_inner | epoch 126:    785 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60492.7, ups=15.28, wpb=3959.9, bsz=218.3, num_updates=169400, lr=7.68322e-05, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.8, wall=2372
2024-01-31 05:23:19 | INFO | train_inner | epoch 126:    885 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=59735.2, ups=15.4, wpb=3878.7, bsz=220, num_updates=169500, lr=7.68095e-05, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.8, wall=2378
2024-01-31 05:23:26 | INFO | train_inner | epoch 126:    985 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60127.6, ups=15.43, wpb=3896.6, bsz=212.2, num_updates=169600, lr=7.67869e-05, gnorm=0.534, loss_scale=16, train_wall=6, gb_free=29.7, wall=2385
2024-01-31 05:23:32 | INFO | train_inner | epoch 126:   1085 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60386.3, ups=15.41, wpb=3917.7, bsz=219.1, num_updates=169700, lr=7.67643e-05, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=2391
2024-01-31 05:23:39 | INFO | train_inner | epoch 126:   1185 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61511.2, ups=15.59, wpb=3944.9, bsz=207.8, num_updates=169800, lr=7.67417e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=2398
2024-01-31 05:23:45 | INFO | train_inner | epoch 126:   1285 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61015.3, ups=15.54, wpb=3926.9, bsz=205.8, num_updates=169900, lr=7.67191e-05, gnorm=0.524, loss_scale=16, train_wall=6, gb_free=29.7, wall=2404
2024-01-31 05:23:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:23:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:23:51 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 2.174 | nll_loss 0.433 | ppl 1.35 | wps 133050 | wpb 3275.2 | bsz 176.5 | num_updates 169964 | best_loss 2.063
2024-01-31 05:23:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 169964 updates
2024-01-31 05:23:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint126.pt
2024-01-31 05:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint126.pt
2024-01-31 05:23:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint126.pt (epoch 126 @ 169964 updates, score 2.174) (writing took 2.2878510439768434 seconds)
2024-01-31 05:23:54 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2024-01-31 05:23:54 | INFO | train | epoch 126 | loss 1.787 | nll_loss 0.19 | ppl 1.14 | wps 57538.5 | ups 14.67 | wpb 3921.2 | bsz 214.5 | num_updates 169964 | lr 7.67046e-05 | gnorm 0.51 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 2413
2024-01-31 05:23:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:23:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:23:54 | INFO | fairseq.trainer | begin training epoch 127
2024-01-31 05:23:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:23:56 | INFO | train_inner | epoch 127:     36 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=35443, ups=8.99, wpb=3944.3, bsz=216.2, num_updates=170000, lr=7.66965e-05, gnorm=0.448, loss_scale=16, train_wall=6, gb_free=29.8, wall=2415
2024-01-31 05:24:03 | INFO | train_inner | epoch 127:    136 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60723.8, ups=15.52, wpb=3913.5, bsz=206.1, num_updates=170100, lr=7.6674e-05, gnorm=0.453, loss_scale=16, train_wall=6, gb_free=29.7, wall=2422
2024-01-31 05:24:09 | INFO | train_inner | epoch 127:    236 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=60433.5, ups=15.58, wpb=3879.8, bsz=202.4, num_updates=170200, lr=7.66514e-05, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.8, wall=2428
2024-01-31 05:24:16 | INFO | train_inner | epoch 127:    336 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61278, ups=15.46, wpb=3963.2, bsz=214.2, num_updates=170300, lr=7.66289e-05, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.7, wall=2434
2024-01-31 05:24:22 | INFO | train_inner | epoch 127:    436 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60996.9, ups=15.4, wpb=3961.2, bsz=220.5, num_updates=170400, lr=7.66064e-05, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=2441
2024-01-31 05:24:28 | INFO | train_inner | epoch 127:    536 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60713, ups=15.55, wpb=3905.2, bsz=202.4, num_updates=170500, lr=7.6584e-05, gnorm=0.538, loss_scale=16, train_wall=6, gb_free=29.8, wall=2447
2024-01-31 05:24:35 | INFO | train_inner | epoch 127:    636 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60549.4, ups=15.42, wpb=3926.8, bsz=229.5, num_updates=170600, lr=7.65615e-05, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.7, wall=2454
2024-01-31 05:24:41 | INFO | train_inner | epoch 127:    736 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60698.4, ups=15.56, wpb=3900.9, bsz=204.9, num_updates=170700, lr=7.65391e-05, gnorm=0.461, loss_scale=16, train_wall=6, gb_free=29.7, wall=2460
2024-01-31 05:24:48 | INFO | train_inner | epoch 127:    836 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=59912.8, ups=15.41, wpb=3887.4, bsz=227.6, num_updates=170800, lr=7.65167e-05, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=2467
2024-01-31 05:24:54 | INFO | train_inner | epoch 127:    936 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60942.3, ups=15.38, wpb=3961.6, bsz=220.9, num_updates=170900, lr=7.64943e-05, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.8, wall=2473
2024-01-31 05:25:01 | INFO | train_inner | epoch 127:   1036 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60853.1, ups=15.52, wpb=3921.2, bsz=221.4, num_updates=171000, lr=7.64719e-05, gnorm=0.468, loss_scale=16, train_wall=6, gb_free=29.9, wall=2480
2024-01-31 05:25:07 | INFO | train_inner | epoch 127:   1136 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60355, ups=15.56, wpb=3877.7, bsz=202.3, num_updates=171100, lr=7.64496e-05, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.8, wall=2486
2024-01-31 05:25:14 | INFO | train_inner | epoch 127:   1236 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60854.6, ups=15.45, wpb=3937.7, bsz=210.6, num_updates=171200, lr=7.64272e-05, gnorm=0.452, loss_scale=16, train_wall=6, gb_free=29.8, wall=2493
2024-01-31 05:25:20 | INFO | train_inner | epoch 127:   1336 / 1349 loss=1.789, nll_loss=0.191, ppl=1.14, wps=60772.1, ups=15.43, wpb=3938, bsz=222.9, num_updates=171300, lr=7.64049e-05, gnorm=0.569, loss_scale=16, train_wall=6, gb_free=29.7, wall=2499
2024-01-31 05:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:25:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:25:23 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 2.173 | nll_loss 0.433 | ppl 1.35 | wps 131727 | wpb 3275.2 | bsz 176.5 | num_updates 171313 | best_loss 2.063
2024-01-31 05:25:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 171313 updates
2024-01-31 05:25:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint127.pt
2024-01-31 05:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint127.pt
2024-01-31 05:25:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint127.pt (epoch 127 @ 171313 updates, score 2.173) (writing took 2.25458023394458 seconds)
2024-01-31 05:25:26 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2024-01-31 05:25:26 | INFO | train | epoch 127 | loss 1.787 | nll_loss 0.19 | ppl 1.14 | wps 57621.4 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 171313 | lr 7.6402e-05 | gnorm 0.5 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 2504
2024-01-31 05:25:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:25:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:25:26 | INFO | fairseq.trainer | begin training epoch 128
2024-01-31 05:25:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:25:31 | INFO | train_inner | epoch 128:     87 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=35432.9, ups=9.02, wpb=3927.7, bsz=212.7, num_updates=171400, lr=7.63826e-05, gnorm=0.488, loss_scale=16, train_wall=6, gb_free=29.8, wall=2510
2024-01-31 05:25:38 | INFO | train_inner | epoch 128:    187 / 1349 loss=1.782, nll_loss=0.184, ppl=1.14, wps=60371.3, ups=15.39, wpb=3923.2, bsz=219.1, num_updates=171500, lr=7.63604e-05, gnorm=0.476, loss_scale=16, train_wall=6, gb_free=29.8, wall=2517
2024-01-31 05:25:44 | INFO | train_inner | epoch 128:    287 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60913.2, ups=15.43, wpb=3947.1, bsz=216.4, num_updates=171600, lr=7.63381e-05, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.7, wall=2523
2024-01-31 05:25:51 | INFO | train_inner | epoch 128:    387 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61317.2, ups=15.52, wpb=3952.1, bsz=214.4, num_updates=171700, lr=7.63159e-05, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=2530
2024-01-31 05:25:57 | INFO | train_inner | epoch 128:    487 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60684.1, ups=15.51, wpb=3911.4, bsz=218.3, num_updates=171800, lr=7.62937e-05, gnorm=0.564, loss_scale=16, train_wall=6, gb_free=29.9, wall=2536
2024-01-31 05:26:04 | INFO | train_inner | epoch 128:    587 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60475.7, ups=15.46, wpb=3912.4, bsz=223, num_updates=171900, lr=7.62715e-05, gnorm=0.545, loss_scale=16, train_wall=6, gb_free=29.8, wall=2543
2024-01-31 05:26:10 | INFO | train_inner | epoch 128:    687 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=59660.4, ups=15.58, wpb=3830, bsz=202.8, num_updates=172000, lr=7.62493e-05, gnorm=0.584, loss_scale=16, train_wall=6, gb_free=29.8, wall=2549
2024-01-31 05:26:16 | INFO | train_inner | epoch 128:    787 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61310.7, ups=15.52, wpb=3949.7, bsz=199.6, num_updates=172100, lr=7.62271e-05, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.8, wall=2555
2024-01-31 05:26:23 | INFO | train_inner | epoch 128:    887 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60631, ups=15.4, wpb=3936, bsz=215.4, num_updates=172200, lr=7.6205e-05, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=2562
2024-01-31 05:26:29 | INFO | train_inner | epoch 128:    987 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60205.5, ups=15.37, wpb=3917.8, bsz=226.4, num_updates=172300, lr=7.61829e-05, gnorm=0.547, loss_scale=16, train_wall=6, gb_free=29.8, wall=2568
2024-01-31 05:26:36 | INFO | train_inner | epoch 128:   1087 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60972.5, ups=15.42, wpb=3955, bsz=221.1, num_updates=172400, lr=7.61608e-05, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=2575
2024-01-31 05:26:42 | INFO | train_inner | epoch 128:   1187 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=60687.3, ups=15.53, wpb=3908.7, bsz=206.6, num_updates=172500, lr=7.61387e-05, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.8, wall=2581
2024-01-31 05:26:49 | INFO | train_inner | epoch 128:   1287 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60645.1, ups=15.5, wpb=3912.2, bsz=206.1, num_updates=172600, lr=7.61166e-05, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=2588
2024-01-31 05:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:26:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:26:55 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 2.178 | nll_loss 0.437 | ppl 1.35 | wps 132899 | wpb 3275.2 | bsz 176.5 | num_updates 172662 | best_loss 2.063
2024-01-31 05:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 172662 updates
2024-01-31 05:26:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint128.pt
2024-01-31 05:26:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint128.pt
2024-01-31 05:26:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint128.pt (epoch 128 @ 172662 updates, score 2.178) (writing took 2.231556835002266 seconds)
2024-01-31 05:26:57 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2024-01-31 05:26:57 | INFO | train | epoch 128 | loss 1.787 | nll_loss 0.19 | ppl 1.14 | wps 57624.5 | ups 14.7 | wpb 3921.2 | bsz 214.5 | num_updates 172662 | lr 7.6103e-05 | gnorm 0.518 | loss_scale 16 | train_wall 84 | gb_free 29.7 | wall 2596
2024-01-31 05:26:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:26:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:26:57 | INFO | fairseq.trainer | begin training epoch 129
2024-01-31 05:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:27:00 | INFO | train_inner | epoch 129:     38 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=35157.6, ups=9.03, wpb=3892.2, bsz=226, num_updates=172700, lr=7.60946e-05, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=2599
2024-01-31 05:27:06 | INFO | train_inner | epoch 129:    138 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60438, ups=15.52, wpb=3895.2, bsz=210.8, num_updates=172800, lr=7.60726e-05, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.8, wall=2605
2024-01-31 05:27:13 | INFO | train_inner | epoch 129:    238 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60933, ups=15.49, wpb=3932.5, bsz=206.5, num_updates=172900, lr=7.60506e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=2612
2024-01-31 05:27:19 | INFO | train_inner | epoch 129:    338 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=60781.8, ups=15.41, wpb=3945.2, bsz=222.2, num_updates=173000, lr=7.60286e-05, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.7, wall=2618
2024-01-31 05:27:26 | INFO | train_inner | epoch 129:    438 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60046.7, ups=15.52, wpb=3868.8, bsz=226.9, num_updates=173100, lr=7.60066e-05, gnorm=0.556, loss_scale=16, train_wall=6, gb_free=29.8, wall=2625
2024-01-31 05:27:32 | INFO | train_inner | epoch 129:    538 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=59665.7, ups=15.46, wpb=3859.7, bsz=207.7, num_updates=173200, lr=7.59847e-05, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.8, wall=2631
2024-01-31 05:27:39 | INFO | train_inner | epoch 129:    638 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60412.2, ups=15.38, wpb=3929, bsz=235.2, num_updates=173300, lr=7.59628e-05, gnorm=0.488, loss_scale=16, train_wall=6, gb_free=29.8, wall=2638
2024-01-31 05:27:45 | INFO | train_inner | epoch 129:    738 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60467.2, ups=15.49, wpb=3903.5, bsz=205.9, num_updates=173400, lr=7.59408e-05, gnorm=0.489, loss_scale=16, train_wall=6, gb_free=29.8, wall=2644
2024-01-31 05:27:52 | INFO | train_inner | epoch 129:    838 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61306, ups=15.51, wpb=3953.8, bsz=213, num_updates=173500, lr=7.5919e-05, gnorm=0.561, loss_scale=16, train_wall=6, gb_free=29.7, wall=2651
2024-01-31 05:27:58 | INFO | train_inner | epoch 129:    938 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=61409.8, ups=15.43, wpb=3980.2, bsz=215, num_updates=173600, lr=7.58971e-05, gnorm=0.434, loss_scale=16, train_wall=6, gb_free=29.7, wall=2657
2024-01-31 05:28:05 | INFO | train_inner | epoch 129:   1038 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61082.1, ups=15.41, wpb=3964.1, bsz=211.9, num_updates=173700, lr=7.58752e-05, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.7, wall=2664
2024-01-31 05:28:11 | INFO | train_inner | epoch 129:   1138 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60419.3, ups=15.59, wpb=3876.4, bsz=211.8, num_updates=173800, lr=7.58534e-05, gnorm=0.593, loss_scale=16, train_wall=6, gb_free=29.8, wall=2670
2024-01-31 05:28:18 | INFO | train_inner | epoch 129:   1238 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60529.4, ups=15.42, wpb=3925.5, bsz=220.9, num_updates=173900, lr=7.58316e-05, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.7, wall=2676
2024-01-31 05:28:24 | INFO | train_inner | epoch 129:   1338 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61292.7, ups=15.56, wpb=3938.2, bsz=196.6, num_updates=174000, lr=7.58098e-05, gnorm=0.526, loss_scale=16, train_wall=6, gb_free=29.8, wall=2683
2024-01-31 05:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:28:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:28:27 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 2.173 | nll_loss 0.432 | ppl 1.35 | wps 132736 | wpb 3275.2 | bsz 176.5 | num_updates 174011 | best_loss 2.063
2024-01-31 05:28:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 174011 updates
2024-01-31 05:28:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint129.pt
2024-01-31 05:28:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint129.pt
2024-01-31 05:28:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint129.pt (epoch 129 @ 174011 updates, score 2.173) (writing took 2.268619226058945 seconds)
2024-01-31 05:28:29 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2024-01-31 05:28:29 | INFO | train | epoch 129 | loss 1.787 | nll_loss 0.19 | ppl 1.14 | wps 57594.1 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 174011 | lr 7.58074e-05 | gnorm 0.513 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 2688
2024-01-31 05:28:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:28:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:28:29 | INFO | fairseq.trainer | begin training epoch 130
2024-01-31 05:28:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:28:35 | INFO | train_inner | epoch 130:     89 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=35122.9, ups=8.96, wpb=3918.9, bsz=218, num_updates=174100, lr=7.5788e-05, gnorm=0.521, loss_scale=16, train_wall=6, gb_free=29.8, wall=2694
2024-01-31 05:28:41 | INFO | train_inner | epoch 130:    189 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60798.4, ups=15.64, wpb=3888.3, bsz=213.2, num_updates=174200, lr=7.57663e-05, gnorm=0.524, loss_scale=16, train_wall=6, gb_free=29.7, wall=2700
2024-01-31 05:28:48 | INFO | train_inner | epoch 130:    289 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60307.2, ups=15.5, wpb=3890.1, bsz=200.6, num_updates=174300, lr=7.57445e-05, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.9, wall=2707
2024-01-31 05:28:54 | INFO | train_inner | epoch 130:    389 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=60519.3, ups=15.42, wpb=3925.3, bsz=217.7, num_updates=174400, lr=7.57228e-05, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.8, wall=2713
2024-01-31 05:29:01 | INFO | train_inner | epoch 130:    489 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60767.4, ups=15.48, wpb=3925.3, bsz=214.2, num_updates=174500, lr=7.57011e-05, gnorm=0.544, loss_scale=16, train_wall=6, gb_free=29.8, wall=2720
2024-01-31 05:29:07 | INFO | train_inner | epoch 130:    589 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60544.4, ups=15.43, wpb=3924.4, bsz=225.1, num_updates=174600, lr=7.56794e-05, gnorm=0.462, loss_scale=16, train_wall=6, gb_free=29.8, wall=2726
2024-01-31 05:29:14 | INFO | train_inner | epoch 130:    689 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60983.1, ups=15.58, wpb=3914.4, bsz=209.5, num_updates=174700, lr=7.56578e-05, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.8, wall=2733
2024-01-31 05:29:20 | INFO | train_inner | epoch 130:    789 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=61121.8, ups=15.55, wpb=3929.8, bsz=211, num_updates=174800, lr=7.56361e-05, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.8, wall=2739
2024-01-31 05:29:27 | INFO | train_inner | epoch 130:    889 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61044.3, ups=15.54, wpb=3927.7, bsz=214.3, num_updates=174900, lr=7.56145e-05, gnorm=0.558, loss_scale=16, train_wall=6, gb_free=29.8, wall=2746
2024-01-31 05:29:33 | INFO | train_inner | epoch 130:    989 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60675.7, ups=15.41, wpb=3938.6, bsz=220.7, num_updates=175000, lr=7.55929e-05, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=2752
2024-01-31 05:29:40 | INFO | train_inner | epoch 130:   1089 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60461.7, ups=15.33, wpb=3943, bsz=221.9, num_updates=175100, lr=7.55713e-05, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=2759
2024-01-31 05:29:46 | INFO | train_inner | epoch 130:   1189 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60465.8, ups=15.39, wpb=3929.8, bsz=218.7, num_updates=175200, lr=7.55497e-05, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.9, wall=2765
2024-01-31 05:29:53 | INFO | train_inner | epoch 130:   1289 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=61276.3, ups=15.58, wpb=3931.9, bsz=199, num_updates=175300, lr=7.55282e-05, gnorm=0.586, loss_scale=16, train_wall=6, gb_free=29.7, wall=2771
2024-01-31 05:29:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:29:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:29:59 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 2.175 | nll_loss 0.433 | ppl 1.35 | wps 132393 | wpb 3275.2 | bsz 176.5 | num_updates 175360 | best_loss 2.063
2024-01-31 05:29:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 175360 updates
2024-01-31 05:29:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint130.pt
2024-01-31 05:30:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint130.pt
2024-01-31 05:30:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint130.pt (epoch 130 @ 175360 updates, score 2.175) (writing took 2.2396880169399083 seconds)
2024-01-31 05:30:01 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2024-01-31 05:30:01 | INFO | train | epoch 130 | loss 1.787 | nll_loss 0.189 | ppl 1.14 | wps 57645.5 | ups 14.7 | wpb 3921.2 | bsz 214.5 | num_updates 175360 | lr 7.55153e-05 | gnorm 0.516 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 2780
2024-01-31 05:30:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:30:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:30:08 | INFO | fairseq.trainer | begin training epoch 131
2024-01-31 05:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:30:16 | INFO | train_inner | epoch 131:     40 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=16767.6, ups=4.3, wpb=3903.9, bsz=220, num_updates=175400, lr=7.55067e-05, gnorm=0.44, loss_scale=16, train_wall=6, gb_free=29.7, wall=2795
2024-01-31 05:30:22 | INFO | train_inner | epoch 131:    140 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=60810.5, ups=15.47, wpb=3931, bsz=212.6, num_updates=175500, lr=7.54851e-05, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=30, wall=2801
2024-01-31 05:30:29 | INFO | train_inner | epoch 131:    240 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=59538.9, ups=15.53, wpb=3833.1, bsz=212.2, num_updates=175600, lr=7.54636e-05, gnorm=0.613, loss_scale=16, train_wall=6, gb_free=29.7, wall=2808
2024-01-31 05:30:35 | INFO | train_inner | epoch 131:    340 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61118.5, ups=15.42, wpb=3963.5, bsz=220.8, num_updates=175700, lr=7.54422e-05, gnorm=0.544, loss_scale=16, train_wall=6, gb_free=29.8, wall=2814
2024-01-31 05:30:42 | INFO | train_inner | epoch 131:    440 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60949.4, ups=15.53, wpb=3925.8, bsz=212.1, num_updates=175800, lr=7.54207e-05, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.8, wall=2821
2024-01-31 05:30:48 | INFO | train_inner | epoch 131:    540 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=61528.1, ups=15.45, wpb=3982.4, bsz=209.8, num_updates=175900, lr=7.53993e-05, gnorm=0.458, loss_scale=16, train_wall=6, gb_free=29.8, wall=2827
2024-01-31 05:30:55 | INFO | train_inner | epoch 131:    640 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60579.9, ups=15.46, wpb=3919.7, bsz=208.4, num_updates=176000, lr=7.53778e-05, gnorm=0.514, loss_scale=16, train_wall=6, gb_free=29.9, wall=2834
2024-01-31 05:31:01 | INFO | train_inner | epoch 131:    740 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60843.5, ups=15.41, wpb=3947.5, bsz=221.2, num_updates=176100, lr=7.53564e-05, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.7, wall=2840
2024-01-31 05:31:08 | INFO | train_inner | epoch 131:    840 / 1349 loss=1.791, nll_loss=0.194, ppl=1.14, wps=60515.2, ups=15.47, wpb=3910.6, bsz=224.4, num_updates=176200, lr=7.5335e-05, gnorm=0.54, loss_scale=16, train_wall=6, gb_free=29.8, wall=2846
2024-01-31 05:31:14 | INFO | train_inner | epoch 131:    940 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60793.6, ups=15.39, wpb=3951.4, bsz=228.4, num_updates=176300, lr=7.53137e-05, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=2853
2024-01-31 05:31:21 | INFO | train_inner | epoch 131:   1040 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60312.4, ups=15.52, wpb=3886.1, bsz=218.3, num_updates=176400, lr=7.52923e-05, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.8, wall=2859
2024-01-31 05:31:27 | INFO | train_inner | epoch 131:   1140 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60904.4, ups=15.59, wpb=3905.8, bsz=199.8, num_updates=176500, lr=7.5271e-05, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.8, wall=2866
2024-01-31 05:31:33 | INFO | train_inner | epoch 131:   1240 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60438.1, ups=15.49, wpb=3900.9, bsz=193.9, num_updates=176600, lr=7.52497e-05, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=2872
2024-01-31 05:31:40 | INFO | train_inner | epoch 131:   1340 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61279.1, ups=15.53, wpb=3944.9, bsz=223.4, num_updates=176700, lr=7.52284e-05, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.8, wall=2879
2024-01-31 05:31:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:31:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:31:43 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 2.175 | nll_loss 0.433 | ppl 1.35 | wps 132142 | wpb 3275.2 | bsz 176.5 | num_updates 176709 | best_loss 2.063
2024-01-31 05:31:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 176709 updates
2024-01-31 05:31:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint131.pt
2024-01-31 05:31:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint131.pt
2024-01-31 05:31:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint131.pt (epoch 131 @ 176709 updates, score 2.175) (writing took 2.2491444599581882 seconds)
2024-01-31 05:31:45 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2024-01-31 05:31:45 | INFO | train | epoch 131 | loss 1.787 | nll_loss 0.189 | ppl 1.14 | wps 50863.4 | ups 12.97 | wpb 3921.2 | bsz 214.5 | num_updates 176709 | lr 7.52265e-05 | gnorm 0.516 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 2884
2024-01-31 05:31:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:31:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:31:45 | INFO | fairseq.trainer | begin training epoch 132
2024-01-31 05:31:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:31:51 | INFO | train_inner | epoch 132:     91 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=35286.9, ups=9.02, wpb=3912.8, bsz=207.8, num_updates=176800, lr=7.52071e-05, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=2890
2024-01-31 05:31:57 | INFO | train_inner | epoch 132:    191 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60898.1, ups=15.51, wpb=3926, bsz=221.3, num_updates=176900, lr=7.51858e-05, gnorm=0.469, loss_scale=16, train_wall=6, gb_free=29.8, wall=2896
2024-01-31 05:32:04 | INFO | train_inner | epoch 132:    291 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=59735, ups=15.6, wpb=3828.4, bsz=203.2, num_updates=177000, lr=7.51646e-05, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=2903
2024-01-31 05:32:10 | INFO | train_inner | epoch 132:    391 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61313.5, ups=15.51, wpb=3951.9, bsz=204.1, num_updates=177100, lr=7.51434e-05, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.7, wall=2909
2024-01-31 05:32:17 | INFO | train_inner | epoch 132:    491 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61042.4, ups=15.49, wpb=3941.6, bsz=209.7, num_updates=177200, lr=7.51222e-05, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.8, wall=2916
2024-01-31 05:32:23 | INFO | train_inner | epoch 132:    591 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60380.8, ups=15.46, wpb=3905.7, bsz=211.7, num_updates=177300, lr=7.5101e-05, gnorm=0.457, loss_scale=16, train_wall=6, gb_free=29.8, wall=2922
2024-01-31 05:32:30 | INFO | train_inner | epoch 132:    691 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60788.6, ups=15.46, wpb=3931.4, bsz=209.8, num_updates=177400, lr=7.50798e-05, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=2929
2024-01-31 05:32:36 | INFO | train_inner | epoch 132:    791 / 1349 loss=1.79, nll_loss=0.193, ppl=1.14, wps=60954.7, ups=15.47, wpb=3939, bsz=219.7, num_updates=177500, lr=7.50587e-05, gnorm=0.565, loss_scale=16, train_wall=6, gb_free=29.7, wall=2935
2024-01-31 05:32:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 05:32:43 | INFO | train_inner | epoch 132:    892 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60862.8, ups=15.29, wpb=3980.4, bsz=222.4, num_updates=177600, lr=7.50375e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.8, wall=2942
2024-01-31 05:32:49 | INFO | train_inner | epoch 132:    992 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60083.9, ups=15.49, wpb=3879.6, bsz=217.4, num_updates=177700, lr=7.50164e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=2948
2024-01-31 05:32:56 | INFO | train_inner | epoch 132:   1092 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60154.5, ups=15.37, wpb=3914.1, bsz=232.2, num_updates=177800, lr=7.49953e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=2954
2024-01-31 05:33:02 | INFO | train_inner | epoch 132:   1192 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60533.1, ups=15.49, wpb=3907.4, bsz=215.4, num_updates=177900, lr=7.49742e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.7, wall=2961
2024-01-31 05:33:09 | INFO | train_inner | epoch 132:   1292 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60716.2, ups=15.41, wpb=3941.3, bsz=211.4, num_updates=178000, lr=7.49532e-05, gnorm=0.551, loss_scale=8, train_wall=6, gb_free=29.8, wall=2967
2024-01-31 05:33:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:33:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:33:14 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 2.173 | nll_loss 0.433 | ppl 1.35 | wps 132520 | wpb 3275.2 | bsz 176.5 | num_updates 178057 | best_loss 2.063
2024-01-31 05:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 178057 updates
2024-01-31 05:33:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint132.pt
2024-01-31 05:33:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint132.pt
2024-01-31 05:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint132.pt (epoch 132 @ 178057 updates, score 2.173) (writing took 2.305149942985736 seconds)
2024-01-31 05:33:17 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2024-01-31 05:33:17 | INFO | train | epoch 132 | loss 1.786 | nll_loss 0.189 | ppl 1.14 | wps 57545.3 | ups 14.68 | wpb 3921 | bsz 214.4 | num_updates 178057 | lr 7.49412e-05 | gnorm 0.509 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 2976
2024-01-31 05:33:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:33:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:33:17 | INFO | fairseq.trainer | begin training epoch 133
2024-01-31 05:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:33:20 | INFO | train_inner | epoch 133:     43 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=35125.9, ups=8.94, wpb=3927.6, bsz=217.8, num_updates=178100, lr=7.49321e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=2979
2024-01-31 05:33:26 | INFO | train_inner | epoch 133:    143 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60712.7, ups=15.53, wpb=3908.9, bsz=219.8, num_updates=178200, lr=7.49111e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=2985
2024-01-31 05:33:33 | INFO | train_inner | epoch 133:    243 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60872.3, ups=15.43, wpb=3944.5, bsz=222.9, num_updates=178300, lr=7.48901e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.7, wall=2992
2024-01-31 05:33:39 | INFO | train_inner | epoch 133:    343 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60528.2, ups=15.5, wpb=3906.1, bsz=217.9, num_updates=178400, lr=7.48691e-05, gnorm=0.534, loss_scale=8, train_wall=6, gb_free=29.7, wall=2998
2024-01-31 05:33:46 | INFO | train_inner | epoch 133:    443 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60605.6, ups=15.53, wpb=3903.5, bsz=215.6, num_updates=178500, lr=7.48481e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=3004
2024-01-31 05:33:52 | INFO | train_inner | epoch 133:    543 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60097.2, ups=15.47, wpb=3884.1, bsz=210.6, num_updates=178600, lr=7.48272e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=3011
2024-01-31 05:33:58 | INFO | train_inner | epoch 133:    643 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60166.9, ups=15.41, wpb=3904.8, bsz=220.7, num_updates=178700, lr=7.48062e-05, gnorm=0.433, loss_scale=8, train_wall=6, gb_free=29.7, wall=3017
2024-01-31 05:34:05 | INFO | train_inner | epoch 133:    743 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60894.7, ups=15.37, wpb=3962.4, bsz=219.1, num_updates=178800, lr=7.47853e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.7, wall=3024
2024-01-31 05:34:11 | INFO | train_inner | epoch 133:    843 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60622.4, ups=15.48, wpb=3916.7, bsz=210.2, num_updates=178900, lr=7.47644e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=3030
2024-01-31 05:34:18 | INFO | train_inner | epoch 133:    943 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60837.3, ups=15.58, wpb=3903.6, bsz=207, num_updates=179000, lr=7.47435e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=3037
2024-01-31 05:34:24 | INFO | train_inner | epoch 133:   1043 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61869.9, ups=15.61, wpb=3964.2, bsz=207, num_updates=179100, lr=7.47226e-05, gnorm=0.545, loss_scale=8, train_wall=6, gb_free=29.7, wall=3043
2024-01-31 05:34:31 | INFO | train_inner | epoch 133:   1143 / 1349 loss=1.787, nll_loss=0.189, ppl=1.14, wps=60676.2, ups=15.51, wpb=3911.6, bsz=200.3, num_updates=179200, lr=7.47018e-05, gnorm=0.459, loss_scale=8, train_wall=6, gb_free=29.7, wall=3050
2024-01-31 05:34:37 | INFO | train_inner | epoch 133:   1243 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60964.6, ups=15.46, wpb=3943.6, bsz=214.2, num_updates=179300, lr=7.4681e-05, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.7, wall=3056
2024-01-31 05:34:44 | INFO | train_inner | epoch 133:   1343 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60969.3, ups=15.49, wpb=3936, bsz=222.6, num_updates=179400, lr=7.46601e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.7, wall=3063
2024-01-31 05:34:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:34:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:34:46 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 2.179 | nll_loss 0.438 | ppl 1.36 | wps 132577 | wpb 3275.2 | bsz 176.5 | num_updates 179406 | best_loss 2.063
2024-01-31 05:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 179406 updates
2024-01-31 05:34:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint133.pt
2024-01-31 05:34:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint133.pt
2024-01-31 05:34:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint133.pt (epoch 133 @ 179406 updates, score 2.179) (writing took 2.279676618054509 seconds)
2024-01-31 05:34:49 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2024-01-31 05:34:49 | INFO | train | epoch 133 | loss 1.786 | nll_loss 0.189 | ppl 1.14 | wps 57616.7 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 179406 | lr 7.46589e-05 | gnorm 0.495 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 3067
2024-01-31 05:34:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:34:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:34:49 | INFO | fairseq.trainer | begin training epoch 134
2024-01-31 05:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:34:55 | INFO | train_inner | epoch 134:     94 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=35187.6, ups=8.98, wpb=3918.1, bsz=216.1, num_updates=179500, lr=7.46393e-05, gnorm=0.433, loss_scale=8, train_wall=6, gb_free=29.8, wall=3074
2024-01-31 05:35:01 | INFO | train_inner | epoch 134:    194 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60665.4, ups=15.5, wpb=3913.3, bsz=213, num_updates=179600, lr=7.46186e-05, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.8, wall=3080
2024-01-31 05:35:08 | INFO | train_inner | epoch 134:    294 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=59917.2, ups=15.36, wpb=3900.4, bsz=230.6, num_updates=179700, lr=7.45978e-05, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.8, wall=3087
2024-01-31 05:35:14 | INFO | train_inner | epoch 134:    394 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=61046.7, ups=15.47, wpb=3945.1, bsz=223.9, num_updates=179800, lr=7.4577e-05, gnorm=0.557, loss_scale=8, train_wall=6, gb_free=29.8, wall=3093
2024-01-31 05:35:21 | INFO | train_inner | epoch 134:    494 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61261.2, ups=15.51, wpb=3950.8, bsz=204.5, num_updates=179900, lr=7.45563e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=3100
2024-01-31 05:35:27 | INFO | train_inner | epoch 134:    594 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=60938.7, ups=15.42, wpb=3950.7, bsz=222.9, num_updates=180000, lr=7.45356e-05, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.8, wall=3106
2024-01-31 05:35:34 | INFO | train_inner | epoch 134:    694 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60205.9, ups=15.57, wpb=3867.8, bsz=209.1, num_updates=180100, lr=7.45149e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=3112
2024-01-31 05:35:40 | INFO | train_inner | epoch 134:    794 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60395.4, ups=15.55, wpb=3882.8, bsz=210.1, num_updates=180200, lr=7.44942e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=3119
2024-01-31 05:35:46 | INFO | train_inner | epoch 134:    894 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=61055.8, ups=15.53, wpb=3931.7, bsz=198.7, num_updates=180300, lr=7.44736e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=3125
2024-01-31 05:35:53 | INFO | train_inner | epoch 134:    994 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61103.4, ups=15.52, wpb=3936.7, bsz=206.1, num_updates=180400, lr=7.44529e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=3132
2024-01-31 05:35:59 | INFO | train_inner | epoch 134:   1094 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60993.4, ups=15.51, wpb=3931.5, bsz=203.4, num_updates=180500, lr=7.44323e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.8, wall=3138
2024-01-31 05:36:06 | INFO | train_inner | epoch 134:   1194 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60631.3, ups=15.35, wpb=3951.1, bsz=227.8, num_updates=180600, lr=7.44117e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.7, wall=3145
2024-01-31 05:36:12 | INFO | train_inner | epoch 134:   1294 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60512.8, ups=15.58, wpb=3883.1, bsz=217.8, num_updates=180700, lr=7.43911e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=3151
2024-01-31 05:36:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:36:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:36:18 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 2.178 | nll_loss 0.438 | ppl 1.35 | wps 133474 | wpb 3275.2 | bsz 176.5 | num_updates 180755 | best_loss 2.063
2024-01-31 05:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 180755 updates
2024-01-31 05:36:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint134.pt
2024-01-31 05:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint134.pt
2024-01-31 05:36:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint134.pt (epoch 134 @ 180755 updates, score 2.178) (writing took 2.2271228280151263 seconds)
2024-01-31 05:36:20 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2024-01-31 05:36:20 | INFO | train | epoch 134 | loss 1.786 | nll_loss 0.189 | ppl 1.14 | wps 57688.3 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 180755 | lr 7.43798e-05 | gnorm 0.512 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 3159
2024-01-31 05:36:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:36:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:36:20 | INFO | fairseq.trainer | begin training epoch 135
2024-01-31 05:36:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:36:23 | INFO | train_inner | epoch 135:     45 / 1349 loss=1.783, nll_loss=0.185, ppl=1.14, wps=35484.8, ups=9.04, wpb=3924.9, bsz=214.5, num_updates=180800, lr=7.43705e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.8, wall=3162
2024-01-31 05:36:30 | INFO | train_inner | epoch 135:    145 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60750.5, ups=15.51, wpb=3916.2, bsz=211.3, num_updates=180900, lr=7.435e-05, gnorm=0.564, loss_scale=8, train_wall=6, gb_free=29.7, wall=3169
2024-01-31 05:36:36 | INFO | train_inner | epoch 135:    245 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=59776.7, ups=15.47, wpb=3862.8, bsz=204.7, num_updates=181000, lr=7.43294e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.7, wall=3175
2024-01-31 05:36:43 | INFO | train_inner | epoch 135:    345 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61240.3, ups=15.52, wpb=3944.7, bsz=223.2, num_updates=181100, lr=7.43089e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=3182
2024-01-31 05:36:49 | INFO | train_inner | epoch 135:    445 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60828.3, ups=15.35, wpb=3962, bsz=232.6, num_updates=181200, lr=7.42884e-05, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.8, wall=3188
2024-01-31 05:36:56 | INFO | train_inner | epoch 135:    545 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61175.3, ups=15.45, wpb=3959.9, bsz=211.7, num_updates=181300, lr=7.42679e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=3195
2024-01-31 05:37:02 | INFO | train_inner | epoch 135:    645 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=59932.9, ups=15.53, wpb=3860, bsz=210.9, num_updates=181400, lr=7.42474e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=3201
2024-01-31 05:37:09 | INFO | train_inner | epoch 135:    745 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60485.1, ups=15.58, wpb=3882.2, bsz=196.1, num_updates=181500, lr=7.4227e-05, gnorm=0.585, loss_scale=8, train_wall=6, gb_free=29.7, wall=3207
2024-01-31 05:37:15 | INFO | train_inner | epoch 135:    845 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60241.2, ups=15.48, wpb=3890.4, bsz=217.6, num_updates=181600, lr=7.42065e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.7, wall=3214
2024-01-31 05:37:21 | INFO | train_inner | epoch 135:    945 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=61385.2, ups=15.47, wpb=3967.2, bsz=218.9, num_updates=181700, lr=7.41861e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=3220
2024-01-31 05:37:28 | INFO | train_inner | epoch 135:   1045 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=61113, ups=15.54, wpb=3932.8, bsz=216.7, num_updates=181800, lr=7.41657e-05, gnorm=0.611, loss_scale=8, train_wall=6, gb_free=29.8, wall=3227
2024-01-31 05:37:34 | INFO | train_inner | epoch 135:   1145 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60691.4, ups=15.46, wpb=3926.4, bsz=215.9, num_updates=181900, lr=7.41453e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=3233
2024-01-31 05:37:41 | INFO | train_inner | epoch 135:   1245 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61415.8, ups=15.44, wpb=3977.3, bsz=216.9, num_updates=182000, lr=7.41249e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.7, wall=3240
2024-01-31 05:37:47 | INFO | train_inner | epoch 135:   1345 / 1349 loss=1.786, nll_loss=0.19, ppl=1.14, wps=60544.9, ups=15.57, wpb=3888.8, bsz=217.7, num_updates=182100, lr=7.41046e-05, gnorm=0.563, loss_scale=8, train_wall=6, gb_free=29.7, wall=3246
2024-01-31 05:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:37:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:37:50 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 2.174 | nll_loss 0.432 | ppl 1.35 | wps 133425 | wpb 3275.2 | bsz 176.5 | num_updates 182104 | best_loss 2.063
2024-01-31 05:37:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 182104 updates
2024-01-31 05:37:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint135.pt
2024-01-31 05:37:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint135.pt
2024-01-31 05:37:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint135.pt (epoch 135 @ 182104 updates, score 2.174) (writing took 2.328037983039394 seconds)
2024-01-31 05:37:52 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2024-01-31 05:37:52 | INFO | train | epoch 135 | loss 1.786 | nll_loss 0.189 | ppl 1.14 | wps 57601.2 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 182104 | lr 7.41038e-05 | gnorm 0.531 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 3251
2024-01-31 05:37:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:37:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:37:52 | INFO | fairseq.trainer | begin training epoch 136
2024-01-31 05:37:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:37:58 | INFO | train_inner | epoch 136:     96 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=34807.4, ups=8.88, wpb=3918.2, bsz=200.6, num_updates=182200, lr=7.40842e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.8, wall=3257
2024-01-31 05:38:06 | INFO | train_inner | epoch 136:    196 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=51717.5, ups=13.06, wpb=3959.2, bsz=207, num_updates=182300, lr=7.40639e-05, gnorm=0.475, loss_scale=8, train_wall=7, gb_free=29.8, wall=3265
2024-01-31 05:38:13 | INFO | train_inner | epoch 136:    296 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60516.9, ups=15.52, wpb=3898.8, bsz=214.6, num_updates=182400, lr=7.40436e-05, gnorm=0.467, loss_scale=8, train_wall=6, gb_free=29.9, wall=3272
2024-01-31 05:38:19 | INFO | train_inner | epoch 136:    396 / 1349 loss=1.786, nll_loss=0.188, ppl=1.14, wps=60764.4, ups=15.55, wpb=3908, bsz=209.1, num_updates=182500, lr=7.40233e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.9, wall=3278
2024-01-31 05:38:26 | INFO | train_inner | epoch 136:    496 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60096.9, ups=15.39, wpb=3904.1, bsz=230.4, num_updates=182600, lr=7.4003e-05, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.8, wall=3284
2024-01-31 05:38:32 | INFO | train_inner | epoch 136:    596 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60296.2, ups=15.41, wpb=3913.8, bsz=216, num_updates=182700, lr=7.39828e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=3291
2024-01-31 05:38:39 | INFO | train_inner | epoch 136:    696 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61291.1, ups=15.44, wpb=3970.8, bsz=213, num_updates=182800, lr=7.39626e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=3297
2024-01-31 05:38:45 | INFO | train_inner | epoch 136:    796 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60848.7, ups=15.53, wpb=3918.1, bsz=221.5, num_updates=182900, lr=7.39423e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=3304
2024-01-31 05:38:51 | INFO | train_inner | epoch 136:    896 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=60694.1, ups=15.47, wpb=3922.8, bsz=214.8, num_updates=183000, lr=7.39221e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.7, wall=3310
2024-01-31 05:38:58 | INFO | train_inner | epoch 136:    996 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60152, ups=15.5, wpb=3880.8, bsz=213.7, num_updates=183100, lr=7.39019e-05, gnorm=0.569, loss_scale=8, train_wall=6, gb_free=29.7, wall=3317
2024-01-31 05:39:04 | INFO | train_inner | epoch 136:   1096 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60729.2, ups=15.39, wpb=3945.8, bsz=232.6, num_updates=183200, lr=7.38818e-05, gnorm=0.542, loss_scale=8, train_wall=6, gb_free=29.8, wall=3323
2024-01-31 05:39:11 | INFO | train_inner | epoch 136:   1196 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60804, ups=15.56, wpb=3906.7, bsz=203.8, num_updates=183300, lr=7.38616e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=3330
2024-01-31 05:39:17 | INFO | train_inner | epoch 136:   1296 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60813.7, ups=15.41, wpb=3946.1, bsz=217.9, num_updates=183400, lr=7.38415e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.8, wall=3336
2024-01-31 05:39:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:39:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:39:23 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 2.176 | nll_loss 0.435 | ppl 1.35 | wps 131036 | wpb 3275.2 | bsz 176.5 | num_updates 183453 | best_loss 2.063
2024-01-31 05:39:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 183453 updates
2024-01-31 05:39:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint136.pt
2024-01-31 05:39:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint136.pt
2024-01-31 05:39:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint136.pt (epoch 136 @ 183453 updates, score 2.176) (writing took 2.3200387489050627 seconds)
2024-01-31 05:39:25 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2024-01-31 05:39:25 | INFO | train | epoch 136 | loss 1.785 | nll_loss 0.188 | ppl 1.14 | wps 56753.2 | ups 14.47 | wpb 3921.2 | bsz 214.5 | num_updates 183453 | lr 7.38308e-05 | gnorm 0.51 | loss_scale 8 | train_wall 85 | gb_free 29.8 | wall 3344
2024-01-31 05:39:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:39:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:39:25 | INFO | fairseq.trainer | begin training epoch 137
2024-01-31 05:39:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:39:28 | INFO | train_inner | epoch 137:     47 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=34897, ups=8.98, wpb=3884.1, bsz=200.1, num_updates=183500, lr=7.38213e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.7, wall=3347
2024-01-31 05:39:35 | INFO | train_inner | epoch 137:    147 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60671.6, ups=15.42, wpb=3933.5, bsz=206.9, num_updates=183600, lr=7.38012e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.7, wall=3354
2024-01-31 05:39:41 | INFO | train_inner | epoch 137:    247 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61289.7, ups=15.45, wpb=3968.2, bsz=221.2, num_updates=183700, lr=7.37812e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=3360
2024-01-31 05:39:48 | INFO | train_inner | epoch 137:    347 / 1349 loss=1.783, nll_loss=0.185, ppl=1.14, wps=59850.3, ups=15.43, wpb=3879.8, bsz=213.4, num_updates=183800, lr=7.37611e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=3367
2024-01-31 05:39:54 | INFO | train_inner | epoch 137:    447 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60987.2, ups=15.49, wpb=3938.4, bsz=211.7, num_updates=183900, lr=7.3741e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=3373
2024-01-31 05:40:01 | INFO | train_inner | epoch 137:    547 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61190, ups=15.32, wpb=3995.2, bsz=233, num_updates=184000, lr=7.3721e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.7, wall=3380
2024-01-31 05:40:07 | INFO | train_inner | epoch 137:    647 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60888.6, ups=15.55, wpb=3914.8, bsz=212.5, num_updates=184100, lr=7.3701e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=3386
2024-01-31 05:40:14 | INFO | train_inner | epoch 137:    747 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60836.5, ups=15.46, wpb=3934.2, bsz=217.9, num_updates=184200, lr=7.36809e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.7, wall=3393
2024-01-31 05:40:20 | INFO | train_inner | epoch 137:    847 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=59686.4, ups=15.6, wpb=3826.1, bsz=217.4, num_updates=184300, lr=7.3661e-05, gnorm=0.621, loss_scale=8, train_wall=6, gb_free=29.9, wall=3399
2024-01-31 05:40:27 | INFO | train_inner | epoch 137:    947 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60742.4, ups=15.42, wpb=3938.8, bsz=225.4, num_updates=184400, lr=7.3641e-05, gnorm=0.476, loss_scale=8, train_wall=6, gb_free=29.8, wall=3406
2024-01-31 05:40:33 | INFO | train_inner | epoch 137:   1047 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=60589.3, ups=15.53, wpb=3902.1, bsz=212.9, num_updates=184500, lr=7.3621e-05, gnorm=0.581, loss_scale=8, train_wall=6, gb_free=29.8, wall=3412
2024-01-31 05:40:40 | INFO | train_inner | epoch 137:   1147 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60685.6, ups=15.55, wpb=3901.4, bsz=203.4, num_updates=184600, lr=7.36011e-05, gnorm=0.569, loss_scale=8, train_wall=6, gb_free=29.8, wall=3418
2024-01-31 05:40:46 | INFO | train_inner | epoch 137:   1247 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61437.3, ups=15.58, wpb=3944.1, bsz=213.9, num_updates=184700, lr=7.35811e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.8, wall=3425
2024-01-31 05:40:52 | INFO | train_inner | epoch 137:   1347 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61042.3, ups=15.63, wpb=3905.4, bsz=205, num_updates=184800, lr=7.35612e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=3431
2024-01-31 05:40:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:40:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:40:55 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 2.182 | nll_loss 0.44 | ppl 1.36 | wps 132776 | wpb 3275.2 | bsz 176.5 | num_updates 184802 | best_loss 2.063
2024-01-31 05:40:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 184802 updates
2024-01-31 05:40:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint137.pt
2024-01-31 05:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint137.pt
2024-01-31 05:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint137.pt (epoch 137 @ 184802 updates, score 2.182) (writing took 2.2600444360869005 seconds)
2024-01-31 05:40:57 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2024-01-31 05:40:57 | INFO | train | epoch 137 | loss 1.785 | nll_loss 0.188 | ppl 1.14 | wps 57642.9 | ups 14.7 | wpb 3921.2 | bsz 214.5 | num_updates 184802 | lr 7.35608e-05 | gnorm 0.525 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 3436
2024-01-31 05:40:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:40:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:40:57 | INFO | fairseq.trainer | begin training epoch 138
2024-01-31 05:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:41:04 | INFO | train_inner | epoch 138:     98 / 1349 loss=1.784, nll_loss=0.186, ppl=1.14, wps=34797.5, ups=8.98, wpb=3874.4, bsz=205, num_updates=184900, lr=7.35413e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=3442
2024-01-31 05:41:10 | INFO | train_inner | epoch 138:    198 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60440.4, ups=15.53, wpb=3892.5, bsz=214.4, num_updates=185000, lr=7.35215e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.9, wall=3449
2024-01-31 05:41:16 | INFO | train_inner | epoch 138:    298 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=60912, ups=15.49, wpb=3931.5, bsz=210, num_updates=185100, lr=7.35016e-05, gnorm=0.534, loss_scale=8, train_wall=6, gb_free=29.8, wall=3455
2024-01-31 05:41:23 | INFO | train_inner | epoch 138:    398 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60650.9, ups=15.59, wpb=3889.9, bsz=213.2, num_updates=185200, lr=7.34818e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=3462
2024-01-31 05:41:29 | INFO | train_inner | epoch 138:    498 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60306, ups=15.46, wpb=3900.3, bsz=223.9, num_updates=185300, lr=7.34619e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.7, wall=3468
2024-01-31 05:41:36 | INFO | train_inner | epoch 138:    598 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60346.4, ups=15.44, wpb=3909.7, bsz=210, num_updates=185400, lr=7.34421e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=3475
2024-01-31 05:41:42 | INFO | train_inner | epoch 138:    698 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60004.5, ups=15.43, wpb=3888.6, bsz=216.6, num_updates=185500, lr=7.34223e-05, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.8, wall=3481
2024-01-31 05:41:49 | INFO | train_inner | epoch 138:    798 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61522, ups=15.43, wpb=3986.7, bsz=221.4, num_updates=185600, lr=7.34025e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=3488
2024-01-31 05:41:55 | INFO | train_inner | epoch 138:    898 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60660.3, ups=15.54, wpb=3903.6, bsz=212, num_updates=185700, lr=7.33828e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.7, wall=3494
2024-01-31 05:42:02 | INFO | train_inner | epoch 138:    998 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61081.8, ups=15.53, wpb=3933.9, bsz=216.7, num_updates=185800, lr=7.3363e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.8, wall=3501
2024-01-31 05:42:08 | INFO | train_inner | epoch 138:   1098 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61086.4, ups=15.5, wpb=3941.3, bsz=207, num_updates=185900, lr=7.33433e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.7, wall=3507
2024-01-31 05:42:15 | INFO | train_inner | epoch 138:   1198 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61514.6, ups=15.5, wpb=3969.2, bsz=215.8, num_updates=186000, lr=7.33236e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.8, wall=3513
2024-01-31 05:42:21 | INFO | train_inner | epoch 138:   1298 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60646.3, ups=15.39, wpb=3941, bsz=219, num_updates=186100, lr=7.33039e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=3520
2024-01-31 05:42:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:42:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:42:26 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 2.179 | nll_loss 0.439 | ppl 1.36 | wps 132228 | wpb 3275.2 | bsz 176.5 | num_updates 186151 | best_loss 2.063
2024-01-31 05:42:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 186151 updates
2024-01-31 05:42:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint138.pt
2024-01-31 05:42:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint138.pt
2024-01-31 05:42:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint138.pt (epoch 138 @ 186151 updates, score 2.179) (writing took 2.44213488092646 seconds)
2024-01-31 05:42:29 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2024-01-31 05:42:29 | INFO | train | epoch 138 | loss 1.785 | nll_loss 0.188 | ppl 1.14 | wps 57545.6 | ups 14.68 | wpb 3921.2 | bsz 214.5 | num_updates 186151 | lr 7.32938e-05 | gnorm 0.51 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 3528
2024-01-31 05:42:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:42:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:42:29 | INFO | fairseq.trainer | begin training epoch 139
2024-01-31 05:42:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:42:32 | INFO | train_inner | epoch 139:     49 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=34891.1, ups=8.82, wpb=3956.9, bsz=224.3, num_updates=186200, lr=7.32842e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=3531
2024-01-31 05:42:39 | INFO | train_inner | epoch 139:    149 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60879.2, ups=15.46, wpb=3937.8, bsz=219.7, num_updates=186300, lr=7.32645e-05, gnorm=0.411, loss_scale=8, train_wall=6, gb_free=29.8, wall=3538
2024-01-31 05:42:45 | INFO | train_inner | epoch 139:    249 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=59890.6, ups=15.56, wpb=3849.4, bsz=209.2, num_updates=186400, lr=7.32448e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.8, wall=3544
2024-01-31 05:42:52 | INFO | train_inner | epoch 139:    349 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60307.5, ups=15.35, wpb=3928.6, bsz=237.9, num_updates=186500, lr=7.32252e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.7, wall=3551
2024-01-31 05:42:58 | INFO | train_inner | epoch 139:    449 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61303.6, ups=15.52, wpb=3950.4, bsz=208.4, num_updates=186600, lr=7.32056e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=3557
2024-01-31 05:43:05 | INFO | train_inner | epoch 139:    549 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60323.1, ups=15.44, wpb=3906.9, bsz=219.1, num_updates=186700, lr=7.3186e-05, gnorm=0.634, loss_scale=8, train_wall=6, gb_free=29.7, wall=3564
2024-01-31 05:43:11 | INFO | train_inner | epoch 139:    649 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60776.4, ups=15.46, wpb=3930.3, bsz=224.3, num_updates=186800, lr=7.31664e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.7, wall=3570
2024-01-31 05:43:18 | INFO | train_inner | epoch 139:    749 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60770.5, ups=15.41, wpb=3942.8, bsz=216, num_updates=186900, lr=7.31468e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.8, wall=3577
2024-01-31 05:43:24 | INFO | train_inner | epoch 139:    849 / 1349 loss=1.788, nll_loss=0.191, ppl=1.14, wps=60424.3, ups=15.58, wpb=3879.3, bsz=200.2, num_updates=187000, lr=7.31272e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=3583
2024-01-31 05:43:31 | INFO | train_inner | epoch 139:    949 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61031, ups=15.5, wpb=3938.2, bsz=214.2, num_updates=187100, lr=7.31077e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=3589
2024-01-31 05:43:37 | INFO | train_inner | epoch 139:   1049 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61588, ups=15.56, wpb=3957.4, bsz=210, num_updates=187200, lr=7.30882e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=3596
2024-01-31 05:43:43 | INFO | train_inner | epoch 139:   1149 / 1349 loss=1.789, nll_loss=0.192, ppl=1.14, wps=60707.1, ups=15.52, wpb=3911, bsz=209.8, num_updates=187300, lr=7.30687e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=3602
2024-01-31 05:43:50 | INFO | train_inner | epoch 139:   1249 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61034.9, ups=15.61, wpb=3911, bsz=205.1, num_updates=187400, lr=7.30492e-05, gnorm=0.553, loss_scale=8, train_wall=6, gb_free=29.7, wall=3609
2024-01-31 05:43:56 | INFO | train_inner | epoch 139:   1349 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61150.1, ups=15.64, wpb=3910.6, bsz=207.4, num_updates=187500, lr=7.30297e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.8, wall=3615
2024-01-31 05:43:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:43:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:43:58 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 2.18 | nll_loss 0.439 | ppl 1.36 | wps 133046 | wpb 3275.2 | bsz 176.5 | num_updates 187500 | best_loss 2.063
2024-01-31 05:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 187500 updates
2024-01-31 05:43:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint139.pt
2024-01-31 05:43:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint139.pt
2024-01-31 05:44:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint139.pt (epoch 139 @ 187500 updates, score 2.18) (writing took 2.3445938129443675 seconds)
2024-01-31 05:44:01 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2024-01-31 05:44:01 | INFO | train | epoch 139 | loss 1.785 | nll_loss 0.188 | ppl 1.14 | wps 57593.3 | ups 14.69 | wpb 3921.2 | bsz 214.5 | num_updates 187500 | lr 7.30297e-05 | gnorm 0.516 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 3620
2024-01-31 05:44:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:44:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:44:01 | INFO | fairseq.trainer | begin training epoch 140
2024-01-31 05:44:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:44:07 | INFO | train_inner | epoch 140:    100 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=34552.6, ups=8.86, wpb=3900.7, bsz=208.6, num_updates=187600, lr=7.30102e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=30, wall=3626
2024-01-31 05:44:14 | INFO | train_inner | epoch 140:    200 / 1349 loss=1.781, nll_loss=0.183, ppl=1.14, wps=60943.9, ups=15.4, wpb=3956.2, bsz=228.5, num_updates=187700, lr=7.29908e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.7, wall=3633
2024-01-31 05:44:20 | INFO | train_inner | epoch 140:    300 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61023.7, ups=15.59, wpb=3915.5, bsz=215, num_updates=187800, lr=7.29713e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=3639
2024-01-31 05:44:27 | INFO | train_inner | epoch 140:    400 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60214.9, ups=15.46, wpb=3895.2, bsz=203.8, num_updates=187900, lr=7.29519e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=3646
2024-01-31 05:44:33 | INFO | train_inner | epoch 140:    500 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=59749.8, ups=15.45, wpb=3866.1, bsz=222.6, num_updates=188000, lr=7.29325e-05, gnorm=0.533, loss_scale=8, train_wall=6, gb_free=29.9, wall=3652
2024-01-31 05:44:40 | INFO | train_inner | epoch 140:    600 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61023.3, ups=15.47, wpb=3943.7, bsz=225.6, num_updates=188100, lr=7.29131e-05, gnorm=0.534, loss_scale=8, train_wall=6, gb_free=29.8, wall=3659
2024-01-31 05:44:46 | INFO | train_inner | epoch 140:    700 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60685.8, ups=15.5, wpb=3914.9, bsz=218.9, num_updates=188200, lr=7.28937e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.7, wall=3665
2024-01-31 05:44:53 | INFO | train_inner | epoch 140:    800 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61867, ups=15.64, wpb=3955.7, bsz=196.2, num_updates=188300, lr=7.28744e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.7, wall=3672
2024-01-31 05:44:59 | INFO | train_inner | epoch 140:    900 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60811.1, ups=15.5, wpb=3922.7, bsz=211.5, num_updates=188400, lr=7.2855e-05, gnorm=0.633, loss_scale=8, train_wall=6, gb_free=29.8, wall=3678
2024-01-31 05:45:05 | INFO | train_inner | epoch 140:   1000 / 1349 loss=1.788, nll_loss=0.19, ppl=1.14, wps=60502.6, ups=15.58, wpb=3883.6, bsz=199.8, num_updates=188500, lr=7.28357e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.7, wall=3684
2024-01-31 05:45:12 | INFO | train_inner | epoch 140:   1100 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60511.9, ups=15.51, wpb=3901.6, bsz=208.3, num_updates=188600, lr=7.28164e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=3691
2024-01-31 05:45:18 | INFO | train_inner | epoch 140:   1200 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61098.7, ups=15.51, wpb=3940.3, bsz=216.8, num_updates=188700, lr=7.27971e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=3697
2024-01-31 05:45:25 | INFO | train_inner | epoch 140:   1300 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61066.3, ups=15.46, wpb=3951, bsz=210.6, num_updates=188800, lr=7.27778e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=3704
2024-01-31 05:45:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:45:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:45:30 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 2.176 | nll_loss 0.437 | ppl 1.35 | wps 132117 | wpb 3275.2 | bsz 176.5 | num_updates 188849 | best_loss 2.063
2024-01-31 05:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 188849 updates
2024-01-31 05:45:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint140.pt
2024-01-31 05:45:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint140.pt
2024-01-31 05:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint140.pt (epoch 140 @ 188849 updates, score 2.176) (writing took 2.399079130962491 seconds)
2024-01-31 05:45:33 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2024-01-31 05:45:33 | INFO | train | epoch 140 | loss 1.785 | nll_loss 0.188 | ppl 1.14 | wps 57571.2 | ups 14.68 | wpb 3921.2 | bsz 214.5 | num_updates 188849 | lr 7.27684e-05 | gnorm 0.516 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 3712
2024-01-31 05:45:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:45:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:45:33 | INFO | fairseq.trainer | begin training epoch 141
2024-01-31 05:45:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:45:36 | INFO | train_inner | epoch 141:     51 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=34450.2, ups=8.83, wpb=3900.4, bsz=236.5, num_updates=188900, lr=7.27585e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=3715
2024-01-31 05:45:43 | INFO | train_inner | epoch 141:    151 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60913.2, ups=15.48, wpb=3935.3, bsz=212.2, num_updates=189000, lr=7.27393e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=3722
2024-01-31 05:45:49 | INFO | train_inner | epoch 141:    251 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61635.5, ups=15.46, wpb=3986.6, bsz=214.3, num_updates=189100, lr=7.27201e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.7, wall=3728
2024-01-31 05:45:56 | INFO | train_inner | epoch 141:    351 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60548.5, ups=15.45, wpb=3918.9, bsz=220.2, num_updates=189200, lr=7.27008e-05, gnorm=0.417, loss_scale=8, train_wall=6, gb_free=29.7, wall=3734
2024-01-31 05:46:02 | INFO | train_inner | epoch 141:    451 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60481.6, ups=15.45, wpb=3914.8, bsz=224.5, num_updates=189300, lr=7.26816e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.8, wall=3741
2024-01-31 05:46:09 | INFO | train_inner | epoch 141:    551 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60909.8, ups=15.42, wpb=3949.9, bsz=212.3, num_updates=189400, lr=7.26624e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.7, wall=3747
2024-01-31 05:46:15 | INFO | train_inner | epoch 141:    651 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60042.9, ups=15.35, wpb=3911.1, bsz=210.7, num_updates=189500, lr=7.26433e-05, gnorm=0.553, loss_scale=8, train_wall=6, gb_free=29.8, wall=3754
2024-01-31 05:46:22 | INFO | train_inner | epoch 141:    751 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=59617, ups=15.51, wpb=3843.4, bsz=213, num_updates=189600, lr=7.26241e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=3760
2024-01-31 05:46:28 | INFO | train_inner | epoch 141:    851 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61027.1, ups=15.51, wpb=3935.4, bsz=200.3, num_updates=189700, lr=7.2605e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.8, wall=3767
2024-01-31 05:46:34 | INFO | train_inner | epoch 141:    951 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60683.2, ups=15.41, wpb=3938.7, bsz=223.2, num_updates=189800, lr=7.25858e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=3773
2024-01-31 05:46:41 | INFO | train_inner | epoch 141:   1051 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60380.6, ups=15.4, wpb=3920.2, bsz=220.2, num_updates=189900, lr=7.25667e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=3780
2024-01-31 05:46:47 | INFO | train_inner | epoch 141:   1151 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60347.9, ups=15.41, wpb=3915, bsz=207.4, num_updates=190000, lr=7.25476e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=3786
2024-01-31 05:46:54 | INFO | train_inner | epoch 141:   1251 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60201.5, ups=15.39, wpb=3911.6, bsz=215, num_updates=190100, lr=7.25285e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=3793
2024-01-31 05:47:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:47:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:47:03 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 2.182 | nll_loss 0.442 | ppl 1.36 | wps 132938 | wpb 3275.2 | bsz 176.5 | num_updates 190198 | best_loss 2.063
2024-01-31 05:47:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 190198 updates
2024-01-31 05:47:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint141.pt
2024-01-31 05:47:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint141.pt
2024-01-31 05:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint141.pt (epoch 141 @ 190198 updates, score 2.182) (writing took 2.2850254339864478 seconds)
2024-01-31 05:47:05 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2024-01-31 05:47:05 | INFO | train | epoch 141 | loss 1.784 | nll_loss 0.187 | ppl 1.14 | wps 57390.9 | ups 14.64 | wpb 3921.2 | bsz 214.5 | num_updates 190198 | lr 7.25099e-05 | gnorm 0.504 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 3804
2024-01-31 05:47:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:47:05 | INFO | fairseq.trainer | begin training epoch 142
2024-01-31 05:47:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:47:05 | INFO | train_inner | epoch 142:      2 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=35202, ups=8.92, wpb=3944.4, bsz=217.7, num_updates=190200, lr=7.25095e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=3804
2024-01-31 05:47:12 | INFO | train_inner | epoch 142:    102 / 1349 loss=1.785, nll_loss=0.187, ppl=1.14, wps=60281.8, ups=15.37, wpb=3921.9, bsz=207.7, num_updates=190300, lr=7.24904e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.8, wall=3811
2024-01-31 05:47:18 | INFO | train_inner | epoch 142:    202 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60871, ups=15.51, wpb=3924.9, bsz=213, num_updates=190400, lr=7.24714e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=3817
2024-01-31 05:47:25 | INFO | train_inner | epoch 142:    302 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60956.7, ups=15.44, wpb=3948.3, bsz=221.4, num_updates=190500, lr=7.24524e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.7, wall=3823
2024-01-31 05:47:31 | INFO | train_inner | epoch 142:    402 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=59788.3, ups=15.43, wpb=3875.3, bsz=221, num_updates=190600, lr=7.24333e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=3830
2024-01-31 05:47:37 | INFO | train_inner | epoch 142:    502 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60754.3, ups=15.55, wpb=3907, bsz=200.6, num_updates=190700, lr=7.24144e-05, gnorm=0.592, loss_scale=8, train_wall=6, gb_free=29.7, wall=3836
2024-01-31 05:47:44 | INFO | train_inner | epoch 142:    602 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60843.2, ups=15.44, wpb=3940.6, bsz=217.8, num_updates=190800, lr=7.23954e-05, gnorm=0.563, loss_scale=8, train_wall=6, gb_free=29.7, wall=3843
2024-01-31 05:47:51 | INFO | train_inner | epoch 142:    702 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=59576.3, ups=15.24, wpb=3909.4, bsz=210.7, num_updates=190900, lr=7.23764e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=3849
2024-01-31 05:47:57 | INFO | train_inner | epoch 142:    802 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60732.5, ups=15.51, wpb=3915.8, bsz=217, num_updates=191000, lr=7.23575e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=3856
2024-01-31 05:48:03 | INFO | train_inner | epoch 142:    902 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61258.6, ups=15.38, wpb=3983.8, bsz=221.3, num_updates=191100, lr=7.23385e-05, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.7, wall=3862
2024-01-31 05:48:10 | INFO | train_inner | epoch 142:   1002 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=60379.3, ups=15.52, wpb=3890, bsz=209.2, num_updates=191200, lr=7.23196e-05, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.7, wall=3869
2024-01-31 05:48:16 | INFO | train_inner | epoch 142:   1102 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60944.9, ups=15.48, wpb=3937.8, bsz=218.6, num_updates=191300, lr=7.23007e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=3875
2024-01-31 05:48:23 | INFO | train_inner | epoch 142:   1202 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60434.4, ups=15.53, wpb=3892.5, bsz=210.1, num_updates=191400, lr=7.22818e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.7, wall=3882
2024-01-31 05:48:29 | INFO | train_inner | epoch 142:   1302 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60550.4, ups=15.53, wpb=3898.5, bsz=213.2, num_updates=191500, lr=7.22629e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.8, wall=3888
2024-01-31 05:48:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:48:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:48:34 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 2.183 | nll_loss 0.443 | ppl 1.36 | wps 133031 | wpb 3275.2 | bsz 176.5 | num_updates 191547 | best_loss 2.063
2024-01-31 05:48:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 191547 updates
2024-01-31 05:48:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint142.pt
2024-01-31 05:48:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint142.pt
2024-01-31 05:48:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint142.pt (epoch 142 @ 191547 updates, score 2.183) (writing took 2.2880176150938496 seconds)
2024-01-31 05:48:37 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2024-01-31 05:48:37 | INFO | train | epoch 142 | loss 1.784 | nll_loss 0.187 | ppl 1.14 | wps 57562.8 | ups 14.68 | wpb 3921.2 | bsz 214.5 | num_updates 191547 | lr 7.22541e-05 | gnorm 0.516 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 3896
2024-01-31 05:48:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:48:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:48:37 | INFO | fairseq.trainer | begin training epoch 143
2024-01-31 05:48:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:48:40 | INFO | train_inner | epoch 143:     53 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=35711.1, ups=8.99, wpb=3970.9, bsz=229.3, num_updates=191600, lr=7.22441e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.8, wall=3899
2024-01-31 05:48:47 | INFO | train_inner | epoch 143:    153 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61293.6, ups=15.5, wpb=3954.2, bsz=208.3, num_updates=191700, lr=7.22252e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=3906
2024-01-31 05:49:11 | INFO | train_inner | epoch 143:    253 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=16206.3, ups=4.14, wpb=3913.7, bsz=212.5, num_updates=191800, lr=7.22064e-05, gnorm=0.5, loss_scale=8, train_wall=10, gb_free=29.7, wall=3930
2024-01-31 05:49:17 | INFO | train_inner | epoch 143:    353 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61062.7, ups=15.56, wpb=3925, bsz=204, num_updates=191900, lr=7.21876e-05, gnorm=0.54, loss_scale=8, train_wall=6, gb_free=29.8, wall=3936
2024-01-31 05:49:24 | INFO | train_inner | epoch 143:    453 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60218.4, ups=15.51, wpb=3881.5, bsz=213, num_updates=192000, lr=7.21688e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=3943
2024-01-31 05:49:30 | INFO | train_inner | epoch 143:    553 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60434.8, ups=15.47, wpb=3906.2, bsz=213.7, num_updates=192100, lr=7.215e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.7, wall=3949
2024-01-31 05:49:37 | INFO | train_inner | epoch 143:    653 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=60833.6, ups=15.5, wpb=3926, bsz=212.4, num_updates=192200, lr=7.21312e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=3956
2024-01-31 05:49:43 | INFO | train_inner | epoch 143:    753 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60394.2, ups=15.6, wpb=3870.6, bsz=208.7, num_updates=192300, lr=7.21125e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=3962
2024-01-31 05:49:50 | INFO | train_inner | epoch 143:    853 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61048.9, ups=15.45, wpb=3951.3, bsz=226, num_updates=192400, lr=7.20937e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=3969
2024-01-31 05:49:56 | INFO | train_inner | epoch 143:    953 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60577.6, ups=15.6, wpb=3882.9, bsz=210.2, num_updates=192500, lr=7.2075e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.8, wall=3975
2024-01-31 05:50:03 | INFO | train_inner | epoch 143:   1053 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60698.8, ups=15.42, wpb=3937.2, bsz=224.1, num_updates=192600, lr=7.20563e-05, gnorm=0.474, loss_scale=8, train_wall=6, gb_free=29.7, wall=3981
2024-01-31 05:50:09 | INFO | train_inner | epoch 143:   1153 / 1349 loss=1.787, nll_loss=0.19, ppl=1.14, wps=61155.9, ups=15.5, wpb=3944.9, bsz=213, num_updates=192700, lr=7.20376e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=3988
2024-01-31 05:50:15 | INFO | train_inner | epoch 143:   1253 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61595.4, ups=15.49, wpb=3977.5, bsz=219.8, num_updates=192800, lr=7.20189e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=3994
2024-01-31 05:50:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:50:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:50:25 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 2.182 | nll_loss 0.44 | ppl 1.36 | wps 124071 | wpb 3275.2 | bsz 176.5 | num_updates 192896 | best_loss 2.063
2024-01-31 05:50:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 192896 updates
2024-01-31 05:50:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint143.pt
2024-01-31 05:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint143.pt
2024-01-31 05:50:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint143.pt (epoch 143 @ 192896 updates, score 2.182) (writing took 2.330920204985887 seconds)
2024-01-31 05:50:27 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2024-01-31 05:50:27 | INFO | train | epoch 143 | loss 1.784 | nll_loss 0.187 | ppl 1.14 | wps 48039.6 | ups 12.25 | wpb 3921.2 | bsz 214.5 | num_updates 192896 | lr 7.2001e-05 | gnorm 0.507 | loss_scale 8 | train_wall 88 | gb_free 29.9 | wall 4006
2024-01-31 05:50:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:50:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:50:27 | INFO | fairseq.trainer | begin training epoch 144
2024-01-31 05:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:50:27 | INFO | train_inner | epoch 144:      4 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=32879.2, ups=8.47, wpb=3880.6, bsz=211.5, num_updates=192900, lr=7.20002e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.8, wall=4006
2024-01-31 05:50:34 | INFO | train_inner | epoch 144:    104 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60557.8, ups=15.34, wpb=3946.6, bsz=224.4, num_updates=193000, lr=7.19816e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.7, wall=4013
2024-01-31 05:50:40 | INFO | train_inner | epoch 144:    204 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61626.4, ups=15.61, wpb=3946.6, bsz=210.5, num_updates=193100, lr=7.19629e-05, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.8, wall=4019
2024-01-31 05:50:47 | INFO | train_inner | epoch 144:    304 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=59597.5, ups=15.4, wpb=3870.3, bsz=227, num_updates=193200, lr=7.19443e-05, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.8, wall=4026
2024-01-31 05:50:53 | INFO | train_inner | epoch 144:    404 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61115.1, ups=15.44, wpb=3959, bsz=220.6, num_updates=193300, lr=7.19257e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.7, wall=4032
2024-01-31 05:51:00 | INFO | train_inner | epoch 144:    504 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60680.3, ups=15.48, wpb=3919.6, bsz=215.7, num_updates=193400, lr=7.19071e-05, gnorm=0.449, loss_scale=8, train_wall=6, gb_free=30, wall=4039
2024-01-31 05:51:06 | INFO | train_inner | epoch 144:    604 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60905.4, ups=15.54, wpb=3919.9, bsz=205.9, num_updates=193500, lr=7.18885e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=4045
2024-01-31 05:51:12 | INFO | train_inner | epoch 144:    704 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=60844.5, ups=15.56, wpb=3911.4, bsz=217.6, num_updates=193600, lr=7.18699e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=4051
2024-01-31 05:51:19 | INFO | train_inner | epoch 144:    804 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60577.6, ups=15.6, wpb=3883.8, bsz=206.9, num_updates=193700, lr=7.18514e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=4058
2024-01-31 05:51:25 | INFO | train_inner | epoch 144:    904 / 1349 loss=1.788, nll_loss=0.192, ppl=1.14, wps=61329.3, ups=15.51, wpb=3953.2, bsz=219.2, num_updates=193800, lr=7.18329e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=4064
2024-01-31 05:51:32 | INFO | train_inner | epoch 144:   1004 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60487.2, ups=15.37, wpb=3935.9, bsz=218.6, num_updates=193900, lr=7.18143e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.7, wall=4071
2024-01-31 05:51:38 | INFO | train_inner | epoch 144:   1104 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60894.9, ups=15.52, wpb=3923.2, bsz=203.7, num_updates=194000, lr=7.17958e-05, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.8, wall=4077
2024-01-31 05:51:45 | INFO | train_inner | epoch 144:   1204 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60286.9, ups=15.52, wpb=3885.2, bsz=209.3, num_updates=194100, lr=7.17773e-05, gnorm=0.648, loss_scale=16, train_wall=6, gb_free=29.8, wall=4084
2024-01-31 05:51:51 | INFO | train_inner | epoch 144:   1304 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=60984.1, ups=15.47, wpb=3942, bsz=215.5, num_updates=194200, lr=7.17588e-05, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.7, wall=4090
2024-01-31 05:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:51:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:51:56 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 2.182 | nll_loss 0.441 | ppl 1.36 | wps 133895 | wpb 3275.2 | bsz 176.5 | num_updates 194245 | best_loss 2.063
2024-01-31 05:51:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 194245 updates
2024-01-31 05:51:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint144.pt
2024-01-31 05:51:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint144.pt
2024-01-31 05:51:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint144.pt (epoch 144 @ 194245 updates, score 2.182) (writing took 2.349517321912572 seconds)
2024-01-31 05:51:59 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2024-01-31 05:51:59 | INFO | train | epoch 144 | loss 1.784 | nll_loss 0.187 | ppl 1.14 | wps 57697 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 194245 | lr 7.17505e-05 | gnorm 0.516 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 4097
2024-01-31 05:51:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:51:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:51:59 | INFO | fairseq.trainer | begin training epoch 145
2024-01-31 05:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:52:02 | INFO | train_inner | epoch 145:     55 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=35029.2, ups=9.03, wpb=3879.6, bsz=204.3, num_updates=194300, lr=7.17404e-05, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=4101
2024-01-31 05:52:09 | INFO | train_inner | epoch 145:    155 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60823.8, ups=15.45, wpb=3936.1, bsz=215.8, num_updates=194400, lr=7.17219e-05, gnorm=0.449, loss_scale=16, train_wall=6, gb_free=29.8, wall=4108
2024-01-31 05:52:15 | INFO | train_inner | epoch 145:    255 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60796.5, ups=15.49, wpb=3925.7, bsz=213.2, num_updates=194500, lr=7.17035e-05, gnorm=0.519, loss_scale=16, train_wall=6, gb_free=29.8, wall=4114
2024-01-31 05:52:22 | INFO | train_inner | epoch 145:    355 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60583.4, ups=15.42, wpb=3928.5, bsz=228.4, num_updates=194600, lr=7.1685e-05, gnorm=0.479, loss_scale=16, train_wall=6, gb_free=29.8, wall=4121
2024-01-31 05:52:28 | INFO | train_inner | epoch 145:    455 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61250.7, ups=15.46, wpb=3961.2, bsz=205.4, num_updates=194700, lr=7.16666e-05, gnorm=0.461, loss_scale=16, train_wall=6, gb_free=29.9, wall=4127
2024-01-31 05:52:35 | INFO | train_inner | epoch 145:    555 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61313.3, ups=15.44, wpb=3971.6, bsz=227.8, num_updates=194800, lr=7.16482e-05, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.8, wall=4134
2024-01-31 05:52:41 | INFO | train_inner | epoch 145:    655 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61319.8, ups=15.59, wpb=3932.9, bsz=199.4, num_updates=194900, lr=7.16299e-05, gnorm=0.549, loss_scale=16, train_wall=6, gb_free=29.9, wall=4140
2024-01-31 05:52:47 | INFO | train_inner | epoch 145:    755 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=60770.2, ups=15.61, wpb=3893.7, bsz=207.5, num_updates=195000, lr=7.16115e-05, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=30, wall=4146
2024-01-31 05:52:54 | INFO | train_inner | epoch 145:    855 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60099.5, ups=15.46, wpb=3887.6, bsz=216.7, num_updates=195100, lr=7.15931e-05, gnorm=0.537, loss_scale=16, train_wall=6, gb_free=29.8, wall=4153
2024-01-31 05:53:00 | INFO | train_inner | epoch 145:    955 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60979.5, ups=15.42, wpb=3954.5, bsz=231, num_updates=195200, lr=7.15748e-05, gnorm=0.478, loss_scale=16, train_wall=6, gb_free=29.8, wall=4159
2024-01-31 05:53:07 | INFO | train_inner | epoch 145:   1055 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60179.1, ups=15.55, wpb=3870.4, bsz=210.6, num_updates=195300, lr=7.15565e-05, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.8, wall=4166
2024-01-31 05:53:13 | INFO | train_inner | epoch 145:   1155 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=61191.1, ups=15.53, wpb=3939.3, bsz=208.2, num_updates=195400, lr=7.15382e-05, gnorm=0.548, loss_scale=16, train_wall=6, gb_free=29.8, wall=4172
2024-01-31 05:53:20 | INFO | train_inner | epoch 145:   1255 / 1349 loss=1.783, nll_loss=0.185, ppl=1.14, wps=59970.5, ups=15.58, wpb=3850.2, bsz=209.3, num_updates=195500, lr=7.15199e-05, gnorm=0.438, loss_scale=16, train_wall=6, gb_free=29.8, wall=4179
2024-01-31 05:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:53:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:53:28 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 2.181 | nll_loss 0.441 | ppl 1.36 | wps 132786 | wpb 3275.2 | bsz 176.5 | num_updates 195594 | best_loss 2.063
2024-01-31 05:53:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 195594 updates
2024-01-31 05:53:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint145.pt
2024-01-31 05:53:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint145.pt
2024-01-31 05:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint145.pt (epoch 145 @ 195594 updates, score 2.181) (writing took 2.2781436179066077 seconds)
2024-01-31 05:53:30 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2024-01-31 05:53:30 | INFO | train | epoch 145 | loss 1.784 | nll_loss 0.187 | ppl 1.14 | wps 57716.2 | ups 14.72 | wpb 3921.2 | bsz 214.5 | num_updates 195594 | lr 7.15027e-05 | gnorm 0.503 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 4189
2024-01-31 05:53:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:53:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:53:30 | INFO | fairseq.trainer | begin training epoch 146
2024-01-31 05:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:53:31 | INFO | train_inner | epoch 146:      6 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=35767.4, ups=9.05, wpb=3950.3, bsz=220.7, num_updates=195600, lr=7.15016e-05, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=4190
2024-01-31 05:53:37 | INFO | train_inner | epoch 146:    106 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60102.4, ups=15.37, wpb=3910, bsz=217.5, num_updates=195700, lr=7.14833e-05, gnorm=0.453, loss_scale=16, train_wall=6, gb_free=29.8, wall=4196
2024-01-31 05:53:44 | INFO | train_inner | epoch 146:    206 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60390.8, ups=15.52, wpb=3890.2, bsz=215.2, num_updates=195800, lr=7.1465e-05, gnorm=0.602, loss_scale=16, train_wall=6, gb_free=29.8, wall=4203
2024-01-31 05:53:50 | INFO | train_inner | epoch 146:    306 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60654.3, ups=15.53, wpb=3906.7, bsz=222.5, num_updates=195900, lr=7.14468e-05, gnorm=0.495, loss_scale=16, train_wall=6, gb_free=29.7, wall=4209
2024-01-31 05:53:57 | INFO | train_inner | epoch 146:    406 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61037.1, ups=15.37, wpb=3971.2, bsz=223, num_updates=196000, lr=7.14286e-05, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=4216
2024-01-31 05:54:03 | INFO | train_inner | epoch 146:    506 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61234.2, ups=15.51, wpb=3947.7, bsz=208.7, num_updates=196100, lr=7.14104e-05, gnorm=0.489, loss_scale=16, train_wall=6, gb_free=29.8, wall=4222
2024-01-31 05:54:10 | INFO | train_inner | epoch 146:    606 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60488.2, ups=15.51, wpb=3901, bsz=204.4, num_updates=196200, lr=7.13922e-05, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=4228
2024-01-31 05:54:16 | INFO | train_inner | epoch 146:    706 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61024.6, ups=15.57, wpb=3918.3, bsz=210.6, num_updates=196300, lr=7.1374e-05, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=29.8, wall=4235
2024-01-31 05:54:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 05:54:22 | INFO | train_inner | epoch 146:    807 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60327.5, ups=15.41, wpb=3915.1, bsz=209.7, num_updates=196400, lr=7.13558e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=4241
2024-01-31 05:54:29 | INFO | train_inner | epoch 146:    907 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60436.9, ups=15.48, wpb=3902.9, bsz=208.6, num_updates=196500, lr=7.13376e-05, gnorm=0.542, loss_scale=8, train_wall=6, gb_free=29.7, wall=4248
2024-01-31 05:54:35 | INFO | train_inner | epoch 146:   1007 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61212.8, ups=15.47, wpb=3955.9, bsz=211.4, num_updates=196600, lr=7.13195e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.7, wall=4254
2024-01-31 05:54:42 | INFO | train_inner | epoch 146:   1107 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60426.2, ups=15.45, wpb=3910.1, bsz=215.8, num_updates=196700, lr=7.13014e-05, gnorm=0.584, loss_scale=8, train_wall=6, gb_free=29.8, wall=4261
2024-01-31 05:54:48 | INFO | train_inner | epoch 146:   1207 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60246, ups=15.46, wpb=3897.2, bsz=227.7, num_updates=196800, lr=7.12832e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=4267
2024-01-31 05:54:55 | INFO | train_inner | epoch 146:   1307 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61230.8, ups=15.51, wpb=3947.4, bsz=219.7, num_updates=196900, lr=7.12651e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=4274
2024-01-31 05:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:54:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:55:00 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 2.181 | nll_loss 0.44 | ppl 1.36 | wps 132974 | wpb 3275.2 | bsz 176.5 | num_updates 196942 | best_loss 2.063
2024-01-31 05:55:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 196942 updates
2024-01-31 05:55:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint146.pt
2024-01-31 05:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint146.pt
2024-01-31 05:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint146.pt (epoch 146 @ 196942 updates, score 2.181) (writing took 2.2798055259045213 seconds)
2024-01-31 05:55:02 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2024-01-31 05:55:02 | INFO | train | epoch 146 | loss 1.783 | nll_loss 0.186 | ppl 1.14 | wps 57651.7 | ups 14.7 | wpb 3921.1 | bsz 214.5 | num_updates 196942 | lr 7.12575e-05 | gnorm 0.512 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 4281
2024-01-31 05:55:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:55:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:55:02 | INFO | fairseq.trainer | begin training epoch 147
2024-01-31 05:55:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:55:06 | INFO | train_inner | epoch 147:     58 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=35130, ups=9.07, wpb=3875.1, bsz=205.8, num_updates=197000, lr=7.1247e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.8, wall=4285
2024-01-31 05:55:12 | INFO | train_inner | epoch 147:    158 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61303.5, ups=15.45, wpb=3968.8, bsz=221.8, num_updates=197100, lr=7.1229e-05, gnorm=0.455, loss_scale=8, train_wall=6, gb_free=29.8, wall=4291
2024-01-31 05:55:19 | INFO | train_inner | epoch 147:    258 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60772.9, ups=15.59, wpb=3899.2, bsz=204.6, num_updates=197200, lr=7.12109e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.8, wall=4298
2024-01-31 05:55:25 | INFO | train_inner | epoch 147:    358 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60247.4, ups=15.45, wpb=3898.4, bsz=222.6, num_updates=197300, lr=7.11929e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.8, wall=4304
2024-01-31 05:55:32 | INFO | train_inner | epoch 147:    458 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60476.3, ups=15.46, wpb=3910.7, bsz=209, num_updates=197400, lr=7.11748e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=4311
2024-01-31 05:55:38 | INFO | train_inner | epoch 147:    558 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60372.6, ups=15.57, wpb=3877.9, bsz=199.6, num_updates=197500, lr=7.11568e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.7, wall=4317
2024-01-31 05:55:45 | INFO | train_inner | epoch 147:    658 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61070.2, ups=15.44, wpb=3956.1, bsz=226.5, num_updates=197600, lr=7.11388e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.7, wall=4323
2024-01-31 05:55:51 | INFO | train_inner | epoch 147:    758 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61490.4, ups=15.55, wpb=3954.2, bsz=204.6, num_updates=197700, lr=7.11208e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=4330
2024-01-31 05:55:57 | INFO | train_inner | epoch 147:    858 / 1349 loss=1.788, nll_loss=0.192, ppl=1.14, wps=61047.6, ups=15.6, wpb=3913.5, bsz=204.5, num_updates=197800, lr=7.11028e-05, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=29.8, wall=4336
2024-01-31 05:56:04 | INFO | train_inner | epoch 147:    958 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60543.4, ups=15.42, wpb=3926.8, bsz=225, num_updates=197900, lr=7.10849e-05, gnorm=0.595, loss_scale=8, train_wall=6, gb_free=29.8, wall=4343
2024-01-31 05:56:10 | INFO | train_inner | epoch 147:   1058 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=59980.9, ups=15.49, wpb=3871.5, bsz=211.1, num_updates=198000, lr=7.10669e-05, gnorm=0.564, loss_scale=8, train_wall=6, gb_free=29.7, wall=4349
2024-01-31 05:56:17 | INFO | train_inner | epoch 147:   1158 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=61212.2, ups=15.6, wpb=3922.8, bsz=207, num_updates=198100, lr=7.1049e-05, gnorm=0.582, loss_scale=8, train_wall=6, gb_free=29.8, wall=4356
2024-01-31 05:56:23 | INFO | train_inner | epoch 147:   1258 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=61559.5, ups=15.48, wpb=3976.9, bsz=220.6, num_updates=198200, lr=7.1031e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=4362
2024-01-31 05:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:56:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:56:31 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 2.188 | nll_loss 0.448 | ppl 1.36 | wps 130897 | wpb 3275.2 | bsz 176.5 | num_updates 198291 | best_loss 2.063
2024-01-31 05:56:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 198291 updates
2024-01-31 05:56:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint147.pt
2024-01-31 05:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint147.pt
2024-01-31 05:56:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint147.pt (epoch 147 @ 198291 updates, score 2.188) (writing took 2.2549172249855474 seconds)
2024-01-31 05:56:34 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2024-01-31 05:56:34 | INFO | train | epoch 147 | loss 1.783 | nll_loss 0.187 | ppl 1.14 | wps 57733.4 | ups 14.72 | wpb 3921.2 | bsz 214.5 | num_updates 198291 | lr 7.10147e-05 | gnorm 0.531 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 4372
2024-01-31 05:56:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:56:34 | INFO | fairseq.trainer | begin training epoch 148
2024-01-31 05:56:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:56:34 | INFO | train_inner | epoch 148:      9 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=35576.8, ups=9.03, wpb=3940.2, bsz=234.8, num_updates=198300, lr=7.10131e-05, gnorm=0.557, loss_scale=8, train_wall=6, gb_free=29.8, wall=4373
2024-01-31 05:56:41 | INFO | train_inner | epoch 148:    109 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60531.8, ups=15.34, wpb=3946.8, bsz=233.4, num_updates=198400, lr=7.09952e-05, gnorm=0.583, loss_scale=8, train_wall=6, gb_free=29.8, wall=4380
2024-01-31 05:56:47 | INFO | train_inner | epoch 148:    209 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60825.6, ups=15.41, wpb=3948.1, bsz=219.2, num_updates=198500, lr=7.09773e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=4386
2024-01-31 05:56:54 | INFO | train_inner | epoch 148:    309 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60988.5, ups=15.62, wpb=3904.8, bsz=208.9, num_updates=198600, lr=7.09595e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=4393
2024-01-31 05:57:00 | INFO | train_inner | epoch 148:    409 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60659.9, ups=15.6, wpb=3889.2, bsz=198.7, num_updates=198700, lr=7.09416e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=4399
2024-01-31 05:57:07 | INFO | train_inner | epoch 148:    509 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60283.4, ups=15.44, wpb=3904.9, bsz=214.9, num_updates=198800, lr=7.09238e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.7, wall=4405
2024-01-31 05:57:13 | INFO | train_inner | epoch 148:    609 / 1349 loss=1.786, nll_loss=0.19, ppl=1.14, wps=60657.5, ups=15.6, wpb=3889.3, bsz=205.1, num_updates=198900, lr=7.09059e-05, gnorm=0.571, loss_scale=8, train_wall=6, gb_free=29.8, wall=4412
2024-01-31 05:57:19 | INFO | train_inner | epoch 148:    709 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60575.2, ups=15.52, wpb=3904.2, bsz=208.4, num_updates=199000, lr=7.08881e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=4418
2024-01-31 05:57:26 | INFO | train_inner | epoch 148:    809 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61427.7, ups=15.51, wpb=3959.6, bsz=211.1, num_updates=199100, lr=7.08703e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.7, wall=4425
2024-01-31 05:57:32 | INFO | train_inner | epoch 148:    909 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=58969.6, ups=15.43, wpb=3820.8, bsz=235, num_updates=199200, lr=7.08525e-05, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.8, wall=4431
2024-01-31 05:57:39 | INFO | train_inner | epoch 148:   1009 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61125, ups=15.42, wpb=3962.8, bsz=220.5, num_updates=199300, lr=7.08347e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.7, wall=4438
2024-01-31 05:57:45 | INFO | train_inner | epoch 148:   1109 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60917.9, ups=15.52, wpb=3924.4, bsz=208.6, num_updates=199400, lr=7.0817e-05, gnorm=0.575, loss_scale=8, train_wall=6, gb_free=29.8, wall=4444
2024-01-31 05:57:52 | INFO | train_inner | epoch 148:   1209 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61451.3, ups=15.55, wpb=3951.5, bsz=209.7, num_updates=199500, lr=7.07992e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=4451
2024-01-31 05:57:58 | INFO | train_inner | epoch 148:   1309 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60701.9, ups=15.4, wpb=3941.2, bsz=214.2, num_updates=199600, lr=7.07815e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=4457
2024-01-31 05:58:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:58:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:58:03 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 2.185 | nll_loss 0.445 | ppl 1.36 | wps 131817 | wpb 3275.2 | bsz 176.5 | num_updates 199640 | best_loss 2.063
2024-01-31 05:58:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 199640 updates
2024-01-31 05:58:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint148.pt
2024-01-31 05:58:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint148.pt
2024-01-31 05:58:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint148.pt (epoch 148 @ 199640 updates, score 2.185) (writing took 2.250701496959664 seconds)
2024-01-31 05:58:05 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2024-01-31 05:58:05 | INFO | train | epoch 148 | loss 1.783 | nll_loss 0.187 | ppl 1.14 | wps 57672.6 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 199640 | lr 7.07744e-05 | gnorm 0.53 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 4464
2024-01-31 05:58:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:58:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:58:05 | INFO | fairseq.trainer | begin training epoch 149
2024-01-31 05:58:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:58:09 | INFO | train_inner | epoch 149:     60 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=35737.5, ups=9, wpb=3969.5, bsz=212.9, num_updates=199700, lr=7.07638e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.7, wall=4468
2024-01-31 05:58:16 | INFO | train_inner | epoch 149:    160 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60894.8, ups=15.52, wpb=3924.6, bsz=209.4, num_updates=199800, lr=7.07461e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.8, wall=4475
2024-01-31 05:58:22 | INFO | train_inner | epoch 149:    260 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60781.3, ups=15.43, wpb=3939.2, bsz=221.5, num_updates=199900, lr=7.07284e-05, gnorm=0.451, loss_scale=8, train_wall=6, gb_free=29.7, wall=4481
2024-01-31 05:58:29 | INFO | train_inner | epoch 149:    360 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61185.1, ups=15.57, wpb=3929.6, bsz=206.5, num_updates=200000, lr=7.07107e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.8, wall=4488
2024-01-31 05:58:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:58:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:58:31 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 2.183 | nll_loss 0.443 | ppl 1.36 | wps 131362 | wpb 3275.2 | bsz 176.5 | num_updates 200000 | best_loss 2.063
2024-01-31 05:58:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 200000 updates
2024-01-31 05:58:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_149_200000.pt
2024-01-31 05:58:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_149_200000.pt
2024-01-31 05:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_149_200000.pt (epoch 149 @ 200000 updates, score 2.183) (writing took 2.328382848063484 seconds)
2024-01-31 05:58:40 | INFO | train_inner | epoch 149:    460 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=35341.9, ups=9.04, wpb=3909.5, bsz=220, num_updates=200100, lr=7.0693e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.8, wall=4499
2024-01-31 05:58:46 | INFO | train_inner | epoch 149:    560 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60293.4, ups=15.45, wpb=3902.3, bsz=217.9, num_updates=200200, lr=7.06753e-05, gnorm=0.607, loss_scale=8, train_wall=6, gb_free=29.8, wall=4505
2024-01-31 05:58:53 | INFO | train_inner | epoch 149:    660 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60242.1, ups=15.52, wpb=3880.4, bsz=203.4, num_updates=200300, lr=7.06577e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.7, wall=4512
2024-01-31 05:58:59 | INFO | train_inner | epoch 149:    760 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60660.9, ups=15.47, wpb=3922.2, bsz=212.4, num_updates=200400, lr=7.06401e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=4518
2024-01-31 05:59:06 | INFO | train_inner | epoch 149:    860 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60474.8, ups=15.45, wpb=3915, bsz=212.5, num_updates=200500, lr=7.06225e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=4524
2024-01-31 05:59:12 | INFO | train_inner | epoch 149:    960 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60804.3, ups=15.56, wpb=3908.8, bsz=218.2, num_updates=200600, lr=7.06049e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=4531
2024-01-31 05:59:18 | INFO | train_inner | epoch 149:   1060 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=61289.5, ups=15.59, wpb=3931.9, bsz=205.8, num_updates=200700, lr=7.05873e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.8, wall=4537
2024-01-31 05:59:25 | INFO | train_inner | epoch 149:   1160 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60647.9, ups=15.41, wpb=3934.7, bsz=223.4, num_updates=200800, lr=7.05697e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.9, wall=4544
2024-01-31 05:59:31 | INFO | train_inner | epoch 149:   1260 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61119.8, ups=15.53, wpb=3936.5, bsz=218.6, num_updates=200900, lr=7.05521e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.7, wall=4550
2024-01-31 05:59:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:59:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:59:39 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 2.185 | nll_loss 0.443 | ppl 1.36 | wps 132104 | wpb 3275.2 | bsz 176.5 | num_updates 200989 | best_loss 2.063
2024-01-31 05:59:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 200989 updates
2024-01-31 05:59:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint149.pt
2024-01-31 05:59:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint149.pt
2024-01-31 05:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint149.pt (epoch 149 @ 200989 updates, score 2.185) (writing took 2.2981081280158833 seconds)
2024-01-31 05:59:42 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2024-01-31 05:59:42 | INFO | train | epoch 149 | loss 1.783 | nll_loss 0.186 | ppl 1.14 | wps 54911.4 | ups 14 | wpb 3921.2 | bsz 214.5 | num_updates 200989 | lr 7.05365e-05 | gnorm 0.513 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 4560
2024-01-31 05:59:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:59:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 05:59:42 | INFO | fairseq.trainer | begin training epoch 150
2024-01-31 05:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:59:42 | INFO | train_inner | epoch 150:     11 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=35331.2, ups=8.99, wpb=3928.2, bsz=219.4, num_updates=201000, lr=7.05346e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=4561
2024-01-31 05:59:49 | INFO | train_inner | epoch 150:    111 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60146.1, ups=15.51, wpb=3877.7, bsz=213.2, num_updates=201100, lr=7.0517e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.7, wall=4568
2024-01-31 05:59:55 | INFO | train_inner | epoch 150:    211 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60441.9, ups=15.48, wpb=3905.7, bsz=225.4, num_updates=201200, lr=7.04995e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.9, wall=4574
2024-01-31 06:00:02 | INFO | train_inner | epoch 150:    311 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61429.8, ups=15.47, wpb=3969.8, bsz=219.6, num_updates=201300, lr=7.0482e-05, gnorm=0.556, loss_scale=8, train_wall=6, gb_free=29.8, wall=4581
2024-01-31 06:00:08 | INFO | train_inner | epoch 150:    411 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60435, ups=15.51, wpb=3895.7, bsz=208.3, num_updates=201400, lr=7.04645e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=4587
2024-01-31 06:00:15 | INFO | train_inner | epoch 150:    511 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61188.3, ups=15.45, wpb=3961.3, bsz=215.4, num_updates=201500, lr=7.0447e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.8, wall=4594
2024-01-31 06:00:21 | INFO | train_inner | epoch 150:    611 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60074.3, ups=15.45, wpb=3888.6, bsz=222.8, num_updates=201600, lr=7.04295e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=4600
2024-01-31 06:00:28 | INFO | train_inner | epoch 150:    711 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60793.3, ups=15.56, wpb=3905.8, bsz=208.6, num_updates=201700, lr=7.04121e-05, gnorm=0.533, loss_scale=8, train_wall=6, gb_free=29.8, wall=4607
2024-01-31 06:00:34 | INFO | train_inner | epoch 150:    811 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60483.2, ups=15.43, wpb=3919.7, bsz=211.7, num_updates=201800, lr=7.03946e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.7, wall=4613
2024-01-31 06:00:41 | INFO | train_inner | epoch 150:    911 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=61056.9, ups=15.55, wpb=3926.4, bsz=214.1, num_updates=201900, lr=7.03772e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=4619
2024-01-31 06:00:47 | INFO | train_inner | epoch 150:   1011 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60961.7, ups=15.64, wpb=3898.6, bsz=214.3, num_updates=202000, lr=7.03598e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=4626
2024-01-31 06:00:53 | INFO | train_inner | epoch 150:   1111 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60734.3, ups=15.42, wpb=3938.2, bsz=228.1, num_updates=202100, lr=7.03423e-05, gnorm=0.433, loss_scale=8, train_wall=6, gb_free=29.8, wall=4632
2024-01-31 06:01:00 | INFO | train_inner | epoch 150:   1211 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=61126.9, ups=15.55, wpb=3931.8, bsz=207.2, num_updates=202200, lr=7.03249e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=4639
2024-01-31 06:01:06 | INFO | train_inner | epoch 150:   1311 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61239.7, ups=15.44, wpb=3966.8, bsz=207.8, num_updates=202300, lr=7.03076e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=4645
2024-01-31 06:01:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:01:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:01:11 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 2.185 | nll_loss 0.444 | ppl 1.36 | wps 132765 | wpb 3275.2 | bsz 176.5 | num_updates 202338 | best_loss 2.063
2024-01-31 06:01:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 202338 updates
2024-01-31 06:01:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint150.pt
2024-01-31 06:01:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint150.pt
2024-01-31 06:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint150.pt (epoch 150 @ 202338 updates, score 2.185) (writing took 2.3047629669308662 seconds)
2024-01-31 06:01:13 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2024-01-31 06:01:13 | INFO | train | epoch 150 | loss 1.783 | nll_loss 0.186 | ppl 1.14 | wps 57674.2 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 202338 | lr 7.0301e-05 | gnorm 0.514 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 4652
2024-01-31 06:01:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:01:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:01:13 | INFO | fairseq.trainer | begin training epoch 151
2024-01-31 06:01:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:01:17 | INFO | train_inner | epoch 151:     62 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=35367.7, ups=9.01, wpb=3926.6, bsz=210.5, num_updates=202400, lr=7.02902e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.7, wall=4656
2024-01-31 06:01:24 | INFO | train_inner | epoch 151:    162 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60351.4, ups=15.54, wpb=3883.8, bsz=214.1, num_updates=202500, lr=7.02728e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=30, wall=4663
2024-01-31 06:01:30 | INFO | train_inner | epoch 151:    262 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60058.1, ups=15.51, wpb=3873.1, bsz=219, num_updates=202600, lr=7.02555e-05, gnorm=0.538, loss_scale=8, train_wall=6, gb_free=29.8, wall=4669
2024-01-31 06:01:37 | INFO | train_inner | epoch 151:    362 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60958.5, ups=15.52, wpb=3928.1, bsz=219.4, num_updates=202700, lr=7.02382e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.7, wall=4676
2024-01-31 06:01:43 | INFO | train_inner | epoch 151:    462 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61347.3, ups=15.51, wpb=3954.4, bsz=207.8, num_updates=202800, lr=7.02208e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.8, wall=4682
2024-01-31 06:01:50 | INFO | train_inner | epoch 151:    562 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60740.4, ups=15.55, wpb=3905.9, bsz=209.8, num_updates=202900, lr=7.02035e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=4689
2024-01-31 06:01:56 | INFO | train_inner | epoch 151:    662 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60664.1, ups=15.38, wpb=3944.6, bsz=220.3, num_updates=203000, lr=7.01862e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.7, wall=4695
2024-01-31 06:02:03 | INFO | train_inner | epoch 151:    762 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=59918.6, ups=15.48, wpb=3870.9, bsz=211.6, num_updates=203100, lr=7.0169e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.7, wall=4701
2024-01-31 06:02:09 | INFO | train_inner | epoch 151:    862 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60691.3, ups=15.53, wpb=3908.2, bsz=206.4, num_updates=203200, lr=7.01517e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.7, wall=4708
2024-01-31 06:02:15 | INFO | train_inner | epoch 151:    962 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60757, ups=15.48, wpb=3923.7, bsz=204.2, num_updates=203300, lr=7.01344e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.8, wall=4714
2024-01-31 06:02:22 | INFO | train_inner | epoch 151:   1062 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61242.3, ups=15.54, wpb=3940.1, bsz=211.7, num_updates=203400, lr=7.01172e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.7, wall=4721
2024-01-31 06:02:28 | INFO | train_inner | epoch 151:   1162 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61287.8, ups=15.57, wpb=3935.8, bsz=219, num_updates=203500, lr=7.01e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.8, wall=4727
2024-01-31 06:02:35 | INFO | train_inner | epoch 151:   1262 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61135.6, ups=15.51, wpb=3942.1, bsz=212.7, num_updates=203600, lr=7.00827e-05, gnorm=0.558, loss_scale=8, train_wall=6, gb_free=29.8, wall=4734
2024-01-31 06:02:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:02:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:02:43 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 2.188 | nll_loss 0.448 | ppl 1.36 | wps 131672 | wpb 3275.2 | bsz 176.5 | num_updates 203687 | best_loss 2.063
2024-01-31 06:02:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 203687 updates
2024-01-31 06:02:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint151.pt
2024-01-31 06:02:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint151.pt
2024-01-31 06:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint151.pt (epoch 151 @ 203687 updates, score 2.188) (writing took 2.290249254903756 seconds)
2024-01-31 06:02:45 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2024-01-31 06:02:45 | INFO | train | epoch 151 | loss 1.783 | nll_loss 0.186 | ppl 1.14 | wps 57701.4 | ups 14.72 | wpb 3921.2 | bsz 214.5 | num_updates 203687 | lr 7.00678e-05 | gnorm 0.523 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 4744
2024-01-31 06:02:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:02:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:02:45 | INFO | fairseq.trainer | begin training epoch 152
2024-01-31 06:02:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:02:46 | INFO | train_inner | epoch 152:     13 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=35336.5, ups=8.97, wpb=3940.7, bsz=231, num_updates=203700, lr=7.00655e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.7, wall=4745
2024-01-31 06:02:52 | INFO | train_inner | epoch 152:    113 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60826.7, ups=15.54, wpb=3914.5, bsz=212.5, num_updates=203800, lr=7.00484e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.7, wall=4751
2024-01-31 06:02:59 | INFO | train_inner | epoch 152:    213 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61497.5, ups=15.62, wpb=3936.3, bsz=206.4, num_updates=203900, lr=7.00312e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=4758
2024-01-31 06:03:05 | INFO | train_inner | epoch 152:    313 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61299.6, ups=15.48, wpb=3958.9, bsz=220.4, num_updates=204000, lr=7.0014e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.7, wall=4764
2024-01-31 06:03:12 | INFO | train_inner | epoch 152:    413 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60421.3, ups=15.51, wpb=3895.9, bsz=206.6, num_updates=204100, lr=6.99969e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=4771
2024-01-31 06:03:18 | INFO | train_inner | epoch 152:    513 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60929.1, ups=15.42, wpb=3951.1, bsz=219.6, num_updates=204200, lr=6.99797e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=4777
2024-01-31 06:03:25 | INFO | train_inner | epoch 152:    613 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61083.3, ups=15.49, wpb=3944.5, bsz=217.9, num_updates=204300, lr=6.99626e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.7, wall=4784
2024-01-31 06:03:31 | INFO | train_inner | epoch 152:    713 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60752.8, ups=15.44, wpb=3935.2, bsz=213.3, num_updates=204400, lr=6.99455e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.9, wall=4790
2024-01-31 06:03:38 | INFO | train_inner | epoch 152:    813 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60863.1, ups=15.34, wpb=3968.3, bsz=228.8, num_updates=204500, lr=6.99284e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=4797
2024-01-31 06:03:44 | INFO | train_inner | epoch 152:    913 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60222.8, ups=15.56, wpb=3870.9, bsz=208.5, num_updates=204600, lr=6.99113e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=4803
2024-01-31 06:03:50 | INFO | train_inner | epoch 152:   1013 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60860.2, ups=15.55, wpb=3913.8, bsz=208.5, num_updates=204700, lr=6.98942e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.9, wall=4809
2024-01-31 06:03:57 | INFO | train_inner | epoch 152:   1113 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=59706.7, ups=15.53, wpb=3844, bsz=220.9, num_updates=204800, lr=6.98771e-05, gnorm=0.563, loss_scale=8, train_wall=6, gb_free=29.8, wall=4816
2024-01-31 06:04:03 | INFO | train_inner | epoch 152:   1213 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=60857.5, ups=15.53, wpb=3917.7, bsz=207, num_updates=204900, lr=6.98601e-05, gnorm=0.562, loss_scale=8, train_wall=6, gb_free=29.8, wall=4822
2024-01-31 06:04:10 | INFO | train_inner | epoch 152:   1313 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60710.1, ups=15.5, wpb=3916.8, bsz=219.4, num_updates=205000, lr=6.9843e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.9, wall=4829
2024-01-31 06:04:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:04:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:04:14 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 2.188 | nll_loss 0.448 | ppl 1.36 | wps 133150 | wpb 3275.2 | bsz 176.5 | num_updates 205036 | best_loss 2.063
2024-01-31 06:04:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 205036 updates
2024-01-31 06:04:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint152.pt
2024-01-31 06:04:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint152.pt
2024-01-31 06:04:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint152.pt (epoch 152 @ 205036 updates, score 2.188) (writing took 2.2896591250319034 seconds)
2024-01-31 06:04:17 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2024-01-31 06:04:17 | INFO | train | epoch 152 | loss 1.783 | nll_loss 0.186 | ppl 1.14 | wps 57686.1 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 205036 | lr 6.98369e-05 | gnorm 0.511 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 4836
2024-01-31 06:04:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:04:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:04:17 | INFO | fairseq.trainer | begin training epoch 153
2024-01-31 06:04:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:04:21 | INFO | train_inner | epoch 153:     64 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=34940.2, ups=9, wpb=3882.1, bsz=203.8, num_updates=205100, lr=6.9826e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.7, wall=4840
2024-01-31 06:04:27 | INFO | train_inner | epoch 153:    164 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60728, ups=15.53, wpb=3910.7, bsz=215.4, num_updates=205200, lr=6.9809e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=4846
2024-01-31 06:04:34 | INFO | train_inner | epoch 153:    264 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60288.4, ups=15.46, wpb=3900.8, bsz=214.8, num_updates=205300, lr=6.9792e-05, gnorm=0.556, loss_scale=8, train_wall=6, gb_free=29.8, wall=4853
2024-01-31 06:04:40 | INFO | train_inner | epoch 153:    364 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61107.1, ups=15.54, wpb=3933.1, bsz=224.6, num_updates=205400, lr=6.9775e-05, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.8, wall=4859
2024-01-31 06:04:47 | INFO | train_inner | epoch 153:    464 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60083.4, ups=15.58, wpb=3857.6, bsz=217, num_updates=205500, lr=6.9758e-05, gnorm=0.554, loss_scale=8, train_wall=6, gb_free=29.8, wall=4866
2024-01-31 06:04:53 | INFO | train_inner | epoch 153:    564 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=61082, ups=15.57, wpb=3923.9, bsz=198.4, num_updates=205600, lr=6.9741e-05, gnorm=0.533, loss_scale=8, train_wall=6, gb_free=29.8, wall=4872
2024-01-31 06:05:00 | INFO | train_inner | epoch 153:    664 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61457.4, ups=15.52, wpb=3958.7, bsz=225.9, num_updates=205700, lr=6.97241e-05, gnorm=0.455, loss_scale=8, train_wall=6, gb_free=29.8, wall=4878
2024-01-31 06:05:06 | INFO | train_inner | epoch 153:    764 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61009.6, ups=15.46, wpb=3946.7, bsz=204.6, num_updates=205800, lr=6.97071e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.7, wall=4885
2024-01-31 06:05:13 | INFO | train_inner | epoch 153:    864 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60405.3, ups=15.36, wpb=3932, bsz=224.8, num_updates=205900, lr=6.96902e-05, gnorm=0.43, loss_scale=8, train_wall=6, gb_free=29.8, wall=4891
2024-01-31 06:05:19 | INFO | train_inner | epoch 153:    964 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61096.4, ups=15.53, wpb=3934.6, bsz=215.4, num_updates=206000, lr=6.96733e-05, gnorm=0.481, loss_scale=8, train_wall=6, gb_free=29.8, wall=4898
2024-01-31 06:05:25 | INFO | train_inner | epoch 153:   1064 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61089.5, ups=15.52, wpb=3937.1, bsz=215.3, num_updates=206100, lr=6.96564e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.7, wall=4904
2024-01-31 06:05:32 | INFO | train_inner | epoch 153:   1164 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60685.6, ups=15.48, wpb=3920.2, bsz=207.7, num_updates=206200, lr=6.96395e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.8, wall=4911
2024-01-31 06:05:38 | INFO | train_inner | epoch 153:   1264 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60532.2, ups=15.5, wpb=3905.7, bsz=218.7, num_updates=206300, lr=6.96226e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=4917
2024-01-31 06:05:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:05:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:05:46 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 2.186 | nll_loss 0.445 | ppl 1.36 | wps 132907 | wpb 3275.2 | bsz 176.5 | num_updates 206385 | best_loss 2.063
2024-01-31 06:05:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 206385 updates
2024-01-31 06:05:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint153.pt
2024-01-31 06:05:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint153.pt
2024-01-31 06:05:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint153.pt (epoch 153 @ 206385 updates, score 2.186) (writing took 2.2871937489835545 seconds)
2024-01-31 06:05:48 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2024-01-31 06:05:48 | INFO | train | epoch 153 | loss 1.782 | nll_loss 0.185 | ppl 1.14 | wps 57697 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 206385 | lr 6.96083e-05 | gnorm 0.504 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 4927
2024-01-31 06:05:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:05:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:05:48 | INFO | fairseq.trainer | begin training epoch 154
2024-01-31 06:05:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:05:49 | INFO | train_inner | epoch 154:     15 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=35743.8, ups=9.01, wpb=3965.4, bsz=211.5, num_updates=206400, lr=6.96058e-05, gnorm=0.553, loss_scale=8, train_wall=6, gb_free=29.8, wall=4928
2024-01-31 06:05:56 | INFO | train_inner | epoch 154:    115 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60755.4, ups=15.58, wpb=3900.4, bsz=207.8, num_updates=206500, lr=6.95889e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.9, wall=4935
2024-01-31 06:06:02 | INFO | train_inner | epoch 154:    215 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61131.5, ups=15.58, wpb=3923, bsz=208.6, num_updates=206600, lr=6.95721e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.9, wall=4941
2024-01-31 06:06:09 | INFO | train_inner | epoch 154:    315 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=61038.2, ups=15.47, wpb=3945.7, bsz=215.8, num_updates=206700, lr=6.95552e-05, gnorm=0.541, loss_scale=8, train_wall=6, gb_free=29.7, wall=4948
2024-01-31 06:06:15 | INFO | train_inner | epoch 154:    415 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=60415.7, ups=15.51, wpb=3896, bsz=217.3, num_updates=206800, lr=6.95384e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.7, wall=4954
2024-01-31 06:06:22 | INFO | train_inner | epoch 154:    515 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60272.3, ups=15.48, wpb=3893.4, bsz=214.8, num_updates=206900, lr=6.95216e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.7, wall=4961
2024-01-31 06:06:28 | INFO | train_inner | epoch 154:    615 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60732.5, ups=15.42, wpb=3939.2, bsz=226.8, num_updates=207000, lr=6.95048e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=4967
2024-01-31 06:06:35 | INFO | train_inner | epoch 154:    715 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60573.6, ups=15.49, wpb=3910.7, bsz=216.7, num_updates=207100, lr=6.9488e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=4973
2024-01-31 06:06:41 | INFO | train_inner | epoch 154:    815 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=61259.5, ups=15.58, wpb=3930.8, bsz=204, num_updates=207200, lr=6.94713e-05, gnorm=0.555, loss_scale=8, train_wall=6, gb_free=29.8, wall=4980
2024-01-31 06:06:47 | INFO | train_inner | epoch 154:    915 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60852.4, ups=15.52, wpb=3921.8, bsz=211.9, num_updates=207300, lr=6.94545e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=4986
2024-01-31 06:06:56 | INFO | train_inner | epoch 154:   1015 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=43844.1, ups=11.2, wpb=3915.7, bsz=217.2, num_updates=207400, lr=6.94377e-05, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.7, wall=4995
2024-01-31 06:07:09 | INFO | train_inner | epoch 154:   1115 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=31960.7, ups=8.14, wpb=3926.8, bsz=219.1, num_updates=207500, lr=6.9421e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.7, wall=5008
2024-01-31 06:07:15 | INFO | train_inner | epoch 154:   1215 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60709.2, ups=15.56, wpb=3902.6, bsz=201.8, num_updates=207600, lr=6.94043e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=5014
2024-01-31 06:07:22 | INFO | train_inner | epoch 154:   1315 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61092.3, ups=15.4, wpb=3966, bsz=224.4, num_updates=207700, lr=6.93876e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.7, wall=5020
2024-01-31 06:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:07:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:07:26 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 2.187 | nll_loss 0.445 | ppl 1.36 | wps 132900 | wpb 3275.2 | bsz 176.5 | num_updates 207734 | best_loss 2.063
2024-01-31 06:07:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 207734 updates
2024-01-31 06:07:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint154.pt
2024-01-31 06:07:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint154.pt
2024-01-31 06:07:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint154.pt (epoch 154 @ 207734 updates, score 2.187) (writing took 2.3568825640249997 seconds)
2024-01-31 06:07:28 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2024-01-31 06:07:28 | INFO | train | epoch 154 | loss 1.782 | nll_loss 0.186 | ppl 1.14 | wps 52864.6 | ups 13.48 | wpb 3921.2 | bsz 214.5 | num_updates 207734 | lr 6.93819e-05 | gnorm 0.528 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 5027
2024-01-31 06:07:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:07:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:07:28 | INFO | fairseq.trainer | begin training epoch 155
2024-01-31 06:07:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:07:33 | INFO | train_inner | epoch 155:     66 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=34823.8, ups=8.9, wpb=3914.1, bsz=215.6, num_updates=207800, lr=6.93709e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=5032
2024-01-31 06:07:39 | INFO | train_inner | epoch 155:    166 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61652.4, ups=15.57, wpb=3960.6, bsz=209, num_updates=207900, lr=6.93542e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.8, wall=5038
2024-01-31 06:07:46 | INFO | train_inner | epoch 155:    266 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61226.8, ups=15.52, wpb=3943.9, bsz=211, num_updates=208000, lr=6.93375e-05, gnorm=0.447, loss_scale=8, train_wall=6, gb_free=29.8, wall=5045
2024-01-31 06:07:52 | INFO | train_inner | epoch 155:    366 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=61373.6, ups=15.43, wpb=3977.7, bsz=234.4, num_updates=208100, lr=6.93209e-05, gnorm=0.427, loss_scale=8, train_wall=6, gb_free=29.7, wall=5051
2024-01-31 06:07:59 | INFO | train_inner | epoch 155:    466 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60318.4, ups=15.55, wpb=3878.2, bsz=212.2, num_updates=208200, lr=6.93042e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=5058
2024-01-31 06:08:05 | INFO | train_inner | epoch 155:    566 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60818.6, ups=15.52, wpb=3919.6, bsz=215.1, num_updates=208300, lr=6.92876e-05, gnorm=0.565, loss_scale=8, train_wall=6, gb_free=29.8, wall=5064
2024-01-31 06:08:11 | INFO | train_inner | epoch 155:    666 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60875.1, ups=15.53, wpb=3920.7, bsz=211, num_updates=208400, lr=6.92709e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.7, wall=5070
2024-01-31 06:08:18 | INFO | train_inner | epoch 155:    766 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=59939.5, ups=15.53, wpb=3859.3, bsz=213.8, num_updates=208500, lr=6.92543e-05, gnorm=0.577, loss_scale=8, train_wall=6, gb_free=29.8, wall=5077
2024-01-31 06:08:24 | INFO | train_inner | epoch 155:    866 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60331.5, ups=15.46, wpb=3903.2, bsz=217, num_updates=208600, lr=6.92377e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.7, wall=5083
2024-01-31 06:08:31 | INFO | train_inner | epoch 155:    966 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60850, ups=15.43, wpb=3942.9, bsz=229.3, num_updates=208700, lr=6.92211e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=5090
2024-01-31 06:08:37 | INFO | train_inner | epoch 155:   1066 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61434.1, ups=15.48, wpb=3967.9, bsz=207, num_updates=208800, lr=6.92046e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=5096
2024-01-31 06:08:44 | INFO | train_inner | epoch 155:   1166 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60336.4, ups=15.49, wpb=3894.8, bsz=203.7, num_updates=208900, lr=6.9188e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=5103
2024-01-31 06:08:50 | INFO | train_inner | epoch 155:   1266 / 1349 loss=1.787, nll_loss=0.191, ppl=1.14, wps=60697.6, ups=15.64, wpb=3881.9, bsz=208.2, num_updates=209000, lr=6.91714e-05, gnorm=0.564, loss_scale=8, train_wall=6, gb_free=29.7, wall=5109
2024-01-31 06:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:08:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:08:58 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 2.184 | nll_loss 0.443 | ppl 1.36 | wps 133665 | wpb 3275.2 | bsz 176.5 | num_updates 209083 | best_loss 2.063
2024-01-31 06:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 209083 updates
2024-01-31 06:08:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint155.pt
2024-01-31 06:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint155.pt
2024-01-31 06:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint155.pt (epoch 155 @ 209083 updates, score 2.184) (writing took 2.241713569033891 seconds)
2024-01-31 06:09:00 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2024-01-31 06:09:00 | INFO | train | epoch 155 | loss 1.782 | nll_loss 0.185 | ppl 1.14 | wps 57750.2 | ups 14.73 | wpb 3921.2 | bsz 214.5 | num_updates 209083 | lr 6.91577e-05 | gnorm 0.516 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 5119
2024-01-31 06:09:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:09:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:09:00 | INFO | fairseq.trainer | begin training epoch 156
2024-01-31 06:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:09:01 | INFO | train_inner | epoch 156:     17 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=35462.6, ups=9.06, wpb=3915.7, bsz=218.7, num_updates=209100, lr=6.91549e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=5120
2024-01-31 06:09:08 | INFO | train_inner | epoch 156:    117 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60229.5, ups=15.44, wpb=3902, bsz=221.9, num_updates=209200, lr=6.91384e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=5127
2024-01-31 06:09:14 | INFO | train_inner | epoch 156:    217 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61005.7, ups=15.47, wpb=3942.8, bsz=219.4, num_updates=209300, lr=6.91219e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.8, wall=5133
2024-01-31 06:09:21 | INFO | train_inner | epoch 156:    317 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60857, ups=15.54, wpb=3916.4, bsz=208.2, num_updates=209400, lr=6.91053e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=5140
2024-01-31 06:09:27 | INFO | train_inner | epoch 156:    417 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61052, ups=15.56, wpb=3923, bsz=215.6, num_updates=209500, lr=6.90889e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=5146
2024-01-31 06:09:33 | INFO | train_inner | epoch 156:    517 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61253.8, ups=15.51, wpb=3948.9, bsz=210.8, num_updates=209600, lr=6.90724e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.7, wall=5152
2024-01-31 06:09:40 | INFO | train_inner | epoch 156:    617 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60849.6, ups=15.45, wpb=3937.4, bsz=232.8, num_updates=209700, lr=6.90559e-05, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.8, wall=5159
2024-01-31 06:09:46 | INFO | train_inner | epoch 156:    717 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60554.1, ups=15.43, wpb=3925.2, bsz=215.3, num_updates=209800, lr=6.90394e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.8, wall=5165
2024-01-31 06:09:53 | INFO | train_inner | epoch 156:    817 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60956.6, ups=15.55, wpb=3920.9, bsz=206.6, num_updates=209900, lr=6.9023e-05, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.8, wall=5172
2024-01-31 06:09:59 | INFO | train_inner | epoch 156:    917 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61463.9, ups=15.61, wpb=3937.4, bsz=203.4, num_updates=210000, lr=6.90066e-05, gnorm=0.534, loss_scale=8, train_wall=6, gb_free=29.7, wall=5178
2024-01-31 06:10:06 | INFO | train_inner | epoch 156:   1017 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=59312.3, ups=15.43, wpb=3842.9, bsz=210.4, num_updates=210100, lr=6.89901e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.8, wall=5185
2024-01-31 06:10:12 | INFO | train_inner | epoch 156:   1117 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61208.8, ups=15.46, wpb=3958.4, bsz=215.2, num_updates=210200, lr=6.89737e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.9, wall=5191
2024-01-31 06:10:19 | INFO | train_inner | epoch 156:   1217 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60341.3, ups=15.53, wpb=3884.9, bsz=212.9, num_updates=210300, lr=6.89573e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=5198
2024-01-31 06:10:25 | INFO | train_inner | epoch 156:   1317 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60950.1, ups=15.45, wpb=3946.1, bsz=218.4, num_updates=210400, lr=6.89409e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=5204
2024-01-31 06:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:10:30 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 2.188 | nll_loss 0.447 | ppl 1.36 | wps 132357 | wpb 3275.2 | bsz 176.5 | num_updates 210432 | best_loss 2.063
2024-01-31 06:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 210432 updates
2024-01-31 06:10:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint156.pt
2024-01-31 06:10:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint156.pt
2024-01-31 06:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint156.pt (epoch 156 @ 210432 updates, score 2.188) (writing took 2.3044213679386303 seconds)
2024-01-31 06:10:32 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2024-01-31 06:10:32 | INFO | train | epoch 156 | loss 1.782 | nll_loss 0.185 | ppl 1.14 | wps 57547.6 | ups 14.68 | wpb 3921.2 | bsz 214.5 | num_updates 210432 | lr 6.89357e-05 | gnorm 0.496 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 5211
2024-01-31 06:10:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:10:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:10:32 | INFO | fairseq.trainer | begin training epoch 157
2024-01-31 06:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:10:37 | INFO | train_inner | epoch 157:     68 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=34356.2, ups=8.78, wpb=3913.8, bsz=228.3, num_updates=210500, lr=6.89246e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.9, wall=5215
2024-01-31 06:10:43 | INFO | train_inner | epoch 157:    168 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=61023.1, ups=15.47, wpb=3944.5, bsz=214.2, num_updates=210600, lr=6.89082e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.7, wall=5222
2024-01-31 06:10:49 | INFO | train_inner | epoch 157:    268 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60858.4, ups=15.68, wpb=3881.8, bsz=205.1, num_updates=210700, lr=6.88918e-05, gnorm=0.61, loss_scale=8, train_wall=6, gb_free=29.8, wall=5228
2024-01-31 06:10:56 | INFO | train_inner | epoch 157:    368 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60252.3, ups=15.58, wpb=3867.4, bsz=197.4, num_updates=210800, lr=6.88755e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.8, wall=5235
2024-01-31 06:11:02 | INFO | train_inner | epoch 157:    468 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61498.2, ups=15.5, wpb=3967.8, bsz=222.2, num_updates=210900, lr=6.88592e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=5241
2024-01-31 06:11:09 | INFO | train_inner | epoch 157:    568 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61057.6, ups=15.47, wpb=3945.7, bsz=221.8, num_updates=211000, lr=6.88428e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.8, wall=5248
2024-01-31 06:11:15 | INFO | train_inner | epoch 157:    668 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61380.3, ups=15.47, wpb=3968, bsz=216.3, num_updates=211100, lr=6.88265e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.7, wall=5254
2024-01-31 06:11:22 | INFO | train_inner | epoch 157:    768 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61035.8, ups=15.48, wpb=3941.7, bsz=220.7, num_updates=211200, lr=6.88102e-05, gnorm=0.443, loss_scale=8, train_wall=6, gb_free=29.8, wall=5261
2024-01-31 06:11:28 | INFO | train_inner | epoch 157:    868 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61023.9, ups=15.51, wpb=3935.1, bsz=214.5, num_updates=211300, lr=6.8794e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=5267
2024-01-31 06:11:35 | INFO | train_inner | epoch 157:    968 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60219.6, ups=15.34, wpb=3924.8, bsz=208.9, num_updates=211400, lr=6.87777e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.9, wall=5273
2024-01-31 06:11:41 | INFO | train_inner | epoch 157:   1068 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=61386.3, ups=15.55, wpb=3948, bsz=210.9, num_updates=211500, lr=6.87614e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=30.1, wall=5280
2024-01-31 06:11:47 | INFO | train_inner | epoch 157:   1168 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=59458.2, ups=15.57, wpb=3819.5, bsz=217.5, num_updates=211600, lr=6.87452e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.7, wall=5286
2024-01-31 06:11:54 | INFO | train_inner | epoch 157:   1268 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61138.6, ups=15.54, wpb=3934.2, bsz=211, num_updates=211700, lr=6.87289e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.7, wall=5293
2024-01-31 06:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:11:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:12:01 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 2.186 | nll_loss 0.445 | ppl 1.36 | wps 132130 | wpb 3275.2 | bsz 176.5 | num_updates 211781 | best_loss 2.063
2024-01-31 06:12:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 211781 updates
2024-01-31 06:12:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint157.pt
2024-01-31 06:12:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint157.pt
2024-01-31 06:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint157.pt (epoch 157 @ 211781 updates, score 2.186) (writing took 2.3484402210451663 seconds)
2024-01-31 06:12:04 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2024-01-31 06:12:04 | INFO | train | epoch 157 | loss 1.782 | nll_loss 0.185 | ppl 1.14 | wps 57650.4 | ups 14.7 | wpb 3921.2 | bsz 214.5 | num_updates 211781 | lr 6.87158e-05 | gnorm 0.506 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 5303
2024-01-31 06:12:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:12:04 | INFO | fairseq.trainer | begin training epoch 158
2024-01-31 06:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:12:05 | INFO | train_inner | epoch 158:     19 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=35088.7, ups=8.97, wpb=3913.2, bsz=212.5, num_updates=211800, lr=6.87127e-05, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.8, wall=5304
2024-01-31 06:12:12 | INFO | train_inner | epoch 158:    119 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=60479.3, ups=15.48, wpb=3906.9, bsz=216.1, num_updates=211900, lr=6.86965e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=5310
2024-01-31 06:12:18 | INFO | train_inner | epoch 158:    219 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60941.9, ups=15.45, wpb=3943.6, bsz=215, num_updates=212000, lr=6.86803e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.7, wall=5317
2024-01-31 06:12:24 | INFO | train_inner | epoch 158:    319 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61269.6, ups=15.49, wpb=3956.5, bsz=219.9, num_updates=212100, lr=6.86641e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.8, wall=5323
2024-01-31 06:12:31 | INFO | train_inner | epoch 158:    419 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=59850.5, ups=15.31, wpb=3910, bsz=208.4, num_updates=212200, lr=6.86479e-05, gnorm=0.595, loss_scale=8, train_wall=6, gb_free=29.8, wall=5330
2024-01-31 06:12:37 | INFO | train_inner | epoch 158:    519 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60103.2, ups=15.58, wpb=3857.6, bsz=215, num_updates=212300, lr=6.86317e-05, gnorm=0.586, loss_scale=8, train_wall=6, gb_free=29.8, wall=5336
2024-01-31 06:12:44 | INFO | train_inner | epoch 158:    619 / 1349 loss=1.78, nll_loss=0.183, ppl=1.13, wps=61053.9, ups=15.5, wpb=3938.2, bsz=211, num_updates=212400, lr=6.86156e-05, gnorm=0.566, loss_scale=8, train_wall=6, gb_free=29.7, wall=5343
2024-01-31 06:12:50 | INFO | train_inner | epoch 158:    719 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61222, ups=15.55, wpb=3936.1, bsz=212.3, num_updates=212500, lr=6.85994e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=5349
2024-01-31 06:12:57 | INFO | train_inner | epoch 158:    819 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60943.6, ups=15.54, wpb=3922.1, bsz=212.1, num_updates=212600, lr=6.85833e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=5356
2024-01-31 06:13:03 | INFO | train_inner | epoch 158:    919 / 1349 loss=1.786, nll_loss=0.189, ppl=1.14, wps=61500.6, ups=15.52, wpb=3962.3, bsz=217.8, num_updates=212700, lr=6.85672e-05, gnorm=0.546, loss_scale=16, train_wall=6, gb_free=29.8, wall=5362
2024-01-31 06:13:10 | INFO | train_inner | epoch 158:   1019 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60432.3, ups=15.54, wpb=3887.6, bsz=206.3, num_updates=212800, lr=6.85511e-05, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.7, wall=5368
2024-01-31 06:13:16 | INFO | train_inner | epoch 158:   1119 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=61073.4, ups=15.4, wpb=3967, bsz=220.9, num_updates=212900, lr=6.8535e-05, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.7, wall=5375
2024-01-31 06:13:23 | INFO | train_inner | epoch 158:   1219 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60522, ups=15.49, wpb=3907.3, bsz=216.1, num_updates=213000, lr=6.85189e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=5381
2024-01-31 06:13:29 | INFO | train_inner | epoch 158:   1319 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60575.8, ups=15.6, wpb=3883.6, bsz=209.6, num_updates=213100, lr=6.85028e-05, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=5388
2024-01-31 06:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:13:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:13:33 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 2.189 | nll_loss 0.449 | ppl 1.37 | wps 131062 | wpb 3275.2 | bsz 176.5 | num_updates 213130 | best_loss 2.063
2024-01-31 06:13:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 213130 updates
2024-01-31 06:13:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint158.pt
2024-01-31 06:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint158.pt
2024-01-31 06:13:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint158.pt (epoch 158 @ 213130 updates, score 2.189) (writing took 2.2529048619326204 seconds)
2024-01-31 06:13:35 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2024-01-31 06:13:35 | INFO | train | epoch 158 | loss 1.781 | nll_loss 0.185 | ppl 1.14 | wps 57670.4 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 213130 | lr 6.8498e-05 | gnorm 0.527 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 5394
2024-01-31 06:13:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:13:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:13:35 | INFO | fairseq.trainer | begin training epoch 159
2024-01-31 06:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:13:40 | INFO | train_inner | epoch 159:     70 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=35289.6, ups=9.03, wpb=3908.8, bsz=221.8, num_updates=213200, lr=6.84867e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=5399
2024-01-31 06:13:46 | INFO | train_inner | epoch 159:    170 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60153.2, ups=15.53, wpb=3872.9, bsz=207.7, num_updates=213300, lr=6.84707e-05, gnorm=0.451, loss_scale=16, train_wall=6, gb_free=29.8, wall=5405
2024-01-31 06:13:53 | INFO | train_inner | epoch 159:    270 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61475.3, ups=15.52, wpb=3959.9, bsz=210.8, num_updates=213400, lr=6.84546e-05, gnorm=0.555, loss_scale=16, train_wall=6, gb_free=29.8, wall=5412
2024-01-31 06:13:59 | INFO | train_inner | epoch 159:    370 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60969.6, ups=15.57, wpb=3916.9, bsz=211.4, num_updates=213500, lr=6.84386e-05, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.8, wall=5418
2024-01-31 06:14:06 | INFO | train_inner | epoch 159:    470 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61302.5, ups=15.53, wpb=3946.3, bsz=205, num_updates=213600, lr=6.84226e-05, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.7, wall=5425
2024-01-31 06:14:12 | INFO | train_inner | epoch 159:    570 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61088.7, ups=15.56, wpb=3926.2, bsz=215.8, num_updates=213700, lr=6.84066e-05, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.8, wall=5431
2024-01-31 06:14:19 | INFO | train_inner | epoch 159:    670 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60843.9, ups=15.51, wpb=3922.1, bsz=213.8, num_updates=213800, lr=6.83906e-05, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=5438
2024-01-31 06:14:25 | INFO | train_inner | epoch 159:    770 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=60707.2, ups=15.41, wpb=3938.4, bsz=223.4, num_updates=213900, lr=6.83746e-05, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.7, wall=5444
2024-01-31 06:14:32 | INFO | train_inner | epoch 159:    870 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=60891.8, ups=15.49, wpb=3931.5, bsz=216.9, num_updates=214000, lr=6.83586e-05, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.7, wall=5450
2024-01-31 06:14:38 | INFO | train_inner | epoch 159:    970 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60053.5, ups=15.48, wpb=3878.8, bsz=218, num_updates=214100, lr=6.83426e-05, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=5457
2024-01-31 06:14:45 | INFO | train_inner | epoch 159:   1070 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60630.3, ups=15.39, wpb=3940.1, bsz=219.6, num_updates=214200, lr=6.83267e-05, gnorm=0.591, loss_scale=16, train_wall=6, gb_free=29.8, wall=5463
2024-01-31 06:14:51 | INFO | train_inner | epoch 159:   1170 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61334.4, ups=15.52, wpb=3951.6, bsz=214.1, num_updates=214300, lr=6.83107e-05, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=5470
2024-01-31 06:14:57 | INFO | train_inner | epoch 159:   1270 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=60154.8, ups=15.47, wpb=3887.9, bsz=215, num_updates=214400, lr=6.82948e-05, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.8, wall=5476
2024-01-31 06:15:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:15:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:15:05 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 2.19 | nll_loss 0.449 | ppl 1.36 | wps 132682 | wpb 3275.2 | bsz 176.5 | num_updates 214479 | best_loss 2.063
2024-01-31 06:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 214479 updates
2024-01-31 06:15:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint159.pt
2024-01-31 06:15:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint159.pt
2024-01-31 06:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint159.pt (epoch 159 @ 214479 updates, score 2.19) (writing took 2.265161645016633 seconds)
2024-01-31 06:15:07 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2024-01-31 06:15:07 | INFO | train | epoch 159 | loss 1.782 | nll_loss 0.185 | ppl 1.14 | wps 57720.4 | ups 14.72 | wpb 3921.2 | bsz 214.5 | num_updates 214479 | lr 6.82822e-05 | gnorm 0.514 | loss_scale 16 | train_wall 84 | gb_free 29.8 | wall 5486
2024-01-31 06:15:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:15:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:15:07 | INFO | fairseq.trainer | begin training epoch 160
2024-01-31 06:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:15:09 | INFO | train_inner | epoch 160:     21 / 1349 loss=1.785, nll_loss=0.188, ppl=1.14, wps=35361.9, ups=9.04, wpb=3913.8, bsz=214.2, num_updates=214500, lr=6.82789e-05, gnorm=0.59, loss_scale=16, train_wall=6, gb_free=29.7, wall=5487
2024-01-31 06:15:15 | INFO | train_inner | epoch 160:    121 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60515.3, ups=15.49, wpb=3907.7, bsz=219.6, num_updates=214600, lr=6.8263e-05, gnorm=0.578, loss_scale=16, train_wall=6, gb_free=29.7, wall=5494
2024-01-31 06:15:21 | INFO | train_inner | epoch 160:    221 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60794.8, ups=15.55, wpb=3910, bsz=225.4, num_updates=214700, lr=6.82471e-05, gnorm=0.564, loss_scale=16, train_wall=6, gb_free=29.7, wall=5500
2024-01-31 06:15:28 | INFO | train_inner | epoch 160:    321 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60415.3, ups=15.54, wpb=3886.6, bsz=216.2, num_updates=214800, lr=6.82312e-05, gnorm=0.572, loss_scale=16, train_wall=6, gb_free=29.8, wall=5507
2024-01-31 06:15:34 | INFO | train_inner | epoch 160:    421 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60858.7, ups=15.49, wpb=3930.1, bsz=214.3, num_updates=214900, lr=6.82153e-05, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=5513
2024-01-31 06:15:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 06:15:41 | INFO | train_inner | epoch 160:    522 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60534.3, ups=15.45, wpb=3918.8, bsz=207.3, num_updates=215000, lr=6.81994e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.7, wall=5520
2024-01-31 06:15:47 | INFO | train_inner | epoch 160:    622 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61505.5, ups=15.5, wpb=3968.1, bsz=215.6, num_updates=215100, lr=6.81836e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.8, wall=5526
2024-01-31 06:15:54 | INFO | train_inner | epoch 160:    722 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60765.4, ups=15.52, wpb=3915.2, bsz=203.1, num_updates=215200, lr=6.81677e-05, gnorm=0.541, loss_scale=8, train_wall=6, gb_free=30, wall=5533
2024-01-31 06:16:00 | INFO | train_inner | epoch 160:    822 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60230.2, ups=15.44, wpb=3901.6, bsz=226.9, num_updates=215300, lr=6.81519e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.7, wall=5539
2024-01-31 06:16:07 | INFO | train_inner | epoch 160:    922 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=60577.5, ups=15.56, wpb=3892.4, bsz=194.3, num_updates=215400, lr=6.81361e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.8, wall=5545
2024-01-31 06:16:13 | INFO | train_inner | epoch 160:   1022 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=61400.1, ups=15.49, wpb=3963.5, bsz=218.3, num_updates=215500, lr=6.81203e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=5552
2024-01-31 06:16:19 | INFO | train_inner | epoch 160:   1122 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60587.6, ups=15.48, wpb=3914.8, bsz=216.2, num_updates=215600, lr=6.81045e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=5558
2024-01-31 06:16:26 | INFO | train_inner | epoch 160:   1222 / 1349 loss=1.784, nll_loss=0.187, ppl=1.14, wps=61053.2, ups=15.58, wpb=3918.9, bsz=203.2, num_updates=215700, lr=6.80887e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.8, wall=5565
2024-01-31 06:16:32 | INFO | train_inner | epoch 160:   1322 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60847.4, ups=15.37, wpb=3958.8, bsz=231.9, num_updates=215800, lr=6.80729e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=5571
2024-01-31 06:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:16:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:16:36 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 2.192 | nll_loss 0.452 | ppl 1.37 | wps 132833 | wpb 3275.2 | bsz 176.5 | num_updates 215827 | best_loss 2.063
2024-01-31 06:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 215827 updates
2024-01-31 06:16:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint160.pt
2024-01-31 06:16:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint160.pt
2024-01-31 06:16:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint160.pt (epoch 160 @ 215827 updates, score 2.192) (writing took 2.233243212918751 seconds)
2024-01-31 06:16:39 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2024-01-31 06:16:39 | INFO | train | epoch 160 | loss 1.782 | nll_loss 0.185 | ppl 1.14 | wps 57727.6 | ups 14.72 | wpb 3921.1 | bsz 214.5 | num_updates 215827 | lr 6.80686e-05 | gnorm 0.529 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 5578
2024-01-31 06:16:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:16:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:16:39 | INFO | fairseq.trainer | begin training epoch 161
2024-01-31 06:16:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:16:43 | INFO | train_inner | epoch 161:     73 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=34880.9, ups=9.03, wpb=3862.4, bsz=224.8, num_updates=215900, lr=6.80571e-05, gnorm=0.555, loss_scale=8, train_wall=6, gb_free=29.8, wall=5582
2024-01-31 06:16:50 | INFO | train_inner | epoch 161:    173 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=61352.6, ups=15.63, wpb=3925, bsz=207.1, num_updates=216000, lr=6.80414e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=5589
2024-01-31 06:16:56 | INFO | train_inner | epoch 161:    273 / 1349 loss=1.78, nll_loss=0.183, ppl=1.13, wps=61006.9, ups=15.53, wpb=3929.1, bsz=200.4, num_updates=216100, lr=6.80256e-05, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=29.8, wall=5595
2024-01-31 06:17:03 | INFO | train_inner | epoch 161:    373 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60395.4, ups=15.45, wpb=3909.7, bsz=214.6, num_updates=216200, lr=6.80099e-05, gnorm=0.587, loss_scale=8, train_wall=6, gb_free=29.8, wall=5602
2024-01-31 06:17:09 | INFO | train_inner | epoch 161:    473 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61406.7, ups=15.57, wpb=3943, bsz=213.4, num_updates=216300, lr=6.79942e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.7, wall=5608
2024-01-31 06:17:16 | INFO | train_inner | epoch 161:    573 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60856.4, ups=15.54, wpb=3915.7, bsz=209.4, num_updates=216400, lr=6.79785e-05, gnorm=0.591, loss_scale=8, train_wall=6, gb_free=29.8, wall=5615
2024-01-31 06:17:22 | INFO | train_inner | epoch 161:    673 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60633.7, ups=15.55, wpb=3898.9, bsz=212.5, num_updates=216500, lr=6.79628e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=5621
2024-01-31 06:17:29 | INFO | train_inner | epoch 161:    773 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61350.3, ups=15.57, wpb=3941.1, bsz=212.8, num_updates=216600, lr=6.79471e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=5627
2024-01-31 06:17:35 | INFO | train_inner | epoch 161:    873 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60375.1, ups=15.56, wpb=3879.8, bsz=209.8, num_updates=216700, lr=6.79314e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=5634
2024-01-31 06:17:41 | INFO | train_inner | epoch 161:    973 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60493.4, ups=15.42, wpb=3923.2, bsz=233.8, num_updates=216800, lr=6.79157e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.7, wall=5640
2024-01-31 06:17:48 | INFO | train_inner | epoch 161:   1073 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61006.2, ups=15.5, wpb=3935, bsz=210.7, num_updates=216900, lr=6.79001e-05, gnorm=0.542, loss_scale=8, train_wall=6, gb_free=29.7, wall=5647
2024-01-31 06:17:54 | INFO | train_inner | epoch 161:   1173 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60901.8, ups=15.41, wpb=3951.4, bsz=222.5, num_updates=217000, lr=6.78844e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.8, wall=5653
2024-01-31 06:18:01 | INFO | train_inner | epoch 161:   1273 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60786.7, ups=15.46, wpb=3931, bsz=215.3, num_updates=217100, lr=6.78688e-05, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.8, wall=5660
2024-01-31 06:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:18:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:18:08 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 2.188 | nll_loss 0.447 | ppl 1.36 | wps 133255 | wpb 3275.2 | bsz 176.5 | num_updates 217176 | best_loss 2.063
2024-01-31 06:18:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 217176 updates
2024-01-31 06:18:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint161.pt
2024-01-31 06:18:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint161.pt
2024-01-31 06:18:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint161.pt (epoch 161 @ 217176 updates, score 2.188) (writing took 2.280488891992718 seconds)
2024-01-31 06:18:10 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2024-01-31 06:18:10 | INFO | train | epoch 161 | loss 1.781 | nll_loss 0.185 | ppl 1.14 | wps 57728.9 | ups 14.72 | wpb 3921.2 | bsz 214.5 | num_updates 217176 | lr 6.78569e-05 | gnorm 0.526 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 5669
2024-01-31 06:18:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:18:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:18:10 | INFO | fairseq.trainer | begin training epoch 162
2024-01-31 06:18:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:18:12 | INFO | train_inner | epoch 162:     24 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=35510.2, ups=9.02, wpb=3938, bsz=215.4, num_updates=217200, lr=6.78532e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=5671
2024-01-31 06:18:18 | INFO | train_inner | epoch 162:    124 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=60886.5, ups=15.47, wpb=3936.3, bsz=220.5, num_updates=217300, lr=6.78375e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=5677
2024-01-31 06:18:25 | INFO | train_inner | epoch 162:    224 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60505.4, ups=15.63, wpb=3872.3, bsz=198.6, num_updates=217400, lr=6.78219e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=5684
2024-01-31 06:18:31 | INFO | train_inner | epoch 162:    324 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=61342, ups=15.48, wpb=3962.7, bsz=217.1, num_updates=217500, lr=6.78064e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=5690
2024-01-31 06:18:38 | INFO | train_inner | epoch 162:    424 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60162.2, ups=15.48, wpb=3887.7, bsz=222.2, num_updates=217600, lr=6.77908e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.7, wall=5697
2024-01-31 06:18:44 | INFO | train_inner | epoch 162:    524 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60866.9, ups=15.51, wpb=3924, bsz=211.3, num_updates=217700, lr=6.77752e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=5703
2024-01-31 06:18:51 | INFO | train_inner | epoch 162:    624 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61302, ups=15.47, wpb=3963.6, bsz=223.7, num_updates=217800, lr=6.77596e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.8, wall=5710
2024-01-31 06:18:57 | INFO | train_inner | epoch 162:    724 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60269.3, ups=15.47, wpb=3896.4, bsz=220.7, num_updates=217900, lr=6.77441e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.8, wall=5716
2024-01-31 06:19:04 | INFO | train_inner | epoch 162:    824 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60809.2, ups=15.49, wpb=3924.8, bsz=229.8, num_updates=218000, lr=6.77285e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=5722
2024-01-31 06:19:10 | INFO | train_inner | epoch 162:    924 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61024.9, ups=15.45, wpb=3949.4, bsz=222.9, num_updates=218100, lr=6.7713e-05, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.7, wall=5729
2024-01-31 06:19:16 | INFO | train_inner | epoch 162:   1024 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=60541.9, ups=15.57, wpb=3888.9, bsz=202.5, num_updates=218200, lr=6.76975e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=5735
2024-01-31 06:19:23 | INFO | train_inner | epoch 162:   1124 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60567.6, ups=15.59, wpb=3884.4, bsz=199, num_updates=218300, lr=6.7682e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.8, wall=5742
2024-01-31 06:19:29 | INFO | train_inner | epoch 162:   1224 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=61381.7, ups=15.49, wpb=3963.2, bsz=221.5, num_updates=218400, lr=6.76665e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=5748
2024-01-31 06:19:36 | INFO | train_inner | epoch 162:   1324 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60721.4, ups=15.47, wpb=3925.3, bsz=201, num_updates=218500, lr=6.7651e-05, gnorm=0.578, loss_scale=8, train_wall=6, gb_free=29.7, wall=5755
2024-01-31 06:19:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:19:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:19:40 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 2.193 | nll_loss 0.452 | ppl 1.37 | wps 132939 | wpb 3275.2 | bsz 176.5 | num_updates 218525 | best_loss 2.063
2024-01-31 06:19:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 218525 updates
2024-01-31 06:19:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint162.pt
2024-01-31 06:19:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint162.pt
2024-01-31 06:19:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint162.pt (epoch 162 @ 218525 updates, score 2.193) (writing took 2.247681980021298 seconds)
2024-01-31 06:19:42 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2024-01-31 06:19:42 | INFO | train | epoch 162 | loss 1.781 | nll_loss 0.185 | ppl 1.14 | wps 57740.7 | ups 14.73 | wpb 3921.2 | bsz 214.5 | num_updates 218525 | lr 6.76471e-05 | gnorm 0.506 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 5761
2024-01-31 06:19:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:19:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:19:42 | INFO | fairseq.trainer | begin training epoch 163
2024-01-31 06:19:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:19:47 | INFO | train_inner | epoch 163:     75 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=35308.5, ups=9.01, wpb=3920.3, bsz=219.1, num_updates=218600, lr=6.76355e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=5766
2024-01-31 06:19:53 | INFO | train_inner | epoch 163:    175 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61111.1, ups=15.51, wpb=3941.1, bsz=213.5, num_updates=218700, lr=6.76201e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=5772
2024-01-31 06:20:00 | INFO | train_inner | epoch 163:    275 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60837.5, ups=15.48, wpb=3929.8, bsz=209.1, num_updates=218800, lr=6.76046e-05, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.8, wall=5779
2024-01-31 06:20:06 | INFO | train_inner | epoch 163:    375 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61407.5, ups=15.56, wpb=3946.4, bsz=217.3, num_updates=218900, lr=6.75892e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=5785
2024-01-31 06:20:13 | INFO | train_inner | epoch 163:    475 / 1349 loss=1.78, nll_loss=0.183, ppl=1.13, wps=61101.7, ups=15.6, wpb=3917.2, bsz=205.5, num_updates=219000, lr=6.75737e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=5792
2024-01-31 06:20:19 | INFO | train_inner | epoch 163:    575 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=61110.3, ups=15.59, wpb=3920.4, bsz=211.1, num_updates=219100, lr=6.75583e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.7, wall=5798
2024-01-31 06:20:25 | INFO | train_inner | epoch 163:    675 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60911.9, ups=15.54, wpb=3920.5, bsz=218.4, num_updates=219200, lr=6.75429e-05, gnorm=0.459, loss_scale=8, train_wall=6, gb_free=29.8, wall=5804
2024-01-31 06:20:32 | INFO | train_inner | epoch 163:    775 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60742.2, ups=15.57, wpb=3901.2, bsz=208, num_updates=219300, lr=6.75275e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=5811
2024-01-31 06:20:38 | INFO | train_inner | epoch 163:    875 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60102.9, ups=15.43, wpb=3895.8, bsz=218.6, num_updates=219400, lr=6.75121e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=5817
2024-01-31 06:20:45 | INFO | train_inner | epoch 163:    975 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61408.4, ups=15.59, wpb=3939.3, bsz=209, num_updates=219500, lr=6.74967e-05, gnorm=0.455, loss_scale=8, train_wall=6, gb_free=29.8, wall=5824
2024-01-31 06:20:51 | INFO | train_inner | epoch 163:   1075 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=59905.2, ups=15.53, wpb=3856.7, bsz=218.9, num_updates=219600, lr=6.74814e-05, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.8, wall=5830
2024-01-31 06:20:58 | INFO | train_inner | epoch 163:   1175 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61100.3, ups=15.48, wpb=3947.6, bsz=213.2, num_updates=219700, lr=6.7466e-05, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.7, wall=5837
2024-01-31 06:21:04 | INFO | train_inner | epoch 163:   1275 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60650.7, ups=15.44, wpb=3927.2, bsz=217.6, num_updates=219800, lr=6.74507e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.7, wall=5843
2024-01-31 06:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:21:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:21:11 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 2.195 | nll_loss 0.452 | ppl 1.37 | wps 132842 | wpb 3275.2 | bsz 176.5 | num_updates 219874 | best_loss 2.063
2024-01-31 06:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 219874 updates
2024-01-31 06:21:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint163.pt
2024-01-31 06:21:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint163.pt
2024-01-31 06:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint163.pt (epoch 163 @ 219874 updates, score 2.195) (writing took 2.301012062933296 seconds)
2024-01-31 06:21:13 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2024-01-31 06:21:13 | INFO | train | epoch 163 | loss 1.781 | nll_loss 0.184 | ppl 1.14 | wps 57731.8 | ups 14.72 | wpb 3921.2 | bsz 214.5 | num_updates 219874 | lr 6.74393e-05 | gnorm 0.511 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 5852
2024-01-31 06:21:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:21:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:21:14 | INFO | fairseq.trainer | begin training epoch 164
2024-01-31 06:21:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:21:15 | INFO | train_inner | epoch 164:     26 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=34881.8, ups=8.97, wpb=3887.7, bsz=218.9, num_updates=219900, lr=6.74353e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=5854
2024-01-31 06:21:22 | INFO | train_inner | epoch 164:    126 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60505.8, ups=15.6, wpb=3878.7, bsz=214.6, num_updates=220000, lr=6.742e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=5861
2024-01-31 06:21:28 | INFO | train_inner | epoch 164:    226 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=61021, ups=15.55, wpb=3925.1, bsz=205.8, num_updates=220100, lr=6.74047e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=5867
2024-01-31 06:21:35 | INFO | train_inner | epoch 164:    326 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60470.2, ups=15.43, wpb=3918.7, bsz=220.9, num_updates=220200, lr=6.73894e-05, gnorm=0.574, loss_scale=8, train_wall=6, gb_free=29.8, wall=5874
2024-01-31 06:21:41 | INFO | train_inner | epoch 164:    426 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=61549.9, ups=15.47, wpb=3979.1, bsz=215.9, num_updates=220300, lr=6.73741e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=5880
2024-01-31 06:21:48 | INFO | train_inner | epoch 164:    526 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61047.3, ups=15.52, wpb=3934.5, bsz=220.2, num_updates=220400, lr=6.73588e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.7, wall=5886
2024-01-31 06:21:54 | INFO | train_inner | epoch 164:    626 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61196.4, ups=15.52, wpb=3943.7, bsz=209, num_updates=220500, lr=6.73435e-05, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.7, wall=5893
2024-01-31 06:22:00 | INFO | train_inner | epoch 164:    726 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61205.2, ups=15.6, wpb=3922.6, bsz=212.6, num_updates=220600, lr=6.73282e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=5899
2024-01-31 06:22:07 | INFO | train_inner | epoch 164:    826 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60634.3, ups=15.47, wpb=3918.8, bsz=212.7, num_updates=220700, lr=6.7313e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.8, wall=5906
2024-01-31 06:22:13 | INFO | train_inner | epoch 164:    926 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60516.8, ups=15.55, wpb=3892.5, bsz=210.6, num_updates=220800, lr=6.72977e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.8, wall=5912
2024-01-31 06:22:20 | INFO | train_inner | epoch 164:   1026 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60582.2, ups=15.44, wpb=3923.7, bsz=226.9, num_updates=220900, lr=6.72825e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.7, wall=5919
2024-01-31 06:22:26 | INFO | train_inner | epoch 164:   1126 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61838.5, ups=15.57, wpb=3971.3, bsz=207.4, num_updates=221000, lr=6.72673e-05, gnorm=0.594, loss_scale=8, train_wall=6, gb_free=29.8, wall=5925
2024-01-31 06:22:33 | INFO | train_inner | epoch 164:   1226 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60487, ups=15.52, wpb=3896.7, bsz=208.7, num_updates=221100, lr=6.72521e-05, gnorm=0.542, loss_scale=8, train_wall=6, gb_free=29.8, wall=5932
2024-01-31 06:22:39 | INFO | train_inner | epoch 164:   1326 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60989.3, ups=15.49, wpb=3937.7, bsz=228.2, num_updates=221200, lr=6.72369e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=5938
2024-01-31 06:22:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:22:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:22:43 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 2.191 | nll_loss 0.449 | ppl 1.36 | wps 130816 | wpb 3275.2 | bsz 176.5 | num_updates 221223 | best_loss 2.063
2024-01-31 06:22:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 221223 updates
2024-01-31 06:22:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint164.pt
2024-01-31 06:22:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint164.pt
2024-01-31 06:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint164.pt (epoch 164 @ 221223 updates, score 2.191) (writing took 2.312765541020781 seconds)
2024-01-31 06:22:45 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2024-01-31 06:22:45 | INFO | train | epoch 164 | loss 1.781 | nll_loss 0.184 | ppl 1.14 | wps 57694.4 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 221223 | lr 6.72334e-05 | gnorm 0.517 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 5944
2024-01-31 06:22:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:22:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:22:45 | INFO | fairseq.trainer | begin training epoch 165
2024-01-31 06:22:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:22:50 | INFO | train_inner | epoch 165:     77 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=34938.1, ups=8.96, wpb=3900.9, bsz=196.5, num_updates=221300, lr=6.72217e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=5949
2024-01-31 06:22:57 | INFO | train_inner | epoch 165:    177 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=61127.7, ups=15.58, wpb=3922.6, bsz=206.3, num_updates=221400, lr=6.72065e-05, gnorm=0.588, loss_scale=8, train_wall=6, gb_free=29.8, wall=5956
2024-01-31 06:23:03 | INFO | train_inner | epoch 165:    277 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=60930, ups=15.36, wpb=3967, bsz=226.4, num_updates=221500, lr=6.71913e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=5962
2024-01-31 06:23:10 | INFO | train_inner | epoch 165:    377 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60374, ups=15.53, wpb=3888.3, bsz=213.4, num_updates=221600, lr=6.71762e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=5969
2024-01-31 06:23:16 | INFO | train_inner | epoch 165:    477 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=60970.8, ups=15.55, wpb=3920.8, bsz=205.5, num_updates=221700, lr=6.7161e-05, gnorm=0.569, loss_scale=8, train_wall=6, gb_free=29.7, wall=5975
2024-01-31 06:23:23 | INFO | train_inner | epoch 165:    577 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60603.6, ups=15.48, wpb=3913.9, bsz=211.1, num_updates=221800, lr=6.71459e-05, gnorm=0.563, loss_scale=8, train_wall=6, gb_free=29.7, wall=5981
2024-01-31 06:23:29 | INFO | train_inner | epoch 165:    677 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60248.6, ups=15.47, wpb=3893.3, bsz=229.8, num_updates=221900, lr=6.71307e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=5988
2024-01-31 06:23:35 | INFO | train_inner | epoch 165:    777 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60928.3, ups=15.55, wpb=3918.5, bsz=212.1, num_updates=222000, lr=6.71156e-05, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.9, wall=5994
2024-01-31 06:23:42 | INFO | train_inner | epoch 165:    877 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60568.2, ups=15.54, wpb=3897.5, bsz=214.4, num_updates=222100, lr=6.71005e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=6001
2024-01-31 06:23:48 | INFO | train_inner | epoch 165:    977 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=60922.7, ups=15.45, wpb=3943, bsz=214.4, num_updates=222200, lr=6.70854e-05, gnorm=0.468, loss_scale=8, train_wall=6, gb_free=29.8, wall=6007
2024-01-31 06:23:55 | INFO | train_inner | epoch 165:   1077 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60138.2, ups=15.49, wpb=3881.7, bsz=227.1, num_updates=222300, lr=6.70703e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.8, wall=6014
2024-01-31 06:24:01 | INFO | train_inner | epoch 165:   1177 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61069.6, ups=15.51, wpb=3936.7, bsz=216.2, num_updates=222400, lr=6.70552e-05, gnorm=0.588, loss_scale=8, train_wall=6, gb_free=29.8, wall=6020
2024-01-31 06:24:08 | INFO | train_inner | epoch 165:   1277 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61556, ups=15.49, wpb=3975, bsz=220.2, num_updates=222500, lr=6.70402e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=6027
2024-01-31 06:24:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:24:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:24:14 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 2.196 | nll_loss 0.456 | ppl 1.37 | wps 132441 | wpb 3275.2 | bsz 176.5 | num_updates 222572 | best_loss 2.063
2024-01-31 06:24:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 222572 updates
2024-01-31 06:24:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint165.pt
2024-01-31 06:24:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint165.pt
2024-01-31 06:24:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint165.pt (epoch 165 @ 222572 updates, score 2.196) (writing took 2.3319994390476495 seconds)
2024-01-31 06:24:17 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2024-01-31 06:24:17 | INFO | train | epoch 165 | loss 1.781 | nll_loss 0.184 | ppl 1.14 | wps 57677.7 | ups 14.71 | wpb 3921.2 | bsz 214.5 | num_updates 222572 | lr 6.70293e-05 | gnorm 0.527 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 6036
2024-01-31 06:24:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:24:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:24:17 | INFO | fairseq.trainer | begin training epoch 166
2024-01-31 06:24:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:24:19 | INFO | train_inner | epoch 166:     28 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=35077.1, ups=9.01, wpb=3892.6, bsz=197.4, num_updates=222600, lr=6.70251e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.8, wall=6038
2024-01-31 06:24:25 | INFO | train_inner | epoch 166:    128 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61034.7, ups=15.48, wpb=3942.3, bsz=217.4, num_updates=222700, lr=6.701e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.9, wall=6044
2024-01-31 06:24:32 | INFO | train_inner | epoch 166:    228 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60932.3, ups=15.53, wpb=3923, bsz=207.6, num_updates=222800, lr=6.6995e-05, gnorm=0.556, loss_scale=8, train_wall=6, gb_free=29.7, wall=6051
2024-01-31 06:24:38 | INFO | train_inner | epoch 166:    328 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60504, ups=15.45, wpb=3916.8, bsz=216.6, num_updates=222900, lr=6.698e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=6057
2024-01-31 06:24:45 | INFO | train_inner | epoch 166:    428 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60262.7, ups=15.54, wpb=3877.3, bsz=224.7, num_updates=223000, lr=6.6965e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=6063
2024-01-31 06:24:51 | INFO | train_inner | epoch 166:    528 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60400.9, ups=15.43, wpb=3915.1, bsz=226.3, num_updates=223100, lr=6.69499e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=6070
2024-01-31 06:24:58 | INFO | train_inner | epoch 166:    628 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61252.4, ups=15.51, wpb=3950, bsz=214.5, num_updates=223200, lr=6.69349e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=6076
2024-01-31 06:25:04 | INFO | train_inner | epoch 166:    728 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=60220.9, ups=15.37, wpb=3917.4, bsz=221.5, num_updates=223300, lr=6.692e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=6083
2024-01-31 06:25:10 | INFO | train_inner | epoch 166:    828 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=61187.8, ups=15.43, wpb=3966.5, bsz=213, num_updates=223400, lr=6.6905e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.7, wall=6089
2024-01-31 06:25:17 | INFO | train_inner | epoch 166:    928 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60905.2, ups=15.6, wpb=3903.2, bsz=201.4, num_updates=223500, lr=6.689e-05, gnorm=0.565, loss_scale=8, train_wall=6, gb_free=29.8, wall=6096
2024-01-31 06:25:23 | INFO | train_inner | epoch 166:   1028 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60577.7, ups=15.48, wpb=3914, bsz=214.5, num_updates=223600, lr=6.6875e-05, gnorm=0.467, loss_scale=8, train_wall=6, gb_free=29.8, wall=6102
2024-01-31 06:25:30 | INFO | train_inner | epoch 166:   1128 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=60753.3, ups=15.53, wpb=3912.6, bsz=219.4, num_updates=223700, lr=6.68601e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.7, wall=6109
2024-01-31 06:25:36 | INFO | train_inner | epoch 166:   1228 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60874.7, ups=15.49, wpb=3930.5, bsz=206.9, num_updates=223800, lr=6.68452e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=6115
2024-01-31 06:25:43 | INFO | train_inner | epoch 166:   1328 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60577.4, ups=15.52, wpb=3902.7, bsz=217.2, num_updates=223900, lr=6.68302e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.7, wall=6122
2024-01-31 06:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:25:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:25:46 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 2.193 | nll_loss 0.453 | ppl 1.37 | wps 132471 | wpb 3275.2 | bsz 176.5 | num_updates 223921 | best_loss 2.063
2024-01-31 06:25:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 223921 updates
2024-01-31 06:25:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint166.pt
2024-01-31 06:25:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint166.pt
2024-01-31 06:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint166.pt (epoch 166 @ 223921 updates, score 2.193) (writing took 2.3239463379140943 seconds)
2024-01-31 06:25:49 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2024-01-31 06:25:49 | INFO | train | epoch 166 | loss 1.781 | nll_loss 0.184 | ppl 1.14 | wps 57658.4 | ups 14.7 | wpb 3921.2 | bsz 214.5 | num_updates 223921 | lr 6.68271e-05 | gnorm 0.514 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 6128
2024-01-31 06:25:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:25:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:25:49 | INFO | fairseq.trainer | begin training epoch 167
2024-01-31 06:25:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:25:54 | INFO | train_inner | epoch 167:     79 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=35511.7, ups=9.02, wpb=3938, bsz=202.4, num_updates=224000, lr=6.68153e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=6133
2024-01-31 06:26:00 | INFO | train_inner | epoch 167:    179 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=60814.2, ups=15.49, wpb=3926.2, bsz=208.9, num_updates=224100, lr=6.68004e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.7, wall=6139
2024-01-31 06:26:07 | INFO | train_inner | epoch 167:    279 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60156.6, ups=15.51, wpb=3878.3, bsz=215.8, num_updates=224200, lr=6.67855e-05, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.8, wall=6146
2024-01-31 06:26:13 | INFO | train_inner | epoch 167:    379 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60962.2, ups=15.52, wpb=3926.8, bsz=202.2, num_updates=224300, lr=6.67706e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.7, wall=6152
2024-01-31 06:26:20 | INFO | train_inner | epoch 167:    479 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60643.3, ups=15.52, wpb=3907.2, bsz=218, num_updates=224400, lr=6.67557e-05, gnorm=0.468, loss_scale=8, train_wall=6, gb_free=29.8, wall=6158
2024-01-31 06:26:26 | INFO | train_inner | epoch 167:    579 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60152.4, ups=15.58, wpb=3860.9, bsz=211.6, num_updates=224500, lr=6.67409e-05, gnorm=0.564, loss_scale=8, train_wall=6, gb_free=29.8, wall=6165
2024-01-31 06:26:32 | INFO | train_inner | epoch 167:    679 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61169.7, ups=15.44, wpb=3961.7, bsz=218.2, num_updates=224600, lr=6.6726e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=6171
2024-01-31 06:26:39 | INFO | train_inner | epoch 167:    779 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60282.3, ups=15.5, wpb=3889.6, bsz=219.6, num_updates=224700, lr=6.67112e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.8, wall=6178
2024-01-31 06:26:45 | INFO | train_inner | epoch 167:    879 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=60032.1, ups=15.46, wpb=3884, bsz=222.9, num_updates=224800, lr=6.66963e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=6184
2024-01-31 06:26:52 | INFO | train_inner | epoch 167:    979 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61273, ups=15.55, wpb=3939.4, bsz=208.2, num_updates=224900, lr=6.66815e-05, gnorm=0.564, loss_scale=8, train_wall=6, gb_free=29.8, wall=6191
2024-01-31 06:26:58 | INFO | train_inner | epoch 167:   1079 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61568.1, ups=15.54, wpb=3963, bsz=228.2, num_updates=225000, lr=6.66667e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=6197
2024-01-31 06:27:05 | INFO | train_inner | epoch 167:   1179 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=61306.9, ups=15.5, wpb=3954.4, bsz=220.7, num_updates=225100, lr=6.66519e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=6204
2024-01-31 06:27:11 | INFO | train_inner | epoch 167:   1279 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=61034.7, ups=15.49, wpb=3939.5, bsz=213.3, num_updates=225200, lr=6.66371e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=6210
2024-01-31 06:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:27:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:27:18 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 2.195 | nll_loss 0.454 | ppl 1.37 | wps 133280 | wpb 3275.2 | bsz 176.5 | num_updates 225270 | best_loss 2.063
2024-01-31 06:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 225270 updates
2024-01-31 06:27:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint167.pt
2024-01-31 06:27:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint167.pt
2024-01-31 06:27:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint167.pt (epoch 167 @ 225270 updates, score 2.195) (writing took 2.2928476210217923 seconds)
2024-01-31 06:27:20 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2024-01-31 06:27:20 | INFO | train | epoch 167 | loss 1.781 | nll_loss 0.184 | ppl 1.14 | wps 57752.6 | ups 14.73 | wpb 3921.2 | bsz 214.5 | num_updates 225270 | lr 6.66267e-05 | gnorm 0.504 | loss_scale 8 | train_wall 84 | gb_free 29.7 | wall 6219
2024-01-31 06:27:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:27:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:27:20 | INFO | fairseq.trainer | begin training epoch 168
2024-01-31 06:27:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:27:22 | INFO | train_inner | epoch 168:     30 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=35425.4, ups=9.02, wpb=3927.5, bsz=209, num_updates=225300, lr=6.66223e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=30, wall=6221
2024-01-31 06:27:29 | INFO | train_inner | epoch 168:    130 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=61902.2, ups=15.59, wpb=3971.8, bsz=224.9, num_updates=225400, lr=6.66075e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=6228
2024-01-31 06:27:35 | INFO | train_inner | epoch 168:    230 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60851.2, ups=15.53, wpb=3917.6, bsz=203.4, num_updates=225500, lr=6.65927e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=6234
2024-01-31 06:27:42 | INFO | train_inner | epoch 168:    330 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60816.7, ups=15.54, wpb=3914.8, bsz=209.5, num_updates=225600, lr=6.6578e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.8, wall=6240
2024-01-31 06:27:48 | INFO | train_inner | epoch 168:    430 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=60878.7, ups=15.53, wpb=3919.6, bsz=218.5, num_updates=225700, lr=6.65632e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=6247
2024-01-31 06:27:54 | INFO | train_inner | epoch 168:    530 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61645.3, ups=15.46, wpb=3986.3, bsz=228.3, num_updates=225800, lr=6.65485e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=6253
2024-01-31 06:28:01 | INFO | train_inner | epoch 168:    630 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60858, ups=15.56, wpb=3911.3, bsz=209.8, num_updates=225900, lr=6.65337e-05, gnorm=0.575, loss_scale=8, train_wall=6, gb_free=29.8, wall=6260
2024-01-31 06:28:07 | INFO | train_inner | epoch 168:    730 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=60815.9, ups=15.49, wpb=3925.1, bsz=219.4, num_updates=226000, lr=6.6519e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.7, wall=6266
2024-01-31 06:28:14 | INFO | train_inner | epoch 168:    830 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61039.5, ups=15.59, wpb=3916.4, bsz=213.6, num_updates=226100, lr=6.65043e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.7, wall=6273
2024-01-31 06:28:20 | INFO | train_inner | epoch 168:    930 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60963.4, ups=15.61, wpb=3905.6, bsz=209, num_updates=226200, lr=6.64896e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=6279
2024-01-31 06:28:27 | INFO | train_inner | epoch 168:   1030 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60547.6, ups=15.52, wpb=3902.2, bsz=216.8, num_updates=226300, lr=6.64749e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=6286
2024-01-31 06:28:33 | INFO | train_inner | epoch 168:   1130 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60514.9, ups=15.61, wpb=3877.5, bsz=205.4, num_updates=226400, lr=6.64602e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.7, wall=6292
2024-01-31 06:28:39 | INFO | train_inner | epoch 168:   1230 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=60922.6, ups=15.49, wpb=3932.2, bsz=223.1, num_updates=226500, lr=6.64455e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.7, wall=6298
2024-01-31 06:28:46 | INFO | train_inner | epoch 168:   1330 / 1349 loss=1.785, nll_loss=0.189, ppl=1.14, wps=60897.4, ups=15.64, wpb=3892.8, bsz=213.8, num_updates=226600, lr=6.64309e-05, gnorm=0.616, loss_scale=8, train_wall=6, gb_free=29.7, wall=6305
2024-01-31 06:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:28:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:28:49 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 2.194 | nll_loss 0.452 | ppl 1.37 | wps 132952 | wpb 3275.2 | bsz 176.5 | num_updates 226619 | best_loss 2.063
2024-01-31 06:28:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 226619 updates
2024-01-31 06:28:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint168.pt
2024-01-31 06:28:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint168.pt
2024-01-31 06:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint168.pt (epoch 168 @ 226619 updates, score 2.194) (writing took 2.3238650480052456 seconds)
2024-01-31 06:28:52 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2024-01-31 06:28:52 | INFO | train | epoch 168 | loss 1.78 | nll_loss 0.184 | ppl 1.14 | wps 57848.3 | ups 14.75 | wpb 3921.2 | bsz 214.5 | num_updates 226619 | lr 6.64281e-05 | gnorm 0.524 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 6311
2024-01-31 06:28:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:28:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:28:52 | INFO | fairseq.trainer | begin training epoch 169
2024-01-31 06:28:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:28:57 | INFO | train_inner | epoch 169:     81 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=35269.6, ups=9.02, wpb=3908.2, bsz=199.4, num_updates=226700, lr=6.64162e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.7, wall=6316
2024-01-31 06:29:03 | INFO | train_inner | epoch 169:    181 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61149.4, ups=15.47, wpb=3953.2, bsz=223.5, num_updates=226800, lr=6.64016e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.8, wall=6322
2024-01-31 06:29:10 | INFO | train_inner | epoch 169:    281 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=61229.3, ups=15.49, wpb=3952.3, bsz=210.2, num_updates=226900, lr=6.6387e-05, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.9, wall=6329
2024-01-31 06:29:16 | INFO | train_inner | epoch 169:    381 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60796.1, ups=15.54, wpb=3912.9, bsz=217.1, num_updates=227000, lr=6.63723e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.8, wall=6335
2024-01-31 06:29:23 | INFO | train_inner | epoch 169:    481 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61025, ups=15.59, wpb=3915.5, bsz=197, num_updates=227100, lr=6.63577e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.8, wall=6342
2024-01-31 06:29:29 | INFO | train_inner | epoch 169:    581 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=60869.3, ups=15.61, wpb=3899, bsz=213.1, num_updates=227200, lr=6.63431e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=6348
2024-01-31 06:29:36 | INFO | train_inner | epoch 169:    681 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61019.5, ups=15.41, wpb=3960, bsz=227.9, num_updates=227300, lr=6.63285e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.7, wall=6355
2024-01-31 06:29:42 | INFO | train_inner | epoch 169:    781 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61037.1, ups=15.47, wpb=3946.7, bsz=221.3, num_updates=227400, lr=6.63139e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=6361
2024-01-31 06:29:48 | INFO | train_inner | epoch 169:    881 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61206.9, ups=15.62, wpb=3917.6, bsz=204.5, num_updates=227500, lr=6.62994e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=6367
2024-01-31 06:29:55 | INFO | train_inner | epoch 169:    981 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=61019, ups=15.6, wpb=3911, bsz=217.9, num_updates=227600, lr=6.62848e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=6374
2024-01-31 06:30:01 | INFO | train_inner | epoch 169:   1081 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60499, ups=15.61, wpb=3874.5, bsz=212.3, num_updates=227700, lr=6.62702e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=6380
2024-01-31 06:30:08 | INFO | train_inner | epoch 169:   1181 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=59862.1, ups=15.5, wpb=3861.9, bsz=222.5, num_updates=227800, lr=6.62557e-05, gnorm=0.538, loss_scale=8, train_wall=6, gb_free=29.8, wall=6387
2024-01-31 06:30:14 | INFO | train_inner | epoch 169:   1281 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61363.8, ups=15.52, wpb=3953.1, bsz=218.6, num_updates=227900, lr=6.62411e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.8, wall=6393
2024-01-31 06:30:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:30:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:30:21 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 2.193 | nll_loss 0.453 | ppl 1.37 | wps 126981 | wpb 3275.2 | bsz 176.5 | num_updates 227968 | best_loss 2.063
2024-01-31 06:30:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 227968 updates
2024-01-31 06:30:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint169.pt
2024-01-31 06:30:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint169.pt
2024-01-31 06:30:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint169.pt (epoch 169 @ 227968 updates, score 2.193) (writing took 2.2749826830113307 seconds)
2024-01-31 06:30:23 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2024-01-31 06:30:23 | INFO | train | epoch 169 | loss 1.78 | nll_loss 0.184 | ppl 1.14 | wps 57796 | ups 14.74 | wpb 3921.2 | bsz 214.5 | num_updates 227968 | lr 6.62313e-05 | gnorm 0.509 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 6402
2024-01-31 06:30:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:30:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:30:23 | INFO | fairseq.trainer | begin training epoch 170
2024-01-31 06:30:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:30:25 | INFO | train_inner | epoch 170:     32 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=35452, ups=8.96, wpb=3956.1, bsz=209.9, num_updates=228000, lr=6.62266e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.8, wall=6404
2024-01-31 06:30:32 | INFO | train_inner | epoch 170:    132 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=61109.9, ups=15.51, wpb=3940.2, bsz=213.5, num_updates=228100, lr=6.62121e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=6411
2024-01-31 06:30:38 | INFO | train_inner | epoch 170:    232 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60901.6, ups=15.58, wpb=3907.9, bsz=208.8, num_updates=228200, lr=6.61976e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=6417
2024-01-31 06:30:45 | INFO | train_inner | epoch 170:    332 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=61488, ups=15.5, wpb=3966.9, bsz=220.2, num_updates=228300, lr=6.61831e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=6424
2024-01-31 06:30:51 | INFO | train_inner | epoch 170:    432 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60473.3, ups=15.55, wpb=3888.8, bsz=213, num_updates=228400, lr=6.61686e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.7, wall=6430
2024-01-31 06:30:58 | INFO | train_inner | epoch 170:    532 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61006.5, ups=15.49, wpb=3937.9, bsz=210.5, num_updates=228500, lr=6.61541e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=6436
2024-01-31 06:31:04 | INFO | train_inner | epoch 170:    632 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=61477.2, ups=15.51, wpb=3964.9, bsz=215.6, num_updates=228600, lr=6.61396e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.8, wall=6443
2024-01-31 06:31:10 | INFO | train_inner | epoch 170:    732 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=61192.4, ups=15.55, wpb=3935.6, bsz=221, num_updates=228700, lr=6.61252e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=6449
2024-01-31 06:31:17 | INFO | train_inner | epoch 170:    832 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60781.8, ups=15.54, wpb=3911.4, bsz=228.6, num_updates=228800, lr=6.61107e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=6456
2024-01-31 06:31:23 | INFO | train_inner | epoch 170:    932 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=60034.2, ups=15.45, wpb=3886.4, bsz=227.3, num_updates=228900, lr=6.60963e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.8, wall=6462
2024-01-31 06:31:30 | INFO | train_inner | epoch 170:   1032 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=61122.3, ups=15.7, wpb=3893, bsz=206.6, num_updates=229000, lr=6.60819e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.9, wall=6469
2024-01-31 06:31:36 | INFO | train_inner | epoch 170:   1132 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61286.1, ups=15.58, wpb=3933.6, bsz=211.2, num_updates=229100, lr=6.60674e-05, gnorm=0.558, loss_scale=8, train_wall=6, gb_free=29.7, wall=6475
2024-01-31 06:31:43 | INFO | train_inner | epoch 170:   1232 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=61246, ups=15.68, wpb=3905.4, bsz=207.8, num_updates=229200, lr=6.6053e-05, gnorm=0.589, loss_scale=8, train_wall=6, gb_free=29.7, wall=6481
2024-01-31 06:31:49 | INFO | train_inner | epoch 170:   1332 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=60434, ups=15.55, wpb=3886.7, bsz=210.7, num_updates=229300, lr=6.60386e-05, gnorm=0.562, loss_scale=8, train_wall=6, gb_free=29.8, wall=6488
2024-01-31 06:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:31:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:31:52 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 2.196 | nll_loss 0.453 | ppl 1.37 | wps 132253 | wpb 3275.2 | bsz 176.5 | num_updates 229317 | best_loss 2.063
2024-01-31 06:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 229317 updates
2024-01-31 06:31:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint170.pt
2024-01-31 06:31:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint170.pt
2024-01-31 06:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint170.pt (epoch 170 @ 229317 updates, score 2.196) (writing took 2.3165223490213975 seconds)
2024-01-31 06:31:55 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2024-01-31 06:31:55 | INFO | train | epoch 170 | loss 1.78 | nll_loss 0.183 | ppl 1.14 | wps 57852 | ups 14.75 | wpb 3921.2 | bsz 214.5 | num_updates 229317 | lr 6.60362e-05 | gnorm 0.52 | loss_scale 8 | train_wall 84 | gb_free 29.8 | wall 6494
2024-01-31 06:31:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:31:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:31:55 | INFO | fairseq.trainer | begin training epoch 171
2024-01-31 06:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:32:00 | INFO | train_inner | epoch 171:     83 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=35701.9, ups=8.99, wpb=3970.3, bsz=204.1, num_updates=229400, lr=6.60242e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.9, wall=6499
2024-01-31 06:32:06 | INFO | train_inner | epoch 171:    183 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=60624.3, ups=15.54, wpb=3900.6, bsz=210.4, num_updates=229500, lr=6.60098e-05, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.8, wall=6505
2024-01-31 06:32:13 | INFO | train_inner | epoch 171:    283 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=60303.7, ups=15.44, wpb=3906.1, bsz=220.4, num_updates=229600, lr=6.59955e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=6512
2024-01-31 06:32:19 | INFO | train_inner | epoch 171:    383 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=60860.2, ups=15.54, wpb=3916.6, bsz=223.7, num_updates=229700, lr=6.59811e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=6518
2024-01-31 06:32:26 | INFO | train_inner | epoch 171:    483 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=60926.3, ups=15.57, wpb=3912.1, bsz=203, num_updates=229800, lr=6.59667e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.7, wall=6525
2024-01-31 06:32:32 | INFO | train_inner | epoch 171:    583 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60990, ups=15.55, wpb=3921.9, bsz=217.8, num_updates=229900, lr=6.59524e-05, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.8, wall=6531
2024-01-31 06:32:39 | INFO | train_inner | epoch 171:    683 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61318, ups=15.56, wpb=3940.1, bsz=202.6, num_updates=230000, lr=6.5938e-05, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.9, wall=6538
2024-01-31 06:32:45 | INFO | train_inner | epoch 171:    783 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=60656.3, ups=15.4, wpb=3939.9, bsz=230.8, num_updates=230100, lr=6.59237e-05, gnorm=0.581, loss_scale=8, train_wall=6, gb_free=29.8, wall=6544
2024-01-31 06:32:52 | INFO | train_inner | epoch 171:    883 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=60564.5, ups=15.57, wpb=3890, bsz=226.7, num_updates=230200, lr=6.59094e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=6551
2024-01-31 06:32:58 | INFO | train_inner | epoch 171:    983 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=61841.7, ups=15.58, wpb=3969.3, bsz=212.5, num_updates=230300, lr=6.58951e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=6557
2024-01-31 06:33:04 | INFO | train_inner | epoch 171:   1083 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=60770, ups=15.52, wpb=3914.7, bsz=223.4, num_updates=230400, lr=6.58808e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=6563
2024-01-31 06:33:11 | INFO | train_inner | epoch 171:   1183 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=61070.7, ups=15.71, wpb=3887.8, bsz=195.4, num_updates=230500, lr=6.58665e-05, gnorm=0.559, loss_scale=8, train_wall=6, gb_free=29.8, wall=6570
2024-01-31 06:33:17 | INFO | train_inner | epoch 171:   1283 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60184.6, ups=15.5, wpb=3882.6, bsz=216.9, num_updates=230600, lr=6.58522e-05, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=29.8, wall=6576
2024-01-31 06:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:33:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:33:24 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 2.198 | nll_loss 0.458 | ppl 1.37 | wps 132647 | wpb 3275.2 | bsz 176.5 | num_updates 230666 | best_loss 2.063
2024-01-31 06:33:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 230666 updates
2024-01-31 06:33:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint171.pt
2024-01-31 06:33:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint171.pt
2024-01-31 06:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint171.pt (epoch 171 @ 230666 updates, score 2.198) (writing took 2.2916326879058033 seconds)
2024-01-31 06:33:26 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2024-01-31 06:33:26 | INFO | train | epoch 171 | loss 1.78 | nll_loss 0.183 | ppl 1.14 | wps 57843.7 | ups 14.75 | wpb 3921.2 | bsz 214.5 | num_updates 230666 | lr 6.58428e-05 | gnorm 0.516 | loss_scale 8 | train_wall 84 | gb_free 29.9 | wall 6585
2024-01-31 06:33:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:33:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 06:33:26 | INFO | fairseq.trainer | begin training epoch 172
2024-01-31 06:33:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:33:28 | INFO | train_inner | epoch 172:     34 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=35587.4, ups=9.02, wpb=3946.2, bsz=215.4, num_updates=230700, lr=6.58379e-05, gnorm=0.558, loss_scale=8, train_wall=6, gb_free=29.7, wall=6587
2024-01-31 06:33:35 | INFO | train_inner | epoch 172:    134 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=61492, ups=15.61, wpb=3939.7, bsz=207, num_updates=230800, lr=6.58237e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=6594
2024-01-31 06:33:41 | INFO | train_inner | epoch 172:    234 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=61480.5, ups=15.54, wpb=3955.1, bsz=214.3, num_updates=230900, lr=6.58094e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.8, wall=6600
2024-01-31 06:33:48 | INFO | train_inner | epoch 172:    334 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=60734.5, ups=15.45, wpb=3931.4, bsz=219.4, num_updates=231000, lr=6.57952e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=6607
2024-01-31 06:33:54 | INFO | train_inner | epoch 172:    434 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=60959.4, ups=15.54, wpb=3923, bsz=218.5, num_updates=231100, lr=6.57809e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=6613
2024-01-31 06:34:01 | INFO | train_inner | epoch 172:    534 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=60930.6, ups=15.58, wpb=3912, bsz=216.6, num_updates=231200, lr=6.57667e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=6619
2024-01-31 06:34:07 | INFO | train_inner | epoch 172:    634 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=61105.5, ups=15.59, wpb=3919.4, bsz=197.2, num_updates=231300, lr=6.57525e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.8, wall=6626
2024-01-31 06:34:13 | INFO | train_inner | epoch 172:    734 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=60724.9, ups=15.51, wpb=3915.9, bsz=216.3, num_updates=231400, lr=6.57383e-05, gnorm=0.483, loss_scale=16, train_wall=6, gb_free=29.7, wall=6632
2024-01-31 06:34:20 | INFO | train_inner | epoch 172:    834 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=60550, ups=15.59, wpb=3882.9, bsz=203.3, num_updates=231500, lr=6.57241e-05, gnorm=0.519, loss_scale=16, train_wall=6, gb_free=29.8, wall=6639
2024-01-31 06:34:26 | INFO | train_inner | epoch 172:    934 / 1349 loss=1.784, nll_loss=0.188, ppl=1.14, wps=61129.1, ups=15.6, wpb=3919.3, bsz=207.3, num_updates=231600, lr=6.57099e-05, gnorm=0.566, loss_scale=16, train_wall=6, gb_free=29.8, wall=6645
2024-01-31 06:34:33 | INFO | train_inner | epoch 172:   1034 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=60813.8, ups=15.42, wpb=3943.3, bsz=242.2, num_updates=231700, lr=6.56957e-05, gnorm=0.452, loss_scale=16, train_wall=6, gb_free=29.8, wall=6652
2024-01-31 06:34:39 | INFO | train_inner | epoch 172:   1134 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=60081.9, ups=15.65, wpb=3838, bsz=205.9, num_updates=231800, lr=6.56815e-05, gnorm=0.537, loss_scale=16, train_wall=6, gb_free=29.7, wall=6658
2024-01-31 06:34:46 | INFO | train_inner | epoch 172:   1234 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=61186.5, ups=15.6, wpb=3923.2, bsz=213, num_updates=231900, lr=6.56674e-05, gnorm=0.625, loss_scale=16, train_wall=6, gb_free=29.7, wall=6664
2024-01-31 06:34:52 | INFO | train_inner | epoch 172:   1334 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=61476.2, ups=15.47, wpb=3974.3, bsz=227.9, num_updates=232000, lr=6.56532e-05, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=6671
2024-01-31 06:34:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:34:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:34:55 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 2.197 | nll_loss 0.455 | ppl 1.37 | wps 133261 | wpb 3275.2 | bsz 176.5 | num_updates 232015 | best_loss 2.063
2024-01-31 06:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 232015 updates
2024-01-31 06:34:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint172.pt
slurmstepd: error: *** JOB 23095537 ON uc2n482 CANCELLED AT 2024-01-31T06:34:55 DUE TO TIME LIMIT ***

============================= JOB FEEDBACK =============================

NodeName=uc2n482
Job ID: 23095537
Cluster: uc2
User/Group: uxude/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 2
CPU Utilized: 00:10:38
CPU Efficiency: 1.36% of 13:00:04 core-walltime
Job Wall-clock time: 06:30:02
Memory Utilized: 2.17 GB
Memory Efficiency: 1.11% of 195.31 GB
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[2024-01-31 10:12:25] [INFO] [Dataset::Prepare Datasets]: Skipping dataset preparation, config file and MT spm data already exists
Training the Punctuation model...
rm: cannot remove '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/preprocess.log': No such file or directory
Binarized data already exists. Skipping binarization.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models
Training time: 6 hours
2024-01-31 10:12:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=50000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-01-31 10:12:46 | INFO | fairseq.tasks.translation | [en] dictionary: 4504 types
2024-01-31 10:12:46 | INFO | fairseq.tasks.translation | [de] dictionary: 6520 types
2024-01-31 10:12:49 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4504, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6520, bias=False)
  )
)
2024-01-31 10:12:49 | INFO | fairseq_cli.train | task: TranslationTask
2024-01-31 10:12:49 | INFO | fairseq_cli.train | model: TransformerModel
2024-01-31 10:12:49 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-31 10:12:49 | INFO | fairseq_cli.train | num. shared model params: 49,782,784 (num. trained: 49,782,784)
2024-01-31 10:12:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-31 10:12:49 | INFO | fairseq.data.data_utils | loaded 15,530 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/valid.en-de.en
2024-01-31 10:12:49 | INFO | fairseq.data.data_utils | loaded 15,530 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/valid.en-de.de
2024-01-31 10:12:49 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset valid en-de 15530 examples
2024-01-31 10:12:50 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-31 10:12:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 10:12:50 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-01-31 10:12:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 10:12:50 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-31 10:12:50 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-01-31 10:12:50 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt
2024-01-31 10:12:54 | INFO | fairseq.trainer | Loaded checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt (epoch 172 @ 230666 updates)
2024-01-31 10:12:54 | INFO | fairseq.trainer | loading train data for epoch 172
2024-01-31 10:12:54 | INFO | fairseq.data.data_utils | loaded 289,365 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/train.en-de.en
2024-01-31 10:12:54 | INFO | fairseq.data.data_utils | loaded 289,365 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/train.en-de.de
2024-01-31 10:12:54 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset train en-de 289365 examples
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 172
2024-01-31 10:12:54 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 10:12:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-31 10:12:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:12:57 | INFO | fairseq.trainer | begin training epoch 172
2024-01-31 10:12:57 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-01-31 10:13:06 | INFO | train_inner | epoch 172:     34 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=48719.2, ups=12.43, wpb=3921.9, bsz=225.2, num_updates=230700, lr=6.58379e-05, gnorm=0.49, loss_scale=8, train_wall=8, gb_free=29.7, wall=16
2024-01-31 10:13:13 | INFO | train_inner | epoch 172:    134 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=55931.3, ups=14.2, wpb=3939.7, bsz=207, num_updates=230800, lr=6.58237e-05, gnorm=0.518, loss_scale=8, train_wall=7, gb_free=29.8, wall=23
2024-01-31 10:13:20 | INFO | train_inner | epoch 172:    234 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=55464, ups=14.02, wpb=3955.1, bsz=214.3, num_updates=230900, lr=6.58094e-05, gnorm=0.572, loss_scale=8, train_wall=7, gb_free=29.8, wall=30
2024-01-31 10:13:27 | INFO | train_inner | epoch 172:    334 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54681, ups=13.91, wpb=3931.4, bsz=219.4, num_updates=231000, lr=6.57952e-05, gnorm=0.496, loss_scale=8, train_wall=7, gb_free=29.8, wall=37
2024-01-31 10:13:34 | INFO | train_inner | epoch 172:    434 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54779.5, ups=13.96, wpb=3923, bsz=218.5, num_updates=231100, lr=6.57809e-05, gnorm=0.494, loss_scale=8, train_wall=7, gb_free=29.8, wall=45
2024-01-31 10:13:41 | INFO | train_inner | epoch 172:    534 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55279, ups=14.13, wpb=3912, bsz=216.6, num_updates=231200, lr=6.57667e-05, gnorm=0.527, loss_scale=8, train_wall=7, gb_free=29.8, wall=52
2024-01-31 10:13:48 | INFO | train_inner | epoch 172:    634 / 1349 loss=1.783, nll_loss=0.186, ppl=1.14, wps=54938.4, ups=14.02, wpb=3919.4, bsz=197.2, num_updates=231300, lr=6.57525e-05, gnorm=0.62, loss_scale=8, train_wall=7, gb_free=29.8, wall=59
2024-01-31 10:13:56 | INFO | train_inner | epoch 172:    734 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=54689.9, ups=13.97, wpb=3915.9, bsz=216.3, num_updates=231400, lr=6.57383e-05, gnorm=0.544, loss_scale=8, train_wall=7, gb_free=29.7, wall=66
2024-01-31 10:14:03 | INFO | train_inner | epoch 172:    834 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=55101, ups=14.19, wpb=3882.9, bsz=203.3, num_updates=231500, lr=6.57241e-05, gnorm=0.525, loss_scale=8, train_wall=7, gb_free=29.8, wall=73
2024-01-31 10:14:10 | INFO | train_inner | epoch 172:    934 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=55009.5, ups=14.04, wpb=3919.3, bsz=207.3, num_updates=231600, lr=6.57099e-05, gnorm=0.516, loss_scale=8, train_wall=7, gb_free=29.8, wall=80
2024-01-31 10:14:17 | INFO | train_inner | epoch 172:   1034 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=54638.9, ups=13.86, wpb=3943.3, bsz=242.2, num_updates=231700, lr=6.56957e-05, gnorm=0.505, loss_scale=8, train_wall=7, gb_free=29.8, wall=87
2024-01-31 10:14:24 | INFO | train_inner | epoch 172:   1134 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54185, ups=14.12, wpb=3838, bsz=205.9, num_updates=231800, lr=6.56815e-05, gnorm=0.512, loss_scale=8, train_wall=7, gb_free=29.7, wall=94
2024-01-31 10:14:31 | INFO | train_inner | epoch 172:   1234 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=56081.1, ups=14.29, wpb=3923.2, bsz=213, num_updates=231900, lr=6.56674e-05, gnorm=0.53, loss_scale=8, train_wall=7, gb_free=29.7, wall=101
2024-01-31 10:14:38 | INFO | train_inner | epoch 172:   1334 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=55345.2, ups=13.93, wpb=3974.3, bsz=227.9, num_updates=232000, lr=6.56532e-05, gnorm=0.458, loss_scale=8, train_wall=7, gb_free=29.8, wall=109
2024-01-31 10:14:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:14:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:14:42 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 2.195 | nll_loss 0.453 | ppl 1.37 | wps 114023 | wpb 3275.2 | bsz 176.5 | num_updates 232015 | best_loss 2.063
2024-01-31 10:14:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 232015 updates
2024-01-31 10:14:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint172.pt
2024-01-31 10:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint172.pt
2024-01-31 10:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint172.pt (epoch 172 @ 232015 updates, score 2.195) (writing took 2.857376971980557 seconds)
2024-01-31 10:14:45 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2024-01-31 10:14:45 | INFO | train | epoch 172 | loss 1.78 | nll_loss 0.183 | ppl 1.14 | wps 52012.8 | ups 13.26 | wpb 3921.2 | bsz 214.5 | num_updates 232015 | lr 6.56511e-05 | gnorm 0.524 | loss_scale 8 | train_wall 98 | gb_free 29.7 | wall 115
2024-01-31 10:14:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:14:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:14:45 | INFO | fairseq.trainer | begin training epoch 173
2024-01-31 10:14:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:14:51 | INFO | train_inner | epoch 173:     85 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=31463.5, ups=7.96, wpb=3951.7, bsz=210.2, num_updates=232100, lr=6.56391e-05, gnorm=0.498, loss_scale=8, train_wall=7, gb_free=29.7, wall=121
2024-01-31 10:14:58 | INFO | train_inner | epoch 173:    185 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55074.2, ups=14.01, wpb=3930, bsz=204.4, num_updates=232200, lr=6.56249e-05, gnorm=0.546, loss_scale=8, train_wall=7, gb_free=29.7, wall=128
2024-01-31 10:15:05 | INFO | train_inner | epoch 173:    285 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=55365.4, ups=14.06, wpb=3939.1, bsz=206.6, num_updates=232300, lr=6.56108e-05, gnorm=0.473, loss_scale=8, train_wall=7, gb_free=29.8, wall=135
2024-01-31 10:15:12 | INFO | train_inner | epoch 173:    385 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54856.6, ups=14.22, wpb=3856.7, bsz=207.5, num_updates=232400, lr=6.55967e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.8, wall=142
2024-01-31 10:15:19 | INFO | train_inner | epoch 173:    485 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55028.6, ups=13.97, wpb=3938.4, bsz=211.8, num_updates=232500, lr=6.55826e-05, gnorm=0.452, loss_scale=8, train_wall=7, gb_free=29.7, wall=150
2024-01-31 10:15:27 | INFO | train_inner | epoch 173:    585 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53933.6, ups=13.68, wpb=3943.4, bsz=243.8, num_updates=232600, lr=6.55685e-05, gnorm=0.482, loss_scale=8, train_wall=7, gb_free=29.8, wall=157
2024-01-31 10:15:34 | INFO | train_inner | epoch 173:    685 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=55176.3, ups=13.94, wpb=3956.7, bsz=222.8, num_updates=232700, lr=6.55544e-05, gnorm=0.542, loss_scale=8, train_wall=7, gb_free=29.7, wall=164
2024-01-31 10:15:41 | INFO | train_inner | epoch 173:    785 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54468.7, ups=14.05, wpb=3876.4, bsz=232.5, num_updates=232800, lr=6.55403e-05, gnorm=0.544, loss_scale=8, train_wall=7, gb_free=29.8, wall=171
2024-01-31 10:15:48 | INFO | train_inner | epoch 173:    885 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=55232.8, ups=13.97, wpb=3953.2, bsz=198.8, num_updates=232900, lr=6.55262e-05, gnorm=0.477, loss_scale=8, train_wall=7, gb_free=29.7, wall=178
2024-01-31 10:15:55 | INFO | train_inner | epoch 173:    985 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=53994.9, ups=13.9, wpb=3884.5, bsz=201, num_updates=233000, lr=6.55122e-05, gnorm=0.509, loss_scale=8, train_wall=7, gb_free=29.7, wall=185
2024-01-31 10:16:02 | INFO | train_inner | epoch 173:   1085 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55533.5, ups=14.24, wpb=3898.7, bsz=220.6, num_updates=233100, lr=6.54981e-05, gnorm=0.485, loss_scale=8, train_wall=7, gb_free=29.8, wall=193
2024-01-31 10:16:09 | INFO | train_inner | epoch 173:   1185 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=55287, ups=14.11, wpb=3917.4, bsz=214.6, num_updates=233200, lr=6.54841e-05, gnorm=0.533, loss_scale=8, train_wall=7, gb_free=29.8, wall=200
2024-01-31 10:16:16 | INFO | train_inner | epoch 173:   1285 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=55107.3, ups=14.04, wpb=3924.7, bsz=206.8, num_updates=233300, lr=6.547e-05, gnorm=0.554, loss_scale=8, train_wall=7, gb_free=29.7, wall=207
2024-01-31 10:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:16:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:16:24 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 2.198 | nll_loss 0.457 | ppl 1.37 | wps 111106 | wpb 3275.2 | bsz 176.5 | num_updates 233364 | best_loss 2.063
2024-01-31 10:16:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 233364 updates
2024-01-31 10:16:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint173.pt
2024-01-31 10:16:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint173.pt
2024-01-31 10:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint173.pt (epoch 173 @ 233364 updates, score 2.198) (writing took 2.470289993972983 seconds)
2024-01-31 10:16:26 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2024-01-31 10:16:26 | INFO | train | epoch 173 | loss 1.78 | nll_loss 0.183 | ppl 1.14 | wps 52186.6 | ups 13.31 | wpb 3921.2 | bsz 214.5 | num_updates 233364 | lr 6.54611e-05 | gnorm 0.511 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 216
2024-01-31 10:16:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:16:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:16:26 | INFO | fairseq.trainer | begin training epoch 174
2024-01-31 10:16:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:16:29 | INFO | train_inner | epoch 174:     36 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=31702.2, ups=8.11, wpb=3908.2, bsz=217.2, num_updates=233400, lr=6.5456e-05, gnorm=0.55, loss_scale=8, train_wall=7, gb_free=29.8, wall=219
2024-01-31 10:16:36 | INFO | train_inner | epoch 174:    136 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54880, ups=13.93, wpb=3940.7, bsz=226.4, num_updates=233500, lr=6.5442e-05, gnorm=0.525, loss_scale=8, train_wall=7, gb_free=29.8, wall=226
2024-01-31 10:16:43 | INFO | train_inner | epoch 174:    236 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55322.4, ups=13.98, wpb=3957.9, bsz=231.8, num_updates=233600, lr=6.5428e-05, gnorm=0.551, loss_scale=8, train_wall=7, gb_free=29.8, wall=233
2024-01-31 10:16:50 | INFO | train_inner | epoch 174:    336 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55090, ups=13.99, wpb=3937.6, bsz=223.7, num_updates=233700, lr=6.5414e-05, gnorm=0.527, loss_scale=8, train_wall=7, gb_free=29.8, wall=241
2024-01-31 10:16:57 | INFO | train_inner | epoch 174:    436 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54979.1, ups=14.02, wpb=3920.7, bsz=215.5, num_updates=233800, lr=6.54e-05, gnorm=0.526, loss_scale=8, train_wall=7, gb_free=29.7, wall=248
2024-01-31 10:17:05 | INFO | train_inner | epoch 174:    536 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54118.7, ups=14.01, wpb=3861.9, bsz=208.2, num_updates=233900, lr=6.5386e-05, gnorm=0.456, loss_scale=8, train_wall=7, gb_free=29.7, wall=255
2024-01-31 10:17:12 | INFO | train_inner | epoch 174:    636 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=55678.5, ups=14.25, wpb=3908.3, bsz=209.1, num_updates=234000, lr=6.5372e-05, gnorm=0.54, loss_scale=8, train_wall=7, gb_free=29.8, wall=262
2024-01-31 10:17:19 | INFO | train_inner | epoch 174:    736 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55132.8, ups=14.05, wpb=3925.2, bsz=204.9, num_updates=234100, lr=6.53581e-05, gnorm=0.56, loss_scale=8, train_wall=7, gb_free=29.8, wall=269
2024-01-31 10:17:26 | INFO | train_inner | epoch 174:    836 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55278.1, ups=13.97, wpb=3956.2, bsz=217.1, num_updates=234200, lr=6.53441e-05, gnorm=0.515, loss_scale=8, train_wall=7, gb_free=29.9, wall=276
2024-01-31 10:17:33 | INFO | train_inner | epoch 174:    936 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55400.9, ups=14.05, wpb=3944.2, bsz=208.6, num_updates=234300, lr=6.53302e-05, gnorm=0.497, loss_scale=8, train_wall=7, gb_free=29.7, wall=283
2024-01-31 10:17:40 | INFO | train_inner | epoch 174:   1036 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54737.8, ups=13.94, wpb=3926.7, bsz=220.4, num_updates=234400, lr=6.53162e-05, gnorm=0.514, loss_scale=8, train_wall=7, gb_free=29.8, wall=290
2024-01-31 10:17:47 | INFO | train_inner | epoch 174:   1136 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=54014.5, ups=14.05, wpb=3843.6, bsz=209.8, num_updates=234500, lr=6.53023e-05, gnorm=0.572, loss_scale=8, train_wall=7, gb_free=29.8, wall=298
2024-01-31 10:17:54 | INFO | train_inner | epoch 174:   1236 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54820.6, ups=13.88, wpb=3949.5, bsz=214.6, num_updates=234600, lr=6.52884e-05, gnorm=0.456, loss_scale=8, train_wall=7, gb_free=29.7, wall=305
2024-01-31 10:18:01 | INFO | train_inner | epoch 174:   1336 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=55787, ups=14.26, wpb=3913.2, bsz=199, num_updates=234700, lr=6.52745e-05, gnorm=0.604, loss_scale=8, train_wall=7, gb_free=29.7, wall=312
2024-01-31 10:18:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:18:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:18:05 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 2.198 | nll_loss 0.456 | ppl 1.37 | wps 107018 | wpb 3275.2 | bsz 176.5 | num_updates 234713 | best_loss 2.063
2024-01-31 10:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 234713 updates
2024-01-31 10:18:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint174.pt
2024-01-31 10:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint174.pt
2024-01-31 10:18:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint174.pt (epoch 174 @ 234713 updates, score 2.198) (writing took 2.4718164560035802 seconds)
2024-01-31 10:18:08 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2024-01-31 10:18:08 | INFO | train | epoch 174 | loss 1.779 | nll_loss 0.183 | ppl 1.14 | wps 52116.2 | ups 13.29 | wpb 3921.2 | bsz 214.5 | num_updates 234713 | lr 6.52727e-05 | gnorm 0.527 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 318
2024-01-31 10:18:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:18:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:18:08 | INFO | fairseq.trainer | begin training epoch 175
2024-01-31 10:18:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:18:14 | INFO | train_inner | epoch 175:     87 / 1349 loss=1.777, nll_loss=0.18, ppl=1.13, wps=31355.4, ups=7.99, wpb=3925.2, bsz=216.7, num_updates=234800, lr=6.52606e-05, gnorm=0.521, loss_scale=8, train_wall=7, gb_free=29.7, wall=324
2024-01-31 10:18:21 | INFO | train_inner | epoch 175:    187 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54975.4, ups=14.1, wpb=3899.6, bsz=228.6, num_updates=234900, lr=6.52467e-05, gnorm=0.517, loss_scale=8, train_wall=7, gb_free=29.8, wall=331
2024-01-31 10:18:28 | INFO | train_inner | epoch 175:    287 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55174.5, ups=13.97, wpb=3948.4, bsz=214.8, num_updates=235000, lr=6.52328e-05, gnorm=0.528, loss_scale=8, train_wall=7, gb_free=29.8, wall=339
2024-01-31 10:18:35 | INFO | train_inner | epoch 175:    387 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55167.7, ups=13.99, wpb=3944.1, bsz=217.7, num_updates=235100, lr=6.52189e-05, gnorm=0.541, loss_scale=8, train_wall=7, gb_free=29.8, wall=346
2024-01-31 10:18:42 | INFO | train_inner | epoch 175:    487 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=54771.4, ups=14.17, wpb=3865.9, bsz=213.6, num_updates=235200, lr=6.52051e-05, gnorm=0.522, loss_scale=8, train_wall=7, gb_free=29.8, wall=353
2024-01-31 10:18:50 | INFO | train_inner | epoch 175:    587 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54292.4, ups=14.01, wpb=3874.6, bsz=205.7, num_updates=235300, lr=6.51912e-05, gnorm=0.578, loss_scale=8, train_wall=7, gb_free=29.8, wall=360
2024-01-31 10:18:57 | INFO | train_inner | epoch 175:    687 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=54993.4, ups=13.91, wpb=3952.4, bsz=213, num_updates=235400, lr=6.51774e-05, gnorm=0.468, loss_scale=8, train_wall=7, gb_free=29.8, wall=367
2024-01-31 10:19:04 | INFO | train_inner | epoch 175:    787 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54376.1, ups=13.9, wpb=3911, bsz=233, num_updates=235500, lr=6.51635e-05, gnorm=0.491, loss_scale=8, train_wall=7, gb_free=29.8, wall=374
2024-01-31 10:19:11 | INFO | train_inner | epoch 175:    887 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=56242.5, ups=14.17, wpb=3967.9, bsz=202.8, num_updates=235600, lr=6.51497e-05, gnorm=0.527, loss_scale=8, train_wall=7, gb_free=29.8, wall=381
2024-01-31 10:19:18 | INFO | train_inner | epoch 175:    987 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=54648.2, ups=14.04, wpb=3893.6, bsz=204.8, num_updates=235700, lr=6.51359e-05, gnorm=0.543, loss_scale=8, train_wall=7, gb_free=29.8, wall=388
2024-01-31 10:19:25 | INFO | train_inner | epoch 175:   1087 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54292.9, ups=13.98, wpb=3884.8, bsz=211.7, num_updates=235800, lr=6.51221e-05, gnorm=0.547, loss_scale=8, train_wall=7, gb_free=29.8, wall=396
2024-01-31 10:19:32 | INFO | train_inner | epoch 175:   1187 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=56083.7, ups=14.19, wpb=3951.2, bsz=218.1, num_updates=235900, lr=6.51083e-05, gnorm=0.528, loss_scale=8, train_wall=7, gb_free=29.7, wall=403
2024-01-31 10:19:39 | INFO | train_inner | epoch 175:   1287 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55442.2, ups=14.1, wpb=3932.4, bsz=213.3, num_updates=236000, lr=6.50945e-05, gnorm=0.518, loss_scale=8, train_wall=7, gb_free=29.8, wall=410
2024-01-31 10:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:19:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:19:46 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 2.2 | nll_loss 0.458 | ppl 1.37 | wps 111905 | wpb 3275.2 | bsz 176.5 | num_updates 236062 | best_loss 2.063
2024-01-31 10:19:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 236062 updates
2024-01-31 10:19:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint175.pt
2024-01-31 10:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint175.pt
2024-01-31 10:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint175.pt (epoch 175 @ 236062 updates, score 2.2) (writing took 2.410994814010337 seconds)
2024-01-31 10:19:49 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2024-01-31 10:19:49 | INFO | train | epoch 175 | loss 1.78 | nll_loss 0.183 | ppl 1.14 | wps 52215.2 | ups 13.32 | wpb 3921.2 | bsz 214.5 | num_updates 236062 | lr 6.50859e-05 | gnorm 0.526 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 419
2024-01-31 10:19:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:19:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:19:49 | INFO | fairseq.trainer | begin training epoch 176
2024-01-31 10:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:19:52 | INFO | train_inner | epoch 176:     38 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=32319.9, ups=8.19, wpb=3946.2, bsz=205.4, num_updates=236100, lr=6.50807e-05, gnorm=0.498, loss_scale=8, train_wall=7, gb_free=29.9, wall=422
2024-01-31 10:19:59 | INFO | train_inner | epoch 176:    138 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53138.5, ups=13.92, wpb=3818.7, bsz=209.3, num_updates=236200, lr=6.50669e-05, gnorm=0.533, loss_scale=8, train_wall=7, gb_free=29.8, wall=429
2024-01-31 10:20:06 | INFO | train_inner | epoch 176:    238 / 1349 loss=1.782, nll_loss=0.185, ppl=1.14, wps=55035.3, ups=14.05, wpb=3917.9, bsz=210.2, num_updates=236300, lr=6.50531e-05, gnorm=0.483, loss_scale=8, train_wall=7, gb_free=29.8, wall=436
2024-01-31 10:20:33 | INFO | train_inner | epoch 176:    338 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=14837.9, ups=3.75, wpb=3954.6, bsz=200.6, num_updates=236400, lr=6.50394e-05, gnorm=0.53, loss_scale=8, train_wall=8, gb_free=29.8, wall=463
2024-01-31 10:20:40 | INFO | train_inner | epoch 176:    438 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53926.4, ups=13.71, wpb=3934.3, bsz=221.7, num_updates=236500, lr=6.50256e-05, gnorm=0.527, loss_scale=8, train_wall=7, gb_free=29.8, wall=470
2024-01-31 10:20:47 | INFO | train_inner | epoch 176:    538 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54204.1, ups=13.83, wpb=3918.4, bsz=222.1, num_updates=236600, lr=6.50119e-05, gnorm=0.556, loss_scale=8, train_wall=7, gb_free=29.8, wall=477
2024-01-31 10:20:54 | INFO | train_inner | epoch 176:    638 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54058.6, ups=13.87, wpb=3896.2, bsz=220.3, num_updates=236700, lr=6.49981e-05, gnorm=0.451, loss_scale=8, train_wall=7, gb_free=29.7, wall=485
2024-01-31 10:21:01 | INFO | train_inner | epoch 176:    738 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55206, ups=13.99, wpb=3945.7, bsz=245.8, num_updates=236800, lr=6.49844e-05, gnorm=0.503, loss_scale=8, train_wall=7, gb_free=29.8, wall=492
2024-01-31 10:21:09 | INFO | train_inner | epoch 176:    838 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=50257.6, ups=12.67, wpb=3966.1, bsz=208.6, num_updates=236900, lr=6.49707e-05, gnorm=0.531, loss_scale=8, train_wall=8, gb_free=29.7, wall=500
2024-01-31 10:21:17 | INFO | train_inner | epoch 176:    938 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=54219, ups=13.89, wpb=3902.1, bsz=207.9, num_updates=237000, lr=6.4957e-05, gnorm=0.516, loss_scale=8, train_wall=7, gb_free=29.8, wall=507
2024-01-31 10:21:24 | INFO | train_inner | epoch 176:   1038 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55757, ups=14.06, wpb=3964.8, bsz=220.4, num_updates=237100, lr=6.49433e-05, gnorm=0.49, loss_scale=8, train_wall=7, gb_free=29.8, wall=514
2024-01-31 10:21:31 | INFO | train_inner | epoch 176:   1138 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55733.2, ups=14.17, wpb=3932.9, bsz=200.6, num_updates=237200, lr=6.49296e-05, gnorm=0.56, loss_scale=8, train_wall=7, gb_free=29.8, wall=521
2024-01-31 10:21:38 | INFO | train_inner | epoch 176:   1238 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=52264.6, ups=13.37, wpb=3908.5, bsz=206.1, num_updates=237300, lr=6.49159e-05, gnorm=0.612, loss_scale=8, train_wall=7, gb_free=29.8, wall=528
2024-01-31 10:21:46 | INFO | train_inner | epoch 176:   1338 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53657.8, ups=13.71, wpb=3913.7, bsz=217, num_updates=237400, lr=6.49022e-05, gnorm=0.561, loss_scale=8, train_wall=7, gb_free=29.8, wall=536
2024-01-31 10:21:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:21:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:21:49 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 2.198 | nll_loss 0.456 | ppl 1.37 | wps 115596 | wpb 3275.2 | bsz 176.5 | num_updates 237411 | best_loss 2.063
2024-01-31 10:21:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 237411 updates
2024-01-31 10:21:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint176.pt
2024-01-31 10:21:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint176.pt
2024-01-31 10:21:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint176.pt (epoch 176 @ 237411 updates, score 2.198) (writing took 2.503768255992327 seconds)
2024-01-31 10:21:51 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2024-01-31 10:21:51 | INFO | train | epoch 176 | loss 1.779 | nll_loss 0.183 | ppl 1.14 | wps 43174.9 | ups 11.01 | wpb 3921.2 | bsz 214.5 | num_updates 237411 | lr 6.49007e-05 | gnorm 0.525 | loss_scale 8 | train_wall 95 | gb_free 29.7 | wall 542
2024-01-31 10:21:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:21:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:21:51 | INFO | fairseq.trainer | begin training epoch 177
2024-01-31 10:21:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:21:59 | INFO | train_inner | epoch 177:     89 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=30053, ups=7.58, wpb=3965.2, bsz=219, num_updates=237500, lr=6.48886e-05, gnorm=0.543, loss_scale=8, train_wall=8, gb_free=29.7, wall=549
2024-01-31 10:22:06 | INFO | train_inner | epoch 177:    189 / 1349 loss=1.777, nll_loss=0.18, ppl=1.13, wps=54389.2, ups=13.8, wpb=3940.5, bsz=210.4, num_updates=237600, lr=6.48749e-05, gnorm=0.429, loss_scale=8, train_wall=7, gb_free=29.8, wall=556
2024-01-31 10:22:13 | INFO | train_inner | epoch 177:    289 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=55771.2, ups=14.23, wpb=3920.2, bsz=210.5, num_updates=237700, lr=6.48613e-05, gnorm=0.53, loss_scale=8, train_wall=7, gb_free=29.8, wall=563
2024-01-31 10:22:20 | INFO | train_inner | epoch 177:    389 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55336.6, ups=13.98, wpb=3958.4, bsz=224.3, num_updates=237800, lr=6.48476e-05, gnorm=0.485, loss_scale=8, train_wall=7, gb_free=29.7, wall=570
2024-01-31 10:22:27 | INFO | train_inner | epoch 177:    489 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53791, ups=13.83, wpb=3888.7, bsz=229, num_updates=237900, lr=6.4834e-05, gnorm=0.481, loss_scale=8, train_wall=7, gb_free=29.8, wall=578
2024-01-31 10:22:35 | INFO | train_inner | epoch 177:    589 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=55378.9, ups=14.04, wpb=3943.7, bsz=204.1, num_updates=238000, lr=6.48204e-05, gnorm=0.548, loss_scale=8, train_wall=7, gb_free=29.7, wall=585
2024-01-31 10:22:42 | INFO | train_inner | epoch 177:    689 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55624.6, ups=14.23, wpb=3908, bsz=215, num_updates=238100, lr=6.48068e-05, gnorm=0.528, loss_scale=8, train_wall=7, gb_free=29.7, wall=592
2024-01-31 10:22:49 | INFO | train_inner | epoch 177:    789 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=51774, ups=13.27, wpb=3902.9, bsz=219.9, num_updates=238200, lr=6.47932e-05, gnorm=0.488, loss_scale=8, train_wall=7, gb_free=29.8, wall=599
2024-01-31 10:22:57 | INFO | train_inner | epoch 177:    889 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53030.8, ups=13.47, wpb=3936.8, bsz=219.9, num_updates=238300, lr=6.47796e-05, gnorm=0.525, loss_scale=8, train_wall=7, gb_free=29.8, wall=607
2024-01-31 10:23:04 | INFO | train_inner | epoch 177:    989 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=54898.2, ups=13.99, wpb=3923.4, bsz=196.7, num_updates=238400, lr=6.4766e-05, gnorm=0.573, loss_scale=8, train_wall=7, gb_free=29.8, wall=614
2024-01-31 10:23:11 | INFO | train_inner | epoch 177:   1089 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54731.8, ups=14.07, wpb=3891.2, bsz=217.7, num_updates=238500, lr=6.47524e-05, gnorm=0.526, loss_scale=8, train_wall=7, gb_free=29.7, wall=621
2024-01-31 10:23:18 | INFO | train_inner | epoch 177:   1189 / 1349 loss=1.783, nll_loss=0.187, ppl=1.14, wps=54249.2, ups=13.92, wpb=3898.2, bsz=201.8, num_updates=238600, lr=6.47388e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.9, wall=628
2024-01-31 10:23:25 | INFO | train_inner | epoch 177:   1289 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54425, ups=13.94, wpb=3904.9, bsz=209.1, num_updates=238700, lr=6.47253e-05, gnorm=0.563, loss_scale=8, train_wall=7, gb_free=29.8, wall=635
2024-01-31 10:23:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:23:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:23:32 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 2.197 | nll_loss 0.456 | ppl 1.37 | wps 116160 | wpb 3275.2 | bsz 176.5 | num_updates 238760 | best_loss 2.063
2024-01-31 10:23:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 238760 updates
2024-01-31 10:23:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint177.pt
2024-01-31 10:23:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint177.pt
2024-01-31 10:23:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint177.pt (epoch 177 @ 238760 updates, score 2.197) (writing took 2.514758592005819 seconds)
2024-01-31 10:23:34 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2024-01-31 10:23:34 | INFO | train | epoch 177 | loss 1.779 | nll_loss 0.183 | ppl 1.14 | wps 51345.9 | ups 13.09 | wpb 3921.2 | bsz 214.5 | num_updates 238760 | lr 6.47171e-05 | gnorm 0.519 | loss_scale 8 | train_wall 94 | gb_free 29.8 | wall 645
2024-01-31 10:23:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:23:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:23:34 | INFO | fairseq.trainer | begin training epoch 178
2024-01-31 10:23:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:23:37 | INFO | train_inner | epoch 178:     40 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=32239, ups=8.16, wpb=3951.2, bsz=236, num_updates=238800, lr=6.47117e-05, gnorm=0.444, loss_scale=8, train_wall=7, gb_free=29.9, wall=648
2024-01-31 10:23:45 | INFO | train_inner | epoch 178:    140 / 1349 loss=1.776, nll_loss=0.179, ppl=1.13, wps=55066.2, ups=14.01, wpb=3931.5, bsz=229.4, num_updates=238900, lr=6.46982e-05, gnorm=0.479, loss_scale=8, train_wall=7, gb_free=29.9, wall=655
2024-01-31 10:23:52 | INFO | train_inner | epoch 178:    240 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55608.4, ups=14.08, wpb=3950.8, bsz=220.7, num_updates=239000, lr=6.46846e-05, gnorm=0.495, loss_scale=8, train_wall=7, gb_free=29.7, wall=662
2024-01-31 10:23:59 | INFO | train_inner | epoch 178:    340 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54057, ups=13.84, wpb=3905.4, bsz=210.4, num_updates=239100, lr=6.46711e-05, gnorm=0.517, loss_scale=8, train_wall=7, gb_free=29.7, wall=669
2024-01-31 10:24:06 | INFO | train_inner | epoch 178:    440 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54827.8, ups=13.89, wpb=3946.1, bsz=209.4, num_updates=239200, lr=6.46576e-05, gnorm=0.555, loss_scale=8, train_wall=7, gb_free=29.7, wall=676
2024-01-31 10:24:13 | INFO | train_inner | epoch 178:    540 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54871.7, ups=14.13, wpb=3884, bsz=204.4, num_updates=239300, lr=6.46441e-05, gnorm=0.558, loss_scale=8, train_wall=7, gb_free=29.9, wall=683
2024-01-31 10:24:21 | INFO | train_inner | epoch 178:    640 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=50216.4, ups=12.88, wpb=3899.8, bsz=216.2, num_updates=239400, lr=6.46306e-05, gnorm=0.476, loss_scale=8, train_wall=7, gb_free=29.8, wall=691
2024-01-31 10:24:28 | INFO | train_inner | epoch 178:    740 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=54312.8, ups=13.92, wpb=3902.8, bsz=202.9, num_updates=239500, lr=6.46171e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.7, wall=698
2024-01-31 10:24:35 | INFO | train_inner | epoch 178:    840 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=54850, ups=13.98, wpb=3924.2, bsz=208.1, num_updates=239600, lr=6.46036e-05, gnorm=0.476, loss_scale=8, train_wall=7, gb_free=29.8, wall=706
2024-01-31 10:24:42 | INFO | train_inner | epoch 178:    940 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55819.2, ups=14.24, wpb=3920.2, bsz=222.1, num_updates=239700, lr=6.45901e-05, gnorm=0.517, loss_scale=8, train_wall=7, gb_free=29.8, wall=713
2024-01-31 10:24:49 | INFO | train_inner | epoch 178:   1040 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54920.4, ups=13.92, wpb=3945.1, bsz=217.1, num_updates=239800, lr=6.45766e-05, gnorm=0.576, loss_scale=8, train_wall=7, gb_free=29.9, wall=720
2024-01-31 10:24:57 | INFO | train_inner | epoch 178:   1140 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=53770.1, ups=14.08, wpb=3819.8, bsz=206.9, num_updates=239900, lr=6.45632e-05, gnorm=0.54, loss_scale=8, train_wall=7, gb_free=29.8, wall=727
2024-01-31 10:25:04 | INFO | train_inner | epoch 178:   1240 / 1349 loss=1.781, nll_loss=0.184, ppl=1.14, wps=56174.9, ups=14.21, wpb=3954.5, bsz=208.9, num_updates=240000, lr=6.45497e-05, gnorm=0.616, loss_scale=8, train_wall=7, gb_free=29.7, wall=734
2024-01-31 10:25:11 | INFO | train_inner | epoch 178:   1340 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=55544.6, ups=14.05, wpb=3952, bsz=218.4, num_updates=240100, lr=6.45363e-05, gnorm=0.513, loss_scale=8, train_wall=7, gb_free=29.7, wall=741
2024-01-31 10:25:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:25:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:25:14 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 2.195 | nll_loss 0.451 | ppl 1.37 | wps 104512 | wpb 3275.2 | bsz 176.5 | num_updates 240109 | best_loss 2.063
2024-01-31 10:25:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 240109 updates
2024-01-31 10:25:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint178.pt
2024-01-31 10:25:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint178.pt
2024-01-31 10:25:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint178.pt (epoch 178 @ 240109 updates, score 2.195) (writing took 2.5092883619945496 seconds)
2024-01-31 10:25:17 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2024-01-31 10:25:17 | INFO | train | epoch 178 | loss 1.779 | nll_loss 0.183 | ppl 1.14 | wps 51722.3 | ups 13.19 | wpb 3921.2 | bsz 214.5 | num_updates 240109 | lr 6.45351e-05 | gnorm 0.526 | loss_scale 8 | train_wall 93 | gb_free 29.7 | wall 747
2024-01-31 10:25:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:25:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:25:17 | INFO | fairseq.trainer | begin training epoch 179
2024-01-31 10:25:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:25:24 | INFO | train_inner | epoch 179:     91 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=29524, ups=7.51, wpb=3929, bsz=215.3, num_updates=240200, lr=6.45228e-05, gnorm=0.518, loss_scale=8, train_wall=7, gb_free=29.8, wall=754
2024-01-31 10:25:31 | INFO | train_inner | epoch 179:    191 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=55437.6, ups=14.08, wpb=3938.6, bsz=205, num_updates=240300, lr=6.45094e-05, gnorm=0.612, loss_scale=8, train_wall=7, gb_free=29.8, wall=761
2024-01-31 10:25:38 | INFO | train_inner | epoch 179:    291 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53948.4, ups=13.93, wpb=3872.5, bsz=208.5, num_updates=240400, lr=6.4496e-05, gnorm=0.508, loss_scale=8, train_wall=7, gb_free=29.8, wall=769
2024-01-31 10:25:45 | INFO | train_inner | epoch 179:    391 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54959.9, ups=14, wpb=3926, bsz=211.4, num_updates=240500, lr=6.44826e-05, gnorm=0.519, loss_scale=8, train_wall=7, gb_free=29.8, wall=776
2024-01-31 10:25:53 | INFO | train_inner | epoch 179:    491 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=56042.3, ups=14.18, wpb=3952.1, bsz=228.6, num_updates=240600, lr=6.44692e-05, gnorm=0.475, loss_scale=8, train_wall=7, gb_free=29.8, wall=783
2024-01-31 10:26:00 | INFO | train_inner | epoch 179:    591 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53671.3, ups=13.88, wpb=3867.9, bsz=216.6, num_updates=240700, lr=6.44558e-05, gnorm=0.567, loss_scale=8, train_wall=7, gb_free=29.9, wall=790
2024-01-31 10:26:07 | INFO | train_inner | epoch 179:    691 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54514.9, ups=13.92, wpb=3916.4, bsz=226, num_updates=240800, lr=6.44424e-05, gnorm=0.491, loss_scale=8, train_wall=7, gb_free=29.8, wall=797
2024-01-31 10:26:14 | INFO | train_inner | epoch 179:    791 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=55077, ups=14.02, wpb=3929.4, bsz=206.7, num_updates=240900, lr=6.4429e-05, gnorm=0.556, loss_scale=8, train_wall=7, gb_free=29.8, wall=804
2024-01-31 10:26:22 | INFO | train_inner | epoch 179:    891 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=50309.6, ups=12.69, wpb=3963.8, bsz=215, num_updates=241000, lr=6.44157e-05, gnorm=0.479, loss_scale=8, train_wall=8, gb_free=29.9, wall=812
2024-01-31 10:26:29 | INFO | train_inner | epoch 179:    991 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54120.3, ups=13.85, wpb=3907.6, bsz=212.1, num_updates=241100, lr=6.44023e-05, gnorm=0.506, loss_scale=8, train_wall=7, gb_free=29.8, wall=819
2024-01-31 10:26:36 | INFO | train_inner | epoch 179:   1091 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54745.7, ups=13.88, wpb=3945.5, bsz=212.6, num_updates=241200, lr=6.4389e-05, gnorm=0.514, loss_scale=8, train_wall=7, gb_free=29.9, wall=827
2024-01-31 10:26:44 | INFO | train_inner | epoch 179:   1191 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54447.3, ups=13.96, wpb=3900, bsz=220.8, num_updates=241300, lr=6.43756e-05, gnorm=0.56, loss_scale=8, train_wall=7, gb_free=29.8, wall=834
2024-01-31 10:26:51 | INFO | train_inner | epoch 179:   1291 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55011.5, ups=13.95, wpb=3942.3, bsz=208.2, num_updates=241400, lr=6.43623e-05, gnorm=0.483, loss_scale=8, train_wall=7, gb_free=29.7, wall=841
2024-01-31 10:26:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:26:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:26:57 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 2.196 | nll_loss 0.454 | ppl 1.37 | wps 112434 | wpb 3275.2 | bsz 176.5 | num_updates 241458 | best_loss 2.063
2024-01-31 10:26:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 241458 updates
2024-01-31 10:26:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint179.pt
2024-01-31 10:26:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint179.pt
2024-01-31 10:27:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint179.pt (epoch 179 @ 241458 updates, score 2.196) (writing took 2.4208642989979126 seconds)
2024-01-31 10:27:00 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2024-01-31 10:27:00 | INFO | train | epoch 179 | loss 1.779 | nll_loss 0.183 | ppl 1.14 | wps 51249.7 | ups 13.07 | wpb 3921.2 | bsz 214.5 | num_updates 241458 | lr 6.43545e-05 | gnorm 0.525 | loss_scale 8 | train_wall 94 | gb_free 29.9 | wall 850
2024-01-31 10:27:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:27:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:27:00 | INFO | fairseq.trainer | begin training epoch 180
2024-01-31 10:27:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:27:03 | INFO | train_inner | epoch 180:     42 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=31550.5, ups=8.11, wpb=3889.1, bsz=213.1, num_updates=241500, lr=6.43489e-05, gnorm=0.56, loss_scale=8, train_wall=7, gb_free=29.9, wall=853
2024-01-31 10:27:11 | INFO | train_inner | epoch 180:    142 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=50742.2, ups=12.9, wpb=3934.7, bsz=209.1, num_updates=241600, lr=6.43356e-05, gnorm=0.636, loss_scale=8, train_wall=7, gb_free=29.9, wall=861
2024-01-31 10:27:18 | INFO | train_inner | epoch 180:    242 / 1349 loss=1.777, nll_loss=0.18, ppl=1.13, wps=54515.2, ups=13.94, wpb=3912.1, bsz=219.6, num_updates=241700, lr=6.43223e-05, gnorm=0.517, loss_scale=8, train_wall=7, gb_free=29.8, wall=868
2024-01-31 10:27:25 | INFO | train_inner | epoch 180:    342 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=54691.6, ups=13.9, wpb=3933.4, bsz=212.2, num_updates=241800, lr=6.4309e-05, gnorm=0.48, loss_scale=8, train_wall=7, gb_free=29.8, wall=875
2024-01-31 10:27:32 | INFO | train_inner | epoch 180:    442 / 1349 loss=1.781, nll_loss=0.186, ppl=1.14, wps=54929.7, ups=14.25, wpb=3854.7, bsz=205.8, num_updates=241900, lr=6.42957e-05, gnorm=0.55, loss_scale=8, train_wall=7, gb_free=29.8, wall=882
2024-01-31 10:27:39 | INFO | train_inner | epoch 180:    542 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54187.6, ups=13.89, wpb=3902.3, bsz=217, num_updates=242000, lr=6.42824e-05, gnorm=0.486, loss_scale=8, train_wall=7, gb_free=29.9, wall=890
2024-01-31 10:27:47 | INFO | train_inner | epoch 180:    642 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54721.3, ups=13.78, wpb=3969.6, bsz=220, num_updates=242100, lr=6.42692e-05, gnorm=0.528, loss_scale=8, train_wall=7, gb_free=29.8, wall=897
2024-01-31 10:27:54 | INFO | train_inner | epoch 180:    742 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=54214.6, ups=13.87, wpb=3909.2, bsz=215.5, num_updates=242200, lr=6.42559e-05, gnorm=0.479, loss_scale=8, train_wall=7, gb_free=29.8, wall=904
2024-01-31 10:28:01 | INFO | train_inner | epoch 180:    842 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=56271.3, ups=14.13, wpb=3982.7, bsz=211.4, num_updates=242300, lr=6.42426e-05, gnorm=0.436, loss_scale=8, train_wall=7, gb_free=29.7, wall=911
2024-01-31 10:28:09 | INFO | train_inner | epoch 180:    942 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=50210.2, ups=12.81, wpb=3919.8, bsz=221.7, num_updates=242400, lr=6.42294e-05, gnorm=0.468, loss_scale=8, train_wall=8, gb_free=29.8, wall=919
2024-01-31 10:28:16 | INFO | train_inner | epoch 180:   1042 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=52437.8, ups=13.27, wpb=3952.8, bsz=222.6, num_updates=242500, lr=6.42161e-05, gnorm=0.487, loss_scale=8, train_wall=7, gb_free=29.7, wall=927
2024-01-31 10:28:23 | INFO | train_inner | epoch 180:   1142 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54900.7, ups=14.16, wpb=3876.8, bsz=218.6, num_updates=242600, lr=6.42029e-05, gnorm=0.5, loss_scale=8, train_wall=7, gb_free=29.8, wall=934
2024-01-31 10:28:30 | INFO | train_inner | epoch 180:   1242 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55227, ups=14.01, wpb=3943.1, bsz=209.2, num_updates=242700, lr=6.41897e-05, gnorm=0.499, loss_scale=8, train_wall=7, gb_free=29.8, wall=941
2024-01-31 10:28:38 | INFO | train_inner | epoch 180:   1342 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=52128, ups=13.28, wpb=3925.2, bsz=211.8, num_updates=242800, lr=6.41764e-05, gnorm=0.65, loss_scale=8, train_wall=7, gb_free=29.8, wall=948
2024-01-31 10:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:28:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:28:41 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 2.2 | nll_loss 0.457 | ppl 1.37 | wps 115931 | wpb 3275.2 | bsz 176.5 | num_updates 242807 | best_loss 2.063
2024-01-31 10:28:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 242807 updates
2024-01-31 10:28:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint180.pt
2024-01-31 10:28:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint180.pt
2024-01-31 10:28:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint180.pt (epoch 180 @ 242807 updates, score 2.2) (writing took 2.3392343079904094 seconds)
2024-01-31 10:28:43 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2024-01-31 10:28:43 | INFO | train | epoch 180 | loss 1.779 | nll_loss 0.183 | ppl 1.13 | wps 51129.9 | ups 13.04 | wpb 3921.2 | bsz 214.5 | num_updates 242807 | lr 6.41755e-05 | gnorm 0.515 | loss_scale 8 | train_wall 95 | gb_free 29.8 | wall 954
2024-01-31 10:28:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:28:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:28:43 | INFO | fairseq.trainer | begin training epoch 181
2024-01-31 10:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:28:51 | INFO | train_inner | epoch 181:     93 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=30255, ups=7.78, wpb=3889.9, bsz=216.3, num_updates=242900, lr=6.41632e-05, gnorm=0.455, loss_scale=8, train_wall=8, gb_free=29.8, wall=961
2024-01-31 10:28:58 | INFO | train_inner | epoch 181:    193 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54223.6, ups=13.78, wpb=3933.7, bsz=229, num_updates=243000, lr=6.415e-05, gnorm=0.494, loss_scale=8, train_wall=7, gb_free=29.8, wall=968
2024-01-31 10:29:05 | INFO | train_inner | epoch 181:    293 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53142.6, ups=13.79, wpb=3853.3, bsz=211.4, num_updates=243100, lr=6.41368e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.8, wall=976
2024-01-31 10:29:13 | INFO | train_inner | epoch 181:    393 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53458.1, ups=13.54, wpb=3949, bsz=222, num_updates=243200, lr=6.41236e-05, gnorm=0.526, loss_scale=8, train_wall=7, gb_free=29.8, wall=983
2024-01-31 10:29:20 | INFO | train_inner | epoch 181:    493 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54019.1, ups=13.86, wpb=3897.3, bsz=211, num_updates=243300, lr=6.41105e-05, gnorm=0.482, loss_scale=8, train_wall=7, gb_free=29.9, wall=990
2024-01-31 10:29:28 | INFO | train_inner | epoch 181:    593 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=50870.5, ups=12.98, wpb=3920, bsz=197.7, num_updates=243400, lr=6.40973e-05, gnorm=0.532, loss_scale=8, train_wall=7, gb_free=29.7, wall=998
2024-01-31 10:29:35 | INFO | train_inner | epoch 181:    693 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54994, ups=13.97, wpb=3937.7, bsz=217.6, num_updates=243500, lr=6.40841e-05, gnorm=0.508, loss_scale=8, train_wall=7, gb_free=29.7, wall=1005
2024-01-31 10:29:42 | INFO | train_inner | epoch 181:    793 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55895.2, ups=14.16, wpb=3946.6, bsz=215.4, num_updates=243600, lr=6.4071e-05, gnorm=0.492, loss_scale=8, train_wall=7, gb_free=29.7, wall=1012
2024-01-31 10:29:50 | INFO | train_inner | epoch 181:    893 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=51251.3, ups=13.06, wpb=3925.2, bsz=197.8, num_updates=243700, lr=6.40578e-05, gnorm=0.525, loss_scale=8, train_wall=7, gb_free=29.9, wall=1020
2024-01-31 10:29:57 | INFO | train_inner | epoch 181:    993 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=53205.6, ups=13.5, wpb=3940, bsz=215.7, num_updates=243800, lr=6.40447e-05, gnorm=0.52, loss_scale=8, train_wall=7, gb_free=29.8, wall=1027
2024-01-31 10:30:04 | INFO | train_inner | epoch 181:   1093 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55109.4, ups=13.87, wpb=3972.7, bsz=232, num_updates=243900, lr=6.40316e-05, gnorm=0.461, loss_scale=8, train_wall=7, gb_free=29.7, wall=1034
2024-01-31 10:30:11 | INFO | train_inner | epoch 181:   1193 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55111.4, ups=14.14, wpb=3896.9, bsz=211.8, num_updates=244000, lr=6.40184e-05, gnorm=0.53, loss_scale=8, train_wall=7, gb_free=29.8, wall=1042
2024-01-31 10:30:18 | INFO | train_inner | epoch 181:   1293 / 1349 loss=1.781, nll_loss=0.186, ppl=1.14, wps=53920.1, ups=13.86, wpb=3890.1, bsz=213.8, num_updates=244100, lr=6.40053e-05, gnorm=0.511, loss_scale=8, train_wall=7, gb_free=29.8, wall=1049
2024-01-31 10:30:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:30:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:30:25 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 2.2 | nll_loss 0.459 | ppl 1.37 | wps 103354 | wpb 3275.2 | bsz 176.5 | num_updates 244156 | best_loss 2.063
2024-01-31 10:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 244156 updates
2024-01-31 10:30:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint181.pt
2024-01-31 10:30:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint181.pt
2024-01-31 10:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint181.pt (epoch 181 @ 244156 updates, score 2.2) (writing took 2.349065579008311 seconds)
2024-01-31 10:30:27 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2024-01-31 10:30:27 | INFO | train | epoch 181 | loss 1.779 | nll_loss 0.182 | ppl 1.13 | wps 50800.8 | ups 12.96 | wpb 3921.2 | bsz 214.5 | num_updates 244156 | lr 6.3998e-05 | gnorm 0.507 | loss_scale 8 | train_wall 95 | gb_free 29.8 | wall 1058
2024-01-31 10:30:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:30:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:30:28 | INFO | fairseq.trainer | begin training epoch 182
2024-01-31 10:30:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:30:31 | INFO | train_inner | epoch 182:     44 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=32105.5, ups=8.14, wpb=3944.3, bsz=218.2, num_updates=244200, lr=6.39922e-05, gnorm=0.528, loss_scale=8, train_wall=7, gb_free=29.8, wall=1061
2024-01-31 10:30:39 | INFO | train_inner | epoch 182:    144 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=50611.4, ups=12.86, wpb=3934.7, bsz=199.9, num_updates=244300, lr=6.39791e-05, gnorm=0.494, loss_scale=8, train_wall=7, gb_free=29.7, wall=1069
2024-01-31 10:30:46 | INFO | train_inner | epoch 182:    244 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=54022, ups=13.82, wpb=3909.1, bsz=213.8, num_updates=244400, lr=6.3966e-05, gnorm=0.552, loss_scale=8, train_wall=7, gb_free=29.7, wall=1076
2024-01-31 10:30:53 | INFO | train_inner | epoch 182:    344 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54574.3, ups=13.93, wpb=3917.4, bsz=213.4, num_updates=244500, lr=6.39529e-05, gnorm=0.575, loss_scale=8, train_wall=7, gb_free=29.8, wall=1083
2024-01-31 10:31:00 | INFO | train_inner | epoch 182:    444 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=54642.2, ups=13.97, wpb=3912.6, bsz=201.1, num_updates=244600, lr=6.39399e-05, gnorm=0.497, loss_scale=8, train_wall=7, gb_free=29.8, wall=1090
2024-01-31 10:31:07 | INFO | train_inner | epoch 182:    544 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54876.8, ups=13.85, wpb=3961.1, bsz=223.8, num_updates=244700, lr=6.39268e-05, gnorm=0.503, loss_scale=8, train_wall=7, gb_free=29.7, wall=1098
2024-01-31 10:31:15 | INFO | train_inner | epoch 182:    644 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=50536, ups=13.23, wpb=3821.1, bsz=211.4, num_updates=244800, lr=6.39137e-05, gnorm=0.598, loss_scale=8, train_wall=7, gb_free=29.8, wall=1105
2024-01-31 10:31:22 | INFO | train_inner | epoch 182:    744 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55629.8, ups=14.09, wpb=3949.4, bsz=206.2, num_updates=244900, lr=6.39007e-05, gnorm=0.525, loss_scale=8, train_wall=7, gb_free=29.8, wall=1112
2024-01-31 10:31:29 | INFO | train_inner | epoch 182:    844 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54323.7, ups=13.72, wpb=3958.8, bsz=229.7, num_updates=245000, lr=6.38877e-05, gnorm=0.588, loss_scale=8, train_wall=7, gb_free=29.8, wall=1120
2024-01-31 10:31:37 | INFO | train_inner | epoch 182:    944 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=52353.7, ups=13.5, wpb=3877.4, bsz=220.2, num_updates=245100, lr=6.38746e-05, gnorm=0.584, loss_scale=8, train_wall=7, gb_free=29.8, wall=1127
2024-01-31 10:31:44 | INFO | train_inner | epoch 182:   1044 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53936.5, ups=13.68, wpb=3943.1, bsz=211.5, num_updates=245200, lr=6.38616e-05, gnorm=0.515, loss_scale=8, train_wall=7, gb_free=29.7, wall=1134
2024-01-31 10:31:51 | INFO | train_inner | epoch 182:   1144 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=52120.2, ups=13.31, wpb=3915.1, bsz=216.8, num_updates=245300, lr=6.38486e-05, gnorm=0.63, loss_scale=8, train_wall=7, gb_free=29.8, wall=1142
2024-01-31 10:32:00 | INFO | train_inner | epoch 182:   1244 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=48845.9, ups=12.44, wpb=3926.4, bsz=215.1, num_updates=245400, lr=6.38356e-05, gnorm=0.519, loss_scale=8, train_wall=8, gb_free=29.8, wall=1150
2024-01-31 10:32:07 | INFO | train_inner | epoch 182:   1344 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55222, ups=14.07, wpb=3923.5, bsz=217.7, num_updates=245500, lr=6.38226e-05, gnorm=0.539, loss_scale=8, train_wall=7, gb_free=29.8, wall=1157
2024-01-31 10:32:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:32:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:32:10 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 2.198 | nll_loss 0.455 | ppl 1.37 | wps 116146 | wpb 3275.2 | bsz 176.5 | num_updates 245505 | best_loss 2.063
2024-01-31 10:32:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 245505 updates
2024-01-31 10:32:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint182.pt
2024-01-31 10:32:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint182.pt
2024-01-31 10:32:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint182.pt (epoch 182 @ 245505 updates, score 2.198) (writing took 2.3509898739866912 seconds)
2024-01-31 10:32:12 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2024-01-31 10:32:12 | INFO | train | epoch 182 | loss 1.779 | nll_loss 0.182 | ppl 1.13 | wps 50654.1 | ups 12.92 | wpb 3921.2 | bsz 214.5 | num_updates 245505 | lr 6.38219e-05 | gnorm 0.547 | loss_scale 8 | train_wall 96 | gb_free 29.7 | wall 1162
2024-01-31 10:32:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:32:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:32:12 | INFO | fairseq.trainer | begin training epoch 183
2024-01-31 10:32:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:32:19 | INFO | train_inner | epoch 183:     95 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=31857.2, ups=8.19, wpb=3888, bsz=212.2, num_updates=245600, lr=6.38096e-05, gnorm=0.52, loss_scale=8, train_wall=7, gb_free=29.8, wall=1169
2024-01-31 10:32:26 | INFO | train_inner | epoch 183:    195 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53768.4, ups=13.82, wpb=3890.7, bsz=215.8, num_updates=245700, lr=6.37966e-05, gnorm=0.52, loss_scale=8, train_wall=7, gb_free=29.8, wall=1176
2024-01-31 10:32:33 | INFO | train_inner | epoch 183:    295 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55159.2, ups=14.06, wpb=3923.4, bsz=215.8, num_updates=245800, lr=6.37836e-05, gnorm=0.457, loss_scale=8, train_wall=7, gb_free=29.8, wall=1183
2024-01-31 10:32:41 | INFO | train_inner | epoch 183:    395 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=52566, ups=13.43, wpb=3913.1, bsz=211.4, num_updates=245900, lr=6.37706e-05, gnorm=0.476, loss_scale=8, train_wall=7, gb_free=29.8, wall=1191
2024-01-31 10:32:48 | INFO | train_inner | epoch 183:    495 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54074.4, ups=13.75, wpb=3932.4, bsz=225.2, num_updates=246000, lr=6.37577e-05, gnorm=0.49, loss_scale=8, train_wall=7, gb_free=29.8, wall=1198
2024-01-31 10:32:55 | INFO | train_inner | epoch 183:    595 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=55005.4, ups=13.94, wpb=3944.7, bsz=226.1, num_updates=246100, lr=6.37447e-05, gnorm=0.539, loss_scale=8, train_wall=7, gb_free=29.8, wall=1205
2024-01-31 10:33:02 | INFO | train_inner | epoch 183:    695 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=56358.2, ups=14.36, wpb=3925.9, bsz=206.4, num_updates=246200, lr=6.37318e-05, gnorm=0.52, loss_scale=8, train_wall=7, gb_free=29.7, wall=1212
2024-01-31 10:33:09 | INFO | train_inner | epoch 183:    795 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53958.2, ups=13.93, wpb=3874.4, bsz=206.9, num_updates=246300, lr=6.37188e-05, gnorm=0.521, loss_scale=8, train_wall=7, gb_free=29.9, wall=1220
2024-01-31 10:33:16 | INFO | train_inner | epoch 183:    895 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=55440.2, ups=13.99, wpb=3961.9, bsz=200, num_updates=246400, lr=6.37059e-05, gnorm=0.569, loss_scale=8, train_wall=7, gb_free=29.8, wall=1227
2024-01-31 10:33:24 | INFO | train_inner | epoch 183:    995 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54716.1, ups=14.05, wpb=3895.2, bsz=215.3, num_updates=246500, lr=6.3693e-05, gnorm=0.505, loss_scale=8, train_wall=7, gb_free=29.8, wall=1234
2024-01-31 10:33:31 | INFO | train_inner | epoch 183:   1095 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55961.2, ups=14.07, wpb=3978.3, bsz=211.9, num_updates=246600, lr=6.36801e-05, gnorm=0.529, loss_scale=8, train_wall=7, gb_free=29.8, wall=1241
2024-01-31 10:33:38 | INFO | train_inner | epoch 183:   1195 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53991.2, ups=13.78, wpb=3918.5, bsz=225.4, num_updates=246700, lr=6.36672e-05, gnorm=0.474, loss_scale=8, train_wall=7, gb_free=29.8, wall=1248
2024-01-31 10:33:45 | INFO | train_inner | epoch 183:   1295 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=55213.6, ups=13.99, wpb=3948, bsz=213.8, num_updates=246800, lr=6.36543e-05, gnorm=0.642, loss_scale=8, train_wall=7, gb_free=29.8, wall=1255
2024-01-31 10:33:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:33:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:33:51 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 2.2 | nll_loss 0.458 | ppl 1.37 | wps 116142 | wpb 3275.2 | bsz 176.5 | num_updates 246854 | best_loss 2.063
2024-01-31 10:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 246854 updates
2024-01-31 10:33:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint183.pt
2024-01-31 10:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint183.pt
2024-01-31 10:33:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint183.pt (epoch 183 @ 246854 updates, score 2.2) (writing took 2.4025746709667146 seconds)
2024-01-31 10:33:54 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2024-01-31 10:33:54 | INFO | train | epoch 183 | loss 1.779 | nll_loss 0.183 | ppl 1.13 | wps 51918.9 | ups 13.24 | wpb 3921.2 | bsz 214.5 | num_updates 246854 | lr 6.36473e-05 | gnorm 0.523 | loss_scale 8 | train_wall 93 | gb_free 29.9 | wall 1264
2024-01-31 10:33:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:33:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:33:54 | INFO | fairseq.trainer | begin training epoch 184
2024-01-31 10:33:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:33:57 | INFO | train_inner | epoch 184:     46 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=32085.4, ups=8.18, wpb=3922.3, bsz=218.2, num_updates=246900, lr=6.36414e-05, gnorm=0.53, loss_scale=8, train_wall=7, gb_free=29.9, wall=1268
2024-01-31 10:34:04 | INFO | train_inner | epoch 184:    146 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54750.6, ups=13.92, wpb=3933, bsz=215.6, num_updates=247000, lr=6.36285e-05, gnorm=0.501, loss_scale=8, train_wall=7, gb_free=29.7, wall=1275
2024-01-31 10:34:11 | INFO | train_inner | epoch 184:    246 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=55287, ups=14.31, wpb=3862.9, bsz=204.5, num_updates=247100, lr=6.36156e-05, gnorm=0.528, loss_scale=16, train_wall=7, gb_free=29.7, wall=1282
2024-01-31 10:34:19 | INFO | train_inner | epoch 184:    346 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54995.3, ups=13.8, wpb=3986.3, bsz=225, num_updates=247200, lr=6.36027e-05, gnorm=0.503, loss_scale=16, train_wall=7, gb_free=29.8, wall=1289
2024-01-31 10:34:26 | INFO | train_inner | epoch 184:    446 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55020.4, ups=14.05, wpb=3916, bsz=215.9, num_updates=247300, lr=6.35899e-05, gnorm=0.512, loss_scale=16, train_wall=7, gb_free=29.7, wall=1296
2024-01-31 10:34:33 | INFO | train_inner | epoch 184:    546 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55015.6, ups=14.26, wpb=3858.8, bsz=207.8, num_updates=247400, lr=6.3577e-05, gnorm=0.578, loss_scale=16, train_wall=7, gb_free=29.7, wall=1303
2024-01-31 10:34:40 | INFO | train_inner | epoch 184:    646 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55005.5, ups=13.95, wpb=3942.2, bsz=213.6, num_updates=247500, lr=6.35642e-05, gnorm=0.508, loss_scale=16, train_wall=7, gb_free=29.8, wall=1310
2024-01-31 10:34:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 10:34:47 | INFO | train_inner | epoch 184:    747 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=53228.2, ups=13.61, wpb=3911.8, bsz=239.5, num_updates=247600, lr=6.35513e-05, gnorm=0.486, loss_scale=8, train_wall=7, gb_free=29.8, wall=1318
2024-01-31 10:34:55 | INFO | train_inner | epoch 184:    847 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=54131.2, ups=13.87, wpb=3902, bsz=220, num_updates=247700, lr=6.35385e-05, gnorm=0.537, loss_scale=8, train_wall=7, gb_free=29.7, wall=1325
2024-01-31 10:35:02 | INFO | train_inner | epoch 184:    947 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=55866.1, ups=14.22, wpb=3929.3, bsz=209.6, num_updates=247800, lr=6.35257e-05, gnorm=0.571, loss_scale=8, train_wall=7, gb_free=29.9, wall=1332
2024-01-31 10:35:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-01-31 10:35:09 | INFO | train_inner | epoch 184:   1048 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=53625.6, ups=13.76, wpb=3897.5, bsz=205, num_updates=247900, lr=6.35129e-05, gnorm=0.58, loss_scale=4, train_wall=7, gb_free=29.7, wall=1339
2024-01-31 10:35:16 | INFO | train_inner | epoch 184:   1148 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=51905.7, ups=13.08, wpb=3966.9, bsz=210, num_updates=248000, lr=6.35001e-05, gnorm=0.484, loss_scale=4, train_wall=7, gb_free=29.9, wall=1347
2024-01-31 10:35:24 | INFO | train_inner | epoch 184:   1248 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54810.7, ups=13.88, wpb=3950.3, bsz=207.8, num_updates=248100, lr=6.34873e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.8, wall=1354
2024-01-31 10:35:31 | INFO | train_inner | epoch 184:   1348 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55948.8, ups=14.34, wpb=3901.7, bsz=215.1, num_updates=248200, lr=6.34745e-05, gnorm=0.509, loss_scale=4, train_wall=7, gb_free=29.7, wall=1361
2024-01-31 10:35:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:35:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:35:33 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 2.202 | nll_loss 0.458 | ppl 1.37 | wps 112955 | wpb 3275.2 | bsz 176.5 | num_updates 248201 | best_loss 2.063
2024-01-31 10:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 248201 updates
2024-01-31 10:35:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint184.pt
2024-01-31 10:35:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint184.pt
2024-01-31 10:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint184.pt (epoch 184 @ 248201 updates, score 2.202) (writing took 2.3517358159879223 seconds)
2024-01-31 10:35:36 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2024-01-31 10:35:36 | INFO | train | epoch 184 | loss 1.779 | nll_loss 0.183 | ppl 1.13 | wps 51808 | ups 13.21 | wpb 3921.1 | bsz 214.4 | num_updates 248201 | lr 6.34743e-05 | gnorm 0.524 | loss_scale 4 | train_wall 93 | gb_free 29.7 | wall 1366
2024-01-31 10:35:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:35:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:35:36 | INFO | fairseq.trainer | begin training epoch 185
2024-01-31 10:35:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:35:43 | INFO | train_inner | epoch 185:     99 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=31952.1, ups=8.15, wpb=3919.4, bsz=209.3, num_updates=248300, lr=6.34617e-05, gnorm=0.517, loss_scale=4, train_wall=7, gb_free=29.7, wall=1373
2024-01-31 10:35:50 | INFO | train_inner | epoch 185:    199 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54499.9, ups=13.76, wpb=3961.8, bsz=238.9, num_updates=248400, lr=6.34489e-05, gnorm=0.445, loss_scale=4, train_wall=7, gb_free=29.7, wall=1380
2024-01-31 10:35:57 | INFO | train_inner | epoch 185:    299 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54177.3, ups=13.99, wpb=3871.3, bsz=202.7, num_updates=248500, lr=6.34361e-05, gnorm=0.586, loss_scale=4, train_wall=7, gb_free=29.8, wall=1388
2024-01-31 10:36:05 | INFO | train_inner | epoch 185:    399 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54153.8, ups=13.84, wpb=3912.5, bsz=211.8, num_updates=248600, lr=6.34234e-05, gnorm=0.524, loss_scale=4, train_wall=7, gb_free=29.7, wall=1395
2024-01-31 10:36:12 | INFO | train_inner | epoch 185:    499 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=55517.1, ups=14.17, wpb=3917.7, bsz=208.3, num_updates=248700, lr=6.34106e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.7, wall=1402
2024-01-31 10:36:19 | INFO | train_inner | epoch 185:    599 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54774.4, ups=13.89, wpb=3943.2, bsz=217.7, num_updates=248800, lr=6.33979e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.8, wall=1409
2024-01-31 10:36:26 | INFO | train_inner | epoch 185:    699 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54515.5, ups=13.95, wpb=3908.7, bsz=211.3, num_updates=248900, lr=6.33852e-05, gnorm=0.558, loss_scale=4, train_wall=7, gb_free=29.8, wall=1416
2024-01-31 10:36:33 | INFO | train_inner | epoch 185:    799 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=54847.5, ups=14.1, wpb=3889.4, bsz=216, num_updates=249000, lr=6.33724e-05, gnorm=0.532, loss_scale=4, train_wall=7, gb_free=29.8, wall=1423
2024-01-31 10:36:40 | INFO | train_inner | epoch 185:    899 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54579.1, ups=13.9, wpb=3926.9, bsz=216.1, num_updates=249100, lr=6.33597e-05, gnorm=0.487, loss_scale=4, train_wall=7, gb_free=29.7, wall=1431
2024-01-31 10:36:47 | INFO | train_inner | epoch 185:    999 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=54706.5, ups=13.9, wpb=3937.1, bsz=215.8, num_updates=249200, lr=6.3347e-05, gnorm=0.566, loss_scale=4, train_wall=7, gb_free=29.8, wall=1438
2024-01-31 10:36:55 | INFO | train_inner | epoch 185:   1099 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=55423, ups=13.96, wpb=3970.9, bsz=215.7, num_updates=249300, lr=6.33343e-05, gnorm=0.477, loss_scale=4, train_wall=7, gb_free=29.8, wall=1445
2024-01-31 10:37:02 | INFO | train_inner | epoch 185:   1199 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=55613.8, ups=14.23, wpb=3907.6, bsz=215.3, num_updates=249400, lr=6.33216e-05, gnorm=0.498, loss_scale=4, train_wall=7, gb_free=29.8, wall=1452
2024-01-31 10:37:10 | INFO | train_inner | epoch 185:   1299 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=49517.7, ups=12.62, wpb=3924.9, bsz=218.1, num_updates=249500, lr=6.33089e-05, gnorm=0.53, loss_scale=4, train_wall=8, gb_free=29.8, wall=1460
2024-01-31 10:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:37:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:37:16 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 2.201 | nll_loss 0.457 | ppl 1.37 | wps 108821 | wpb 3275.2 | bsz 176.5 | num_updates 249550 | best_loss 2.063
2024-01-31 10:37:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 249550 updates
2024-01-31 10:37:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint185.pt
2024-01-31 10:37:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint185.pt
2024-01-31 10:37:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint185.pt (epoch 185 @ 249550 updates, score 2.201) (writing took 2.3168868119828403 seconds)
2024-01-31 10:37:18 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2024-01-31 10:37:18 | INFO | train | epoch 185 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51653 | ups 13.17 | wpb 3921.2 | bsz 214.5 | num_updates 249550 | lr 6.33026e-05 | gnorm 0.523 | loss_scale 4 | train_wall 94 | gb_free 29.8 | wall 1468
2024-01-31 10:37:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:37:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:37:18 | INFO | fairseq.trainer | begin training epoch 186
2024-01-31 10:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:37:22 | INFO | train_inner | epoch 186:     50 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=32130.2, ups=8.19, wpb=3921, bsz=206.6, num_updates=249600, lr=6.32962e-05, gnorm=0.509, loss_scale=4, train_wall=7, gb_free=29.8, wall=1472
2024-01-31 10:37:29 | INFO | train_inner | epoch 186:    150 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=53928.4, ups=13.76, wpb=3920.3, bsz=213.3, num_updates=249700, lr=6.32835e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.8, wall=1479
2024-01-31 10:37:37 | INFO | train_inner | epoch 186:    250 / 1349 loss=1.78, nll_loss=0.183, ppl=1.14, wps=50608.6, ups=12.93, wpb=3913.5, bsz=199.7, num_updates=249800, lr=6.32709e-05, gnorm=0.617, loss_scale=4, train_wall=7, gb_free=29.7, wall=1487
2024-01-31 10:37:44 | INFO | train_inner | epoch 186:    350 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54317.4, ups=13.77, wpb=3943.7, bsz=215, num_updates=249900, lr=6.32582e-05, gnorm=0.557, loss_scale=4, train_wall=7, gb_free=29.7, wall=1494
2024-01-31 10:37:51 | INFO | train_inner | epoch 186:    450 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54926.9, ups=14.12, wpb=3889.4, bsz=210.9, num_updates=250000, lr=6.32456e-05, gnorm=0.521, loss_scale=4, train_wall=7, gb_free=29.7, wall=1501
2024-01-31 10:37:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:37:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:37:54 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 2.202 | nll_loss 0.46 | ppl 1.38 | wps 111708 | wpb 3275.2 | bsz 176.5 | num_updates 250000 | best_loss 2.063
2024-01-31 10:37:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 250000 updates
2024-01-31 10:37:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_186_250000.pt
2024-01-31 10:37:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_186_250000.pt
2024-01-31 10:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_186_250000.pt (epoch 186 @ 250000 updates, score 2.202) (writing took 2.4016903159790672 seconds)
2024-01-31 10:38:03 | INFO | train_inner | epoch 186:    550 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=32491, ups=8.18, wpb=3972.8, bsz=209, num_updates=250100, lr=6.32329e-05, gnorm=0.604, loss_scale=4, train_wall=7, gb_free=29.8, wall=1514
2024-01-31 10:38:11 | INFO | train_inner | epoch 186:    650 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55106.8, ups=13.97, wpb=3945.2, bsz=215.6, num_updates=250200, lr=6.32203e-05, gnorm=0.499, loss_scale=4, train_wall=7, gb_free=29.7, wall=1521
2024-01-31 10:38:18 | INFO | train_inner | epoch 186:    750 / 1349 loss=1.777, nll_loss=0.18, ppl=1.13, wps=53913.4, ups=13.76, wpb=3918.4, bsz=224.4, num_updates=250300, lr=6.32076e-05, gnorm=0.497, loss_scale=4, train_wall=7, gb_free=29.7, wall=1528
2024-01-31 10:38:25 | INFO | train_inner | epoch 186:    850 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54117.4, ups=13.89, wpb=3897.1, bsz=219.5, num_updates=250400, lr=6.3195e-05, gnorm=0.592, loss_scale=4, train_wall=7, gb_free=29.8, wall=1535
2024-01-31 10:38:32 | INFO | train_inner | epoch 186:    950 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55236.1, ups=14.1, wpb=3917.1, bsz=213.8, num_updates=250500, lr=6.31824e-05, gnorm=0.547, loss_scale=4, train_wall=7, gb_free=29.8, wall=1542
2024-01-31 10:38:39 | INFO | train_inner | epoch 186:   1050 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=53764.8, ups=13.86, wpb=3878, bsz=208.7, num_updates=250600, lr=6.31698e-05, gnorm=0.709, loss_scale=4, train_wall=7, gb_free=29.8, wall=1550
2024-01-31 10:38:47 | INFO | train_inner | epoch 186:   1150 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53701.1, ups=13.79, wpb=3894.5, bsz=221.4, num_updates=250700, lr=6.31572e-05, gnorm=0.581, loss_scale=4, train_wall=7, gb_free=29.8, wall=1557
2024-01-31 10:38:54 | INFO | train_inner | epoch 186:   1250 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55078.8, ups=13.91, wpb=3959.3, bsz=225.5, num_updates=250800, lr=6.31446e-05, gnorm=0.544, loss_scale=4, train_wall=7, gb_free=29.7, wall=1564
2024-01-31 10:39:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:39:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:39:03 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 2.2 | nll_loss 0.457 | ppl 1.37 | wps 110709 | wpb 3275.2 | bsz 176.5 | num_updates 250899 | best_loss 2.063
2024-01-31 10:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 250899 updates
2024-01-31 10:39:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint186.pt
2024-01-31 10:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint186.pt
2024-01-31 10:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint186.pt (epoch 186 @ 250899 updates, score 2.2) (writing took 2.5016754519892856 seconds)
2024-01-31 10:39:06 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2024-01-31 10:39:06 | INFO | train | epoch 186 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 49105.3 | ups 12.52 | wpb 3921.2 | bsz 214.5 | num_updates 250899 | lr 6.31321e-05 | gnorm 0.559 | loss_scale 4 | train_wall 94 | gb_free 29.7 | wall 1576
2024-01-31 10:39:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:39:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:39:06 | INFO | fairseq.trainer | begin training epoch 187
2024-01-31 10:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:39:06 | INFO | train_inner | epoch 187:      1 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=31980.3, ups=8.17, wpb=3912, bsz=210.2, num_updates=250900, lr=6.3132e-05, gnorm=0.527, loss_scale=4, train_wall=7, gb_free=29.8, wall=1576
2024-01-31 10:39:13 | INFO | train_inner | epoch 187:    101 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=53851, ups=13.79, wpb=3904.5, bsz=207, num_updates=251000, lr=6.31194e-05, gnorm=0.475, loss_scale=4, train_wall=7, gb_free=29.7, wall=1584
2024-01-31 10:39:20 | INFO | train_inner | epoch 187:    201 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55304.4, ups=14, wpb=3950.6, bsz=223.4, num_updates=251100, lr=6.31069e-05, gnorm=0.489, loss_scale=4, train_wall=7, gb_free=29.8, wall=1591
2024-01-31 10:39:28 | INFO | train_inner | epoch 187:    301 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54522, ups=13.83, wpb=3943.2, bsz=227.1, num_updates=251200, lr=6.30943e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.9, wall=1598
2024-01-31 10:39:35 | INFO | train_inner | epoch 187:    401 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54804.3, ups=13.98, wpb=3921.6, bsz=203.7, num_updates=251300, lr=6.30818e-05, gnorm=0.647, loss_scale=4, train_wall=7, gb_free=29.8, wall=1605
2024-01-31 10:39:42 | INFO | train_inner | epoch 187:    501 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=56006.6, ups=14.06, wpb=3982.4, bsz=219, num_updates=251400, lr=6.30692e-05, gnorm=0.525, loss_scale=4, train_wall=7, gb_free=29.7, wall=1612
2024-01-31 10:39:49 | INFO | train_inner | epoch 187:    601 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54484.2, ups=13.89, wpb=3923.2, bsz=224.8, num_updates=251500, lr=6.30567e-05, gnorm=0.501, loss_scale=4, train_wall=7, gb_free=29.8, wall=1619
2024-01-31 10:39:57 | INFO | train_inner | epoch 187:    701 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=50201.6, ups=12.97, wpb=3869.3, bsz=204.1, num_updates=251600, lr=6.30441e-05, gnorm=0.496, loss_scale=4, train_wall=7, gb_free=29.8, wall=1627
2024-01-31 10:40:04 | INFO | train_inner | epoch 187:    801 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54219.6, ups=13.85, wpb=3914.6, bsz=227, num_updates=251700, lr=6.30316e-05, gnorm=0.514, loss_scale=4, train_wall=7, gb_free=29.8, wall=1634
2024-01-31 10:40:11 | INFO | train_inner | epoch 187:    901 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55428.8, ups=14.08, wpb=3937.2, bsz=228.2, num_updates=251800, lr=6.30191e-05, gnorm=0.455, loss_scale=4, train_wall=7, gb_free=29.8, wall=1641
2024-01-31 10:40:18 | INFO | train_inner | epoch 187:   1001 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=54808, ups=14.05, wpb=3902.1, bsz=195, num_updates=251900, lr=6.30066e-05, gnorm=0.522, loss_scale=4, train_wall=7, gb_free=29.7, wall=1649
2024-01-31 10:40:25 | INFO | train_inner | epoch 187:   1101 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54323, ups=14.04, wpb=3870.2, bsz=211.4, num_updates=252000, lr=6.29941e-05, gnorm=0.5, loss_scale=4, train_wall=7, gb_free=29.9, wall=1656
2024-01-31 10:40:33 | INFO | train_inner | epoch 187:   1201 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=52252.2, ups=13.32, wpb=3923.7, bsz=219.8, num_updates=252100, lr=6.29816e-05, gnorm=0.469, loss_scale=4, train_wall=7, gb_free=29.8, wall=1663
2024-01-31 10:40:40 | INFO | train_inner | epoch 187:   1301 / 1349 loss=1.781, nll_loss=0.185, ppl=1.14, wps=52054, ups=13.23, wpb=3934.7, bsz=196.2, num_updates=252200, lr=6.29691e-05, gnorm=0.475, loss_scale=4, train_wall=7, gb_free=29.8, wall=1671
2024-01-31 10:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:40:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:40:47 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 2.199 | nll_loss 0.455 | ppl 1.37 | wps 103711 | wpb 3275.2 | bsz 176.5 | num_updates 252248 | best_loss 2.063
2024-01-31 10:40:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 252248 updates
2024-01-31 10:40:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint187.pt
2024-01-31 10:40:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint187.pt
2024-01-31 10:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint187.pt (epoch 187 @ 252248 updates, score 2.199) (writing took 2.532906907959841 seconds)
2024-01-31 10:40:49 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2024-01-31 10:40:49 | INFO | train | epoch 187 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51170.3 | ups 13.05 | wpb 3921.2 | bsz 214.5 | num_updates 252248 | lr 6.29631e-05 | gnorm 0.508 | loss_scale 4 | train_wall 94 | gb_free 29.8 | wall 1679
2024-01-31 10:40:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:40:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:40:49 | INFO | fairseq.trainer | begin training epoch 188
2024-01-31 10:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:40:53 | INFO | train_inner | epoch 188:     52 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=30915.3, ups=7.93, wpb=3898.6, bsz=212, num_updates=252300, lr=6.29566e-05, gnorm=0.571, loss_scale=4, train_wall=7, gb_free=29.7, wall=1683
2024-01-31 10:41:00 | INFO | train_inner | epoch 188:    152 / 1349 loss=1.776, nll_loss=0.179, ppl=1.13, wps=55050.2, ups=14, wpb=3932.9, bsz=215.6, num_updates=252400, lr=6.29441e-05, gnorm=0.625, loss_scale=4, train_wall=7, gb_free=29.8, wall=1690
2024-01-31 10:41:07 | INFO | train_inner | epoch 188:    252 / 1349 loss=1.777, nll_loss=0.18, ppl=1.13, wps=54451.2, ups=13.96, wpb=3901.1, bsz=217.9, num_updates=252500, lr=6.29317e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.8, wall=1698
2024-01-31 10:41:15 | INFO | train_inner | epoch 188:    352 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54894.3, ups=14, wpb=3920.7, bsz=207.9, num_updates=252600, lr=6.29192e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.7, wall=1705
2024-01-31 10:41:21 | INFO | train_inner | epoch 188:    452 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=55661.8, ups=14.34, wpb=3881.7, bsz=206.2, num_updates=252700, lr=6.29068e-05, gnorm=0.542, loss_scale=4, train_wall=7, gb_free=29.8, wall=1712
2024-01-31 10:41:29 | INFO | train_inner | epoch 188:    552 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54799, ups=14.01, wpb=3910.6, bsz=215.7, num_updates=252800, lr=6.28943e-05, gnorm=0.495, loss_scale=4, train_wall=7, gb_free=29.8, wall=1719
2024-01-31 10:41:36 | INFO | train_inner | epoch 188:    652 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55612.5, ups=13.98, wpb=3976.7, bsz=215.2, num_updates=252900, lr=6.28819e-05, gnorm=0.49, loss_scale=4, train_wall=7, gb_free=29.7, wall=1726
2024-01-31 10:41:44 | INFO | train_inner | epoch 188:    752 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=49945.9, ups=12.72, wpb=3927.6, bsz=217.4, num_updates=253000, lr=6.28695e-05, gnorm=0.552, loss_scale=4, train_wall=8, gb_free=29.8, wall=1734
2024-01-31 10:41:51 | INFO | train_inner | epoch 188:    852 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55825.1, ups=14.2, wpb=3930.4, bsz=211.4, num_updates=253100, lr=6.2857e-05, gnorm=0.433, loss_scale=4, train_wall=7, gb_free=29.8, wall=1741
2024-01-31 10:41:58 | INFO | train_inner | epoch 188:    952 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=53930.9, ups=13.9, wpb=3880.6, bsz=224.7, num_updates=253200, lr=6.28446e-05, gnorm=0.505, loss_scale=4, train_wall=7, gb_free=29.9, wall=1748
2024-01-31 10:42:05 | INFO | train_inner | epoch 188:   1052 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55291.2, ups=13.95, wpb=3963.3, bsz=227.1, num_updates=253300, lr=6.28322e-05, gnorm=0.506, loss_scale=4, train_wall=7, gb_free=29.7, wall=1755
2024-01-31 10:42:12 | INFO | train_inner | epoch 188:   1152 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=56340.2, ups=14.24, wpb=3956.7, bsz=215.9, num_updates=253400, lr=6.28198e-05, gnorm=0.527, loss_scale=4, train_wall=7, gb_free=29.8, wall=1762
2024-01-31 10:42:19 | INFO | train_inner | epoch 188:   1252 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=55056.3, ups=14.06, wpb=3914.5, bsz=201, num_updates=253500, lr=6.28074e-05, gnorm=0.553, loss_scale=4, train_wall=7, gb_free=29.8, wall=1769
2024-01-31 10:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:42:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:42:29 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 2.202 | nll_loss 0.459 | ppl 1.37 | wps 114246 | wpb 3275.2 | bsz 176.5 | num_updates 253597 | best_loss 2.063
2024-01-31 10:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 253597 updates
2024-01-31 10:42:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint188.pt
2024-01-31 10:42:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint188.pt
2024-01-31 10:42:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint188.pt (epoch 188 @ 253597 updates, score 2.202) (writing took 2.323732986988034 seconds)
2024-01-31 10:42:31 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2024-01-31 10:42:31 | INFO | train | epoch 188 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 52004.4 | ups 13.26 | wpb 3921.2 | bsz 214.5 | num_updates 253597 | lr 6.27954e-05 | gnorm 0.527 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 1781
2024-01-31 10:42:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:42:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:42:31 | INFO | fairseq.trainer | begin training epoch 189
2024-01-31 10:42:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:42:31 | INFO | train_inner | epoch 189:      3 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=32304.8, ups=8.28, wpb=3901.3, bsz=215.4, num_updates=253600, lr=6.2795e-05, gnorm=0.495, loss_scale=4, train_wall=7, gb_free=29.8, wall=1782
2024-01-31 10:42:39 | INFO | train_inner | epoch 189:    103 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54315.7, ups=13.77, wpb=3943.3, bsz=208.2, num_updates=253700, lr=6.27827e-05, gnorm=0.547, loss_scale=4, train_wall=7, gb_free=29.8, wall=1789
2024-01-31 10:42:46 | INFO | train_inner | epoch 189:    203 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54667, ups=13.91, wpb=3930.9, bsz=220.4, num_updates=253800, lr=6.27703e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.7, wall=1796
2024-01-31 10:42:53 | INFO | train_inner | epoch 189:    303 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55642.9, ups=14.19, wpb=3921.4, bsz=204.6, num_updates=253900, lr=6.27579e-05, gnorm=0.521, loss_scale=4, train_wall=7, gb_free=29.8, wall=1803
2024-01-31 10:43:00 | INFO | train_inner | epoch 189:    403 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54267.6, ups=13.96, wpb=3888.3, bsz=212.3, num_updates=254000, lr=6.27456e-05, gnorm=0.546, loss_scale=4, train_wall=7, gb_free=29.8, wall=1810
2024-01-31 10:43:07 | INFO | train_inner | epoch 189:    503 / 1349 loss=1.776, nll_loss=0.179, ppl=1.13, wps=54442.2, ups=13.74, wpb=3960.9, bsz=220.1, num_updates=254100, lr=6.27332e-05, gnorm=0.496, loss_scale=4, train_wall=7, gb_free=29.8, wall=1817
2024-01-31 10:43:14 | INFO | train_inner | epoch 189:    603 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53820.2, ups=13.93, wpb=3864.2, bsz=197.8, num_updates=254200, lr=6.27209e-05, gnorm=0.569, loss_scale=4, train_wall=7, gb_free=29.8, wall=1825
2024-01-31 10:43:21 | INFO | train_inner | epoch 189:    703 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=55681.9, ups=14.2, wpb=3921.1, bsz=220.5, num_updates=254300, lr=6.27086e-05, gnorm=0.529, loss_scale=4, train_wall=7, gb_free=29.7, wall=1832
2024-01-31 10:43:29 | INFO | train_inner | epoch 189:    803 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=50378.1, ups=12.81, wpb=3932.2, bsz=217.4, num_updates=254400, lr=6.26962e-05, gnorm=0.563, loss_scale=4, train_wall=8, gb_free=29.8, wall=1839
2024-01-31 10:43:36 | INFO | train_inner | epoch 189:    903 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54125.9, ups=13.9, wpb=3893.4, bsz=211.3, num_updates=254500, lr=6.26839e-05, gnorm=0.545, loss_scale=4, train_wall=7, gb_free=29.7, wall=1847
2024-01-31 10:43:44 | INFO | train_inner | epoch 189:   1003 / 1349 loss=1.778, nll_loss=0.183, ppl=1.14, wps=55083.9, ups=14.06, wpb=3917.2, bsz=227.6, num_updates=254600, lr=6.26716e-05, gnorm=0.514, loss_scale=4, train_wall=7, gb_free=29.8, wall=1854
2024-01-31 10:43:51 | INFO | train_inner | epoch 189:   1103 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55913.6, ups=14.3, wpb=3909.2, bsz=225.8, num_updates=254700, lr=6.26593e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.8, wall=1861
2024-01-31 10:43:58 | INFO | train_inner | epoch 189:   1203 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55194, ups=13.93, wpb=3961.3, bsz=217.6, num_updates=254800, lr=6.2647e-05, gnorm=0.569, loss_scale=4, train_wall=7, gb_free=29.7, wall=1868
2024-01-31 10:44:05 | INFO | train_inner | epoch 189:   1303 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54408.8, ups=13.9, wpb=3913.4, bsz=208.1, num_updates=254900, lr=6.26347e-05, gnorm=0.57, loss_scale=4, train_wall=7, gb_free=29.7, wall=1875
2024-01-31 10:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:44:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:44:11 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 2.203 | nll_loss 0.46 | ppl 1.38 | wps 115809 | wpb 3275.2 | bsz 176.5 | num_updates 254946 | best_loss 2.063
2024-01-31 10:44:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 254946 updates
2024-01-31 10:44:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint189.pt
2024-01-31 10:44:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint189.pt
2024-01-31 10:44:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint189.pt (epoch 189 @ 254946 updates, score 2.203) (writing took 2.440202771977056 seconds)
2024-01-31 10:44:13 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2024-01-31 10:44:13 | INFO | train | epoch 189 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51827.2 | ups 13.22 | wpb 3921.2 | bsz 214.5 | num_updates 254946 | lr 6.26291e-05 | gnorm 0.539 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 1883
2024-01-31 10:44:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:44:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:44:13 | INFO | fairseq.trainer | begin training epoch 190
2024-01-31 10:44:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:44:17 | INFO | train_inner | epoch 190:     54 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=32351.1, ups=8.19, wpb=3951.1, bsz=220.5, num_updates=255000, lr=6.26224e-05, gnorm=0.49, loss_scale=4, train_wall=7, gb_free=29.8, wall=1887
2024-01-31 10:44:24 | INFO | train_inner | epoch 190:    154 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=53533.9, ups=13.59, wpb=3939.6, bsz=240.2, num_updates=255100, lr=6.26102e-05, gnorm=0.548, loss_scale=4, train_wall=7, gb_free=30, wall=1895
2024-01-31 10:44:31 | INFO | train_inner | epoch 190:    254 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=54445.5, ups=14.35, wpb=3793.2, bsz=201.1, num_updates=255200, lr=6.25979e-05, gnorm=0.529, loss_scale=4, train_wall=7, gb_free=29.7, wall=1902
2024-01-31 10:44:39 | INFO | train_inner | epoch 190:    354 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54546.3, ups=13.88, wpb=3928.9, bsz=206.4, num_updates=255300, lr=6.25856e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.8, wall=1909
2024-01-31 10:44:46 | INFO | train_inner | epoch 190:    454 / 1349 loss=1.777, nll_loss=0.18, ppl=1.13, wps=54558.1, ups=13.93, wpb=3917.4, bsz=217, num_updates=255400, lr=6.25734e-05, gnorm=0.525, loss_scale=4, train_wall=7, gb_free=29.8, wall=1916
2024-01-31 10:44:53 | INFO | train_inner | epoch 190:    554 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55223.6, ups=14.14, wpb=3904.9, bsz=220.6, num_updates=255500, lr=6.25611e-05, gnorm=0.551, loss_scale=4, train_wall=7, gb_free=29.8, wall=1923
2024-01-31 10:45:00 | INFO | train_inner | epoch 190:    654 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54791.5, ups=14.05, wpb=3899.1, bsz=212.4, num_updates=255600, lr=6.25489e-05, gnorm=0.548, loss_scale=4, train_wall=7, gb_free=29.8, wall=1930
2024-01-31 10:45:08 | INFO | train_inner | epoch 190:    754 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=50592.3, ups=12.77, wpb=3961.3, bsz=222.8, num_updates=255700, lr=6.25367e-05, gnorm=0.525, loss_scale=4, train_wall=8, gb_free=29.9, wall=1938
2024-01-31 10:45:15 | INFO | train_inner | epoch 190:    854 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=53719.3, ups=13.74, wpb=3910.5, bsz=212.9, num_updates=255800, lr=6.25244e-05, gnorm=0.481, loss_scale=4, train_wall=7, gb_free=29.8, wall=1945
2024-01-31 10:45:22 | INFO | train_inner | epoch 190:    954 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=56328.2, ups=14.23, wpb=3957.3, bsz=213.3, num_updates=255900, lr=6.25122e-05, gnorm=0.569, loss_scale=4, train_wall=7, gb_free=29.8, wall=1952
2024-01-31 10:45:29 | INFO | train_inner | epoch 190:   1054 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53686.9, ups=13.68, wpb=3923.5, bsz=206.2, num_updates=256000, lr=6.25e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.8, wall=1960
2024-01-31 10:45:37 | INFO | train_inner | epoch 190:   1154 / 1349 loss=1.78, nll_loss=0.185, ppl=1.14, wps=54433.1, ups=13.78, wpb=3948.7, bsz=224.9, num_updates=256100, lr=6.24878e-05, gnorm=0.546, loss_scale=4, train_wall=7, gb_free=29.8, wall=1967
2024-01-31 10:45:44 | INFO | train_inner | epoch 190:   1254 / 1349 loss=1.782, nll_loss=0.186, ppl=1.14, wps=55378.9, ups=14.01, wpb=3952.7, bsz=193.4, num_updates=256200, lr=6.24756e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.7, wall=1974
2024-01-31 10:45:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:45:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:45:53 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 2.206 | nll_loss 0.462 | ppl 1.38 | wps 110085 | wpb 3275.2 | bsz 176.5 | num_updates 256295 | best_loss 2.063
2024-01-31 10:45:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 256295 updates
2024-01-31 10:45:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint190.pt
2024-01-31 10:45:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint190.pt
2024-01-31 10:45:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint190.pt (epoch 190 @ 256295 updates, score 2.206) (writing took 2.3840379570028745 seconds)
2024-01-31 10:45:56 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2024-01-31 10:45:56 | INFO | train | epoch 190 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51563 | ups 13.15 | wpb 3921.2 | bsz 214.5 | num_updates 256295 | lr 6.2464e-05 | gnorm 0.529 | loss_scale 4 | train_wall 94 | gb_free 29.8 | wall 1986
2024-01-31 10:45:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:45:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:45:56 | INFO | fairseq.trainer | begin training epoch 191
2024-01-31 10:45:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:45:56 | INFO | train_inner | epoch 191:      5 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=32064.1, ups=8.17, wpb=3923.6, bsz=210, num_updates=256300, lr=6.24634e-05, gnorm=0.507, loss_scale=4, train_wall=7, gb_free=29.8, wall=1986
2024-01-31 10:46:03 | INFO | train_inner | epoch 191:    105 / 1349 loss=1.774, nll_loss=0.177, ppl=1.13, wps=55138, ups=14.01, wpb=3936.2, bsz=231, num_updates=256400, lr=6.24512e-05, gnorm=0.523, loss_scale=4, train_wall=7, gb_free=29.8, wall=1993
2024-01-31 10:46:10 | INFO | train_inner | epoch 191:    205 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55526.4, ups=14.03, wpb=3958.4, bsz=217.2, num_updates=256500, lr=6.24391e-05, gnorm=0.534, loss_scale=4, train_wall=7, gb_free=29.8, wall=2001
2024-01-31 10:46:18 | INFO | train_inner | epoch 191:    305 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54020.5, ups=13.8, wpb=3915.7, bsz=212.6, num_updates=256600, lr=6.24269e-05, gnorm=0.504, loss_scale=4, train_wall=7, gb_free=29.9, wall=2008
2024-01-31 10:46:25 | INFO | train_inner | epoch 191:    405 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54165.7, ups=13.92, wpb=3890.8, bsz=206.6, num_updates=256700, lr=6.24147e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.8, wall=2015
2024-01-31 10:46:32 | INFO | train_inner | epoch 191:    505 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55378.3, ups=14.13, wpb=3920, bsz=213.8, num_updates=256800, lr=6.24026e-05, gnorm=0.485, loss_scale=4, train_wall=7, gb_free=29.7, wall=2022
2024-01-31 10:46:39 | INFO | train_inner | epoch 191:    605 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53968.7, ups=13.74, wpb=3926.8, bsz=214.5, num_updates=256900, lr=6.23904e-05, gnorm=0.529, loss_scale=4, train_wall=7, gb_free=29.7, wall=2029
2024-01-31 10:46:46 | INFO | train_inner | epoch 191:    705 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54009.6, ups=13.78, wpb=3918.2, bsz=211.3, num_updates=257000, lr=6.23783e-05, gnorm=0.494, loss_scale=4, train_wall=7, gb_free=29.8, wall=2037
2024-01-31 10:46:54 | INFO | train_inner | epoch 191:    805 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54783.7, ups=13.96, wpb=3923.9, bsz=216.1, num_updates=257100, lr=6.23662e-05, gnorm=0.501, loss_scale=4, train_wall=7, gb_free=29.8, wall=2044
2024-01-31 10:47:01 | INFO | train_inner | epoch 191:    905 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54570.5, ups=14.03, wpb=3889.7, bsz=218.4, num_updates=257200, lr=6.2354e-05, gnorm=0.606, loss_scale=4, train_wall=7, gb_free=29.9, wall=2051
2024-01-31 10:47:08 | INFO | train_inner | epoch 191:   1005 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54468.7, ups=13.78, wpb=3953.5, bsz=216.4, num_updates=257300, lr=6.23419e-05, gnorm=0.51, loss_scale=4, train_wall=7, gb_free=29.7, wall=2058
2024-01-31 10:47:15 | INFO | train_inner | epoch 191:   1105 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54561.7, ups=13.96, wpb=3908, bsz=213.8, num_updates=257400, lr=6.23298e-05, gnorm=0.497, loss_scale=4, train_wall=7, gb_free=29.7, wall=2065
2024-01-31 10:47:22 | INFO | train_inner | epoch 191:   1205 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55952.2, ups=14.19, wpb=3942.1, bsz=223.4, num_updates=257500, lr=6.23177e-05, gnorm=0.481, loss_scale=4, train_wall=7, gb_free=29.8, wall=2072
2024-01-31 10:47:29 | INFO | train_inner | epoch 191:   1305 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54970.7, ups=13.9, wpb=3954.8, bsz=200.3, num_updates=257600, lr=6.23056e-05, gnorm=0.565, loss_scale=4, train_wall=7, gb_free=29.7, wall=2080
2024-01-31 10:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:47:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:47:35 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 2.202 | nll_loss 0.459 | ppl 1.37 | wps 100642 | wpb 3275.2 | bsz 176.5 | num_updates 257644 | best_loss 2.063
2024-01-31 10:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 257644 updates
2024-01-31 10:47:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint191.pt
2024-01-31 10:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint191.pt
2024-01-31 10:47:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint191.pt (epoch 191 @ 257644 updates, score 2.202) (writing took 2.5646062799496576 seconds)
2024-01-31 10:47:38 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2024-01-31 10:47:38 | INFO | train | epoch 191 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51758.3 | ups 13.2 | wpb 3921.2 | bsz 214.5 | num_updates 257644 | lr 6.23003e-05 | gnorm 0.524 | loss_scale 4 | train_wall 93 | gb_free 29.7 | wall 2088
2024-01-31 10:47:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:47:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:47:38 | INFO | fairseq.trainer | begin training epoch 192
2024-01-31 10:47:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:47:42 | INFO | train_inner | epoch 192:     56 / 1349 loss=1.779, nll_loss=0.182, ppl=1.13, wps=30586.7, ups=7.98, wpb=3835.2, bsz=201.9, num_updates=257700, lr=6.22935e-05, gnorm=0.558, loss_scale=4, train_wall=7, gb_free=29.7, wall=2092
2024-01-31 10:47:49 | INFO | train_inner | epoch 192:    156 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=53424.8, ups=13.59, wpb=3931.2, bsz=218.9, num_updates=257800, lr=6.22814e-05, gnorm=0.484, loss_scale=4, train_wall=7, gb_free=29.7, wall=2100
2024-01-31 10:47:56 | INFO | train_inner | epoch 192:    256 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54367.7, ups=13.79, wpb=3943.7, bsz=226, num_updates=257900, lr=6.22693e-05, gnorm=0.558, loss_scale=4, train_wall=7, gb_free=29.7, wall=2107
2024-01-31 10:48:04 | INFO | train_inner | epoch 192:    356 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55189.5, ups=14.03, wpb=3932.8, bsz=209.9, num_updates=258000, lr=6.22573e-05, gnorm=0.551, loss_scale=4, train_wall=7, gb_free=29.8, wall=2114
2024-01-31 10:48:11 | INFO | train_inner | epoch 192:    456 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=55545.5, ups=13.97, wpb=3977, bsz=226.2, num_updates=258100, lr=6.22452e-05, gnorm=0.501, loss_scale=4, train_wall=7, gb_free=29.7, wall=2121
2024-01-31 10:48:18 | INFO | train_inner | epoch 192:    556 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=54281.5, ups=13.85, wpb=3918.2, bsz=206.5, num_updates=258200, lr=6.22332e-05, gnorm=0.601, loss_scale=4, train_wall=7, gb_free=29.7, wall=2128
2024-01-31 10:48:25 | INFO | train_inner | epoch 192:    656 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53182.5, ups=13.87, wpb=3833.8, bsz=210.1, num_updates=258300, lr=6.22211e-05, gnorm=0.595, loss_scale=4, train_wall=7, gb_free=29.7, wall=2135
2024-01-31 10:48:32 | INFO | train_inner | epoch 192:    756 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55825.3, ups=14.23, wpb=3922.6, bsz=218.1, num_updates=258400, lr=6.22091e-05, gnorm=0.471, loss_scale=4, train_wall=7, gb_free=29.7, wall=2142
2024-01-31 10:48:39 | INFO | train_inner | epoch 192:    856 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54318.2, ups=13.81, wpb=3934.4, bsz=221, num_updates=258500, lr=6.2197e-05, gnorm=0.504, loss_scale=4, train_wall=7, gb_free=29.8, wall=2150
2024-01-31 10:48:47 | INFO | train_inner | epoch 192:    956 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=54892.4, ups=13.91, wpb=3947.2, bsz=204.3, num_updates=258600, lr=6.2185e-05, gnorm=0.526, loss_scale=4, train_wall=7, gb_free=29.8, wall=2157
2024-01-31 10:48:54 | INFO | train_inner | epoch 192:   1056 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55061.3, ups=13.98, wpb=3937.5, bsz=205.2, num_updates=258700, lr=6.2173e-05, gnorm=0.597, loss_scale=4, train_wall=7, gb_free=29.8, wall=2164
2024-01-31 10:49:01 | INFO | train_inner | epoch 192:   1156 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54416.7, ups=13.94, wpb=3903.2, bsz=221.8, num_updates=258800, lr=6.2161e-05, gnorm=0.509, loss_scale=4, train_wall=7, gb_free=29.7, wall=2171
2024-01-31 10:49:08 | INFO | train_inner | epoch 192:   1256 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54529.2, ups=13.83, wpb=3941.8, bsz=213.6, num_updates=258900, lr=6.2149e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.8, wall=2178
2024-01-31 10:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:49:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:49:17 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 2.201 | nll_loss 0.459 | ppl 1.37 | wps 115360 | wpb 3275.2 | bsz 176.5 | num_updates 258993 | best_loss 2.063
2024-01-31 10:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 258993 updates
2024-01-31 10:49:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint192.pt
2024-01-31 10:49:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint192.pt
2024-01-31 10:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint192.pt (epoch 192 @ 258993 updates, score 2.201) (writing took 2.5994156529777683 seconds)
2024-01-31 10:49:20 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2024-01-31 10:49:20 | INFO | train | epoch 192 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51761 | ups 13.2 | wpb 3921.2 | bsz 214.5 | num_updates 258993 | lr 6.21378e-05 | gnorm 0.537 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 2190
2024-01-31 10:49:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:49:20 | INFO | fairseq.trainer | begin training epoch 193
2024-01-31 10:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:49:21 | INFO | train_inner | epoch 193:      7 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=31377.2, ups=8.08, wpb=3881.5, bsz=211.7, num_updates=259000, lr=6.2137e-05, gnorm=0.554, loss_scale=4, train_wall=7, gb_free=29.8, wall=2191
2024-01-31 10:49:28 | INFO | train_inner | epoch 193:    107 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54125.5, ups=13.7, wpb=3950.7, bsz=211.8, num_updates=259100, lr=6.2125e-05, gnorm=0.443, loss_scale=4, train_wall=7, gb_free=29.8, wall=2198
2024-01-31 10:49:35 | INFO | train_inner | epoch 193:    207 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54208.7, ups=13.75, wpb=3943.2, bsz=228.6, num_updates=259200, lr=6.2113e-05, gnorm=0.499, loss_scale=4, train_wall=7, gb_free=29.8, wall=2205
2024-01-31 10:49:42 | INFO | train_inner | epoch 193:    307 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55065.8, ups=14.06, wpb=3917, bsz=219.8, num_updates=259300, lr=6.2101e-05, gnorm=0.491, loss_scale=4, train_wall=7, gb_free=29.8, wall=2213
2024-01-31 10:49:49 | INFO | train_inner | epoch 193:    407 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54389.9, ups=13.9, wpb=3912, bsz=218.2, num_updates=259400, lr=6.2089e-05, gnorm=0.597, loss_scale=4, train_wall=7, gb_free=29.7, wall=2220
2024-01-31 10:49:57 | INFO | train_inner | epoch 193:    507 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54117.6, ups=13.97, wpb=3874.5, bsz=203, num_updates=259500, lr=6.20771e-05, gnorm=0.551, loss_scale=4, train_wall=7, gb_free=29.7, wall=2227
2024-01-31 10:50:04 | INFO | train_inner | epoch 193:    607 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54007.5, ups=13.89, wpb=3888, bsz=214.9, num_updates=259600, lr=6.20651e-05, gnorm=0.489, loss_scale=4, train_wall=7, gb_free=29.7, wall=2234
2024-01-31 10:50:11 | INFO | train_inner | epoch 193:    707 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55885, ups=14.17, wpb=3944.9, bsz=207.8, num_updates=259700, lr=6.20532e-05, gnorm=0.512, loss_scale=4, train_wall=7, gb_free=29.7, wall=2241
2024-01-31 10:50:18 | INFO | train_inner | epoch 193:    807 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=54337.9, ups=13.88, wpb=3915.9, bsz=210.6, num_updates=259800, lr=6.20412e-05, gnorm=0.522, loss_scale=4, train_wall=7, gb_free=29.7, wall=2248
2024-01-31 10:50:25 | INFO | train_inner | epoch 193:    907 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55042.6, ups=13.98, wpb=3937.3, bsz=211.4, num_updates=259900, lr=6.20293e-05, gnorm=0.605, loss_scale=4, train_wall=7, gb_free=29.9, wall=2256
2024-01-31 10:50:32 | INFO | train_inner | epoch 193:   1007 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=56117.5, ups=14.12, wpb=3975.1, bsz=227.3, num_updates=260000, lr=6.20174e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.8, wall=2263
2024-01-31 10:50:40 | INFO | train_inner | epoch 193:   1107 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54013.9, ups=13.93, wpb=3876.8, bsz=210.5, num_updates=260100, lr=6.20054e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.7, wall=2270
2024-01-31 10:50:47 | INFO | train_inner | epoch 193:   1207 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54088, ups=13.8, wpb=3919.6, bsz=215.8, num_updates=260200, lr=6.19935e-05, gnorm=0.57, loss_scale=4, train_wall=7, gb_free=29.8, wall=2277
2024-01-31 10:50:54 | INFO | train_inner | epoch 193:   1307 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54929.6, ups=14, wpb=3923.8, bsz=219.9, num_updates=260300, lr=6.19816e-05, gnorm=0.544, loss_scale=4, train_wall=7, gb_free=29.8, wall=2284
2024-01-31 10:50:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:50:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:50:59 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 2.206 | nll_loss 0.463 | ppl 1.38 | wps 114626 | wpb 3275.2 | bsz 176.5 | num_updates 260342 | best_loss 2.063
2024-01-31 10:50:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 260342 updates
2024-01-31 10:50:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint193.pt
2024-01-31 10:51:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint193.pt
2024-01-31 10:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint193.pt (epoch 193 @ 260342 updates, score 2.206) (writing took 2.513059081044048 seconds)
2024-01-31 10:51:02 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2024-01-31 10:51:02 | INFO | train | epoch 193 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 51930.7 | ups 13.24 | wpb 3921.2 | bsz 214.5 | num_updates 260342 | lr 6.19766e-05 | gnorm 0.53 | loss_scale 4 | train_wall 93 | gb_free 29.7 | wall 2292
2024-01-31 10:51:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:51:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:51:02 | INFO | fairseq.trainer | begin training epoch 194
2024-01-31 10:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:51:06 | INFO | train_inner | epoch 194:     58 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=31492.1, ups=8.07, wpb=3904.8, bsz=205.4, num_updates=260400, lr=6.19697e-05, gnorm=0.548, loss_scale=4, train_wall=7, gb_free=29.7, wall=2297
2024-01-31 10:51:13 | INFO | train_inner | epoch 194:    158 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=54146.6, ups=13.98, wpb=3874.4, bsz=221.6, num_updates=260500, lr=6.19578e-05, gnorm=0.497, loss_scale=4, train_wall=7, gb_free=29.9, wall=2304
2024-01-31 10:51:21 | INFO | train_inner | epoch 194:    258 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55616.1, ups=14.08, wpb=3948.8, bsz=224.5, num_updates=260600, lr=6.19459e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.8, wall=2311
2024-01-31 10:51:28 | INFO | train_inner | epoch 194:    358 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55025.3, ups=13.92, wpb=3952, bsz=205.9, num_updates=260700, lr=6.19341e-05, gnorm=0.479, loss_scale=4, train_wall=7, gb_free=29.8, wall=2318
2024-01-31 10:51:35 | INFO | train_inner | epoch 194:    458 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54994, ups=14.01, wpb=3926.3, bsz=215.8, num_updates=260800, lr=6.19222e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.8, wall=2325
2024-01-31 10:51:42 | INFO | train_inner | epoch 194:    558 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55285.3, ups=14.26, wpb=3876.8, bsz=204.3, num_updates=260900, lr=6.19103e-05, gnorm=0.572, loss_scale=4, train_wall=7, gb_free=29.7, wall=2332
2024-01-31 10:51:49 | INFO | train_inner | epoch 194:    658 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54304.7, ups=13.8, wpb=3935, bsz=216.8, num_updates=261000, lr=6.18984e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.8, wall=2339
2024-01-31 10:51:56 | INFO | train_inner | epoch 194:    758 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54023.9, ups=13.88, wpb=3893.3, bsz=209.3, num_updates=261100, lr=6.18866e-05, gnorm=0.586, loss_scale=4, train_wall=7, gb_free=29.9, wall=2347
2024-01-31 10:52:03 | INFO | train_inner | epoch 194:    858 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55427.3, ups=14.03, wpb=3951.3, bsz=216.7, num_updates=261200, lr=6.18747e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.8, wall=2354
2024-01-31 10:52:11 | INFO | train_inner | epoch 194:    958 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55360.2, ups=14.07, wpb=3933.6, bsz=211.9, num_updates=261300, lr=6.18629e-05, gnorm=0.567, loss_scale=4, train_wall=7, gb_free=29.9, wall=2361
2024-01-31 10:52:18 | INFO | train_inner | epoch 194:   1058 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54929.2, ups=13.86, wpb=3963.5, bsz=216.5, num_updates=261400, lr=6.18511e-05, gnorm=0.499, loss_scale=4, train_wall=7, gb_free=29.7, wall=2368
2024-01-31 10:52:25 | INFO | train_inner | epoch 194:   1158 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55189.8, ups=14.06, wpb=3926.7, bsz=215.8, num_updates=261500, lr=6.18392e-05, gnorm=0.506, loss_scale=4, train_wall=7, gb_free=29.7, wall=2375
2024-01-31 10:52:32 | INFO | train_inner | epoch 194:   1258 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55423.7, ups=14.15, wpb=3915.8, bsz=214.9, num_updates=261600, lr=6.18274e-05, gnorm=0.541, loss_scale=4, train_wall=7, gb_free=29.8, wall=2382
2024-01-31 10:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:52:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:52:41 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 2.206 | nll_loss 0.462 | ppl 1.38 | wps 116196 | wpb 3275.2 | bsz 176.5 | num_updates 261691 | best_loss 2.063
2024-01-31 10:52:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 261691 updates
2024-01-31 10:52:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint194.pt
2024-01-31 10:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint194.pt
2024-01-31 10:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint194.pt (epoch 194 @ 261691 updates, score 2.206) (writing took 2.457282571005635 seconds)
2024-01-31 10:52:43 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2024-01-31 10:52:43 | INFO | train | epoch 194 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 52044.9 | ups 13.27 | wpb 3921.2 | bsz 214.5 | num_updates 261691 | lr 6.18167e-05 | gnorm 0.532 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 2394
2024-01-31 10:52:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:52:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:52:44 | INFO | fairseq.trainer | begin training epoch 195
2024-01-31 10:52:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:52:44 | INFO | train_inner | epoch 195:      9 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=31408.8, ups=8.08, wpb=3886.8, bsz=210, num_updates=261700, lr=6.18156e-05, gnorm=0.576, loss_scale=4, train_wall=7, gb_free=29.8, wall=2395
2024-01-31 10:52:51 | INFO | train_inner | epoch 195:    109 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=56090.2, ups=14.13, wpb=3968.5, bsz=212.2, num_updates=261800, lr=6.18038e-05, gnorm=0.434, loss_scale=4, train_wall=7, gb_free=29.8, wall=2402
2024-01-31 10:52:59 | INFO | train_inner | epoch 195:    209 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54881.7, ups=13.95, wpb=3935, bsz=212.2, num_updates=261900, lr=6.1792e-05, gnorm=0.503, loss_scale=4, train_wall=7, gb_free=29.8, wall=2409
2024-01-31 10:53:06 | INFO | train_inner | epoch 195:    309 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54283.4, ups=13.8, wpb=3932.2, bsz=221.8, num_updates=262000, lr=6.17802e-05, gnorm=0.607, loss_scale=4, train_wall=7, gb_free=29.8, wall=2416
2024-01-31 10:53:13 | INFO | train_inner | epoch 195:    409 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=55470.8, ups=14.22, wpb=3900.1, bsz=198.4, num_updates=262100, lr=6.17684e-05, gnorm=0.53, loss_scale=4, train_wall=7, gb_free=29.8, wall=2423
2024-01-31 10:53:20 | INFO | train_inner | epoch 195:    509 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=53665.9, ups=13.89, wpb=3864.8, bsz=223.7, num_updates=262200, lr=6.17566e-05, gnorm=0.495, loss_scale=4, train_wall=7, gb_free=29.7, wall=2430
2024-01-31 10:53:27 | INFO | train_inner | epoch 195:    609 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=53964.7, ups=13.93, wpb=3874.4, bsz=213.9, num_updates=262300, lr=6.17449e-05, gnorm=0.514, loss_scale=4, train_wall=7, gb_free=29.8, wall=2438
2024-01-31 10:53:34 | INFO | train_inner | epoch 195:    709 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54639.3, ups=14, wpb=3901.8, bsz=219.2, num_updates=262400, lr=6.17331e-05, gnorm=0.583, loss_scale=4, train_wall=7, gb_free=29.7, wall=2445
2024-01-31 10:53:41 | INFO | train_inner | epoch 195:    809 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=56056.5, ups=14.2, wpb=3948, bsz=234.5, num_updates=262500, lr=6.17213e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.7, wall=2452
2024-01-31 10:53:49 | INFO | train_inner | epoch 195:    909 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54591.2, ups=13.96, wpb=3911.7, bsz=205.4, num_updates=262600, lr=6.17096e-05, gnorm=0.527, loss_scale=4, train_wall=7, gb_free=29.7, wall=2459
2024-01-31 10:53:56 | INFO | train_inner | epoch 195:   1009 / 1349 loss=1.778, nll_loss=0.181, ppl=1.13, wps=54770.5, ups=13.95, wpb=3925.2, bsz=206.6, num_updates=262700, lr=6.16978e-05, gnorm=0.59, loss_scale=4, train_wall=7, gb_free=29.7, wall=2466
2024-01-31 10:54:03 | INFO | train_inner | epoch 195:   1109 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54950.8, ups=13.98, wpb=3929.5, bsz=209.9, num_updates=262800, lr=6.16861e-05, gnorm=0.523, loss_scale=4, train_wall=7, gb_free=29.8, wall=2473
2024-01-31 10:54:10 | INFO | train_inner | epoch 195:   1209 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=54684, ups=13.93, wpb=3926.3, bsz=209.3, num_updates=262900, lr=6.16744e-05, gnorm=0.572, loss_scale=4, train_wall=7, gb_free=29.9, wall=2480
2024-01-31 10:54:17 | INFO | train_inner | epoch 195:   1309 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54262.4, ups=13.83, wpb=3923.7, bsz=212.6, num_updates=263000, lr=6.16626e-05, gnorm=0.483, loss_scale=4, train_wall=7, gb_free=29.8, wall=2488
2024-01-31 10:54:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:54:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:54:23 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 2.203 | nll_loss 0.458 | ppl 1.37 | wps 112312 | wpb 3275.2 | bsz 176.5 | num_updates 263040 | best_loss 2.063
2024-01-31 10:54:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 263040 updates
2024-01-31 10:54:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint195.pt
2024-01-31 10:54:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint195.pt
2024-01-31 10:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint195.pt (epoch 195 @ 263040 updates, score 2.203) (writing took 2.4767365700099617 seconds)
2024-01-31 10:54:25 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2024-01-31 10:54:25 | INFO | train | epoch 195 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 51948.1 | ups 13.25 | wpb 3921.2 | bsz 214.5 | num_updates 263040 | lr 6.1658e-05 | gnorm 0.522 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 2496
2024-01-31 10:54:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:54:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:54:25 | INFO | fairseq.trainer | begin training epoch 196
2024-01-31 10:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:54:30 | INFO | train_inner | epoch 196:     60 / 1349 loss=1.774, nll_loss=0.177, ppl=1.13, wps=31699, ups=8.06, wpb=3933.5, bsz=224.7, num_updates=263100, lr=6.16509e-05, gnorm=0.442, loss_scale=4, train_wall=7, gb_free=29.7, wall=2500
2024-01-31 10:54:37 | INFO | train_inner | epoch 196:    160 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54746.4, ups=13.9, wpb=3938.3, bsz=217.2, num_updates=263200, lr=6.16392e-05, gnorm=0.598, loss_scale=4, train_wall=7, gb_free=29.8, wall=2507
2024-01-31 10:54:44 | INFO | train_inner | epoch 196:    260 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55221.6, ups=14.24, wpb=3879.1, bsz=216.3, num_updates=263300, lr=6.16275e-05, gnorm=0.495, loss_scale=4, train_wall=7, gb_free=29.8, wall=2514
2024-01-31 10:54:51 | INFO | train_inner | epoch 196:    360 / 1349 loss=1.776, nll_loss=0.179, ppl=1.13, wps=55256.9, ups=14.04, wpb=3934.6, bsz=218.3, num_updates=263400, lr=6.16158e-05, gnorm=0.511, loss_scale=4, train_wall=7, gb_free=29.7, wall=2521
2024-01-31 10:54:58 | INFO | train_inner | epoch 196:    460 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54939.6, ups=13.81, wpb=3979.4, bsz=222.2, num_updates=263500, lr=6.16041e-05, gnorm=0.553, loss_scale=4, train_wall=7, gb_free=29.8, wall=2529
2024-01-31 10:55:06 | INFO | train_inner | epoch 196:    560 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=54442.4, ups=13.9, wpb=3917.9, bsz=217.8, num_updates=263600, lr=6.15924e-05, gnorm=0.575, loss_scale=4, train_wall=7, gb_free=29.9, wall=2536
2024-01-31 10:55:13 | INFO | train_inner | epoch 196:    660 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55130.9, ups=14.09, wpb=3912.2, bsz=212.8, num_updates=263700, lr=6.15807e-05, gnorm=0.55, loss_scale=4, train_wall=7, gb_free=29.7, wall=2543
2024-01-31 10:55:29 | INFO | train_inner | epoch 196:    760 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=23931.5, ups=6.09, wpb=3927.3, bsz=217.5, num_updates=263800, lr=6.15691e-05, gnorm=0.527, loss_scale=4, train_wall=15, gb_free=29.8, wall=2559
2024-01-31 10:55:36 | INFO | train_inner | epoch 196:    860 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=53587.2, ups=13.82, wpb=3878.8, bsz=225.8, num_updates=263900, lr=6.15574e-05, gnorm=0.548, loss_scale=4, train_wall=7, gb_free=29.7, wall=2567
2024-01-31 10:55:43 | INFO | train_inner | epoch 196:    960 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55585.7, ups=14.17, wpb=3922.7, bsz=205, num_updates=264000, lr=6.15457e-05, gnorm=0.562, loss_scale=4, train_wall=7, gb_free=29.8, wall=2574
2024-01-31 10:55:50 | INFO | train_inner | epoch 196:   1060 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=55426.3, ups=14.12, wpb=3925.1, bsz=213.4, num_updates=264100, lr=6.15341e-05, gnorm=0.613, loss_scale=4, train_wall=7, gb_free=29.8, wall=2581
2024-01-31 10:55:58 | INFO | train_inner | epoch 196:   1160 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=55515.8, ups=14.05, wpb=3951.1, bsz=210.6, num_updates=264200, lr=6.15224e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.8, wall=2588
2024-01-31 10:56:05 | INFO | train_inner | epoch 196:   1260 / 1349 loss=1.78, nll_loss=0.184, ppl=1.14, wps=53824.9, ups=13.85, wpb=3886.2, bsz=211.6, num_updates=264300, lr=6.15108e-05, gnorm=0.559, loss_scale=8, train_wall=7, gb_free=29.7, wall=2595
2024-01-31 10:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:56:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:56:14 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 2.206 | nll_loss 0.462 | ppl 1.38 | wps 108849 | wpb 3275.2 | bsz 176.5 | num_updates 264389 | best_loss 2.063
2024-01-31 10:56:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 264389 updates
2024-01-31 10:56:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint196.pt
2024-01-31 10:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint196.pt
2024-01-31 10:56:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint196.pt (epoch 196 @ 264389 updates, score 2.206) (writing took 2.641928441997152 seconds)
2024-01-31 10:56:16 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2024-01-31 10:56:16 | INFO | train | epoch 196 | loss 1.778 | nll_loss 0.182 | ppl 1.13 | wps 47684.5 | ups 12.16 | wpb 3921.2 | bsz 214.5 | num_updates 264389 | lr 6.15005e-05 | gnorm 0.551 | loss_scale 8 | train_wall 101 | gb_free 29.8 | wall 2607
2024-01-31 10:56:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:56:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:56:16 | INFO | fairseq.trainer | begin training epoch 197
2024-01-31 10:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:56:17 | INFO | train_inner | epoch 197:     11 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=31768.9, ups=8.05, wpb=3947.1, bsz=203, num_updates=264400, lr=6.14992e-05, gnorm=0.602, loss_scale=8, train_wall=7, gb_free=29.7, wall=2607
2024-01-31 10:56:24 | INFO | train_inner | epoch 197:    111 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54529, ups=14.02, wpb=3888.6, bsz=205.3, num_updates=264500, lr=6.14875e-05, gnorm=0.562, loss_scale=8, train_wall=7, gb_free=29.8, wall=2615
2024-01-31 10:56:31 | INFO | train_inner | epoch 197:    211 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=56481.4, ups=14.28, wpb=3956.2, bsz=215, num_updates=264600, lr=6.14759e-05, gnorm=0.489, loss_scale=8, train_wall=7, gb_free=29.8, wall=2622
2024-01-31 10:56:38 | INFO | train_inner | epoch 197:    311 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54411, ups=13.99, wpb=3889.5, bsz=210.5, num_updates=264700, lr=6.14643e-05, gnorm=0.512, loss_scale=8, train_wall=7, gb_free=29.8, wall=2629
2024-01-31 10:56:46 | INFO | train_inner | epoch 197:    411 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54653, ups=13.91, wpb=3929.9, bsz=233, num_updates=264800, lr=6.14527e-05, gnorm=0.502, loss_scale=8, train_wall=7, gb_free=29.8, wall=2636
2024-01-31 10:56:53 | INFO | train_inner | epoch 197:    511 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=56059.6, ups=14.18, wpb=3953.8, bsz=217.7, num_updates=264900, lr=6.14411e-05, gnorm=0.59, loss_scale=8, train_wall=7, gb_free=29.7, wall=2643
2024-01-31 10:57:00 | INFO | train_inner | epoch 197:    611 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=55129, ups=14.09, wpb=3911.8, bsz=205.3, num_updates=265000, lr=6.14295e-05, gnorm=0.531, loss_scale=8, train_wall=7, gb_free=29.7, wall=2650
2024-01-31 10:57:07 | INFO | train_inner | epoch 197:    711 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=52126.7, ups=13.53, wpb=3853.3, bsz=210.9, num_updates=265100, lr=6.14179e-05, gnorm=0.474, loss_scale=8, train_wall=7, gb_free=29.7, wall=2657
2024-01-31 10:57:15 | INFO | train_inner | epoch 197:    811 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53675.7, ups=13.63, wpb=3937, bsz=216.6, num_updates=265200, lr=6.14063e-05, gnorm=0.533, loss_scale=8, train_wall=7, gb_free=29.8, wall=2665
2024-01-31 10:57:22 | INFO | train_inner | epoch 197:    911 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55957, ups=14.27, wpb=3921.4, bsz=217.4, num_updates=265300, lr=6.13948e-05, gnorm=0.523, loss_scale=8, train_wall=7, gb_free=29.7, wall=2672
2024-01-31 10:57:29 | INFO | train_inner | epoch 197:   1011 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54509.2, ups=13.77, wpb=3959.9, bsz=226.4, num_updates=265400, lr=6.13832e-05, gnorm=0.558, loss_scale=8, train_wall=7, gb_free=29.8, wall=2679
2024-01-31 10:57:36 | INFO | train_inner | epoch 197:   1111 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54739.2, ups=13.82, wpb=3960.3, bsz=208.9, num_updates=265500, lr=6.13716e-05, gnorm=0.55, loss_scale=8, train_wall=7, gb_free=29.7, wall=2686
2024-01-31 10:57:43 | INFO | train_inner | epoch 197:   1211 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55477.2, ups=14.32, wpb=3874.5, bsz=211.9, num_updates=265600, lr=6.13601e-05, gnorm=0.595, loss_scale=8, train_wall=7, gb_free=29.7, wall=2693
2024-01-31 10:57:50 | INFO | train_inner | epoch 197:   1311 / 1349 loss=1.778, nll_loss=0.183, ppl=1.14, wps=55228.2, ups=14.05, wpb=3931.1, bsz=218.2, num_updates=265700, lr=6.13485e-05, gnorm=0.5, loss_scale=8, train_wall=7, gb_free=29.8, wall=2700
2024-01-31 10:57:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:57:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:57:56 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 2.203 | nll_loss 0.46 | ppl 1.38 | wps 103721 | wpb 3275.2 | bsz 176.5 | num_updates 265738 | best_loss 2.063
2024-01-31 10:57:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 265738 updates
2024-01-31 10:57:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint197.pt
2024-01-31 10:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint197.pt
2024-01-31 10:57:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint197.pt (epoch 197 @ 265738 updates, score 2.203) (writing took 2.658338192035444 seconds)
2024-01-31 10:57:58 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2024-01-31 10:57:58 | INFO | train | epoch 197 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 51868.1 | ups 13.23 | wpb 3921.2 | bsz 214.5 | num_updates 265738 | lr 6.13442e-05 | gnorm 0.533 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 2708
2024-01-31 10:57:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:57:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:57:58 | INFO | fairseq.trainer | begin training epoch 198
2024-01-31 10:57:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:58:03 | INFO | train_inner | epoch 198:     62 / 1349 loss=1.779, nll_loss=0.183, ppl=1.13, wps=30025.7, ups=7.64, wpb=3929.3, bsz=193.6, num_updates=265800, lr=6.1337e-05, gnorm=0.514, loss_scale=8, train_wall=7, gb_free=29.7, wall=2713
2024-01-31 10:58:11 | INFO | train_inner | epoch 198:    162 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=48732.5, ups=12.53, wpb=3890.2, bsz=211.5, num_updates=265900, lr=6.13255e-05, gnorm=0.526, loss_scale=8, train_wall=8, gb_free=29.8, wall=2721
2024-01-31 10:58:18 | INFO | train_inner | epoch 198:    262 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53631.5, ups=13.87, wpb=3866.6, bsz=230.6, num_updates=266000, lr=6.13139e-05, gnorm=0.517, loss_scale=8, train_wall=7, gb_free=29.8, wall=2729
2024-01-31 10:58:26 | INFO | train_inner | epoch 198:    362 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54120.6, ups=13.88, wpb=3898.1, bsz=213.8, num_updates=266100, lr=6.13024e-05, gnorm=0.597, loss_scale=8, train_wall=7, gb_free=29.7, wall=2736
2024-01-31 10:58:33 | INFO | train_inner | epoch 198:    462 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=56518.8, ups=14.19, wpb=3982.3, bsz=223.7, num_updates=266200, lr=6.12909e-05, gnorm=0.524, loss_scale=8, train_wall=7, gb_free=29.8, wall=2743
2024-01-31 10:58:40 | INFO | train_inner | epoch 198:    562 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54919.3, ups=13.99, wpb=3926.7, bsz=210.1, num_updates=266300, lr=6.12794e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.8, wall=2750
2024-01-31 10:58:47 | INFO | train_inner | epoch 198:    662 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55142.4, ups=14, wpb=3939.6, bsz=202.6, num_updates=266400, lr=6.12679e-05, gnorm=0.565, loss_scale=8, train_wall=7, gb_free=29.8, wall=2757
2024-01-31 10:58:54 | INFO | train_inner | epoch 198:    762 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54435.5, ups=13.89, wpb=3918.1, bsz=220.3, num_updates=266500, lr=6.12564e-05, gnorm=0.505, loss_scale=8, train_wall=7, gb_free=29.8, wall=2764
2024-01-31 10:59:01 | INFO | train_inner | epoch 198:    862 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55940, ups=14.28, wpb=3917.9, bsz=209.6, num_updates=266600, lr=6.12449e-05, gnorm=0.493, loss_scale=8, train_wall=7, gb_free=29.8, wall=2771
2024-01-31 10:59:08 | INFO | train_inner | epoch 198:    962 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54489.3, ups=13.87, wpb=3928.3, bsz=218, num_updates=266700, lr=6.12334e-05, gnorm=0.58, loss_scale=8, train_wall=7, gb_free=29.7, wall=2779
2024-01-31 10:59:16 | INFO | train_inner | epoch 198:   1062 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55566.8, ups=14.02, wpb=3963, bsz=207.4, num_updates=266800, lr=6.12219e-05, gnorm=0.589, loss_scale=8, train_wall=7, gb_free=29.7, wall=2786
2024-01-31 10:59:22 | INFO | train_inner | epoch 198:   1162 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=56010.6, ups=14.33, wpb=3908.5, bsz=211.9, num_updates=266900, lr=6.12105e-05, gnorm=0.521, loss_scale=8, train_wall=7, gb_free=29.8, wall=2793
2024-01-31 10:59:30 | INFO | train_inner | epoch 198:   1262 / 1349 loss=1.778, nll_loss=0.183, ppl=1.14, wps=54197.8, ups=13.92, wpb=3893.8, bsz=210.2, num_updates=267000, lr=6.1199e-05, gnorm=0.547, loss_scale=8, train_wall=7, gb_free=29.7, wall=2800
2024-01-31 10:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 10:59:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:59:38 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 2.208 | nll_loss 0.465 | ppl 1.38 | wps 116322 | wpb 3275.2 | bsz 176.5 | num_updates 267087 | best_loss 2.063
2024-01-31 10:59:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 267087 updates
2024-01-31 10:59:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint198.pt
2024-01-31 10:59:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint198.pt
2024-01-31 10:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint198.pt (epoch 198 @ 267087 updates, score 2.208) (writing took 2.4840576379792765 seconds)
2024-01-31 10:59:41 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2024-01-31 10:59:41 | INFO | train | epoch 198 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 51477.5 | ups 13.13 | wpb 3921.2 | bsz 214.5 | num_updates 267087 | lr 6.1189e-05 | gnorm 0.54 | loss_scale 8 | train_wall 94 | gb_free 29.7 | wall 2811
2024-01-31 10:59:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 10:59:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 10:59:41 | INFO | fairseq.trainer | begin training epoch 199
2024-01-31 10:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 10:59:42 | INFO | train_inner | epoch 199:     13 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=31798.7, ups=8.1, wpb=3925.6, bsz=224.5, num_updates=267100, lr=6.11875e-05, gnorm=0.527, loss_scale=8, train_wall=7, gb_free=29.8, wall=2812
2024-01-31 10:59:49 | INFO | train_inner | epoch 199:    113 / 1349 loss=1.776, nll_loss=0.179, ppl=1.13, wps=54279.8, ups=13.86, wpb=3915.5, bsz=217, num_updates=267200, lr=6.11761e-05, gnorm=0.543, loss_scale=8, train_wall=7, gb_free=29.8, wall=2820
2024-01-31 10:59:56 | INFO | train_inner | epoch 199:    213 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54810.7, ups=13.82, wpb=3965, bsz=212.2, num_updates=267300, lr=6.11647e-05, gnorm=0.477, loss_scale=8, train_wall=7, gb_free=29.7, wall=2827
2024-01-31 11:00:04 | INFO | train_inner | epoch 199:    313 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54461.2, ups=14.07, wpb=3870.9, bsz=218.5, num_updates=267400, lr=6.11532e-05, gnorm=0.484, loss_scale=8, train_wall=7, gb_free=29.8, wall=2834
2024-01-31 11:00:11 | INFO | train_inner | epoch 199:    413 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55832, ups=14.21, wpb=3928.9, bsz=214.6, num_updates=267500, lr=6.11418e-05, gnorm=0.503, loss_scale=8, train_wall=7, gb_free=29.8, wall=2841
2024-01-31 11:00:18 | INFO | train_inner | epoch 199:    513 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54301.5, ups=13.92, wpb=3902.1, bsz=220.2, num_updates=267600, lr=6.11304e-05, gnorm=0.488, loss_scale=8, train_wall=7, gb_free=29.8, wall=2848
2024-01-31 11:00:25 | INFO | train_inner | epoch 199:    613 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54572.6, ups=13.9, wpb=3927.2, bsz=229.4, num_updates=267700, lr=6.11189e-05, gnorm=0.535, loss_scale=8, train_wall=7, gb_free=29.7, wall=2855
2024-01-31 11:00:32 | INFO | train_inner | epoch 199:    713 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=56253.8, ups=14.24, wpb=3951.5, bsz=215.4, num_updates=267800, lr=6.11075e-05, gnorm=0.461, loss_scale=8, train_wall=7, gb_free=29.8, wall=2862
2024-01-31 11:00:39 | INFO | train_inner | epoch 199:    813 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54872.6, ups=14, wpb=3919.2, bsz=209.8, num_updates=267900, lr=6.10961e-05, gnorm=0.51, loss_scale=8, train_wall=7, gb_free=29.8, wall=2869
2024-01-31 11:00:46 | INFO | train_inner | epoch 199:    913 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=54072.9, ups=13.81, wpb=3915, bsz=206.3, num_updates=268000, lr=6.10847e-05, gnorm=0.566, loss_scale=8, train_wall=7, gb_free=29.9, wall=2877
2024-01-31 11:00:53 | INFO | train_inner | epoch 199:   1013 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=55316, ups=14.18, wpb=3902.3, bsz=211.5, num_updates=268100, lr=6.10733e-05, gnorm=0.555, loss_scale=8, train_wall=7, gb_free=29.8, wall=2884
2024-01-31 11:01:01 | INFO | train_inner | epoch 199:   1113 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55485.9, ups=14.11, wpb=3932, bsz=211.6, num_updates=268200, lr=6.10619e-05, gnorm=0.612, loss_scale=8, train_wall=7, gb_free=29.8, wall=2891
2024-01-31 11:01:08 | INFO | train_inner | epoch 199:   1213 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54337.5, ups=13.85, wpb=3923.4, bsz=220.9, num_updates=268300, lr=6.10506e-05, gnorm=0.574, loss_scale=8, train_wall=7, gb_free=29.8, wall=2898
2024-01-31 11:01:15 | INFO | train_inner | epoch 199:   1313 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55706.3, ups=14.21, wpb=3921.5, bsz=210.7, num_updates=268400, lr=6.10392e-05, gnorm=0.471, loss_scale=8, train_wall=7, gb_free=29.7, wall=2905
2024-01-31 11:01:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:01:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:01:20 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 2.203 | nll_loss 0.46 | ppl 1.38 | wps 116237 | wpb 3275.2 | bsz 176.5 | num_updates 268436 | best_loss 2.063
2024-01-31 11:01:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 268436 updates
2024-01-31 11:01:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint199.pt
2024-01-31 11:01:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint199.pt
2024-01-31 11:01:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint199.pt (epoch 199 @ 268436 updates, score 2.203) (writing took 2.5254272799938917 seconds)
2024-01-31 11:01:22 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2024-01-31 11:01:22 | INFO | train | epoch 199 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 52199.9 | ups 13.31 | wpb 3921.2 | bsz 214.5 | num_updates 268436 | lr 6.10351e-05 | gnorm 0.522 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 2913
2024-01-31 11:01:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:01:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:01:22 | INFO | fairseq.trainer | begin training epoch 200
2024-01-31 11:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:01:27 | INFO | train_inner | epoch 200:     64 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=31564.6, ups=8.1, wpb=3897.2, bsz=214.7, num_updates=268500, lr=6.10278e-05, gnorm=0.535, loss_scale=8, train_wall=7, gb_free=29.8, wall=2917
2024-01-31 11:01:34 | INFO | train_inner | epoch 200:    164 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54887.4, ups=13.95, wpb=3933.7, bsz=217.3, num_updates=268600, lr=6.10165e-05, gnorm=0.619, loss_scale=8, train_wall=7, gb_free=29.7, wall=2925
2024-01-31 11:01:41 | INFO | train_inner | epoch 200:    264 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55598.8, ups=14.14, wpb=3932, bsz=219.6, num_updates=268700, lr=6.10051e-05, gnorm=0.528, loss_scale=8, train_wall=7, gb_free=29.8, wall=2932
2024-01-31 11:01:49 | INFO | train_inner | epoch 200:    364 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55167.5, ups=14.01, wpb=3938.9, bsz=214.6, num_updates=268800, lr=6.09938e-05, gnorm=0.523, loss_scale=8, train_wall=7, gb_free=29.7, wall=2939
2024-01-31 11:01:56 | INFO | train_inner | epoch 200:    464 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54499.4, ups=13.91, wpb=3916.8, bsz=213.7, num_updates=268900, lr=6.09824e-05, gnorm=0.545, loss_scale=8, train_wall=7, gb_free=29.7, wall=2946
2024-01-31 11:02:03 | INFO | train_inner | epoch 200:    564 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54876.9, ups=14.29, wpb=3839.4, bsz=206.4, num_updates=269000, lr=6.09711e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.7, wall=2953
2024-01-31 11:02:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-01-31 11:02:10 | INFO | train_inner | epoch 200:    665 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54586, ups=13.88, wpb=3933.6, bsz=218, num_updates=269100, lr=6.09597e-05, gnorm=0.579, loss_scale=4, train_wall=7, gb_free=29.7, wall=2960
2024-01-31 11:02:17 | INFO | train_inner | epoch 200:    765 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55031.4, ups=13.92, wpb=3953.6, bsz=223.8, num_updates=269200, lr=6.09484e-05, gnorm=0.434, loss_scale=4, train_wall=7, gb_free=29.9, wall=2967
2024-01-31 11:02:24 | INFO | train_inner | epoch 200:    865 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55395.9, ups=14.24, wpb=3889.7, bsz=206.2, num_updates=269300, lr=6.09371e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.8, wall=2974
2024-01-31 11:02:31 | INFO | train_inner | epoch 200:    965 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=56096.7, ups=14.23, wpb=3941.3, bsz=222.2, num_updates=269400, lr=6.09258e-05, gnorm=0.474, loss_scale=4, train_wall=7, gb_free=29.8, wall=2981
2024-01-31 11:02:38 | INFO | train_inner | epoch 200:   1065 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=55554.5, ups=14.09, wpb=3943.6, bsz=203.1, num_updates=269500, lr=6.09145e-05, gnorm=0.5, loss_scale=4, train_wall=7, gb_free=29.7, wall=2989
2024-01-31 11:02:45 | INFO | train_inner | epoch 200:   1165 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54810.4, ups=14.05, wpb=3901.8, bsz=201.3, num_updates=269600, lr=6.09032e-05, gnorm=0.486, loss_scale=4, train_wall=7, gb_free=29.8, wall=2996
2024-01-31 11:02:52 | INFO | train_inner | epoch 200:   1265 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=56222.6, ups=14.27, wpb=3941.2, bsz=219.5, num_updates=269700, lr=6.08919e-05, gnorm=0.573, loss_scale=4, train_wall=7, gb_free=29.7, wall=3003
2024-01-31 11:02:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:02:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:03:01 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 2.203 | nll_loss 0.46 | ppl 1.38 | wps 114932 | wpb 3275.2 | bsz 176.5 | num_updates 269784 | best_loss 2.063
2024-01-31 11:03:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 269784 updates
2024-01-31 11:03:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint200.pt
2024-01-31 11:03:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint200.pt
2024-01-31 11:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint200.pt (epoch 200 @ 269784 updates, score 2.203) (writing took 2.4288623180473223 seconds)
2024-01-31 11:03:03 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2024-01-31 11:03:03 | INFO | train | epoch 200 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 52306.8 | ups 13.34 | wpb 3921.1 | bsz 214.5 | num_updates 269784 | lr 6.08824e-05 | gnorm 0.53 | loss_scale 4 | train_wall 92 | gb_free 29.8 | wall 3014
2024-01-31 11:03:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:03:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:03:03 | INFO | fairseq.trainer | begin training epoch 201
2024-01-31 11:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:03:05 | INFO | train_inner | epoch 201:     16 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=31789.6, ups=8.08, wpb=3936.3, bsz=228.2, num_updates=269800, lr=6.08806e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.8, wall=3015
2024-01-31 11:03:12 | INFO | train_inner | epoch 201:    116 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55836.9, ups=14.31, wpb=3901.1, bsz=216.6, num_updates=269900, lr=6.08693e-05, gnorm=0.464, loss_scale=4, train_wall=7, gb_free=29.7, wall=3022
2024-01-31 11:03:19 | INFO | train_inner | epoch 201:    216 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53880.9, ups=13.96, wpb=3858.4, bsz=209.3, num_updates=270000, lr=6.08581e-05, gnorm=0.593, loss_scale=4, train_wall=7, gb_free=29.9, wall=3029
2024-01-31 11:03:26 | INFO | train_inner | epoch 201:    316 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54146.6, ups=13.91, wpb=3892.1, bsz=216.6, num_updates=270100, lr=6.08468e-05, gnorm=0.586, loss_scale=4, train_wall=7, gb_free=29.8, wall=3036
2024-01-31 11:03:33 | INFO | train_inner | epoch 201:    416 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55771.6, ups=14.19, wpb=3931.3, bsz=225.5, num_updates=270200, lr=6.08355e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.9, wall=3043
2024-01-31 11:03:40 | INFO | train_inner | epoch 201:    516 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55594.5, ups=14.25, wpb=3901.6, bsz=203.7, num_updates=270300, lr=6.08243e-05, gnorm=0.485, loss_scale=4, train_wall=7, gb_free=29.8, wall=3050
2024-01-31 11:03:47 | INFO | train_inner | epoch 201:    616 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54724.4, ups=13.96, wpb=3921.2, bsz=207, num_updates=270400, lr=6.0813e-05, gnorm=0.547, loss_scale=4, train_wall=7, gb_free=29.8, wall=3058
2024-01-31 11:03:55 | INFO | train_inner | epoch 201:    716 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54589.5, ups=13.93, wpb=3917.5, bsz=204.4, num_updates=270500, lr=6.08018e-05, gnorm=0.52, loss_scale=4, train_wall=7, gb_free=29.7, wall=3065
2024-01-31 11:04:02 | INFO | train_inner | epoch 201:    816 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=56367.3, ups=14.26, wpb=3952, bsz=203.5, num_updates=270600, lr=6.07906e-05, gnorm=0.624, loss_scale=4, train_wall=7, gb_free=29.7, wall=3072
2024-01-31 11:04:09 | INFO | train_inner | epoch 201:    916 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55258.5, ups=13.95, wpb=3961.7, bsz=222.2, num_updates=270700, lr=6.07793e-05, gnorm=0.439, loss_scale=4, train_wall=7, gb_free=29.7, wall=3079
2024-01-31 11:04:16 | INFO | train_inner | epoch 201:   1016 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54954.3, ups=13.86, wpb=3964.2, bsz=209.5, num_updates=270800, lr=6.07681e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.7, wall=3086
2024-01-31 11:04:23 | INFO | train_inner | epoch 201:   1116 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55162, ups=14.12, wpb=3905.3, bsz=223.5, num_updates=270900, lr=6.07569e-05, gnorm=0.554, loss_scale=4, train_wall=7, gb_free=29.8, wall=3093
2024-01-31 11:04:30 | INFO | train_inner | epoch 201:   1216 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54911.4, ups=13.98, wpb=3928.9, bsz=211.9, num_updates=271000, lr=6.07457e-05, gnorm=0.576, loss_scale=4, train_wall=7, gb_free=29.7, wall=3100
2024-01-31 11:04:37 | INFO | train_inner | epoch 201:   1316 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54604.5, ups=13.92, wpb=3922, bsz=213.8, num_updates=271100, lr=6.07345e-05, gnorm=0.519, loss_scale=4, train_wall=7, gb_free=29.7, wall=3108
2024-01-31 11:04:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:04:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:04:42 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 2.204 | nll_loss 0.461 | ppl 1.38 | wps 117463 | wpb 3275.2 | bsz 176.5 | num_updates 271133 | best_loss 2.063
2024-01-31 11:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 271133 updates
2024-01-31 11:04:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint201.pt
2024-01-31 11:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint201.pt
2024-01-31 11:04:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint201.pt (epoch 201 @ 271133 updates, score 2.204) (writing took 2.416323689976707 seconds)
2024-01-31 11:04:45 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2024-01-31 11:04:45 | INFO | train | epoch 201 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 52292.6 | ups 13.34 | wpb 3921.2 | bsz 214.5 | num_updates 271133 | lr 6.07308e-05 | gnorm 0.534 | loss_scale 4 | train_wall 92 | gb_free 29.7 | wall 3115
2024-01-31 11:04:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:04:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:04:45 | INFO | fairseq.trainer | begin training epoch 202
2024-01-31 11:04:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:04:49 | INFO | train_inner | epoch 202:     67 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=32715.3, ups=8.29, wpb=3947.7, bsz=232.2, num_updates=271200, lr=6.07233e-05, gnorm=0.503, loss_scale=4, train_wall=7, gb_free=29.7, wall=3120
2024-01-31 11:04:57 | INFO | train_inner | epoch 202:    167 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54309.6, ups=14, wpb=3878.4, bsz=216.5, num_updates=271300, lr=6.07121e-05, gnorm=0.541, loss_scale=4, train_wall=7, gb_free=29.8, wall=3127
2024-01-31 11:05:04 | INFO | train_inner | epoch 202:    267 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55204.5, ups=14.13, wpb=3907.6, bsz=210.2, num_updates=271400, lr=6.07009e-05, gnorm=0.563, loss_scale=4, train_wall=7, gb_free=29.7, wall=3134
2024-01-31 11:05:11 | INFO | train_inner | epoch 202:    367 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=56008.6, ups=14.25, wpb=3929.6, bsz=207.8, num_updates=271500, lr=6.06897e-05, gnorm=0.581, loss_scale=4, train_wall=7, gb_free=29.8, wall=3141
2024-01-31 11:05:18 | INFO | train_inner | epoch 202:    467 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55365, ups=14.04, wpb=3943.9, bsz=209.1, num_updates=271600, lr=6.06785e-05, gnorm=0.517, loss_scale=4, train_wall=7, gb_free=29.8, wall=3148
2024-01-31 11:05:25 | INFO | train_inner | epoch 202:    567 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55399.2, ups=14.05, wpb=3942.9, bsz=213.6, num_updates=271700, lr=6.06674e-05, gnorm=0.545, loss_scale=4, train_wall=7, gb_free=29.8, wall=3155
2024-01-31 11:05:32 | INFO | train_inner | epoch 202:    667 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55858, ups=14.17, wpb=3940.7, bsz=208.6, num_updates=271800, lr=6.06562e-05, gnorm=0.467, loss_scale=4, train_wall=7, gb_free=29.8, wall=3162
2024-01-31 11:05:39 | INFO | train_inner | epoch 202:    767 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54334.5, ups=14.05, wpb=3867.2, bsz=200.5, num_updates=271900, lr=6.06451e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.8, wall=3169
2024-01-31 11:05:46 | INFO | train_inner | epoch 202:    867 / 1349 loss=1.778, nll_loss=0.183, ppl=1.14, wps=55109.2, ups=14.02, wpb=3931.2, bsz=207.4, num_updates=272000, lr=6.06339e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.9, wall=3176
2024-01-31 11:05:53 | INFO | train_inner | epoch 202:    967 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55980.4, ups=14.22, wpb=3935.8, bsz=223, num_updates=272100, lr=6.06228e-05, gnorm=0.549, loss_scale=4, train_wall=7, gb_free=29.8, wall=3183
2024-01-31 11:06:00 | INFO | train_inner | epoch 202:   1067 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53912.7, ups=14.01, wpb=3847.2, bsz=211.4, num_updates=272200, lr=6.06116e-05, gnorm=0.549, loss_scale=4, train_wall=7, gb_free=29.8, wall=3191
2024-01-31 11:06:08 | INFO | train_inner | epoch 202:   1167 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54452.1, ups=13.84, wpb=3934, bsz=228.2, num_updates=272300, lr=6.06005e-05, gnorm=0.528, loss_scale=4, train_wall=7, gb_free=29.8, wall=3198
2024-01-31 11:06:15 | INFO | train_inner | epoch 202:   1267 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55122.2, ups=13.88, wpb=3972.4, bsz=234.6, num_updates=272400, lr=6.05894e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.7, wall=3205
2024-01-31 11:06:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:06:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:06:23 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 2.202 | nll_loss 0.461 | ppl 1.38 | wps 111050 | wpb 3275.2 | bsz 176.5 | num_updates 272482 | best_loss 2.063
2024-01-31 11:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 272482 updates
2024-01-31 11:06:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint202.pt
2024-01-31 11:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint202.pt
2024-01-31 11:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint202.pt (epoch 202 @ 272482 updates, score 2.202) (writing took 2.5010292360093445 seconds)
2024-01-31 11:06:26 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2024-01-31 11:06:26 | INFO | train | epoch 202 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 52318.7 | ups 13.34 | wpb 3921.2 | bsz 214.5 | num_updates 272482 | lr 6.05803e-05 | gnorm 0.537 | loss_scale 4 | train_wall 92 | gb_free 29.7 | wall 3216
2024-01-31 11:06:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:06:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:06:26 | INFO | fairseq.trainer | begin training epoch 203
2024-01-31 11:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:06:27 | INFO | train_inner | epoch 203:     18 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=32080.9, ups=8.14, wpb=3942.3, bsz=214.9, num_updates=272500, lr=6.05783e-05, gnorm=0.571, loss_scale=4, train_wall=7, gb_free=29.9, wall=3217
2024-01-31 11:06:34 | INFO | train_inner | epoch 203:    118 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54375.9, ups=13.98, wpb=3890.8, bsz=215.6, num_updates=272600, lr=6.05671e-05, gnorm=0.582, loss_scale=4, train_wall=7, gb_free=29.8, wall=3224
2024-01-31 11:06:41 | INFO | train_inner | epoch 203:    218 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=56627.2, ups=14.23, wpb=3980.7, bsz=215.8, num_updates=272700, lr=6.0556e-05, gnorm=0.494, loss_scale=4, train_wall=7, gb_free=29.7, wall=3232
2024-01-31 11:06:48 | INFO | train_inner | epoch 203:    318 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54580.3, ups=13.92, wpb=3921, bsz=218.9, num_updates=272800, lr=6.05449e-05, gnorm=0.492, loss_scale=4, train_wall=7, gb_free=29.8, wall=3239
2024-01-31 11:06:56 | INFO | train_inner | epoch 203:    418 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54948.1, ups=13.96, wpb=3936.4, bsz=222.4, num_updates=272900, lr=6.05338e-05, gnorm=0.492, loss_scale=4, train_wall=7, gb_free=29.8, wall=3246
2024-01-31 11:07:03 | INFO | train_inner | epoch 203:    518 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55949.9, ups=14.15, wpb=3952.9, bsz=218, num_updates=273000, lr=6.05228e-05, gnorm=0.505, loss_scale=4, train_wall=7, gb_free=29.7, wall=3253
2024-01-31 11:07:14 | INFO | train_inner | epoch 203:    618 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=32861.3, ups=8.47, wpb=3881.9, bsz=214.9, num_updates=273100, lr=6.05117e-05, gnorm=0.551, loss_scale=4, train_wall=11, gb_free=29.8, wall=3265
2024-01-31 11:07:22 | INFO | train_inner | epoch 203:    718 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55249.6, ups=14.17, wpb=3900.4, bsz=221.7, num_updates=273200, lr=6.05006e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=30, wall=3272
2024-01-31 11:07:29 | INFO | train_inner | epoch 203:    818 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54717.4, ups=13.92, wpb=3931.8, bsz=210.2, num_updates=273300, lr=6.04895e-05, gnorm=0.469, loss_scale=4, train_wall=7, gb_free=29.8, wall=3279
2024-01-31 11:07:36 | INFO | train_inner | epoch 203:    918 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55014.1, ups=13.97, wpb=3939.2, bsz=208.1, num_updates=273400, lr=6.04785e-05, gnorm=0.521, loss_scale=4, train_wall=7, gb_free=29.8, wall=3286
2024-01-31 11:07:43 | INFO | train_inner | epoch 203:   1018 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55132.9, ups=14.03, wpb=3930.9, bsz=212.7, num_updates=273500, lr=6.04674e-05, gnorm=0.489, loss_scale=4, train_wall=7, gb_free=29.7, wall=3293
2024-01-31 11:07:50 | INFO | train_inner | epoch 203:   1118 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54962.1, ups=14.07, wpb=3906.4, bsz=202.9, num_updates=273600, lr=6.04564e-05, gnorm=0.549, loss_scale=4, train_wall=7, gb_free=29.8, wall=3300
2024-01-31 11:07:58 | INFO | train_inner | epoch 203:   1218 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=51535.1, ups=13.19, wpb=3906.4, bsz=217, num_updates=273700, lr=6.04453e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.7, wall=3308
2024-01-31 11:08:05 | INFO | train_inner | epoch 203:   1318 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54307.7, ups=13.92, wpb=3902.2, bsz=213.3, num_updates=273800, lr=6.04343e-05, gnorm=0.551, loss_scale=4, train_wall=7, gb_free=29.9, wall=3315
2024-01-31 11:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:08:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:08:10 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 2.208 | nll_loss 0.465 | ppl 1.38 | wps 116168 | wpb 3275.2 | bsz 176.5 | num_updates 273831 | best_loss 2.063
2024-01-31 11:08:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 273831 updates
2024-01-31 11:08:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint203.pt
2024-01-31 11:08:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint203.pt
2024-01-31 11:08:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint203.pt (epoch 203 @ 273831 updates, score 2.208) (writing took 2.8612707649590448 seconds)
2024-01-31 11:08:13 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2024-01-31 11:08:13 | INFO | train | epoch 203 | loss 1.776 | nll_loss 0.181 | ppl 1.13 | wps 49447.8 | ups 12.61 | wpb 3921.2 | bsz 214.5 | num_updates 273831 | lr 6.04308e-05 | gnorm 0.523 | loss_scale 4 | train_wall 97 | gb_free 29.8 | wall 3323
2024-01-31 11:08:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:08:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:08:13 | INFO | fairseq.trainer | begin training epoch 204
2024-01-31 11:08:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:08:18 | INFO | train_inner | epoch 204:     69 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=30629.6, ups=7.8, wpb=3927.2, bsz=202.4, num_updates=273900, lr=6.04232e-05, gnorm=0.461, loss_scale=4, train_wall=7, gb_free=29.7, wall=3328
2024-01-31 11:08:25 | INFO | train_inner | epoch 204:    169 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54545.6, ups=13.88, wpb=3930.6, bsz=221.1, num_updates=274000, lr=6.04122e-05, gnorm=0.47, loss_scale=4, train_wall=7, gb_free=29.7, wall=3335
2024-01-31 11:08:32 | INFO | train_inner | epoch 204:    269 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55957, ups=14.38, wpb=3892, bsz=214.4, num_updates=274100, lr=6.04012e-05, gnorm=0.592, loss_scale=4, train_wall=7, gb_free=29.8, wall=3342
2024-01-31 11:08:39 | INFO | train_inner | epoch 204:    369 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54638.1, ups=13.99, wpb=3905.9, bsz=221.4, num_updates=274200, lr=6.03902e-05, gnorm=0.532, loss_scale=4, train_wall=7, gb_free=29.8, wall=3349
2024-01-31 11:08:46 | INFO | train_inner | epoch 204:    469 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54938.8, ups=13.87, wpb=3962.3, bsz=212, num_updates=274300, lr=6.03792e-05, gnorm=0.511, loss_scale=4, train_wall=7, gb_free=29.8, wall=3357
2024-01-31 11:08:53 | INFO | train_inner | epoch 204:    569 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55264.5, ups=14.07, wpb=3927.8, bsz=222.6, num_updates=274400, lr=6.03682e-05, gnorm=0.524, loss_scale=4, train_wall=7, gb_free=29.7, wall=3364
2024-01-31 11:09:01 | INFO | train_inner | epoch 204:    669 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55158.4, ups=13.99, wpb=3942.2, bsz=216.6, num_updates=274500, lr=6.03572e-05, gnorm=0.56, loss_scale=4, train_wall=7, gb_free=29.7, wall=3371
2024-01-31 11:09:08 | INFO | train_inner | epoch 204:    769 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=53950.7, ups=13.96, wpb=3864.3, bsz=205.4, num_updates=274600, lr=6.03462e-05, gnorm=0.595, loss_scale=4, train_wall=7, gb_free=29.7, wall=3378
2024-01-31 11:09:15 | INFO | train_inner | epoch 204:    869 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55056.2, ups=14.01, wpb=3930.4, bsz=215.9, num_updates=274700, lr=6.03352e-05, gnorm=0.505, loss_scale=4, train_wall=7, gb_free=29.8, wall=3385
2024-01-31 11:09:22 | INFO | train_inner | epoch 204:    969 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55670.3, ups=14.21, wpb=3918.1, bsz=213.1, num_updates=274800, lr=6.03242e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.8, wall=3392
2024-01-31 11:09:29 | INFO | train_inner | epoch 204:   1069 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54436.5, ups=13.82, wpb=3940.1, bsz=220.8, num_updates=274900, lr=6.03132e-05, gnorm=0.543, loss_scale=4, train_wall=7, gb_free=29.8, wall=3399
2024-01-31 11:09:36 | INFO | train_inner | epoch 204:   1169 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53930.1, ups=13.83, wpb=3900.6, bsz=206.2, num_updates=275000, lr=6.03023e-05, gnorm=0.504, loss_scale=4, train_wall=7, gb_free=29.8, wall=3407
2024-01-31 11:09:43 | INFO | train_inner | epoch 204:   1269 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55508.9, ups=14.07, wpb=3946.2, bsz=218.6, num_updates=275100, lr=6.02913e-05, gnorm=0.63, loss_scale=4, train_wall=7, gb_free=29.8, wall=3414
2024-01-31 11:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:09:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:09:52 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 2.21 | nll_loss 0.467 | ppl 1.38 | wps 115940 | wpb 3275.2 | bsz 176.5 | num_updates 275180 | best_loss 2.063
2024-01-31 11:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 275180 updates
2024-01-31 11:09:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint204.pt
2024-01-31 11:09:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint204.pt
2024-01-31 11:09:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint204.pt (epoch 204 @ 275180 updates, score 2.21) (writing took 2.5241065690061077 seconds)
2024-01-31 11:09:54 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2024-01-31 11:09:54 | INFO | train | epoch 204 | loss 1.777 | nll_loss 0.181 | ppl 1.13 | wps 52062.4 | ups 13.28 | wpb 3921.2 | bsz 214.5 | num_updates 275180 | lr 6.02825e-05 | gnorm 0.537 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 3424
2024-01-31 11:09:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:09:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:09:54 | INFO | fairseq.trainer | begin training epoch 205
2024-01-31 11:09:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:09:56 | INFO | train_inner | epoch 205:     20 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=31617.8, ups=8.14, wpb=3885.5, bsz=208.4, num_updates=275200, lr=6.02804e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.8, wall=3426
2024-01-31 11:10:03 | INFO | train_inner | epoch 205:    120 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55095.9, ups=13.98, wpb=3942.4, bsz=213, num_updates=275300, lr=6.02694e-05, gnorm=0.574, loss_scale=4, train_wall=7, gb_free=29.7, wall=3433
2024-01-31 11:10:10 | INFO | train_inner | epoch 205:    220 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55767.8, ups=14.08, wpb=3960.1, bsz=213.4, num_updates=275400, lr=6.02585e-05, gnorm=0.508, loss_scale=4, train_wall=7, gb_free=29.8, wall=3440
2024-01-31 11:10:17 | INFO | train_inner | epoch 205:    320 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53856, ups=13.94, wpb=3864.6, bsz=215.2, num_updates=275500, lr=6.02475e-05, gnorm=0.479, loss_scale=4, train_wall=7, gb_free=29.7, wall=3447
2024-01-31 11:10:24 | INFO | train_inner | epoch 205:    420 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55099.9, ups=13.97, wpb=3943.6, bsz=214.9, num_updates=275600, lr=6.02366e-05, gnorm=0.508, loss_scale=4, train_wall=7, gb_free=30, wall=3455
2024-01-31 11:10:31 | INFO | train_inner | epoch 205:    520 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55789.6, ups=14.23, wpb=3920.5, bsz=212.5, num_updates=275700, lr=6.02257e-05, gnorm=0.501, loss_scale=4, train_wall=7, gb_free=29.7, wall=3462
2024-01-31 11:10:39 | INFO | train_inner | epoch 205:    620 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=53427.8, ups=13.78, wpb=3876.1, bsz=221.5, num_updates=275800, lr=6.02147e-05, gnorm=0.599, loss_scale=4, train_wall=7, gb_free=29.7, wall=3469
2024-01-31 11:10:46 | INFO | train_inner | epoch 205:    720 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53493.5, ups=13.9, wpb=3847.2, bsz=216.2, num_updates=275900, lr=6.02038e-05, gnorm=0.509, loss_scale=4, train_wall=7, gb_free=29.8, wall=3476
2024-01-31 11:10:53 | INFO | train_inner | epoch 205:    820 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55793.7, ups=14.26, wpb=3913.2, bsz=214.2, num_updates=276000, lr=6.01929e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.7, wall=3483
2024-01-31 11:11:00 | INFO | train_inner | epoch 205:    920 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54992.1, ups=14.02, wpb=3923.1, bsz=204.1, num_updates=276100, lr=6.0182e-05, gnorm=0.547, loss_scale=4, train_wall=7, gb_free=29.8, wall=3490
2024-01-31 11:11:07 | INFO | train_inner | epoch 205:   1020 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54773, ups=13.91, wpb=3938.5, bsz=220.7, num_updates=276200, lr=6.01711e-05, gnorm=0.563, loss_scale=4, train_wall=7, gb_free=29.8, wall=3497
2024-01-31 11:11:14 | INFO | train_inner | epoch 205:   1120 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54514.8, ups=13.91, wpb=3918.4, bsz=215, num_updates=276300, lr=6.01602e-05, gnorm=0.535, loss_scale=4, train_wall=7, gb_free=29.8, wall=3505
2024-01-31 11:11:21 | INFO | train_inner | epoch 205:   1220 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=56153.3, ups=14.18, wpb=3960.7, bsz=213, num_updates=276400, lr=6.01494e-05, gnorm=0.543, loss_scale=4, train_wall=7, gb_free=29.8, wall=3512
2024-01-31 11:11:29 | INFO | train_inner | epoch 205:   1320 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55465.2, ups=13.98, wpb=3968.3, bsz=209.2, num_updates=276500, lr=6.01385e-05, gnorm=0.574, loss_scale=4, train_wall=7, gb_free=29.8, wall=3519
2024-01-31 11:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:11:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:11:33 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 2.209 | nll_loss 0.464 | ppl 1.38 | wps 110384 | wpb 3275.2 | bsz 176.5 | num_updates 276529 | best_loss 2.063
2024-01-31 11:11:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 276529 updates
2024-01-31 11:11:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint205.pt
2024-01-31 11:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint205.pt
2024-01-31 11:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint205.pt (epoch 205 @ 276529 updates, score 2.209) (writing took 2.484584652003832 seconds)
2024-01-31 11:11:36 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2024-01-31 11:11:36 | INFO | train | epoch 205 | loss 1.776 | nll_loss 0.181 | ppl 1.13 | wps 52118.7 | ups 13.29 | wpb 3921.2 | bsz 214.5 | num_updates 276529 | lr 6.01353e-05 | gnorm 0.531 | loss_scale 4 | train_wall 92 | gb_free 29.7 | wall 3526
2024-01-31 11:11:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:11:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:11:36 | INFO | fairseq.trainer | begin training epoch 206
2024-01-31 11:11:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:11:41 | INFO | train_inner | epoch 206:     71 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=31531.2, ups=8.13, wpb=3876.4, bsz=225.4, num_updates=276600, lr=6.01276e-05, gnorm=0.519, loss_scale=4, train_wall=7, gb_free=29.7, wall=3531
2024-01-31 11:11:48 | INFO | train_inner | epoch 206:    171 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55161.4, ups=14, wpb=3939.4, bsz=215.6, num_updates=276700, lr=6.01167e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.8, wall=3538
2024-01-31 11:11:55 | INFO | train_inner | epoch 206:    271 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54491.5, ups=14.05, wpb=3877.9, bsz=212.3, num_updates=276800, lr=6.01059e-05, gnorm=0.508, loss_scale=4, train_wall=7, gb_free=29.8, wall=3545
2024-01-31 11:12:02 | INFO | train_inner | epoch 206:    371 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55631.2, ups=14.08, wpb=3952.2, bsz=228.1, num_updates=276900, lr=6.0095e-05, gnorm=0.452, loss_scale=4, train_wall=7, gb_free=29.8, wall=3553
2024-01-31 11:12:09 | INFO | train_inner | epoch 206:    471 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54377.8, ups=14.01, wpb=3880.1, bsz=205.4, num_updates=277000, lr=6.00842e-05, gnorm=0.497, loss_scale=4, train_wall=7, gb_free=29.8, wall=3560
2024-01-31 11:12:17 | INFO | train_inner | epoch 206:    571 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55146.9, ups=13.89, wpb=3968.9, bsz=208.3, num_updates=277100, lr=6.00733e-05, gnorm=0.442, loss_scale=4, train_wall=7, gb_free=29.8, wall=3567
2024-01-31 11:12:24 | INFO | train_inner | epoch 206:    671 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=55183.3, ups=13.9, wpb=3971, bsz=221.3, num_updates=277200, lr=6.00625e-05, gnorm=0.498, loss_scale=4, train_wall=7, gb_free=29.8, wall=3574
2024-01-31 11:12:31 | INFO | train_inner | epoch 206:    771 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54921.8, ups=14.23, wpb=3858.6, bsz=211.9, num_updates=277300, lr=6.00517e-05, gnorm=0.497, loss_scale=4, train_wall=7, gb_free=29.8, wall=3581
2024-01-31 11:12:38 | INFO | train_inner | epoch 206:    871 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54912.8, ups=13.94, wpb=3939.3, bsz=213.1, num_updates=277400, lr=6.00408e-05, gnorm=0.474, loss_scale=4, train_wall=7, gb_free=29.9, wall=3588
2024-01-31 11:12:45 | INFO | train_inner | epoch 206:    971 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55251.5, ups=13.97, wpb=3954.4, bsz=210.2, num_updates=277500, lr=6.003e-05, gnorm=0.523, loss_scale=4, train_wall=7, gb_free=29.8, wall=3595
2024-01-31 11:12:52 | INFO | train_inner | epoch 206:   1071 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55573.7, ups=14.24, wpb=3902.1, bsz=213, num_updates=277600, lr=6.00192e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.8, wall=3602
2024-01-31 11:12:59 | INFO | train_inner | epoch 206:   1171 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54844.7, ups=13.99, wpb=3919.8, bsz=210.4, num_updates=277700, lr=6.00084e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.7, wall=3610
2024-01-31 11:13:07 | INFO | train_inner | epoch 206:   1271 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54275.1, ups=13.87, wpb=3913, bsz=219.9, num_updates=277800, lr=5.99976e-05, gnorm=0.506, loss_scale=4, train_wall=7, gb_free=29.7, wall=3617
2024-01-31 11:13:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:13:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:13:15 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 2.206 | nll_loss 0.463 | ppl 1.38 | wps 104563 | wpb 3275.2 | bsz 176.5 | num_updates 277878 | best_loss 2.063
2024-01-31 11:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 277878 updates
2024-01-31 11:13:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint206.pt
2024-01-31 11:13:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint206.pt
2024-01-31 11:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint206.pt (epoch 206 @ 277878 updates, score 2.206) (writing took 2.4548908460419625 seconds)
2024-01-31 11:13:17 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2024-01-31 11:13:17 | INFO | train | epoch 206 | loss 1.776 | nll_loss 0.181 | ppl 1.13 | wps 52119.8 | ups 13.29 | wpb 3921.2 | bsz 214.5 | num_updates 277878 | lr 5.99892e-05 | gnorm 0.504 | loss_scale 4 | train_wall 92 | gb_free 29.8 | wall 3627
2024-01-31 11:13:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:13:17 | INFO | fairseq.trainer | begin training epoch 207
2024-01-31 11:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:13:19 | INFO | train_inner | epoch 207:     22 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=32069.8, ups=8.09, wpb=3962.7, bsz=210.5, num_updates=277900, lr=5.99868e-05, gnorm=0.596, loss_scale=4, train_wall=7, gb_free=29.8, wall=3629
2024-01-31 11:13:26 | INFO | train_inner | epoch 207:    122 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54507.9, ups=13.83, wpb=3940.2, bsz=218.4, num_updates=278000, lr=5.9976e-05, gnorm=0.453, loss_scale=4, train_wall=7, gb_free=29.8, wall=3636
2024-01-31 11:13:33 | INFO | train_inner | epoch 207:    222 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54761.6, ups=13.93, wpb=3931.7, bsz=217.1, num_updates=278100, lr=5.99652e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.8, wall=3644
2024-01-31 11:13:40 | INFO | train_inner | epoch 207:    322 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55536.4, ups=14.06, wpb=3950.3, bsz=205.6, num_updates=278200, lr=5.99545e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.9, wall=3651
2024-01-31 11:13:48 | INFO | train_inner | epoch 207:    422 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54369.4, ups=13.93, wpb=3903.3, bsz=209.4, num_updates=278300, lr=5.99437e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.9, wall=3658
2024-01-31 11:13:55 | INFO | train_inner | epoch 207:    522 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=53766.1, ups=13.82, wpb=3889.5, bsz=222.2, num_updates=278400, lr=5.99329e-05, gnorm=0.452, loss_scale=4, train_wall=7, gb_free=29.8, wall=3665
2024-01-31 11:14:02 | INFO | train_inner | epoch 207:    622 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55540.1, ups=14.1, wpb=3940, bsz=224.7, num_updates=278500, lr=5.99222e-05, gnorm=0.541, loss_scale=4, train_wall=7, gb_free=29.9, wall=3672
2024-01-31 11:14:09 | INFO | train_inner | epoch 207:    722 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55357.5, ups=14.07, wpb=3934.5, bsz=209.8, num_updates=278600, lr=5.99114e-05, gnorm=0.583, loss_scale=4, train_wall=7, gb_free=29.8, wall=3679
2024-01-31 11:14:16 | INFO | train_inner | epoch 207:    822 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54959.8, ups=13.92, wpb=3948.1, bsz=219.3, num_updates=278700, lr=5.99006e-05, gnorm=0.514, loss_scale=4, train_wall=7, gb_free=29.7, wall=3686
2024-01-31 11:14:23 | INFO | train_inner | epoch 207:    922 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55128.5, ups=14.11, wpb=3907.8, bsz=219.8, num_updates=278800, lr=5.98899e-05, gnorm=0.543, loss_scale=4, train_wall=7, gb_free=29.8, wall=3694
2024-01-31 11:14:30 | INFO | train_inner | epoch 207:   1022 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55471.5, ups=14.2, wpb=3905.8, bsz=197.1, num_updates=278900, lr=5.98792e-05, gnorm=0.556, loss_scale=4, train_wall=7, gb_free=29.8, wall=3701
2024-01-31 11:14:37 | INFO | train_inner | epoch 207:   1122 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54489.4, ups=13.95, wpb=3905.1, bsz=214.7, num_updates=279000, lr=5.98684e-05, gnorm=0.521, loss_scale=4, train_wall=7, gb_free=29.8, wall=3708
2024-01-31 11:14:45 | INFO | train_inner | epoch 207:   1222 / 1349 loss=1.779, nll_loss=0.183, ppl=1.14, wps=54041.4, ups=13.81, wpb=3914.2, bsz=219.4, num_updates=279100, lr=5.98577e-05, gnorm=0.572, loss_scale=4, train_wall=7, gb_free=29.8, wall=3715
2024-01-31 11:14:52 | INFO | train_inner | epoch 207:   1322 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=55383.5, ups=14.23, wpb=3892.6, bsz=213.5, num_updates=279200, lr=5.9847e-05, gnorm=0.623, loss_scale=4, train_wall=7, gb_free=29.8, wall=3722
2024-01-31 11:14:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:14:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:14:56 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 2.208 | nll_loss 0.464 | ppl 1.38 | wps 111089 | wpb 3275.2 | bsz 176.5 | num_updates 279227 | best_loss 2.063
2024-01-31 11:14:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 279227 updates
2024-01-31 11:14:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint207.pt
2024-01-31 11:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint207.pt
2024-01-31 11:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint207.pt (epoch 207 @ 279227 updates, score 2.208) (writing took 2.471474716090597 seconds)
2024-01-31 11:14:59 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2024-01-31 11:14:59 | INFO | train | epoch 207 | loss 1.776 | nll_loss 0.181 | ppl 1.13 | wps 52068.7 | ups 13.28 | wpb 3921.2 | bsz 214.5 | num_updates 279227 | lr 5.98441e-05 | gnorm 0.536 | loss_scale 4 | train_wall 93 | gb_free 29.8 | wall 3729
2024-01-31 11:14:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:14:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:14:59 | INFO | fairseq.trainer | begin training epoch 208
2024-01-31 11:14:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:15:04 | INFO | train_inner | epoch 208:     73 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=31777.1, ups=8, wpb=3970.5, bsz=212.2, num_updates=279300, lr=5.98363e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.7, wall=3735
2024-01-31 11:15:11 | INFO | train_inner | epoch 208:    173 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55643.3, ups=14.23, wpb=3911.7, bsz=216.5, num_updates=279400, lr=5.98256e-05, gnorm=0.514, loss_scale=4, train_wall=7, gb_free=29.7, wall=3742
2024-01-31 11:15:18 | INFO | train_inner | epoch 208:    273 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54665.2, ups=14.08, wpb=3882.6, bsz=210.2, num_updates=279500, lr=5.98149e-05, gnorm=0.553, loss_scale=4, train_wall=7, gb_free=29.8, wall=3749
2024-01-31 11:15:26 | INFO | train_inner | epoch 208:    373 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55167.5, ups=13.96, wpb=3951.2, bsz=214.4, num_updates=279600, lr=5.98042e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.7, wall=3756
2024-01-31 11:15:33 | INFO | train_inner | epoch 208:    473 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=55659, ups=14.15, wpb=3934.6, bsz=212, num_updates=279700, lr=5.97935e-05, gnorm=0.577, loss_scale=4, train_wall=7, gb_free=29.8, wall=3763
2024-01-31 11:15:40 | INFO | train_inner | epoch 208:    573 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54727.6, ups=14.03, wpb=3899.7, bsz=214.6, num_updates=279800, lr=5.97828e-05, gnorm=0.56, loss_scale=4, train_wall=7, gb_free=29.8, wall=3770
2024-01-31 11:15:47 | INFO | train_inner | epoch 208:    673 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54955.6, ups=13.94, wpb=3942.3, bsz=214.4, num_updates=279900, lr=5.97721e-05, gnorm=0.589, loss_scale=4, train_wall=7, gb_free=29.8, wall=3777
2024-01-31 11:15:54 | INFO | train_inner | epoch 208:    773 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54801.4, ups=13.94, wpb=3930.4, bsz=208, num_updates=280000, lr=5.97614e-05, gnorm=0.498, loss_scale=4, train_wall=7, gb_free=29.8, wall=3784
2024-01-31 11:16:01 | INFO | train_inner | epoch 208:    873 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54713.7, ups=14.17, wpb=3861.4, bsz=207, num_updates=280100, lr=5.97508e-05, gnorm=0.543, loss_scale=4, train_wall=7, gb_free=29.8, wall=3791
2024-01-31 11:16:08 | INFO | train_inner | epoch 208:    973 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55047, ups=13.87, wpb=3968.3, bsz=219.6, num_updates=280200, lr=5.97401e-05, gnorm=0.509, loss_scale=4, train_wall=7, gb_free=29.7, wall=3799
2024-01-31 11:16:16 | INFO | train_inner | epoch 208:   1073 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53914.4, ups=13.85, wpb=3891.6, bsz=216.1, num_updates=280300, lr=5.97294e-05, gnorm=0.539, loss_scale=4, train_wall=7, gb_free=29.8, wall=3806
2024-01-31 11:16:23 | INFO | train_inner | epoch 208:   1173 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55987.1, ups=14.17, wpb=3952.2, bsz=216.8, num_updates=280400, lr=5.97188e-05, gnorm=0.469, loss_scale=4, train_wall=7, gb_free=29.8, wall=3813
2024-01-31 11:16:30 | INFO | train_inner | epoch 208:   1273 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54289.8, ups=14.01, wpb=3876, bsz=223.2, num_updates=280500, lr=5.97081e-05, gnorm=0.51, loss_scale=4, train_wall=7, gb_free=29.8, wall=3820
2024-01-31 11:16:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:16:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:16:38 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 2.21 | nll_loss 0.468 | ppl 1.38 | wps 114554 | wpb 3275.2 | bsz 176.5 | num_updates 280576 | best_loss 2.063
2024-01-31 11:16:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 280576 updates
2024-01-31 11:16:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint208.pt
2024-01-31 11:16:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint208.pt
2024-01-31 11:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint208.pt (epoch 208 @ 280576 updates, score 2.21) (writing took 2.54692479793448 seconds)
2024-01-31 11:16:40 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2024-01-31 11:16:40 | INFO | train | epoch 208 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 52023.2 | ups 13.27 | wpb 3921.2 | bsz 214.5 | num_updates 280576 | lr 5.97001e-05 | gnorm 0.529 | loss_scale 4 | train_wall 93 | gb_free 29.7 | wall 3831
2024-01-31 11:16:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:16:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:16:40 | INFO | fairseq.trainer | begin training epoch 209
2024-01-31 11:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:16:42 | INFO | train_inner | epoch 209:     24 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=31297.7, ups=7.98, wpb=3923.2, bsz=217.7, num_updates=280600, lr=5.96975e-05, gnorm=0.52, loss_scale=4, train_wall=7, gb_free=29.9, wall=3833
2024-01-31 11:16:50 | INFO | train_inner | epoch 209:    124 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54696.2, ups=13.77, wpb=3973.2, bsz=219.8, num_updates=280700, lr=5.96869e-05, gnorm=0.556, loss_scale=4, train_wall=7, gb_free=29.8, wall=3840
2024-01-31 11:16:57 | INFO | train_inner | epoch 209:    224 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54349.8, ups=13.89, wpb=3912.4, bsz=215.8, num_updates=280800, lr=5.96762e-05, gnorm=0.573, loss_scale=4, train_wall=7, gb_free=29.9, wall=3847
2024-01-31 11:17:04 | INFO | train_inner | epoch 209:    324 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54560.4, ups=13.95, wpb=3912.1, bsz=217.5, num_updates=280900, lr=5.96656e-05, gnorm=0.59, loss_scale=4, train_wall=7, gb_free=29.9, wall=3854
2024-01-31 11:17:11 | INFO | train_inner | epoch 209:    424 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55624.6, ups=14.2, wpb=3918, bsz=210.6, num_updates=281000, lr=5.9655e-05, gnorm=0.583, loss_scale=4, train_wall=7, gb_free=29.8, wall=3861
2024-01-31 11:17:18 | INFO | train_inner | epoch 209:    524 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=55220.4, ups=14, wpb=3943.1, bsz=204.6, num_updates=281100, lr=5.96444e-05, gnorm=0.55, loss_scale=4, train_wall=7, gb_free=29.7, wall=3868
2024-01-31 11:17:25 | INFO | train_inner | epoch 209:    624 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54530.9, ups=13.91, wpb=3919.2, bsz=215.5, num_updates=281200, lr=5.96338e-05, gnorm=0.503, loss_scale=4, train_wall=7, gb_free=29.8, wall=3876
2024-01-31 11:17:32 | INFO | train_inner | epoch 209:    724 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55041.2, ups=14.29, wpb=3851.2, bsz=214.6, num_updates=281300, lr=5.96232e-05, gnorm=0.468, loss_scale=4, train_wall=7, gb_free=29.8, wall=3883
2024-01-31 11:17:39 | INFO | train_inner | epoch 209:    824 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54593, ups=14.1, wpb=3871.9, bsz=215.9, num_updates=281400, lr=5.96126e-05, gnorm=0.578, loss_scale=4, train_wall=7, gb_free=29.7, wall=3890
2024-01-31 11:17:47 | INFO | train_inner | epoch 209:    924 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54849.9, ups=13.87, wpb=3954.1, bsz=221, num_updates=281500, lr=5.9602e-05, gnorm=0.506, loss_scale=4, train_wall=7, gb_free=29.8, wall=3897
2024-01-31 11:17:54 | INFO | train_inner | epoch 209:   1024 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55304.5, ups=14.07, wpb=3930.5, bsz=203.9, num_updates=281600, lr=5.95914e-05, gnorm=0.491, loss_scale=4, train_wall=7, gb_free=29.8, wall=3904
2024-01-31 11:18:01 | INFO | train_inner | epoch 209:   1124 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=55530.6, ups=14.14, wpb=3926.9, bsz=205.8, num_updates=281700, lr=5.95808e-05, gnorm=0.565, loss_scale=4, train_wall=7, gb_free=29.7, wall=3911
2024-01-31 11:18:08 | INFO | train_inner | epoch 209:   1224 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54816.5, ups=13.84, wpb=3959.7, bsz=227, num_updates=281800, lr=5.95703e-05, gnorm=0.524, loss_scale=4, train_wall=7, gb_free=29.8, wall=3918
2024-01-31 11:18:15 | INFO | train_inner | epoch 209:   1324 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54225.5, ups=13.78, wpb=3935.3, bsz=223.4, num_updates=281900, lr=5.95597e-05, gnorm=0.529, loss_scale=4, train_wall=7, gb_free=29.7, wall=3926
2024-01-31 11:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:18:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:18:19 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 2.208 | nll_loss 0.464 | ppl 1.38 | wps 116302 | wpb 3275.2 | bsz 176.5 | num_updates 281925 | best_loss 2.063
2024-01-31 11:18:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 281925 updates
2024-01-31 11:18:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint209.pt
2024-01-31 11:18:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint209.pt
2024-01-31 11:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint209.pt (epoch 209 @ 281925 updates, score 2.208) (writing took 2.468379686004482 seconds)
2024-01-31 11:18:22 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2024-01-31 11:18:22 | INFO | train | epoch 209 | loss 1.776 | nll_loss 0.181 | ppl 1.13 | wps 52071.1 | ups 13.28 | wpb 3921.2 | bsz 214.5 | num_updates 281925 | lr 5.95571e-05 | gnorm 0.54 | loss_scale 4 | train_wall 93 | gb_free 30.1 | wall 3932
2024-01-31 11:18:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:18:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:18:22 | INFO | fairseq.trainer | begin training epoch 210
2024-01-31 11:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:18:28 | INFO | train_inner | epoch 210:     75 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=31779.6, ups=8.04, wpb=3951.9, bsz=214.2, num_updates=282000, lr=5.95491e-05, gnorm=0.447, loss_scale=4, train_wall=7, gb_free=29.8, wall=3938
2024-01-31 11:18:35 | INFO | train_inner | epoch 210:    175 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54695.5, ups=13.94, wpb=3923.1, bsz=209.4, num_updates=282100, lr=5.95386e-05, gnorm=0.513, loss_scale=4, train_wall=7, gb_free=29.9, wall=3945
2024-01-31 11:18:42 | INFO | train_inner | epoch 210:    275 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55753.5, ups=14.24, wpb=3915.6, bsz=213.4, num_updates=282200, lr=5.9528e-05, gnorm=0.532, loss_scale=4, train_wall=7, gb_free=29.8, wall=3952
2024-01-31 11:19:09 | INFO | train_inner | epoch 210:    375 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=14335.6, ups=3.71, wpb=3863.8, bsz=215.4, num_updates=282300, lr=5.95175e-05, gnorm=0.599, loss_scale=4, train_wall=22, gb_free=29.8, wall=3979
2024-01-31 11:19:16 | INFO | train_inner | epoch 210:    475 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54669.3, ups=13.89, wpb=3936.9, bsz=216.7, num_updates=282400, lr=5.95069e-05, gnorm=0.481, loss_scale=4, train_wall=7, gb_free=29.8, wall=3987
2024-01-31 11:19:23 | INFO | train_inner | epoch 210:    575 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54436.4, ups=13.96, wpb=3898.4, bsz=219.6, num_updates=282500, lr=5.94964e-05, gnorm=0.471, loss_scale=4, train_wall=7, gb_free=29.9, wall=3994
2024-01-31 11:19:31 | INFO | train_inner | epoch 210:    675 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55412.6, ups=14.12, wpb=3923.4, bsz=212.9, num_updates=282600, lr=5.94859e-05, gnorm=0.504, loss_scale=4, train_wall=7, gb_free=29.8, wall=4001
2024-01-31 11:19:38 | INFO | train_inner | epoch 210:    775 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=54140.4, ups=14.02, wpb=3862.3, bsz=211.8, num_updates=282700, lr=5.94754e-05, gnorm=0.569, loss_scale=4, train_wall=7, gb_free=29.8, wall=4008
2024-01-31 11:19:45 | INFO | train_inner | epoch 210:    875 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=53912.6, ups=14.06, wpb=3834.8, bsz=205.8, num_updates=282800, lr=5.94648e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.8, wall=4015
2024-01-31 11:19:52 | INFO | train_inner | epoch 210:    975 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=56140.5, ups=14.34, wpb=3916.2, bsz=207.8, num_updates=282900, lr=5.94543e-05, gnorm=0.549, loss_scale=4, train_wall=7, gb_free=29.9, wall=4022
2024-01-31 11:19:59 | INFO | train_inner | epoch 210:   1075 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55359.8, ups=14.02, wpb=3948, bsz=205.7, num_updates=283000, lr=5.94438e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.8, wall=4029
2024-01-31 11:20:06 | INFO | train_inner | epoch 210:   1175 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55206.7, ups=13.91, wpb=3969.3, bsz=228.6, num_updates=283100, lr=5.94333e-05, gnorm=0.575, loss_scale=4, train_wall=7, gb_free=29.7, wall=4036
2024-01-31 11:20:13 | INFO | train_inner | epoch 210:   1275 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=56039, ups=14.1, wpb=3973.5, bsz=227, num_updates=283200, lr=5.94228e-05, gnorm=0.476, loss_scale=4, train_wall=7, gb_free=29.7, wall=4043
2024-01-31 11:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:20:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:20:22 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 2.206 | nll_loss 0.462 | ppl 1.38 | wps 109861 | wpb 3275.2 | bsz 176.5 | num_updates 283274 | best_loss 2.063
2024-01-31 11:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 283274 updates
2024-01-31 11:20:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint210.pt
2024-01-31 11:20:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint210.pt
2024-01-31 11:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint210.pt (epoch 210 @ 283274 updates, score 2.206) (writing took 3.0285485489293933 seconds)
2024-01-31 11:20:25 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2024-01-31 11:20:25 | INFO | train | epoch 210 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 43146.1 | ups 11 | wpb 3921.2 | bsz 214.5 | num_updates 283274 | lr 5.94151e-05 | gnorm 0.529 | loss_scale 4 | train_wall 107 | gb_free 29.7 | wall 4055
2024-01-31 11:20:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:20:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:20:25 | INFO | fairseq.trainer | begin training epoch 211
2024-01-31 11:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:20:27 | INFO | train_inner | epoch 211:     26 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=28979.6, ups=7.34, wpb=3949.4, bsz=203, num_updates=283300, lr=5.94123e-05, gnorm=0.601, loss_scale=4, train_wall=7, gb_free=29.8, wall=4057
2024-01-31 11:20:34 | INFO | train_inner | epoch 211:    126 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55691.8, ups=14.08, wpb=3956.2, bsz=217, num_updates=283400, lr=5.94019e-05, gnorm=0.484, loss_scale=4, train_wall=7, gb_free=29.8, wall=4064
2024-01-31 11:20:42 | INFO | train_inner | epoch 211:    226 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=50890.3, ups=12.83, wpb=3967.1, bsz=221.8, num_updates=283500, lr=5.93914e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.8, wall=4072
2024-01-31 11:20:49 | INFO | train_inner | epoch 211:    326 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54200.2, ups=13.9, wpb=3898.9, bsz=204.7, num_updates=283600, lr=5.93809e-05, gnorm=0.558, loss_scale=4, train_wall=7, gb_free=29.8, wall=4079
2024-01-31 11:20:57 | INFO | train_inner | epoch 211:    426 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=46082.9, ups=11.64, wpb=3959.2, bsz=212.2, num_updates=283700, lr=5.93704e-05, gnorm=0.465, loss_scale=4, train_wall=8, gb_free=29.8, wall=4088
2024-01-31 11:21:05 | INFO | train_inner | epoch 211:    526 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54590.9, ups=13.87, wpb=3936.5, bsz=212.7, num_updates=283800, lr=5.936e-05, gnorm=0.497, loss_scale=4, train_wall=7, gb_free=29.9, wall=4095
2024-01-31 11:21:12 | INFO | train_inner | epoch 211:    626 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=55669.8, ups=14.31, wpb=3889.8, bsz=216.7, num_updates=283900, lr=5.93495e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.7, wall=4102
2024-01-31 11:21:19 | INFO | train_inner | epoch 211:    726 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54721.4, ups=13.94, wpb=3925, bsz=210.4, num_updates=284000, lr=5.93391e-05, gnorm=0.504, loss_scale=4, train_wall=7, gb_free=29.8, wall=4109
2024-01-31 11:21:26 | INFO | train_inner | epoch 211:    826 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54649.7, ups=14.01, wpb=3899.8, bsz=218.6, num_updates=284100, lr=5.93286e-05, gnorm=0.559, loss_scale=4, train_wall=7, gb_free=29.7, wall=4116
2024-01-31 11:21:35 | INFO | train_inner | epoch 211:    926 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=44909.2, ups=11.6, wpb=3870.7, bsz=204.7, num_updates=284200, lr=5.93182e-05, gnorm=0.501, loss_scale=4, train_wall=8, gb_free=29.8, wall=4125
2024-01-31 11:21:42 | INFO | train_inner | epoch 211:   1026 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=56384.3, ups=14.41, wpb=3912.1, bsz=206.6, num_updates=284300, lr=5.93078e-05, gnorm=0.525, loss_scale=4, train_wall=7, gb_free=29.7, wall=4132
2024-01-31 11:21:49 | INFO | train_inner | epoch 211:   1126 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54370.4, ups=13.79, wpb=3941.5, bsz=223.4, num_updates=284400, lr=5.92973e-05, gnorm=0.512, loss_scale=4, train_wall=7, gb_free=29.7, wall=4139
2024-01-31 11:21:56 | INFO | train_inner | epoch 211:   1226 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54436.8, ups=13.94, wpb=3906.4, bsz=213.7, num_updates=284500, lr=5.92869e-05, gnorm=0.53, loss_scale=4, train_wall=7, gb_free=29.9, wall=4146
2024-01-31 11:22:03 | INFO | train_inner | epoch 211:   1326 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55231.5, ups=14.17, wpb=3898.8, bsz=226.8, num_updates=284600, lr=5.92765e-05, gnorm=0.519, loss_scale=4, train_wall=7, gb_free=29.8, wall=4153
2024-01-31 11:22:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:22:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:22:07 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 2.212 | nll_loss 0.469 | ppl 1.38 | wps 117028 | wpb 3275.2 | bsz 176.5 | num_updates 284623 | best_loss 2.063
2024-01-31 11:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 284623 updates
2024-01-31 11:22:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint211.pt
2024-01-31 11:22:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint211.pt
2024-01-31 11:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint211.pt (epoch 211 @ 284623 updates, score 2.212) (writing took 2.51838095090352 seconds)
2024-01-31 11:22:10 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2024-01-31 11:22:10 | INFO | train | epoch 211 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 50336 | ups 12.84 | wpb 3921.2 | bsz 214.5 | num_updates 284623 | lr 5.92741e-05 | gnorm 0.512 | loss_scale 4 | train_wall 95 | gb_free 29.8 | wall 4160
2024-01-31 11:22:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:22:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:22:10 | INFO | fairseq.trainer | begin training epoch 212
2024-01-31 11:22:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:22:17 | INFO | train_inner | epoch 212:     77 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=29623.2, ups=7.42, wpb=3990.5, bsz=220.6, num_updates=284700, lr=5.92661e-05, gnorm=0.519, loss_scale=4, train_wall=7, gb_free=29.7, wall=4167
2024-01-31 11:22:24 | INFO | train_inner | epoch 212:    177 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55488, ups=14.1, wpb=3936, bsz=210.6, num_updates=284800, lr=5.92557e-05, gnorm=0.546, loss_scale=4, train_wall=7, gb_free=29.7, wall=4174
2024-01-31 11:22:31 | INFO | train_inner | epoch 212:    277 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55021.1, ups=14.07, wpb=3911.1, bsz=205.3, num_updates=284900, lr=5.92453e-05, gnorm=0.495, loss_scale=4, train_wall=7, gb_free=29.8, wall=4181
2024-01-31 11:22:38 | INFO | train_inner | epoch 212:    377 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55896.5, ups=14.08, wpb=3968.6, bsz=216.5, num_updates=285000, lr=5.92349e-05, gnorm=0.501, loss_scale=4, train_wall=7, gb_free=29.8, wall=4188
2024-01-31 11:22:45 | INFO | train_inner | epoch 212:    477 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54371.8, ups=13.9, wpb=3911.9, bsz=197.7, num_updates=285100, lr=5.92245e-05, gnorm=0.621, loss_scale=4, train_wall=7, gb_free=29.8, wall=4195
2024-01-31 11:22:52 | INFO | train_inner | epoch 212:    577 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=55582.6, ups=14.19, wpb=3917.5, bsz=228.3, num_updates=285200, lr=5.92141e-05, gnorm=0.543, loss_scale=4, train_wall=7, gb_free=29.8, wall=4202
2024-01-31 11:22:59 | INFO | train_inner | epoch 212:    677 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=54443.4, ups=13.89, wpb=3919.8, bsz=213.1, num_updates=285300, lr=5.92037e-05, gnorm=0.48, loss_scale=4, train_wall=7, gb_free=29.8, wall=4210
2024-01-31 11:23:06 | INFO | train_inner | epoch 212:    777 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55129.3, ups=13.91, wpb=3963.1, bsz=226.4, num_updates=285400, lr=5.91934e-05, gnorm=0.541, loss_scale=4, train_wall=7, gb_free=29.7, wall=4217
2024-01-31 11:23:14 | INFO | train_inner | epoch 212:    877 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55544.9, ups=13.98, wpb=3973.3, bsz=221.9, num_updates=285500, lr=5.9183e-05, gnorm=0.561, loss_scale=8, train_wall=7, gb_free=29.8, wall=4224
2024-01-31 11:23:21 | INFO | train_inner | epoch 212:    977 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55478, ups=14.17, wpb=3915.2, bsz=212, num_updates=285600, lr=5.91726e-05, gnorm=0.587, loss_scale=8, train_wall=7, gb_free=29.8, wall=4231
2024-01-31 11:23:28 | INFO | train_inner | epoch 212:   1077 / 1349 loss=1.78, nll_loss=0.185, ppl=1.14, wps=54154.4, ups=14.04, wpb=3858.1, bsz=212.9, num_updates=285700, lr=5.91623e-05, gnorm=0.554, loss_scale=8, train_wall=7, gb_free=29.7, wall=4238
2024-01-31 11:23:35 | INFO | train_inner | epoch 212:   1177 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54755.5, ups=14.03, wpb=3903.5, bsz=215.1, num_updates=285800, lr=5.91519e-05, gnorm=0.559, loss_scale=8, train_wall=7, gb_free=29.8, wall=4245
2024-01-31 11:23:42 | INFO | train_inner | epoch 212:   1277 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55729.6, ups=14.33, wpb=3888.7, bsz=217.3, num_updates=285900, lr=5.91416e-05, gnorm=0.598, loss_scale=8, train_wall=7, gb_free=29.8, wall=4252
2024-01-31 11:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:23:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:23:50 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 2.21 | nll_loss 0.466 | ppl 1.38 | wps 113722 | wpb 3275.2 | bsz 176.5 | num_updates 285972 | best_loss 2.063
2024-01-31 11:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 285972 updates
2024-01-31 11:23:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint212.pt
2024-01-31 11:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint212.pt
2024-01-31 11:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint212.pt (epoch 212 @ 285972 updates, score 2.21) (writing took 2.4250030270777643 seconds)
2024-01-31 11:23:52 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2024-01-31 11:23:52 | INFO | train | epoch 212 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 51723.6 | ups 13.19 | wpb 3921.2 | bsz 214.5 | num_updates 285972 | lr 5.91341e-05 | gnorm 0.55 | loss_scale 8 | train_wall 93 | gb_free 29.8 | wall 4262
2024-01-31 11:23:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:23:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:23:52 | INFO | fairseq.trainer | begin training epoch 213
2024-01-31 11:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:23:54 | INFO | train_inner | epoch 213:     28 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=31252.8, ups=8.13, wpb=3842.1, bsz=204, num_updates=286000, lr=5.91312e-05, gnorm=0.55, loss_scale=8, train_wall=7, gb_free=29.9, wall=4264
2024-01-31 11:24:01 | INFO | train_inner | epoch 213:    128 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=56334.5, ups=14.28, wpb=3945.5, bsz=217.3, num_updates=286100, lr=5.91209e-05, gnorm=0.517, loss_scale=8, train_wall=7, gb_free=29.7, wall=4271
2024-01-31 11:24:08 | INFO | train_inner | epoch 213:    228 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54710.1, ups=13.82, wpb=3959.8, bsz=223.2, num_updates=286200, lr=5.91106e-05, gnorm=0.571, loss_scale=8, train_wall=7, gb_free=29.8, wall=4279
2024-01-31 11:24:16 | INFO | train_inner | epoch 213:    328 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54937.2, ups=13.87, wpb=3960.6, bsz=205.8, num_updates=286300, lr=5.91003e-05, gnorm=0.467, loss_scale=8, train_wall=7, gb_free=29.7, wall=4286
2024-01-31 11:24:23 | INFO | train_inner | epoch 213:    428 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55763, ups=14.3, wpb=3899.8, bsz=211.6, num_updates=286400, lr=5.90899e-05, gnorm=0.512, loss_scale=8, train_wall=7, gb_free=29.7, wall=4293
2024-01-31 11:24:30 | INFO | train_inner | epoch 213:    528 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54793.3, ups=13.98, wpb=3919, bsz=224.4, num_updates=286500, lr=5.90796e-05, gnorm=0.518, loss_scale=8, train_wall=7, gb_free=29.8, wall=4300
2024-01-31 11:24:37 | INFO | train_inner | epoch 213:    628 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55138.5, ups=14.02, wpb=3933.1, bsz=212.6, num_updates=286600, lr=5.90693e-05, gnorm=0.56, loss_scale=8, train_wall=7, gb_free=29.8, wall=4307
2024-01-31 11:24:44 | INFO | train_inner | epoch 213:    728 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54044, ups=14.08, wpb=3839.7, bsz=220.4, num_updates=286700, lr=5.9059e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.7, wall=4314
2024-01-31 11:24:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-01-31 11:24:51 | INFO | train_inner | epoch 213:    829 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54558.9, ups=13.96, wpb=3909.3, bsz=210.1, num_updates=286800, lr=5.90487e-05, gnorm=0.479, loss_scale=4, train_wall=7, gb_free=29.7, wall=4321
2024-01-31 11:24:58 | INFO | train_inner | epoch 213:    929 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55218.8, ups=14, wpb=3944.2, bsz=223.8, num_updates=286900, lr=5.90384e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.9, wall=4329
2024-01-31 11:25:06 | INFO | train_inner | epoch 213:   1029 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54413.2, ups=13.97, wpb=3896.3, bsz=211.7, num_updates=287000, lr=5.90281e-05, gnorm=0.558, loss_scale=4, train_wall=7, gb_free=29.8, wall=4336
2024-01-31 11:25:12 | INFO | train_inner | epoch 213:   1129 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=56570.8, ups=14.32, wpb=3950.6, bsz=212.6, num_updates=287100, lr=5.90179e-05, gnorm=0.6, loss_scale=4, train_wall=7, gb_free=29.7, wall=4343
2024-01-31 11:25:20 | INFO | train_inner | epoch 213:   1229 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54348.2, ups=13.88, wpb=3914.7, bsz=218.1, num_updates=287200, lr=5.90076e-05, gnorm=0.515, loss_scale=4, train_wall=7, gb_free=29.7, wall=4350
2024-01-31 11:25:27 | INFO | train_inner | epoch 213:   1329 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55126, ups=13.97, wpb=3945.7, bsz=201.2, num_updates=287300, lr=5.89973e-05, gnorm=0.593, loss_scale=4, train_wall=7, gb_free=29.8, wall=4357
2024-01-31 11:25:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:25:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:25:31 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 2.211 | nll_loss 0.468 | ppl 1.38 | wps 116950 | wpb 3275.2 | bsz 176.5 | num_updates 287320 | best_loss 2.063
2024-01-31 11:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 287320 updates
2024-01-31 11:25:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint213.pt
2024-01-31 11:25:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint213.pt
2024-01-31 11:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint213.pt (epoch 213 @ 287320 updates, score 2.211) (writing took 2.411298446939327 seconds)
2024-01-31 11:25:33 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2024-01-31 11:25:33 | INFO | train | epoch 213 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 52283.5 | ups 13.33 | wpb 3921.1 | bsz 214.5 | num_updates 287320 | lr 5.89953e-05 | gnorm 0.534 | loss_scale 4 | train_wall 92 | gb_free 29.9 | wall 4363
2024-01-31 11:25:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:25:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:25:33 | INFO | fairseq.trainer | begin training epoch 214
2024-01-31 11:25:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:25:39 | INFO | train_inner | epoch 214:     80 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=32144.7, ups=8.24, wpb=3902.4, bsz=224.9, num_updates=287400, lr=5.8987e-05, gnorm=0.493, loss_scale=4, train_wall=7, gb_free=29.8, wall=4369
2024-01-31 11:25:48 | INFO | train_inner | epoch 214:    180 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=41737.6, ups=10.63, wpb=3926.6, bsz=213.1, num_updates=287500, lr=5.89768e-05, gnorm=0.546, loss_scale=4, train_wall=9, gb_free=29.7, wall=4379
2024-01-31 11:25:56 | INFO | train_inner | epoch 214:    280 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55896.6, ups=14.19, wpb=3940.3, bsz=212.2, num_updates=287600, lr=5.89665e-05, gnorm=0.51, loss_scale=4, train_wall=7, gb_free=29.9, wall=4386
2024-01-31 11:26:03 | INFO | train_inner | epoch 214:    380 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=48865.8, ups=12.66, wpb=3858.9, bsz=203, num_updates=287700, lr=5.89563e-05, gnorm=0.56, loss_scale=4, train_wall=7, gb_free=29.7, wall=4394
2024-01-31 11:26:10 | INFO | train_inner | epoch 214:    480 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55862.6, ups=14.23, wpb=3924.5, bsz=218, num_updates=287800, lr=5.8946e-05, gnorm=0.509, loss_scale=4, train_wall=7, gb_free=29.7, wall=4401
2024-01-31 11:26:18 | INFO | train_inner | epoch 214:    580 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54250.8, ups=13.95, wpb=3890, bsz=217.3, num_updates=287900, lr=5.89358e-05, gnorm=0.547, loss_scale=4, train_wall=7, gb_free=29.9, wall=4408
2024-01-31 11:26:25 | INFO | train_inner | epoch 214:    680 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55137.1, ups=14.05, wpb=3924.7, bsz=214.5, num_updates=288000, lr=5.89256e-05, gnorm=0.514, loss_scale=4, train_wall=7, gb_free=29.7, wall=4415
2024-01-31 11:26:33 | INFO | train_inner | epoch 214:    780 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=49638.5, ups=12.69, wpb=3912.9, bsz=217.8, num_updates=288100, lr=5.89153e-05, gnorm=0.637, loss_scale=4, train_wall=8, gb_free=29.7, wall=4423
2024-01-31 11:26:40 | INFO | train_inner | epoch 214:    880 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=56207.5, ups=14.14, wpb=3973.9, bsz=210.6, num_updates=288200, lr=5.89051e-05, gnorm=0.515, loss_scale=4, train_wall=7, gb_free=29.8, wall=4430
2024-01-31 11:26:47 | INFO | train_inner | epoch 214:    980 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55125.9, ups=14.15, wpb=3895.9, bsz=207.6, num_updates=288300, lr=5.88949e-05, gnorm=0.549, loss_scale=4, train_wall=7, gb_free=29.7, wall=4437
2024-01-31 11:26:54 | INFO | train_inner | epoch 214:   1080 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55423.1, ups=14.06, wpb=3941.9, bsz=228.6, num_updates=288400, lr=5.88847e-05, gnorm=0.502, loss_scale=4, train_wall=7, gb_free=30, wall=4444
2024-01-31 11:27:01 | INFO | train_inner | epoch 214:   1180 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55855.3, ups=14.27, wpb=3914.8, bsz=212.3, num_updates=288500, lr=5.88745e-05, gnorm=0.58, loss_scale=4, train_wall=7, gb_free=29.8, wall=4451
2024-01-31 11:27:08 | INFO | train_inner | epoch 214:   1280 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55023.9, ups=14.15, wpb=3888, bsz=208.3, num_updates=288600, lr=5.88643e-05, gnorm=0.602, loss_scale=4, train_wall=7, gb_free=29.7, wall=4458
2024-01-31 11:27:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:27:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:27:16 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 2.213 | nll_loss 0.469 | ppl 1.38 | wps 96474.2 | wpb 3275.2 | bsz 176.5 | num_updates 288669 | best_loss 2.063
2024-01-31 11:27:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 288669 updates
2024-01-31 11:27:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint214.pt
2024-01-31 11:27:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint214.pt
2024-01-31 11:27:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint214.pt (epoch 214 @ 288669 updates, score 2.213) (writing took 2.6497156630503014 seconds)
2024-01-31 11:27:19 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2024-01-31 11:27:19 | INFO | train | epoch 214 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 50147.4 | ups 12.79 | wpb 3921.2 | bsz 214.5 | num_updates 288669 | lr 5.88572e-05 | gnorm 0.54 | loss_scale 4 | train_wall 95 | gb_free 29.8 | wall 4469
2024-01-31 11:27:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:27:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:27:19 | INFO | fairseq.trainer | begin training epoch 215
2024-01-31 11:27:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:27:22 | INFO | train_inner | epoch 215:     31 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=29007.6, ups=7.32, wpb=3963.4, bsz=219.1, num_updates=288700, lr=5.88541e-05, gnorm=0.463, loss_scale=4, train_wall=7, gb_free=29.8, wall=4472
2024-01-31 11:27:29 | INFO | train_inner | epoch 215:    131 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54897.9, ups=13.93, wpb=3939.9, bsz=195.2, num_updates=288800, lr=5.88439e-05, gnorm=0.54, loss_scale=4, train_wall=7, gb_free=29.7, wall=4479
2024-01-31 11:27:36 | INFO | train_inner | epoch 215:    231 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54736.3, ups=13.94, wpb=3925.9, bsz=228.2, num_updates=288900, lr=5.88337e-05, gnorm=0.435, loss_scale=4, train_wall=7, gb_free=29.7, wall=4486
2024-01-31 11:27:43 | INFO | train_inner | epoch 215:    331 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54957.1, ups=13.97, wpb=3933.9, bsz=226, num_updates=289000, lr=5.88235e-05, gnorm=0.57, loss_scale=4, train_wall=7, gb_free=29.9, wall=4493
2024-01-31 11:27:50 | INFO | train_inner | epoch 215:    431 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=55884.1, ups=14.07, wpb=3971.5, bsz=218.8, num_updates=289100, lr=5.88134e-05, gnorm=0.504, loss_scale=4, train_wall=7, gb_free=29.8, wall=4501
2024-01-31 11:27:57 | INFO | train_inner | epoch 215:    531 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54761.2, ups=13.99, wpb=3915.4, bsz=224.7, num_updates=289200, lr=5.88032e-05, gnorm=0.513, loss_scale=4, train_wall=7, gb_free=29.8, wall=4508
2024-01-31 11:28:06 | INFO | train_inner | epoch 215:    631 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=46160.6, ups=12, wpb=3845.8, bsz=200.3, num_updates=289300, lr=5.8793e-05, gnorm=0.54, loss_scale=4, train_wall=8, gb_free=29.8, wall=4516
2024-01-31 11:28:13 | INFO | train_inner | epoch 215:    731 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=56355.8, ups=14.28, wpb=3947.7, bsz=213.7, num_updates=289400, lr=5.87829e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.8, wall=4523
2024-01-31 11:28:20 | INFO | train_inner | epoch 215:    831 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=53537.1, ups=14.03, wpb=3814.7, bsz=222.9, num_updates=289500, lr=5.87727e-05, gnorm=0.634, loss_scale=4, train_wall=7, gb_free=29.8, wall=4530
2024-01-31 11:28:27 | INFO | train_inner | epoch 215:    931 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=55546.9, ups=14.2, wpb=3911.5, bsz=201.5, num_updates=289600, lr=5.87626e-05, gnorm=0.631, loss_scale=4, train_wall=7, gb_free=29.7, wall=4537
2024-01-31 11:28:35 | INFO | train_inner | epoch 215:   1031 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=51275.7, ups=13.05, wpb=3929.1, bsz=209.8, num_updates=289700, lr=5.87524e-05, gnorm=0.523, loss_scale=4, train_wall=7, gb_free=29.7, wall=4545
2024-01-31 11:28:42 | INFO | train_inner | epoch 215:   1131 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54205.2, ups=13.81, wpb=3926.1, bsz=212.4, num_updates=289800, lr=5.87423e-05, gnorm=0.55, loss_scale=4, train_wall=7, gb_free=29.7, wall=4552
2024-01-31 11:28:49 | INFO | train_inner | epoch 215:   1231 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55748.9, ups=14.04, wpb=3971.2, bsz=221, num_updates=289900, lr=5.87321e-05, gnorm=0.503, loss_scale=4, train_wall=7, gb_free=29.8, wall=4559
2024-01-31 11:28:56 | INFO | train_inner | epoch 215:   1331 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55409.5, ups=13.99, wpb=3960.4, bsz=215.1, num_updates=290000, lr=5.8722e-05, gnorm=0.482, loss_scale=4, train_wall=7, gb_free=29.8, wall=4566
2024-01-31 11:28:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:28:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:29:00 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 2.214 | nll_loss 0.469 | ppl 1.38 | wps 117744 | wpb 3275.2 | bsz 176.5 | num_updates 290018 | best_loss 2.063
2024-01-31 11:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 290018 updates
2024-01-31 11:29:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint215.pt
2024-01-31 11:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint215.pt
2024-01-31 11:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint215.pt (epoch 215 @ 290018 updates, score 2.214) (writing took 2.542238984024152 seconds)
2024-01-31 11:29:02 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2024-01-31 11:29:02 | INFO | train | epoch 215 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 50962.9 | ups 13 | wpb 3921.2 | bsz 214.5 | num_updates 290018 | lr 5.87202e-05 | gnorm 0.531 | loss_scale 4 | train_wall 95 | gb_free 29.8 | wall 4573
2024-01-31 11:29:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:29:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:29:02 | INFO | fairseq.trainer | begin training epoch 216
2024-01-31 11:29:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:29:09 | INFO | train_inner | epoch 216:     82 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=29733.4, ups=7.72, wpb=3852.1, bsz=211.4, num_updates=290100, lr=5.87119e-05, gnorm=0.568, loss_scale=4, train_wall=8, gb_free=29.7, wall=4579
2024-01-31 11:29:16 | INFO | train_inner | epoch 216:    182 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55207.7, ups=14, wpb=3944.5, bsz=224.8, num_updates=290200, lr=5.87018e-05, gnorm=0.528, loss_scale=4, train_wall=7, gb_free=29.9, wall=4587
2024-01-31 11:29:23 | INFO | train_inner | epoch 216:    282 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=56299.5, ups=14.19, wpb=3968.5, bsz=225.4, num_updates=290300, lr=5.86917e-05, gnorm=0.505, loss_scale=4, train_wall=7, gb_free=29.7, wall=4594
2024-01-31 11:29:31 | INFO | train_inner | epoch 216:    382 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=49027, ups=12.52, wpb=3915.9, bsz=204.6, num_updates=290400, lr=5.86816e-05, gnorm=0.514, loss_scale=4, train_wall=8, gb_free=29.9, wall=4602
2024-01-31 11:29:38 | INFO | train_inner | epoch 216:    482 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54915.1, ups=13.97, wpb=3930.7, bsz=224.2, num_updates=290500, lr=5.86715e-05, gnorm=0.529, loss_scale=4, train_wall=7, gb_free=29.7, wall=4609
2024-01-31 11:29:46 | INFO | train_inner | epoch 216:    582 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55539, ups=14.13, wpb=3929.5, bsz=204.2, num_updates=290600, lr=5.86614e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.8, wall=4616
2024-01-31 11:29:53 | INFO | train_inner | epoch 216:    682 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=51720.7, ups=13.05, wpb=3964.6, bsz=214.7, num_updates=290700, lr=5.86513e-05, gnorm=0.512, loss_scale=4, train_wall=7, gb_free=29.7, wall=4623
2024-01-31 11:30:00 | INFO | train_inner | epoch 216:    782 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54368.7, ups=14, wpb=3884.1, bsz=208.1, num_updates=290800, lr=5.86412e-05, gnorm=0.517, loss_scale=4, train_wall=7, gb_free=29.8, wall=4631
2024-01-31 11:30:07 | INFO | train_inner | epoch 216:    882 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54139.3, ups=13.97, wpb=3875.3, bsz=205.2, num_updates=290900, lr=5.86311e-05, gnorm=0.595, loss_scale=4, train_wall=7, gb_free=29.8, wall=4638
2024-01-31 11:30:15 | INFO | train_inner | epoch 216:    982 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54771.3, ups=14.08, wpb=3889, bsz=220, num_updates=291000, lr=5.8621e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.8, wall=4645
2024-01-31 11:30:22 | INFO | train_inner | epoch 216:   1082 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=52276.6, ups=13.35, wpb=3915.9, bsz=208.9, num_updates=291100, lr=5.8611e-05, gnorm=0.598, loss_scale=4, train_wall=7, gb_free=29.8, wall=4652
2024-01-31 11:30:29 | INFO | train_inner | epoch 216:   1182 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54730.8, ups=13.9, wpb=3936.5, bsz=217.4, num_updates=291200, lr=5.86009e-05, gnorm=0.501, loss_scale=4, train_wall=7, gb_free=29.8, wall=4660
2024-01-31 11:30:38 | INFO | train_inner | epoch 216:   1282 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=43127.8, ups=10.91, wpb=3954.2, bsz=220.9, num_updates=291300, lr=5.85908e-05, gnorm=0.512, loss_scale=4, train_wall=9, gb_free=29.8, wall=4669
2024-01-31 11:30:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:30:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:30:46 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 2.217 | nll_loss 0.474 | ppl 1.39 | wps 113260 | wpb 3275.2 | bsz 176.5 | num_updates 291367 | best_loss 2.063
2024-01-31 11:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 291367 updates
2024-01-31 11:30:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint216.pt
2024-01-31 11:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint216.pt
2024-01-31 11:30:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint216.pt (epoch 216 @ 291367 updates, score 2.217) (writing took 2.480945092975162 seconds)
2024-01-31 11:30:48 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2024-01-31 11:30:48 | INFO | train | epoch 216 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 49980.1 | ups 12.75 | wpb 3921.2 | bsz 214.5 | num_updates 291367 | lr 5.85841e-05 | gnorm 0.537 | loss_scale 4 | train_wall 97 | gb_free 29.8 | wall 4678
2024-01-31 11:30:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:30:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:30:48 | INFO | fairseq.trainer | begin training epoch 217
2024-01-31 11:30:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:30:51 | INFO | train_inner | epoch 217:     33 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=32242.8, ups=8.17, wpb=3948.1, bsz=209.6, num_updates=291400, lr=5.85808e-05, gnorm=0.587, loss_scale=4, train_wall=7, gb_free=29.8, wall=4681
2024-01-31 11:30:58 | INFO | train_inner | epoch 217:    133 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55043.6, ups=14.03, wpb=3922.6, bsz=217.4, num_updates=291500, lr=5.85707e-05, gnorm=0.464, loss_scale=4, train_wall=7, gb_free=29.7, wall=4688
2024-01-31 11:31:05 | INFO | train_inner | epoch 217:    233 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=53713.8, ups=13.89, wpb=3866.3, bsz=227.6, num_updates=291600, lr=5.85607e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.8, wall=4695
2024-01-31 11:31:12 | INFO | train_inner | epoch 217:    333 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=56479.8, ups=14.21, wpb=3975, bsz=220, num_updates=291700, lr=5.85507e-05, gnorm=0.458, loss_scale=4, train_wall=7, gb_free=29.8, wall=4702
2024-01-31 11:31:19 | INFO | train_inner | epoch 217:    433 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55351.2, ups=14.03, wpb=3943.8, bsz=213.7, num_updates=291800, lr=5.85406e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.8, wall=4709
2024-01-31 11:31:26 | INFO | train_inner | epoch 217:    533 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54874.5, ups=14.06, wpb=3903.5, bsz=205.3, num_updates=291900, lr=5.85306e-05, gnorm=0.589, loss_scale=4, train_wall=7, gb_free=29.7, wall=4717
2024-01-31 11:31:33 | INFO | train_inner | epoch 217:    633 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=55403.5, ups=14.08, wpb=3934.3, bsz=210.2, num_updates=292000, lr=5.85206e-05, gnorm=0.526, loss_scale=4, train_wall=7, gb_free=29.7, wall=4724
2024-01-31 11:31:40 | INFO | train_inner | epoch 217:    733 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54803.8, ups=14.16, wpb=3869, bsz=199.4, num_updates=292100, lr=5.85106e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.8, wall=4731
2024-01-31 11:31:48 | INFO | train_inner | epoch 217:    833 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54443.2, ups=13.89, wpb=3920.2, bsz=213.1, num_updates=292200, lr=5.85005e-05, gnorm=0.562, loss_scale=4, train_wall=7, gb_free=29.9, wall=4738
2024-01-31 11:31:55 | INFO | train_inner | epoch 217:    933 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54329.2, ups=13.77, wpb=3946.8, bsz=234.1, num_updates=292300, lr=5.84905e-05, gnorm=0.482, loss_scale=4, train_wall=7, gb_free=29.7, wall=4745
2024-01-31 11:32:02 | INFO | train_inner | epoch 217:   1033 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=55958.4, ups=14.22, wpb=3934.4, bsz=217.3, num_updates=292400, lr=5.84805e-05, gnorm=0.508, loss_scale=4, train_wall=7, gb_free=29.8, wall=4752
2024-01-31 11:32:09 | INFO | train_inner | epoch 217:   1133 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54895.1, ups=14, wpb=3921.1, bsz=204.9, num_updates=292500, lr=5.84705e-05, gnorm=0.523, loss_scale=4, train_wall=7, gb_free=29.8, wall=4759
2024-01-31 11:32:16 | INFO | train_inner | epoch 217:   1233 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=54428.4, ups=13.94, wpb=3903.1, bsz=206.4, num_updates=292600, lr=5.84605e-05, gnorm=0.579, loss_scale=4, train_wall=7, gb_free=29.8, wall=4767
2024-01-31 11:32:23 | INFO | train_inner | epoch 217:   1333 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55848.4, ups=14.2, wpb=3932.4, bsz=214.5, num_updates=292700, lr=5.84506e-05, gnorm=0.64, loss_scale=4, train_wall=7, gb_free=29.9, wall=4774
2024-01-31 11:32:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:32:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:32:27 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 2.213 | nll_loss 0.468 | ppl 1.38 | wps 113237 | wpb 3275.2 | bsz 176.5 | num_updates 292716 | best_loss 2.063
2024-01-31 11:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 292716 updates
2024-01-31 11:32:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint217.pt
2024-01-31 11:32:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint217.pt
2024-01-31 11:32:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint217.pt (epoch 217 @ 292716 updates, score 2.213) (writing took 2.537461402011104 seconds)
2024-01-31 11:32:30 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2024-01-31 11:32:30 | INFO | train | epoch 217 | loss 1.776 | nll_loss 0.18 | ppl 1.13 | wps 52194.6 | ups 13.31 | wpb 3921.2 | bsz 214.5 | num_updates 292716 | lr 5.8449e-05 | gnorm 0.533 | loss_scale 4 | train_wall 92 | gb_free 30 | wall 4780
2024-01-31 11:32:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:32:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:32:30 | INFO | fairseq.trainer | begin training epoch 218
2024-01-31 11:32:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:32:36 | INFO | train_inner | epoch 218:     84 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=31098.4, ups=8.06, wpb=3856.7, bsz=208.6, num_updates=292800, lr=5.84406e-05, gnorm=0.569, loss_scale=4, train_wall=7, gb_free=29.8, wall=4786
2024-01-31 11:32:43 | INFO | train_inner | epoch 218:    184 / 1349 loss=1.772, nll_loss=0.177, ppl=1.13, wps=55879.9, ups=14.22, wpb=3930.6, bsz=224.6, num_updates=292900, lr=5.84306e-05, gnorm=0.541, loss_scale=4, train_wall=7, gb_free=29.9, wall=4793
2024-01-31 11:32:51 | INFO | train_inner | epoch 218:    284 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=50205.7, ups=12.71, wpb=3948.8, bsz=223.4, num_updates=293000, lr=5.84206e-05, gnorm=0.53, loss_scale=4, train_wall=8, gb_free=29.8, wall=4801
2024-01-31 11:32:58 | INFO | train_inner | epoch 218:    384 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54790.9, ups=14.04, wpb=3901.2, bsz=201.8, num_updates=293100, lr=5.84107e-05, gnorm=0.465, loss_scale=4, train_wall=7, gb_free=29.8, wall=4808
2024-01-31 11:33:05 | INFO | train_inner | epoch 218:    484 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54251.1, ups=13.82, wpb=3925.2, bsz=216.1, num_updates=293200, lr=5.84007e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.8, wall=4815
2024-01-31 11:33:12 | INFO | train_inner | epoch 218:    584 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=53933.7, ups=13.73, wpb=3927.3, bsz=221, num_updates=293300, lr=5.83907e-05, gnorm=0.641, loss_scale=4, train_wall=7, gb_free=29.7, wall=4823
2024-01-31 11:33:19 | INFO | train_inner | epoch 218:    684 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=53955, ups=13.83, wpb=3902.6, bsz=222.3, num_updates=293400, lr=5.83808e-05, gnorm=0.498, loss_scale=4, train_wall=7, gb_free=29.8, wall=4830
2024-01-31 11:33:27 | INFO | train_inner | epoch 218:    784 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55608.4, ups=14.03, wpb=3964.2, bsz=212.9, num_updates=293500, lr=5.83708e-05, gnorm=0.543, loss_scale=4, train_wall=7, gb_free=29.8, wall=4837
2024-01-31 11:33:34 | INFO | train_inner | epoch 218:    884 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=53776.9, ups=13.75, wpb=3911.8, bsz=217.2, num_updates=293600, lr=5.83609e-05, gnorm=0.609, loss_scale=4, train_wall=7, gb_free=29.7, wall=4844
2024-01-31 11:33:41 | INFO | train_inner | epoch 218:    984 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=55842.2, ups=14.22, wpb=3928.1, bsz=208.6, num_updates=293700, lr=5.8351e-05, gnorm=0.527, loss_scale=4, train_wall=7, gb_free=29.8, wall=4851
2024-01-31 11:33:48 | INFO | train_inner | epoch 218:   1084 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=52689, ups=13.38, wpb=3937.9, bsz=205.8, num_updates=293800, lr=5.8341e-05, gnorm=0.53, loss_scale=4, train_wall=7, gb_free=29.8, wall=4859
2024-01-31 11:33:56 | INFO | train_inner | epoch 218:   1184 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54711.3, ups=13.9, wpb=3937.3, bsz=219.2, num_updates=293900, lr=5.83311e-05, gnorm=0.466, loss_scale=4, train_wall=7, gb_free=29.8, wall=4866
2024-01-31 11:34:03 | INFO | train_inner | epoch 218:   1284 / 1349 loss=1.779, nll_loss=0.184, ppl=1.14, wps=53461.6, ups=13.72, wpb=3896.2, bsz=222.2, num_updates=294000, lr=5.83212e-05, gnorm=0.556, loss_scale=4, train_wall=7, gb_free=29.8, wall=4873
2024-01-31 11:34:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:34:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:34:10 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 2.21 | nll_loss 0.467 | ppl 1.38 | wps 115293 | wpb 3275.2 | bsz 176.5 | num_updates 294065 | best_loss 2.063
2024-01-31 11:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 294065 updates
2024-01-31 11:34:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint218.pt
2024-01-31 11:34:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint218.pt
2024-01-31 11:34:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint218.pt (epoch 218 @ 294065 updates, score 2.21) (writing took 2.504310946096666 seconds)
2024-01-31 11:34:13 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2024-01-31 11:34:13 | INFO | train | epoch 218 | loss 1.775 | nll_loss 0.18 | ppl 1.13 | wps 51165.5 | ups 13.05 | wpb 3921.2 | bsz 214.5 | num_updates 294065 | lr 5.83147e-05 | gnorm 0.539 | loss_scale 4 | train_wall 94 | gb_free 29.8 | wall 4883
2024-01-31 11:34:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:34:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:34:13 | INFO | fairseq.trainer | begin training epoch 219
2024-01-31 11:34:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:34:16 | INFO | train_inner | epoch 219:     35 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=30897.8, ups=7.85, wpb=3936.4, bsz=203.6, num_updates=294100, lr=5.83113e-05, gnorm=0.505, loss_scale=4, train_wall=7, gb_free=29.8, wall=4886
2024-01-31 11:34:23 | INFO | train_inner | epoch 219:    135 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=52056.4, ups=13.33, wpb=3906.7, bsz=198.8, num_updates=294200, lr=5.83014e-05, gnorm=0.538, loss_scale=4, train_wall=7, gb_free=29.8, wall=4893
2024-01-31 11:34:30 | INFO | train_inner | epoch 219:    235 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=54708.8, ups=13.91, wpb=3933, bsz=202.2, num_updates=294300, lr=5.82915e-05, gnorm=0.506, loss_scale=4, train_wall=7, gb_free=29.8, wall=4901
2024-01-31 11:34:38 | INFO | train_inner | epoch 219:    335 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=51115.4, ups=13.01, wpb=3929.2, bsz=210.4, num_updates=294400, lr=5.82816e-05, gnorm=0.512, loss_scale=4, train_wall=7, gb_free=29.8, wall=4908
2024-01-31 11:34:45 | INFO | train_inner | epoch 219:    435 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55346.6, ups=13.99, wpb=3956.1, bsz=212.2, num_updates=294500, lr=5.82717e-05, gnorm=0.513, loss_scale=4, train_wall=7, gb_free=29.7, wall=4915
2024-01-31 11:34:52 | INFO | train_inner | epoch 219:    535 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55436.7, ups=14.17, wpb=3911.1, bsz=222.2, num_updates=294600, lr=5.82618e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.7, wall=4922
2024-01-31 11:35:00 | INFO | train_inner | epoch 219:    635 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=50740, ups=13, wpb=3904.2, bsz=229.4, num_updates=294700, lr=5.82519e-05, gnorm=0.46, loss_scale=4, train_wall=7, gb_free=29.7, wall=4930
2024-01-31 11:35:08 | INFO | train_inner | epoch 219:    735 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=52255.1, ups=13.13, wpb=3979.2, bsz=217.5, num_updates=294800, lr=5.8242e-05, gnorm=0.511, loss_scale=4, train_wall=7, gb_free=29.7, wall=4938
2024-01-31 11:35:15 | INFO | train_inner | epoch 219:    835 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=54152.9, ups=13.87, wpb=3904.5, bsz=217.7, num_updates=294900, lr=5.82321e-05, gnorm=0.555, loss_scale=4, train_wall=7, gb_free=29.7, wall=4945
2024-01-31 11:35:23 | INFO | train_inner | epoch 219:    935 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=50606, ups=12.86, wpb=3936.5, bsz=210.2, num_updates=295000, lr=5.82223e-05, gnorm=0.471, loss_scale=4, train_wall=7, gb_free=29.7, wall=4953
2024-01-31 11:35:30 | INFO | train_inner | epoch 219:   1035 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54376.7, ups=13.9, wpb=3913, bsz=216.3, num_updates=295100, lr=5.82124e-05, gnorm=0.517, loss_scale=4, train_wall=7, gb_free=29.8, wall=4960
2024-01-31 11:35:37 | INFO | train_inner | epoch 219:   1135 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=54446.1, ups=13.9, wpb=3916.5, bsz=214.1, num_updates=295200, lr=5.82025e-05, gnorm=0.578, loss_scale=4, train_wall=7, gb_free=29.8, wall=4967
2024-01-31 11:35:44 | INFO | train_inner | epoch 219:   1235 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=51666.3, ups=13.29, wpb=3887, bsz=218.4, num_updates=295300, lr=5.81927e-05, gnorm=0.554, loss_scale=4, train_wall=7, gb_free=29.7, wall=4975
2024-01-31 11:35:52 | INFO | train_inner | epoch 219:   1335 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=52979.8, ups=13.6, wpb=3894.9, bsz=216.3, num_updates=295400, lr=5.81828e-05, gnorm=0.561, loss_scale=4, train_wall=7, gb_free=29.8, wall=4982
2024-01-31 11:35:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:35:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:35:55 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 2.215 | nll_loss 0.472 | ppl 1.39 | wps 108048 | wpb 3275.2 | bsz 176.5 | num_updates 295414 | best_loss 2.063
2024-01-31 11:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 295414 updates
2024-01-31 11:35:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint219.pt
2024-01-31 11:35:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint219.pt
2024-01-31 11:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint219.pt (epoch 219 @ 295414 updates, score 2.215) (writing took 2.4909606020664796 seconds)
2024-01-31 11:35:58 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2024-01-31 11:35:58 | INFO | train | epoch 219 | loss 1.775 | nll_loss 0.18 | ppl 1.13 | wps 50356.5 | ups 12.84 | wpb 3921.2 | bsz 214.5 | num_updates 295414 | lr 5.81814e-05 | gnorm 0.522 | loss_scale 4 | train_wall 96 | gb_free 29.7 | wall 4988
2024-01-31 11:35:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:35:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:35:58 | INFO | fairseq.trainer | begin training epoch 220
2024-01-31 11:35:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:36:05 | INFO | train_inner | epoch 220:     86 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=30106.9, ups=7.77, wpb=3876.3, bsz=207, num_updates=295500, lr=5.8173e-05, gnorm=0.517, loss_scale=4, train_wall=7, gb_free=29.8, wall=4995
2024-01-31 11:36:12 | INFO | train_inner | epoch 220:    186 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=52379.5, ups=13.38, wpb=3915.7, bsz=213.3, num_updates=295600, lr=5.81631e-05, gnorm=0.488, loss_scale=4, train_wall=7, gb_free=29.7, wall=5002
2024-01-31 11:36:20 | INFO | train_inner | epoch 220:    286 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=52317.8, ups=13.41, wpb=3902.3, bsz=202.8, num_updates=295700, lr=5.81533e-05, gnorm=0.572, loss_scale=4, train_wall=7, gb_free=29.8, wall=5010
2024-01-31 11:36:27 | INFO | train_inner | epoch 220:    386 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=50947.5, ups=13.1, wpb=3890.6, bsz=214.8, num_updates=295800, lr=5.81435e-05, gnorm=0.49, loss_scale=4, train_wall=7, gb_free=29.8, wall=5018
2024-01-31 11:36:35 | INFO | train_inner | epoch 220:    486 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=53883.4, ups=13.79, wpb=3908.7, bsz=230.5, num_updates=295900, lr=5.81336e-05, gnorm=0.573, loss_scale=4, train_wall=7, gb_free=29.8, wall=5025
2024-01-31 11:36:42 | INFO | train_inner | epoch 220:    586 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=52655.3, ups=13.33, wpb=3950.9, bsz=217.4, num_updates=296000, lr=5.81238e-05, gnorm=0.444, loss_scale=4, train_wall=7, gb_free=29.8, wall=5032
2024-01-31 11:36:50 | INFO | train_inner | epoch 220:    686 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=51899.6, ups=13.3, wpb=3903.5, bsz=219.7, num_updates=296100, lr=5.8114e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.7, wall=5040
2024-01-31 11:36:57 | INFO | train_inner | epoch 220:    786 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=50054.8, ups=12.78, wpb=3917.2, bsz=209, num_updates=296200, lr=5.81042e-05, gnorm=0.601, loss_scale=4, train_wall=8, gb_free=29.8, wall=5048
2024-01-31 11:37:05 | INFO | train_inner | epoch 220:    886 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=52389.8, ups=13.31, wpb=3935.1, bsz=215.5, num_updates=296300, lr=5.80944e-05, gnorm=0.606, loss_scale=4, train_wall=7, gb_free=29.8, wall=5055
2024-01-31 11:37:12 | INFO | train_inner | epoch 220:    986 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=53704.9, ups=13.59, wpb=3952.4, bsz=224.6, num_updates=296400, lr=5.80846e-05, gnorm=0.537, loss_scale=4, train_wall=7, gb_free=29.7, wall=5063
2024-01-31 11:37:20 | INFO | train_inner | epoch 220:   1086 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=52862.4, ups=13.56, wpb=3898.4, bsz=211, num_updates=296500, lr=5.80748e-05, gnorm=0.566, loss_scale=4, train_wall=7, gb_free=29.9, wall=5070
2024-01-31 11:37:27 | INFO | train_inner | epoch 220:   1186 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=51057.2, ups=12.99, wpb=3930.5, bsz=209.3, num_updates=296600, lr=5.8065e-05, gnorm=0.498, loss_scale=4, train_wall=7, gb_free=29.8, wall=5078
2024-01-31 11:37:35 | INFO | train_inner | epoch 220:   1286 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=51592.2, ups=13.04, wpb=3956.2, bsz=214.4, num_updates=296700, lr=5.80552e-05, gnorm=0.498, loss_scale=4, train_wall=7, gb_free=29.7, wall=5085
2024-01-31 11:37:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:37:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:37:42 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 2.216 | nll_loss 0.473 | ppl 1.39 | wps 114193 | wpb 3275.2 | bsz 176.5 | num_updates 296763 | best_loss 2.063
2024-01-31 11:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 296763 updates
2024-01-31 11:37:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint220.pt
2024-01-31 11:37:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint220.pt
2024-01-31 11:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint220.pt (epoch 220 @ 296763 updates, score 2.216) (writing took 2.555820288020186 seconds)
2024-01-31 11:37:45 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2024-01-31 11:37:45 | INFO | train | epoch 220 | loss 1.775 | nll_loss 0.18 | ppl 1.13 | wps 49507.5 | ups 12.63 | wpb 3921.2 | bsz 214.5 | num_updates 296763 | lr 5.80491e-05 | gnorm 0.53 | loss_scale 4 | train_wall 98 | gb_free 29.8 | wall 5095
2024-01-31 11:37:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:37:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:37:45 | INFO | fairseq.trainer | begin training epoch 221
2024-01-31 11:37:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:37:48 | INFO | train_inner | epoch 221:     37 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=31104.9, ups=7.9, wpb=3937.8, bsz=216.4, num_updates=296800, lr=5.80454e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.8, wall=5098
2024-01-31 11:37:56 | INFO | train_inner | epoch 221:    137 / 1349 loss=1.772, nll_loss=0.177, ppl=1.13, wps=47345.3, ups=12.03, wpb=3935, bsz=210.1, num_updates=296900, lr=5.80357e-05, gnorm=0.503, loss_scale=4, train_wall=8, gb_free=29.8, wall=5106
2024-01-31 11:38:04 | INFO | train_inner | epoch 221:    237 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=49427.9, ups=12.61, wpb=3920.4, bsz=206.2, num_updates=297000, lr=5.80259e-05, gnorm=0.489, loss_scale=4, train_wall=8, gb_free=29.7, wall=5114
2024-01-31 11:38:12 | INFO | train_inner | epoch 221:    337 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=48812.5, ups=12.31, wpb=3966.7, bsz=220.1, num_updates=297100, lr=5.80161e-05, gnorm=0.497, loss_scale=4, train_wall=8, gb_free=29.7, wall=5122
2024-01-31 11:38:21 | INFO | train_inner | epoch 221:    437 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=43230.8, ups=11.14, wpb=3881.7, bsz=209.6, num_updates=297200, lr=5.80064e-05, gnorm=0.505, loss_scale=4, train_wall=9, gb_free=29.7, wall=5131
2024-01-31 11:38:29 | INFO | train_inner | epoch 221:    537 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=50967.4, ups=13.02, wpb=3915.1, bsz=210.2, num_updates=297300, lr=5.79966e-05, gnorm=0.506, loss_scale=4, train_wall=7, gb_free=29.7, wall=5139
2024-01-31 11:38:37 | INFO | train_inner | epoch 221:    637 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=47519.3, ups=12.05, wpb=3943.6, bsz=223.5, num_updates=297400, lr=5.79869e-05, gnorm=0.503, loss_scale=4, train_wall=8, gb_free=29.8, wall=5147
2024-01-31 11:38:45 | INFO | train_inner | epoch 221:    737 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=47336.7, ups=12.24, wpb=3868.9, bsz=221.2, num_updates=297500, lr=5.79771e-05, gnorm=0.513, loss_scale=4, train_wall=8, gb_free=29.8, wall=5155
2024-01-31 11:38:53 | INFO | train_inner | epoch 221:    837 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=48963.1, ups=12.39, wpb=3952.6, bsz=223.8, num_updates=297600, lr=5.79674e-05, gnorm=0.505, loss_scale=4, train_wall=8, gb_free=29.7, wall=5164
2024-01-31 11:39:01 | INFO | train_inner | epoch 221:    937 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54513.9, ups=13.8, wpb=3950.4, bsz=222.2, num_updates=297700, lr=5.79576e-05, gnorm=0.531, loss_scale=4, train_wall=7, gb_free=29.8, wall=5171
2024-01-31 11:39:08 | INFO | train_inner | epoch 221:   1037 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=50557.8, ups=12.88, wpb=3924.1, bsz=211.2, num_updates=297800, lr=5.79479e-05, gnorm=0.456, loss_scale=4, train_wall=7, gb_free=29.7, wall=5179
2024-01-31 11:39:17 | INFO | train_inner | epoch 221:   1137 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=46451, ups=11.9, wpb=3903.8, bsz=213, num_updates=297900, lr=5.79382e-05, gnorm=0.594, loss_scale=4, train_wall=8, gb_free=29.8, wall=5187
2024-01-31 11:39:25 | INFO | train_inner | epoch 221:   1237 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=47853, ups=12.2, wpb=3924, bsz=208.5, num_updates=298000, lr=5.79284e-05, gnorm=0.505, loss_scale=4, train_wall=8, gb_free=29.8, wall=5195
2024-01-31 11:39:32 | INFO | train_inner | epoch 221:   1337 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54447.6, ups=13.95, wpb=3903.5, bsz=209.4, num_updates=298100, lr=5.79187e-05, gnorm=0.524, loss_scale=4, train_wall=7, gb_free=29.8, wall=5202
2024-01-31 11:39:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:39:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:39:36 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 2.216 | nll_loss 0.472 | ppl 1.39 | wps 107983 | wpb 3275.2 | bsz 176.5 | num_updates 298112 | best_loss 2.063
2024-01-31 11:39:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 298112 updates
2024-01-31 11:39:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint221.pt
2024-01-31 11:39:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint221.pt
2024-01-31 11:39:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint221.pt (epoch 221 @ 298112 updates, score 2.216) (writing took 2.35740004805848 seconds)
2024-01-31 11:39:38 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2024-01-31 11:39:38 | INFO | train | epoch 221 | loss 1.775 | nll_loss 0.18 | ppl 1.13 | wps 46724.9 | ups 11.92 | wpb 3921.2 | bsz 214.5 | num_updates 298112 | lr 5.79176e-05 | gnorm 0.514 | loss_scale 4 | train_wall 104 | gb_free 29.9 | wall 5208
2024-01-31 11:39:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:39:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:39:38 | INFO | fairseq.trainer | begin training epoch 222
2024-01-31 11:39:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:39:46 | INFO | train_inner | epoch 222:     88 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=27008.6, ups=7.02, wpb=3846.4, bsz=215.4, num_updates=298200, lr=5.7909e-05, gnorm=0.601, loss_scale=4, train_wall=8, gb_free=29.8, wall=5217
2024-01-31 11:39:55 | INFO | train_inner | epoch 222:    188 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=44951, ups=11.39, wpb=3945.6, bsz=213.4, num_updates=298300, lr=5.78993e-05, gnorm=0.605, loss_scale=4, train_wall=8, gb_free=29.8, wall=5225
2024-01-31 11:40:03 | INFO | train_inner | epoch 222:    288 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=49293.1, ups=12.57, wpb=3922.7, bsz=225.4, num_updates=298400, lr=5.78896e-05, gnorm=0.477, loss_scale=4, train_wall=8, gb_free=29.7, wall=5233
2024-01-31 11:40:12 | INFO | train_inner | epoch 222:    388 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=43823.4, ups=11.07, wpb=3959.9, bsz=212.8, num_updates=298500, lr=5.78799e-05, gnorm=0.521, loss_scale=4, train_wall=9, gb_free=29.8, wall=5242
2024-01-31 11:40:20 | INFO | train_inner | epoch 222:    488 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=51755.5, ups=13.24, wpb=3910.5, bsz=219.9, num_updates=298600, lr=5.78702e-05, gnorm=0.529, loss_scale=4, train_wall=7, gb_free=29.8, wall=5250
2024-01-31 11:40:27 | INFO | train_inner | epoch 222:    588 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=50851.8, ups=12.95, wpb=3927.8, bsz=215.1, num_updates=298700, lr=5.78605e-05, gnorm=0.542, loss_scale=4, train_wall=7, gb_free=29.8, wall=5258
2024-01-31 11:40:35 | INFO | train_inner | epoch 222:    688 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=48143.2, ups=12.43, wpb=3871.6, bsz=203.4, num_updates=298800, lr=5.78508e-05, gnorm=0.51, loss_scale=4, train_wall=8, gb_free=29.8, wall=5266
2024-01-31 11:40:44 | INFO | train_inner | epoch 222:    788 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=47348.9, ups=11.96, wpb=3958.5, bsz=224.3, num_updates=298900, lr=5.78412e-05, gnorm=0.488, loss_scale=4, train_wall=8, gb_free=29.9, wall=5274
2024-01-31 11:40:51 | INFO | train_inner | epoch 222:    888 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=53492, ups=13.7, wpb=3903.7, bsz=217.7, num_updates=299000, lr=5.78315e-05, gnorm=0.533, loss_scale=4, train_wall=7, gb_free=29.8, wall=5281
2024-01-31 11:40:59 | INFO | train_inner | epoch 222:    988 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=51529.7, ups=13.14, wpb=3920.8, bsz=219.7, num_updates=299100, lr=5.78218e-05, gnorm=0.528, loss_scale=4, train_wall=7, gb_free=29.8, wall=5289
2024-01-31 11:41:06 | INFO | train_inner | epoch 222:   1088 / 1349 loss=1.777, nll_loss=0.181, ppl=1.13, wps=51088.8, ups=12.99, wpb=3933.1, bsz=203, num_updates=299200, lr=5.78122e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.7, wall=5297
2024-01-31 11:41:14 | INFO | train_inner | epoch 222:   1188 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=48804.2, ups=12.42, wpb=3930.4, bsz=205.6, num_updates=299300, lr=5.78025e-05, gnorm=0.578, loss_scale=4, train_wall=8, gb_free=29.7, wall=5305
2024-01-31 11:41:23 | INFO | train_inner | epoch 222:   1288 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=43726, ups=11.13, wpb=3928.2, bsz=215.6, num_updates=299400, lr=5.77928e-05, gnorm=0.457, loss_scale=4, train_wall=9, gb_free=29.8, wall=5314
2024-01-31 11:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:41:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:41:30 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 2.213 | nll_loss 0.468 | ppl 1.38 | wps 114499 | wpb 3275.2 | bsz 176.5 | num_updates 299461 | best_loss 2.063
2024-01-31 11:41:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 299461 updates
2024-01-31 11:41:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint222.pt
2024-01-31 11:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint222.pt
2024-01-31 11:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint222.pt (epoch 222 @ 299461 updates, score 2.213) (writing took 2.504879873013124 seconds)
2024-01-31 11:41:33 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2024-01-31 11:41:33 | INFO | train | epoch 222 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 46006.9 | ups 11.73 | wpb 3921.2 | bsz 214.5 | num_updates 299461 | lr 5.7787e-05 | gnorm 0.531 | loss_scale 4 | train_wall 105 | gb_free 29.7 | wall 5323
2024-01-31 11:41:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:41:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:41:33 | INFO | fairseq.trainer | begin training epoch 223
2024-01-31 11:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:41:38 | INFO | train_inner | epoch 223:     39 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=26709.3, ups=6.85, wpb=3901.9, bsz=222.8, num_updates=299500, lr=5.77832e-05, gnorm=0.552, loss_scale=4, train_wall=9, gb_free=29.7, wall=5328
2024-01-31 11:41:46 | INFO | train_inner | epoch 223:    139 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=49476.1, ups=12.51, wpb=3954.5, bsz=212, num_updates=299600, lr=5.77736e-05, gnorm=0.522, loss_scale=4, train_wall=8, gb_free=29.8, wall=5336
2024-01-31 11:41:57 | INFO | train_inner | epoch 223:    239 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=36154.4, ups=9.3, wpb=3889, bsz=206.6, num_updates=299700, lr=5.77639e-05, gnorm=0.554, loss_scale=4, train_wall=10, gb_free=29.8, wall=5347
2024-01-31 11:42:04 | INFO | train_inner | epoch 223:    339 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=53931.9, ups=13.7, wpb=3935.5, bsz=218.2, num_updates=299800, lr=5.77543e-05, gnorm=0.517, loss_scale=4, train_wall=7, gb_free=29.7, wall=5354
2024-01-31 11:42:12 | INFO | train_inner | epoch 223:    439 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=52683.7, ups=13.36, wpb=3943.7, bsz=222.4, num_updates=299900, lr=5.77447e-05, gnorm=0.52, loss_scale=4, train_wall=7, gb_free=29.7, wall=5362
2024-01-31 11:42:20 | INFO | train_inner | epoch 223:    539 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=49027.4, ups=12.35, wpb=3969.4, bsz=215.6, num_updates=300000, lr=5.7735e-05, gnorm=0.54, loss_scale=4, train_wall=8, gb_free=29.9, wall=5370
2024-01-31 11:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:42:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:42:22 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 2.215 | nll_loss 0.471 | ppl 1.39 | wps 113867 | wpb 3275.2 | bsz 176.5 | num_updates 300000 | best_loss 2.063
2024-01-31 11:42:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 300000 updates
2024-01-31 11:42:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_223_300000.pt
2024-01-31 11:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_223_300000.pt
2024-01-31 11:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_223_300000.pt (epoch 223 @ 300000 updates, score 2.215) (writing took 2.396621444961056 seconds)
2024-01-31 11:42:34 | INFO | train_inner | epoch 223:    639 / 1349 loss=1.778, nll_loss=0.183, ppl=1.13, wps=26617.8, ups=6.77, wpb=3931.7, bsz=203.4, num_updates=300100, lr=5.77254e-05, gnorm=0.603, loss_scale=4, train_wall=9, gb_free=29.8, wall=5385
2024-01-31 11:42:42 | INFO | train_inner | epoch 223:    739 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=51214.3, ups=13.04, wpb=3926.1, bsz=207.3, num_updates=300200, lr=5.77158e-05, gnorm=0.547, loss_scale=4, train_wall=7, gb_free=29.8, wall=5392
2024-01-31 11:42:51 | INFO | train_inner | epoch 223:    839 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=44175.7, ups=11.31, wpb=3907.3, bsz=196.6, num_updates=300300, lr=5.77062e-05, gnorm=0.519, loss_scale=4, train_wall=9, gb_free=29.8, wall=5401
2024-01-31 11:42:58 | INFO | train_inner | epoch 223:    939 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=51944.7, ups=13.39, wpb=3880.4, bsz=215.6, num_updates=300400, lr=5.76966e-05, gnorm=0.564, loss_scale=4, train_wall=7, gb_free=29.8, wall=5409
2024-01-31 11:43:06 | INFO | train_inner | epoch 223:   1039 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=50417.1, ups=12.85, wpb=3924.6, bsz=227.3, num_updates=300500, lr=5.7687e-05, gnorm=0.551, loss_scale=4, train_wall=8, gb_free=29.7, wall=5416
2024-01-31 11:43:14 | INFO | train_inner | epoch 223:   1139 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=53475.4, ups=13.64, wpb=3921.3, bsz=223.4, num_updates=300600, lr=5.76774e-05, gnorm=0.499, loss_scale=4, train_wall=7, gb_free=29.8, wall=5424
2024-01-31 11:43:21 | INFO | train_inner | epoch 223:   1239 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=52829.9, ups=13.66, wpb=3868.3, bsz=216.3, num_updates=300700, lr=5.76678e-05, gnorm=0.655, loss_scale=4, train_wall=7, gb_free=29.8, wall=5431
2024-01-31 11:43:28 | INFO | train_inner | epoch 223:   1339 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=52432.2, ups=13.34, wpb=3931.7, bsz=210.2, num_updates=300800, lr=5.76582e-05, gnorm=0.584, loss_scale=4, train_wall=7, gb_free=29.8, wall=5439
2024-01-31 11:43:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:43:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:43:32 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 2.214 | nll_loss 0.469 | ppl 1.38 | wps 115480 | wpb 3275.2 | bsz 176.5 | num_updates 300810 | best_loss 2.063
2024-01-31 11:43:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 300810 updates
2024-01-31 11:43:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint223.pt
2024-01-31 11:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint223.pt
2024-01-31 11:43:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint223.pt (epoch 223 @ 300810 updates, score 2.214) (writing took 2.371825542068109 seconds)
2024-01-31 11:43:34 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2024-01-31 11:43:34 | INFO | train | epoch 223 | loss 1.775 | nll_loss 0.18 | ppl 1.13 | wps 43703.5 | ups 11.15 | wpb 3921.2 | bsz 214.5 | num_updates 300810 | lr 5.76572e-05 | gnorm 0.553 | loss_scale 4 | train_wall 106 | gb_free 29.8 | wall 5444
2024-01-31 11:43:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:43:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:43:34 | INFO | fairseq.trainer | begin training epoch 224
2024-01-31 11:43:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:43:42 | INFO | train_inner | epoch 224:     90 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=28000.1, ups=7.12, wpb=3934.6, bsz=203, num_updates=300900, lr=5.76486e-05, gnorm=0.547, loss_scale=4, train_wall=8, gb_free=29.7, wall=5453
2024-01-31 11:43:51 | INFO | train_inner | epoch 224:    190 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=49000.6, ups=12.29, wpb=3985.4, bsz=216.4, num_updates=301000, lr=5.7639e-05, gnorm=0.527, loss_scale=4, train_wall=8, gb_free=29.7, wall=5461
2024-01-31 11:43:59 | INFO | train_inner | epoch 224:    290 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=45987, ups=11.83, wpb=3887.7, bsz=216.2, num_updates=301100, lr=5.76295e-05, gnorm=0.532, loss_scale=4, train_wall=8, gb_free=29.8, wall=5469
2024-01-31 11:44:08 | INFO | train_inner | epoch 224:    390 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=44225.2, ups=11.27, wpb=3923.2, bsz=206, num_updates=301200, lr=5.76199e-05, gnorm=0.5, loss_scale=4, train_wall=9, gb_free=29.8, wall=5478
2024-01-31 11:44:16 | INFO | train_inner | epoch 224:    490 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=46970.3, ups=12.02, wpb=3909.1, bsz=219.1, num_updates=301300, lr=5.76103e-05, gnorm=0.521, loss_scale=4, train_wall=8, gb_free=29.8, wall=5486
2024-01-31 11:44:24 | INFO | train_inner | epoch 224:    590 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=51015.8, ups=12.98, wpb=3930.4, bsz=215.8, num_updates=301400, lr=5.76008e-05, gnorm=0.563, loss_scale=4, train_wall=7, gb_free=29.8, wall=5494
2024-01-31 11:44:32 | INFO | train_inner | epoch 224:    690 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=50875.7, ups=12.98, wpb=3920.8, bsz=224.4, num_updates=301500, lr=5.75912e-05, gnorm=0.475, loss_scale=4, train_wall=7, gb_free=29.8, wall=5502
2024-01-31 11:44:39 | INFO | train_inner | epoch 224:    790 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=51031.7, ups=13.16, wpb=3877.2, bsz=215.4, num_updates=301600, lr=5.75817e-05, gnorm=0.562, loss_scale=4, train_wall=7, gb_free=29.8, wall=5510
2024-01-31 11:44:48 | INFO | train_inner | epoch 224:    890 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=47364.8, ups=12.06, wpb=3928.3, bsz=213.7, num_updates=301700, lr=5.75721e-05, gnorm=0.486, loss_scale=4, train_wall=8, gb_free=29.8, wall=5518
2024-01-31 11:44:57 | INFO | train_inner | epoch 224:    990 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=41717.8, ups=10.69, wpb=3902.3, bsz=203.8, num_updates=301800, lr=5.75626e-05, gnorm=0.544, loss_scale=4, train_wall=9, gb_free=29.8, wall=5527
2024-01-31 11:45:04 | INFO | train_inner | epoch 224:   1090 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=53823, ups=13.63, wpb=3947.6, bsz=214.1, num_updates=301900, lr=5.75531e-05, gnorm=0.592, loss_scale=4, train_wall=7, gb_free=29.7, wall=5534
2024-01-31 11:45:12 | INFO | train_inner | epoch 224:   1190 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=51517, ups=13.19, wpb=3904.9, bsz=216.6, num_updates=302000, lr=5.75435e-05, gnorm=0.536, loss_scale=4, train_wall=7, gb_free=29.8, wall=5542
2024-01-31 11:45:19 | INFO | train_inner | epoch 224:   1290 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=52505, ups=13.36, wpb=3928.6, bsz=220.6, num_updates=302100, lr=5.7534e-05, gnorm=0.516, loss_scale=4, train_wall=7, gb_free=29.7, wall=5550
2024-01-31 11:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:45:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:45:26 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 2.212 | nll_loss 0.467 | ppl 1.38 | wps 112926 | wpb 3275.2 | bsz 176.5 | num_updates 302159 | best_loss 2.063
2024-01-31 11:45:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 302159 updates
2024-01-31 11:45:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint224.pt
2024-01-31 11:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint224.pt
2024-01-31 11:45:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint224.pt (epoch 224 @ 302159 updates, score 2.212) (writing took 2.432709714048542 seconds)
2024-01-31 11:45:29 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2024-01-31 11:45:29 | INFO | train | epoch 224 | loss 1.775 | nll_loss 0.18 | ppl 1.13 | wps 46155.3 | ups 11.77 | wpb 3921.2 | bsz 214.5 | num_updates 302159 | lr 5.75284e-05 | gnorm 0.536 | loss_scale 4 | train_wall 105 | gb_free 29.8 | wall 5559
2024-01-31 11:45:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:45:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:45:29 | INFO | fairseq.trainer | begin training epoch 225
2024-01-31 11:45:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:45:32 | INFO | train_inner | epoch 225:     41 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=31170.6, ups=7.97, wpb=3910.7, bsz=231.3, num_updates=302200, lr=5.75245e-05, gnorm=0.598, loss_scale=4, train_wall=7, gb_free=29.8, wall=5562
2024-01-31 11:45:39 | INFO | train_inner | epoch 225:    141 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55150, ups=14.08, wpb=3916.5, bsz=213.3, num_updates=302300, lr=5.7515e-05, gnorm=0.481, loss_scale=4, train_wall=7, gb_free=29.8, wall=5569
2024-01-31 11:45:47 | INFO | train_inner | epoch 225:    241 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=47678.7, ups=12.09, wpb=3942.1, bsz=216.8, num_updates=302400, lr=5.75055e-05, gnorm=0.493, loss_scale=4, train_wall=8, gb_free=29.8, wall=5577
2024-01-31 11:45:56 | INFO | train_inner | epoch 225:    341 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=45373.8, ups=11.51, wpb=3940.7, bsz=205.3, num_updates=302500, lr=5.7496e-05, gnorm=0.565, loss_scale=4, train_wall=8, gb_free=29.7, wall=5586
2024-01-31 11:46:11 | INFO | train_inner | epoch 225:    441 / 1349 loss=1.772, nll_loss=0.177, ppl=1.13, wps=26987.6, ups=6.81, wpb=3964.8, bsz=217.3, num_updates=302600, lr=5.74865e-05, gnorm=0.429, loss_scale=4, train_wall=8, gb_free=29.7, wall=5601
2024-01-31 11:46:18 | INFO | train_inner | epoch 225:    541 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=54157.1, ups=13.84, wpb=3912.5, bsz=224.1, num_updates=302700, lr=5.7477e-05, gnorm=0.511, loss_scale=4, train_wall=7, gb_free=29.8, wall=5608
2024-01-31 11:46:26 | INFO | train_inner | epoch 225:    641 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=49725.6, ups=12.7, wpb=3916.8, bsz=216.4, num_updates=302800, lr=5.74675e-05, gnorm=0.594, loss_scale=4, train_wall=8, gb_free=29.9, wall=5616
2024-01-31 11:46:34 | INFO | train_inner | epoch 225:    741 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=48323.7, ups=12.26, wpb=3941, bsz=210.6, num_updates=302900, lr=5.7458e-05, gnorm=0.567, loss_scale=4, train_wall=8, gb_free=29.7, wall=5624
2024-01-31 11:46:42 | INFO | train_inner | epoch 225:    841 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=50844.1, ups=12.98, wpb=3918.6, bsz=209.7, num_updates=303000, lr=5.74485e-05, gnorm=0.549, loss_scale=4, train_wall=7, gb_free=29.8, wall=5632
2024-01-31 11:46:50 | INFO | train_inner | epoch 225:    941 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=48525.6, ups=12.37, wpb=3923.4, bsz=218.2, num_updates=303100, lr=5.7439e-05, gnorm=0.584, loss_scale=4, train_wall=7, gb_free=29.8, wall=5640
2024-01-31 11:46:57 | INFO | train_inner | epoch 225:   1041 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=52813.3, ups=13.63, wpb=3874.9, bsz=208.3, num_updates=303200, lr=5.74295e-05, gnorm=0.515, loss_scale=8, train_wall=7, gb_free=29.8, wall=5647
2024-01-31 11:47:05 | INFO | train_inner | epoch 225:   1141 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=49402.7, ups=12.67, wpb=3900.6, bsz=212.8, num_updates=303300, lr=5.74201e-05, gnorm=0.58, loss_scale=8, train_wall=8, gb_free=29.8, wall=5655
2024-01-31 11:47:12 | INFO | train_inner | epoch 225:   1241 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=52637, ups=13.44, wpb=3917.8, bsz=214.8, num_updates=303400, lr=5.74106e-05, gnorm=0.596, loss_scale=8, train_wall=7, gb_free=29.8, wall=5663
2024-01-31 11:47:20 | INFO | train_inner | epoch 225:   1341 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=52729.1, ups=13.48, wpb=3912, bsz=207.2, num_updates=303500, lr=5.74012e-05, gnorm=0.555, loss_scale=8, train_wall=7, gb_free=29.8, wall=5670
2024-01-31 11:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:47:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:47:23 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 2.214 | nll_loss 0.468 | ppl 1.38 | wps 111546 | wpb 3275.2 | bsz 176.5 | num_updates 303508 | best_loss 2.063
2024-01-31 11:47:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 303508 updates
2024-01-31 11:47:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint225.pt
2024-01-31 11:47:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint225.pt
2024-01-31 11:47:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint225.pt (epoch 225 @ 303508 updates, score 2.214) (writing took 2.450218101963401 seconds)
2024-01-31 11:47:25 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2024-01-31 11:47:25 | INFO | train | epoch 225 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 45284.9 | ups 11.55 | wpb 3921.2 | bsz 214.5 | num_updates 303508 | lr 5.74004e-05 | gnorm 0.538 | loss_scale 8 | train_wall 101 | gb_free 29.7 | wall 5676
2024-01-31 11:47:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:47:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:47:26 | INFO | fairseq.trainer | begin training epoch 226
2024-01-31 11:47:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:47:34 | INFO | train_inner | epoch 226:     92 / 1349 loss=1.771, nll_loss=0.175, ppl=1.13, wps=28322, ups=7.19, wpb=3937, bsz=215.4, num_updates=303600, lr=5.73917e-05, gnorm=0.451, loss_scale=8, train_wall=8, gb_free=29.8, wall=5684
2024-01-31 11:47:42 | INFO | train_inner | epoch 226:    192 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=45303.6, ups=11.51, wpb=3935.2, bsz=215.9, num_updates=303700, lr=5.73823e-05, gnorm=0.527, loss_scale=8, train_wall=8, gb_free=29.8, wall=5693
2024-01-31 11:47:51 | INFO | train_inner | epoch 226:    292 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=44842.1, ups=11.63, wpb=3856.6, bsz=198.2, num_updates=303800, lr=5.73728e-05, gnorm=0.537, loss_scale=8, train_wall=8, gb_free=29.7, wall=5701
2024-01-31 11:48:00 | INFO | train_inner | epoch 226:    392 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=44761.3, ups=11.42, wpb=3920.3, bsz=222.2, num_updates=303900, lr=5.73634e-05, gnorm=0.529, loss_scale=8, train_wall=8, gb_free=29.8, wall=5710
2024-01-31 11:48:07 | INFO | train_inner | epoch 226:    492 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54928.1, ups=14.07, wpb=3902.6, bsz=223.3, num_updates=304000, lr=5.73539e-05, gnorm=0.516, loss_scale=8, train_wall=7, gb_free=29.8, wall=5717
2024-01-31 11:48:15 | INFO | train_inner | epoch 226:    592 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=48298.1, ups=12.34, wpb=3913.2, bsz=218, num_updates=304100, lr=5.73445e-05, gnorm=0.509, loss_scale=8, train_wall=8, gb_free=29.8, wall=5725
2024-01-31 11:48:23 | INFO | train_inner | epoch 226:    692 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=49319.5, ups=12.43, wpb=3968.5, bsz=206.8, num_updates=304200, lr=5.73351e-05, gnorm=0.464, loss_scale=8, train_wall=8, gb_free=29.9, wall=5733
2024-01-31 11:48:31 | INFO | train_inner | epoch 226:    792 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=47040, ups=12.05, wpb=3902.8, bsz=217.2, num_updates=304300, lr=5.73257e-05, gnorm=0.581, loss_scale=8, train_wall=8, gb_free=29.7, wall=5742
2024-01-31 11:48:40 | INFO | train_inner | epoch 226:    892 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=46128.3, ups=11.77, wpb=3917.5, bsz=206.6, num_updates=304400, lr=5.73162e-05, gnorm=0.51, loss_scale=8, train_wall=8, gb_free=29.7, wall=5750
2024-01-31 11:48:47 | INFO | train_inner | epoch 226:    992 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=53107.6, ups=13.45, wpb=3949.6, bsz=232.7, num_updates=304500, lr=5.73068e-05, gnorm=0.536, loss_scale=8, train_wall=7, gb_free=29.8, wall=5757
2024-01-31 11:48:55 | INFO | train_inner | epoch 226:   1092 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=52046.1, ups=13.22, wpb=3937, bsz=208.2, num_updates=304600, lr=5.72974e-05, gnorm=0.498, loss_scale=8, train_wall=7, gb_free=29.8, wall=5765
2024-01-31 11:49:03 | INFO | train_inner | epoch 226:   1192 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=50990.1, ups=12.87, wpb=3962.9, bsz=214.6, num_updates=304700, lr=5.7288e-05, gnorm=0.458, loss_scale=8, train_wall=8, gb_free=29.7, wall=5773
2024-01-31 11:49:11 | INFO | train_inner | epoch 226:   1292 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=45253, ups=11.62, wpb=3893.2, bsz=218.2, num_updates=304800, lr=5.72786e-05, gnorm=0.546, loss_scale=8, train_wall=8, gb_free=29.8, wall=5781
2024-01-31 11:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:49:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:49:18 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 2.214 | nll_loss 0.467 | ppl 1.38 | wps 112304 | wpb 3275.2 | bsz 176.5 | num_updates 304857 | best_loss 2.063
2024-01-31 11:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 304857 updates
2024-01-31 11:49:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint226.pt
2024-01-31 11:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint226.pt
2024-01-31 11:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint226.pt (epoch 226 @ 304857 updates, score 2.214) (writing took 2.341703003970906 seconds)
2024-01-31 11:49:20 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2024-01-31 11:49:20 | INFO | train | epoch 226 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 46101.2 | ups 11.76 | wpb 3921.2 | bsz 214.5 | num_updates 304857 | lr 5.72733e-05 | gnorm 0.519 | loss_scale 8 | train_wall 106 | gb_free 29.7 | wall 5790
2024-01-31 11:49:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:49:20 | INFO | fairseq.trainer | begin training epoch 227
2024-01-31 11:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:49:25 | INFO | train_inner | epoch 227:     43 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=28733.2, ups=7.34, wpb=3915.3, bsz=208, num_updates=304900, lr=5.72692e-05, gnorm=0.597, loss_scale=8, train_wall=8, gb_free=29.8, wall=5795
2024-01-31 11:49:34 | INFO | train_inner | epoch 227:    143 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=44092.8, ups=11.16, wpb=3952.4, bsz=220.6, num_updates=305000, lr=5.72598e-05, gnorm=0.487, loss_scale=8, train_wall=9, gb_free=29.9, wall=5804
2024-01-31 11:49:42 | INFO | train_inner | epoch 227:    243 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=46822.7, ups=11.79, wpb=3970.9, bsz=219.9, num_updates=305100, lr=5.72504e-05, gnorm=0.527, loss_scale=8, train_wall=8, gb_free=29.8, wall=5813
2024-01-31 11:49:50 | INFO | train_inner | epoch 227:    343 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=53392.3, ups=13.65, wpb=3912.7, bsz=202.3, num_updates=305200, lr=5.72411e-05, gnorm=0.475, loss_scale=8, train_wall=7, gb_free=29.8, wall=5820
2024-01-31 11:49:57 | INFO | train_inner | epoch 227:    443 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=53775.1, ups=13.68, wpb=3931.6, bsz=205.5, num_updates=305300, lr=5.72317e-05, gnorm=0.483, loss_scale=8, train_wall=7, gb_free=29.8, wall=5827
2024-01-31 11:50:04 | INFO | train_inner | epoch 227:    543 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=54085, ups=14.1, wpb=3836.2, bsz=215.9, num_updates=305400, lr=5.72223e-05, gnorm=0.613, loss_scale=8, train_wall=7, gb_free=29.9, wall=5834
2024-01-31 11:50:11 | INFO | train_inner | epoch 227:    643 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=55880.7, ups=14.15, wpb=3948.3, bsz=209.8, num_updates=305500, lr=5.7213e-05, gnorm=0.66, loss_scale=8, train_wall=7, gb_free=29.8, wall=5841
2024-01-31 11:50:18 | INFO | train_inner | epoch 227:    743 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55587.1, ups=13.96, wpb=3983.1, bsz=211.5, num_updates=305600, lr=5.72036e-05, gnorm=0.527, loss_scale=8, train_wall=7, gb_free=29.8, wall=5848
2024-01-31 11:50:25 | INFO | train_inner | epoch 227:    843 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=54936.2, ups=14.12, wpb=3891.9, bsz=206.6, num_updates=305700, lr=5.71942e-05, gnorm=0.558, loss_scale=8, train_wall=7, gb_free=29.8, wall=5856
2024-01-31 11:50:32 | INFO | train_inner | epoch 227:    943 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55828.6, ups=14.43, wpb=3867.9, bsz=203.7, num_updates=305800, lr=5.71849e-05, gnorm=0.53, loss_scale=8, train_wall=7, gb_free=29.7, wall=5862
2024-01-31 11:50:39 | INFO | train_inner | epoch 227:   1043 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55590.9, ups=14.07, wpb=3949.7, bsz=209.4, num_updates=305900, lr=5.71755e-05, gnorm=0.548, loss_scale=8, train_wall=7, gb_free=29.8, wall=5870
2024-01-31 11:50:48 | INFO | train_inner | epoch 227:   1143 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=45319.3, ups=11.57, wpb=3917.1, bsz=222.6, num_updates=306000, lr=5.71662e-05, gnorm=0.517, loss_scale=8, train_wall=8, gb_free=29.8, wall=5878
2024-01-31 11:50:56 | INFO | train_inner | epoch 227:   1243 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=47134.8, ups=12.02, wpb=3921.1, bsz=225.4, num_updates=306100, lr=5.71569e-05, gnorm=0.54, loss_scale=8, train_wall=8, gb_free=29.8, wall=5887
2024-01-31 11:51:04 | INFO | train_inner | epoch 227:   1343 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=48292.8, ups=12.45, wpb=3879, bsz=236.6, num_updates=306200, lr=5.71475e-05, gnorm=0.493, loss_scale=8, train_wall=8, gb_free=29.7, wall=5895
2024-01-31 11:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:51:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:51:07 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 2.216 | nll_loss 0.472 | ppl 1.39 | wps 115718 | wpb 3275.2 | bsz 176.5 | num_updates 306206 | best_loss 2.063
2024-01-31 11:51:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 306206 updates
2024-01-31 11:51:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint227.pt
2024-01-31 11:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint227.pt
2024-01-31 11:51:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint227.pt (epoch 227 @ 306206 updates, score 2.216) (writing took 2.330827232915908 seconds)
2024-01-31 11:51:10 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2024-01-31 11:51:10 | INFO | train | epoch 227 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 48336.1 | ups 12.33 | wpb 3921.2 | bsz 214.5 | num_updates 306206 | lr 5.7147e-05 | gnorm 0.534 | loss_scale 8 | train_wall 100 | gb_free 29.7 | wall 5900
2024-01-31 11:51:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:51:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:51:10 | INFO | fairseq.trainer | begin training epoch 228
2024-01-31 11:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:51:17 | INFO | train_inner | epoch 228:     94 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=30747.7, ups=7.8, wpb=3942.1, bsz=201, num_updates=306300, lr=5.71382e-05, gnorm=0.582, loss_scale=8, train_wall=8, gb_free=29.7, wall=5907
2024-01-31 11:51:25 | INFO | train_inner | epoch 228:    194 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=48888.8, ups=12.45, wpb=3926.8, bsz=212.4, num_updates=306400, lr=5.71289e-05, gnorm=0.603, loss_scale=8, train_wall=8, gb_free=29.8, wall=5915
2024-01-31 11:51:33 | INFO | train_inner | epoch 228:    294 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=48617.6, ups=12.48, wpb=3897, bsz=215.4, num_updates=306500, lr=5.71195e-05, gnorm=0.589, loss_scale=8, train_wall=8, gb_free=29.7, wall=5923
2024-01-31 11:51:40 | INFO | train_inner | epoch 228:    394 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54819.2, ups=14.11, wpb=3886.4, bsz=214.2, num_updates=306600, lr=5.71102e-05, gnorm=0.552, loss_scale=8, train_wall=7, gb_free=29.7, wall=5931
2024-01-31 11:51:48 | INFO | train_inner | epoch 228:    494 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=52984, ups=13.3, wpb=3983, bsz=218.1, num_updates=306700, lr=5.71009e-05, gnorm=0.485, loss_scale=8, train_wall=7, gb_free=29.8, wall=5938
2024-01-31 11:51:55 | INFO | train_inner | epoch 228:    594 / 1349 loss=1.772, nll_loss=0.177, ppl=1.13, wps=52906.2, ups=13.31, wpb=3974.4, bsz=226.1, num_updates=306800, lr=5.70916e-05, gnorm=0.53, loss_scale=8, train_wall=7, gb_free=29.7, wall=5946
2024-01-31 11:52:03 | INFO | train_inner | epoch 228:    694 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=52614.8, ups=13.29, wpb=3958.1, bsz=225.5, num_updates=306900, lr=5.70823e-05, gnorm=0.5, loss_scale=8, train_wall=7, gb_free=29.8, wall=5953
2024-01-31 11:52:11 | INFO | train_inner | epoch 228:    794 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=48144.5, ups=12.33, wpb=3903.2, bsz=216.4, num_updates=307000, lr=5.7073e-05, gnorm=0.532, loss_scale=8, train_wall=8, gb_free=29.8, wall=5961
2024-01-31 11:52:18 | INFO | train_inner | epoch 228:    894 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=54460.1, ups=14.01, wpb=3887.9, bsz=222.2, num_updates=307100, lr=5.70637e-05, gnorm=0.564, loss_scale=8, train_wall=7, gb_free=29.9, wall=5968
2024-01-31 11:52:26 | INFO | train_inner | epoch 228:    994 / 1349 loss=1.778, nll_loss=0.182, ppl=1.13, wps=48742.5, ups=12.56, wpb=3880.6, bsz=203.4, num_updates=307200, lr=5.70544e-05, gnorm=0.593, loss_scale=8, train_wall=8, gb_free=29.8, wall=5976
2024-01-31 11:52:34 | INFO | train_inner | epoch 228:   1094 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=52027.7, ups=13.08, wpb=3976.9, bsz=202.2, num_updates=307300, lr=5.70451e-05, gnorm=0.491, loss_scale=8, train_wall=7, gb_free=29.7, wall=5984
2024-01-31 11:52:41 | INFO | train_inner | epoch 228:   1194 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=53238.1, ups=13.71, wpb=3884.4, bsz=217.4, num_updates=307400, lr=5.70359e-05, gnorm=0.553, loss_scale=8, train_wall=7, gb_free=29.7, wall=5991
2024-01-31 11:52:48 | INFO | train_inner | epoch 228:   1294 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=53809, ups=13.94, wpb=3860, bsz=204.7, num_updates=307500, lr=5.70266e-05, gnorm=0.543, loss_scale=8, train_wall=7, gb_free=29.8, wall=5998
2024-01-31 11:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:52:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:52:55 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 2.214 | nll_loss 0.469 | ppl 1.38 | wps 104890 | wpb 3275.2 | bsz 176.5 | num_updates 307555 | best_loss 2.063
2024-01-31 11:52:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 307555 updates
2024-01-31 11:52:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint228.pt
2024-01-31 11:52:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint228.pt
2024-01-31 11:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint228.pt (epoch 228 @ 307555 updates, score 2.214) (writing took 2.355074711027555 seconds)
2024-01-31 11:52:57 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2024-01-31 11:52:57 | INFO | train | epoch 228 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 49162.2 | ups 12.54 | wpb 3921.2 | bsz 214.5 | num_updates 307555 | lr 5.70215e-05 | gnorm 0.546 | loss_scale 8 | train_wall 98 | gb_free 29.8 | wall 6007
2024-01-31 11:52:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:52:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:52:58 | INFO | fairseq.trainer | begin training epoch 229
2024-01-31 11:52:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:53:02 | INFO | train_inner | epoch 229:     45 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=29607.5, ups=7.51, wpb=3943.7, bsz=223.6, num_updates=307600, lr=5.70173e-05, gnorm=0.496, loss_scale=8, train_wall=7, gb_free=29.8, wall=6012
2024-01-31 11:53:09 | INFO | train_inner | epoch 229:    145 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=50351, ups=12.82, wpb=3928, bsz=204.7, num_updates=307700, lr=5.70081e-05, gnorm=0.574, loss_scale=8, train_wall=8, gb_free=29.7, wall=6020
2024-01-31 11:53:18 | INFO | train_inner | epoch 229:    245 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=46980.1, ups=12.07, wpb=3893.3, bsz=210.6, num_updates=307800, lr=5.69988e-05, gnorm=0.526, loss_scale=8, train_wall=8, gb_free=29.9, wall=6028
2024-01-31 11:53:26 | INFO | train_inner | epoch 229:    345 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=46734, ups=11.85, wpb=3945, bsz=204.6, num_updates=307900, lr=5.69895e-05, gnorm=0.488, loss_scale=8, train_wall=8, gb_free=29.9, wall=6036
2024-01-31 11:53:33 | INFO | train_inner | epoch 229:    445 / 1349 loss=1.771, nll_loss=0.175, ppl=1.13, wps=53184.8, ups=13.5, wpb=3940.2, bsz=218.6, num_updates=308000, lr=5.69803e-05, gnorm=0.456, loss_scale=8, train_wall=7, gb_free=29.8, wall=6044
2024-01-31 11:53:41 | INFO | train_inner | epoch 229:    545 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=49623.8, ups=12.72, wpb=3901.7, bsz=217.8, num_updates=308100, lr=5.6971e-05, gnorm=0.512, loss_scale=8, train_wall=8, gb_free=29.8, wall=6052
2024-01-31 11:53:49 | INFO | train_inner | epoch 229:    645 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=48248.3, ups=12.29, wpb=3927.2, bsz=213.6, num_updates=308200, lr=5.69618e-05, gnorm=0.64, loss_scale=8, train_wall=8, gb_free=29.8, wall=6060
2024-01-31 11:53:59 | INFO | train_inner | epoch 229:    745 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=43077, ups=11.07, wpb=3891, bsz=205.8, num_updates=308300, lr=5.69526e-05, gnorm=0.544, loss_scale=8, train_wall=9, gb_free=29.7, wall=6069
2024-01-31 11:54:06 | INFO | train_inner | epoch 229:    845 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=54596.7, ups=13.8, wpb=3957.5, bsz=220.3, num_updates=308400, lr=5.69433e-05, gnorm=0.484, loss_scale=8, train_wall=7, gb_free=29.7, wall=6076
2024-01-31 11:54:13 | INFO | train_inner | epoch 229:    945 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=53656.9, ups=13.6, wpb=3946.2, bsz=226.7, num_updates=308500, lr=5.69341e-05, gnorm=0.578, loss_scale=8, train_wall=7, gb_free=29.8, wall=6083
2024-01-31 11:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-01-31 11:54:21 | INFO | train_inner | epoch 229:   1046 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=51002.8, ups=13.17, wpb=3872.8, bsz=214.6, num_updates=308600, lr=5.69249e-05, gnorm=0.551, loss_scale=4, train_wall=7, gb_free=29.7, wall=6091
2024-01-31 11:54:29 | INFO | train_inner | epoch 229:   1146 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=50069.6, ups=12.78, wpb=3918.8, bsz=217.8, num_updates=308700, lr=5.69156e-05, gnorm=0.523, loss_scale=4, train_wall=8, gb_free=29.7, wall=6099
2024-01-31 11:54:37 | INFO | train_inner | epoch 229:   1246 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=47450, ups=12.15, wpb=3904.6, bsz=217.4, num_updates=308800, lr=5.69064e-05, gnorm=0.51, loss_scale=4, train_wall=8, gb_free=29.9, wall=6107
2024-01-31 11:54:44 | INFO | train_inner | epoch 229:   1346 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=56084.1, ups=14.23, wpb=3942, bsz=215.3, num_updates=308900, lr=5.68972e-05, gnorm=0.483, loss_scale=4, train_wall=7, gb_free=29.8, wall=6114
2024-01-31 11:54:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:54:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:54:47 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 2.216 | nll_loss 0.472 | ppl 1.39 | wps 116095 | wpb 3275.2 | bsz 176.5 | num_updates 308903 | best_loss 2.063
2024-01-31 11:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 308903 updates
2024-01-31 11:54:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint229.pt
2024-01-31 11:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint229.pt
2024-01-31 11:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint229.pt (epoch 229 @ 308903 updates, score 2.216) (writing took 2.3278303190600127 seconds)
2024-01-31 11:54:49 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2024-01-31 11:54:49 | INFO | train | epoch 229 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 47321.3 | ups 12.07 | wpb 3921.2 | bsz 214.5 | num_updates 308903 | lr 5.68969e-05 | gnorm 0.526 | loss_scale 4 | train_wall 103 | gb_free 29.8 | wall 6119
2024-01-31 11:54:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:54:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:54:49 | INFO | fairseq.trainer | begin training epoch 230
2024-01-31 11:54:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:54:56 | INFO | train_inner | epoch 230:     97 / 1349 loss=1.772, nll_loss=0.176, ppl=1.13, wps=32037.7, ups=8.16, wpb=3927.7, bsz=208.5, num_updates=309000, lr=5.6888e-05, gnorm=0.518, loss_scale=4, train_wall=7, gb_free=29.7, wall=6126
2024-01-31 11:55:03 | INFO | train_inner | epoch 230:    197 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=55667.1, ups=14.13, wpb=3939.7, bsz=209.9, num_updates=309100, lr=5.68788e-05, gnorm=0.634, loss_scale=4, train_wall=7, gb_free=29.8, wall=6133
2024-01-31 11:55:10 | INFO | train_inner | epoch 230:    297 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=55378.9, ups=14.13, wpb=3920.1, bsz=202.5, num_updates=309200, lr=5.68696e-05, gnorm=0.583, loss_scale=4, train_wall=7, gb_free=29.8, wall=6140
2024-01-31 11:55:17 | INFO | train_inner | epoch 230:    397 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=55113, ups=13.86, wpb=3975.1, bsz=219, num_updates=309300, lr=5.68604e-05, gnorm=0.558, loss_scale=4, train_wall=7, gb_free=29.9, wall=6148
2024-01-31 11:55:26 | INFO | train_inner | epoch 230:    497 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=47225.5, ups=12.21, wpb=3869, bsz=220.9, num_updates=309400, lr=5.68512e-05, gnorm=0.616, loss_scale=4, train_wall=8, gb_free=29.8, wall=6156
2024-01-31 11:55:34 | INFO | train_inner | epoch 230:    597 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=44605.3, ups=11.4, wpb=3911.3, bsz=215.7, num_updates=309500, lr=5.6842e-05, gnorm=0.502, loss_scale=4, train_wall=8, gb_free=29.9, wall=6165
2024-01-31 11:55:42 | INFO | train_inner | epoch 230:    697 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=51122.7, ups=12.99, wpb=3935.1, bsz=212.2, num_updates=309600, lr=5.68329e-05, gnorm=0.507, loss_scale=4, train_wall=7, gb_free=29.8, wall=6172
2024-01-31 11:55:50 | INFO | train_inner | epoch 230:    797 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=49887.8, ups=12.84, wpb=3886.5, bsz=224.3, num_updates=309700, lr=5.68237e-05, gnorm=0.498, loss_scale=4, train_wall=8, gb_free=29.8, wall=6180
2024-01-31 11:55:57 | INFO | train_inner | epoch 230:    897 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54570.2, ups=14.07, wpb=3878.5, bsz=214.5, num_updates=309800, lr=5.68145e-05, gnorm=0.553, loss_scale=4, train_wall=7, gb_free=29.7, wall=6187
2024-01-31 11:56:04 | INFO | train_inner | epoch 230:    997 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=53916.8, ups=13.56, wpb=3975.8, bsz=224.6, num_updates=309900, lr=5.68053e-05, gnorm=0.49, loss_scale=4, train_wall=7, gb_free=29.8, wall=6195
2024-01-31 11:56:12 | INFO | train_inner | epoch 230:   1097 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=51540.4, ups=13.03, wpb=3956.7, bsz=219.3, num_updates=310000, lr=5.67962e-05, gnorm=0.572, loss_scale=4, train_wall=7, gb_free=29.8, wall=6202
2024-01-31 11:56:20 | INFO | train_inner | epoch 230:   1197 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=49685, ups=12.8, wpb=3882.5, bsz=210, num_updates=310100, lr=5.6787e-05, gnorm=0.602, loss_scale=4, train_wall=8, gb_free=29.7, wall=6210
2024-01-31 11:56:27 | INFO | train_inner | epoch 230:   1297 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=52860.6, ups=13.49, wpb=3919.1, bsz=209.4, num_updates=310200, lr=5.67779e-05, gnorm=0.52, loss_scale=4, train_wall=7, gb_free=29.8, wall=6218
2024-01-31 11:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:56:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:56:34 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 2.216 | nll_loss 0.472 | ppl 1.39 | wps 104314 | wpb 3275.2 | bsz 176.5 | num_updates 310252 | best_loss 2.063
2024-01-31 11:56:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 310252 updates
2024-01-31 11:56:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint230.pt
2024-01-31 11:56:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint230.pt
2024-01-31 11:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint230.pt (epoch 230 @ 310252 updates, score 2.216) (writing took 2.3879301169654354 seconds)
2024-01-31 11:56:36 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2024-01-31 11:56:36 | INFO | train | epoch 230 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 49200.4 | ups 12.55 | wpb 3921.2 | bsz 214.5 | num_updates 310252 | lr 5.67731e-05 | gnorm 0.55 | loss_scale 4 | train_wall 98 | gb_free 29.9 | wall 6227
2024-01-31 11:56:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:56:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:56:37 | INFO | fairseq.trainer | begin training epoch 231
2024-01-31 11:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:56:40 | INFO | train_inner | epoch 231:     48 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=29497.6, ups=7.59, wpb=3884.4, bsz=209.4, num_updates=310300, lr=5.67687e-05, gnorm=0.521, loss_scale=4, train_wall=7, gb_free=29.8, wall=6231
2024-01-31 11:56:49 | INFO | train_inner | epoch 231:    148 / 1349 loss=1.774, nll_loss=0.178, ppl=1.13, wps=47848.9, ups=12.31, wpb=3887.7, bsz=213.4, num_updates=310400, lr=5.67596e-05, gnorm=0.535, loss_scale=4, train_wall=8, gb_free=29.7, wall=6239
2024-01-31 11:56:56 | INFO | train_inner | epoch 231:    248 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=50199.7, ups=12.73, wpb=3943.1, bsz=199.6, num_updates=310500, lr=5.67504e-05, gnorm=0.551, loss_scale=4, train_wall=8, gb_free=29.7, wall=6247
2024-01-31 11:57:04 | INFO | train_inner | epoch 231:    348 / 1349 loss=1.775, nll_loss=0.179, ppl=1.13, wps=54340.1, ups=13.92, wpb=3902.5, bsz=202.6, num_updates=310600, lr=5.67413e-05, gnorm=0.566, loss_scale=4, train_wall=7, gb_free=29.8, wall=6254
2024-01-31 11:57:11 | INFO | train_inner | epoch 231:    448 / 1349 loss=1.772, nll_loss=0.176, ppl=1.13, wps=53210.5, ups=13.43, wpb=3963.3, bsz=205.1, num_updates=310700, lr=5.67322e-05, gnorm=0.518, loss_scale=4, train_wall=7, gb_free=29.8, wall=6261
2024-01-31 11:57:19 | INFO | train_inner | epoch 231:    548 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=49014.9, ups=12.41, wpb=3949, bsz=222.7, num_updates=310800, lr=5.6723e-05, gnorm=0.477, loss_scale=4, train_wall=8, gb_free=29.7, wall=6269
2024-01-31 11:57:27 | INFO | train_inner | epoch 231:    648 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=50096.9, ups=12.87, wpb=3893.7, bsz=213.4, num_updates=310900, lr=5.67139e-05, gnorm=0.631, loss_scale=4, train_wall=7, gb_free=29.8, wall=6277
2024-01-31 11:57:35 | INFO | train_inner | epoch 231:    748 / 1349 loss=1.772, nll_loss=0.176, ppl=1.13, wps=47873, ups=12.19, wpb=3926.3, bsz=214.8, num_updates=311000, lr=5.67048e-05, gnorm=0.517, loss_scale=4, train_wall=8, gb_free=29.8, wall=6285
2024-01-31 11:57:43 | INFO | train_inner | epoch 231:    848 / 1349 loss=1.773, nll_loss=0.178, ppl=1.13, wps=52150.1, ups=13.19, wpb=3954.7, bsz=234.1, num_updates=311100, lr=5.66957e-05, gnorm=0.489, loss_scale=4, train_wall=7, gb_free=29.7, wall=6293
2024-01-31 11:57:51 | INFO | train_inner | epoch 231:    948 / 1349 loss=1.775, nll_loss=0.18, ppl=1.13, wps=49327, ups=12.56, wpb=3927.7, bsz=229, num_updates=311200, lr=5.66866e-05, gnorm=0.5, loss_scale=4, train_wall=8, gb_free=29.7, wall=6301
2024-01-31 11:57:59 | INFO | train_inner | epoch 231:   1048 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=48500.6, ups=12.34, wpb=3929.2, bsz=214.9, num_updates=311300, lr=5.66775e-05, gnorm=0.548, loss_scale=4, train_wall=8, gb_free=29.8, wall=6309
2024-01-31 11:58:07 | INFO | train_inner | epoch 231:   1148 / 1349 loss=1.776, nll_loss=0.181, ppl=1.13, wps=48507.7, ups=12.42, wpb=3906.4, bsz=220, num_updates=311400, lr=5.66684e-05, gnorm=0.54, loss_scale=4, train_wall=8, gb_free=29.8, wall=6317
2024-01-31 11:58:15 | INFO | train_inner | epoch 231:   1248 / 1349 loss=1.777, nll_loss=0.182, ppl=1.13, wps=45945.3, ups=11.77, wpb=3904, bsz=213.5, num_updates=311500, lr=5.66593e-05, gnorm=0.565, loss_scale=4, train_wall=8, gb_free=29.8, wall=6326
2024-01-31 11:58:22 | INFO | train_inner | epoch 231:   1348 / 1349 loss=1.776, nll_loss=0.18, ppl=1.13, wps=57207.9, ups=14.58, wpb=3923.7, bsz=208.5, num_updates=311600, lr=5.66502e-05, gnorm=0.503, loss_scale=4, train_wall=7, gb_free=29.7, wall=6332
2024-01-31 11:58:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:58:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:58:25 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 2.214 | nll_loss 0.469 | ppl 1.38 | wps 106794 | wpb 3275.2 | bsz 176.5 | num_updates 311601 | best_loss 2.063
2024-01-31 11:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 311601 updates
2024-01-31 11:58:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint231.pt
2024-01-31 11:58:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint231.pt
2024-01-31 11:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint231.pt (epoch 231 @ 311601 updates, score 2.214) (writing took 2.3912124990019947 seconds)
2024-01-31 11:58:27 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2024-01-31 11:58:27 | INFO | train | epoch 231 | loss 1.775 | nll_loss 0.179 | ppl 1.13 | wps 47637.2 | ups 12.15 | wpb 3921.2 | bsz 214.5 | num_updates 311601 | lr 5.66501e-05 | gnorm 0.533 | loss_scale 4 | train_wall 102 | gb_free 29.8 | wall 6338
2024-01-31 11:58:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:58:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-31 11:58:28 | INFO | fairseq.trainer | begin training epoch 232
2024-01-31 11:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 11:58:37 | INFO | train_inner | epoch 232:     99 / 1349 loss=1.773, nll_loss=0.177, ppl=1.13, wps=26948.3, ups=6.87, wpb=3924.6, bsz=217.8, num_updates=311700, lr=5.66411e-05, gnorm=0.558, loss_scale=4, train_wall=9, gb_free=29.8, wall=6347
2024-01-31 11:58:46 | INFO | train_inner | epoch 232:    199 / 1349 loss=1.774, nll_loss=0.179, ppl=1.13, wps=40871.2, ups=10.37, wpb=3942.5, bsz=226.2, num_updates=311800, lr=5.6632e-05, gnorm=0.449, loss_scale=4, train_wall=9, gb_free=29.8, wall=6357
2024-01-31 11:58:53 | INFO | fairseq_cli.train | Stopping training due to cumulative_training_time: 6.000020810431904 > stop_time_hours: 6.0 hour(s)
2024-01-31 11:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 11:58:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 11:58:55 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 2.216 | nll_loss 0.472 | ppl 1.39 | wps 103298 | wpb 3275.2 | bsz 176.5 | num_updates 311886 | best_loss 2.063
2024-01-31 11:58:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 311886 updates
2024-01-31 11:58:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt
2024-01-31 11:58:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt
2024-01-31 11:58:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt (epoch 232 @ 311886 updates, score 2.216) (writing took 1.0405027590459213 seconds)
2024-01-31 11:58:57 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2024-01-31 11:58:57 | INFO | train | epoch 232 | loss 1.774 | nll_loss 0.178 | ppl 1.13 | wps 38697.2 | ups 9.81 | wpb 3944.4 | bsz 220.5 | num_updates 311886 | lr 5.66242e-05 | gnorm 0.506 | loss_scale 4 | train_wall 24 | gb_free 29.8 | wall 6367
2024-01-31 11:58:57 | INFO | fairseq_cli.train | done training in 6359.4 seconds
Training complete.
Training complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 4504 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.en: 15529 sents, 225049 tokens, 0.00355% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.de: 15529 sents, 256398 tokens, 0.0215% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
averaging checkpoints:  ['/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint231.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint230.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint229.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint228.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint227.pt']
Finished writing averaged checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/train_punctuation_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 4504 types
INFO:fairseq.tasks.translation:[de] dictionary: 6520 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
INFO:fairseq.data.data_utils:loaded 15,529 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/test.en-de.en
INFO:fairseq.data.data_utils:loaded 15,529 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/test.en-de.de
INFO:fairseq.tasks.translation:/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset test en-de 15529 examples
INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True
INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True
INFO:fairseq.tasks.fairseq_task:rebuild_batches = False
INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1
INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2
INFO:fairseq_cli.generate:Translated 15,528 sentences (253,144 tokens) in 34.4s (451.84 sentences/s, 7366.12 tokens/s)
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/train_punctuation_covost/hyp.txt and /home/kit/stud/uxude/predictions/train_punctuation_covost/ref.txt
Sample predictions:
Sample: What became of him?
Reference: What became of him?
Sample: Do you know him?
Reference: Do you know him?
WER:
Generate test with beam=4: BLEU4 = 84.97, 92.3/87.4/82.8/78.1 (BP=1.000, ratio=1.000, syslen=141244, reflen=141268)
BLEU:
{
 "name": "BLEU",
 "score": 85.0,
 "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:none|version:2.4.0",
 "verbose_score": "92.3/87.4/82.8/78.1 (BP = 1.000 ratio = 1.000 hyp_len = 141244 ref_len = 141268)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "none",
 "smooth": "none",
 "version": "2.4.0"
}

============================= JOB FEEDBACK =============================
