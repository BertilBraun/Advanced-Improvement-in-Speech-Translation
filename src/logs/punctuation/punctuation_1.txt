(base) [uxude@uc2n994 train]$ cat train_punctuation_covost_23092674.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[2024-01-30 20:04:27] [INFO] [Dataset::Prepare Datasets]: Skipping dataset preparation, config file and MT spm data already exists
Training the Punctuation model...
rm: cannot remove '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/preprocess.log': No such file or directory
Binarizing the data...
Binarized data will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Training data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train
Validation data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train', validpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev', testpref=None, align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 4504 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train.en: 289365 sents, 4750058 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[en] Dictionary: 4504 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev.en: 15530 sents, 258128 tokens, 0.00232% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train.de: 289365 sents, 5289915 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev.de: 15530 sents, 288297 tokens, 0.00902% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Binarization complete.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models
Training time: 6 hours
2024-01-30 20:06:24 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=50000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=1, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-01-30 20:06:24 | INFO | fairseq.tasks.translation | [en] dictionary: 4504 types
2024-01-30 20:06:24 | INFO | fairseq.tasks.translation | [de] dictionary: 6520 types
2024-01-30 20:06:26 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4504, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6520, bias=False)
  )
)
2024-01-30 20:06:26 | INFO | fairseq_cli.train | task: TranslationTask
2024-01-30 20:06:26 | INFO | fairseq_cli.train | model: TransformerModel
2024-01-30 20:06:26 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-30 20:06:26 | INFO | fairseq_cli.train | num. shared model params: 49,782,784 (num. trained: 49,782,784)
2024-01-30 20:06:26 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-30 20:06:26 | INFO | fairseq.data.data_utils | loaded 15,530 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/valid.en-de.en
2024-01-30 20:06:26 | INFO | fairseq.data.data_utils | loaded 15,530 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/valid.en-de.de
2024-01-30 20:06:26 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset valid en-de 15530 examples
2024-01-30 20:06:27 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-30 20:06:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-30 20:06:27 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-01-30 20:06:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-30 20:06:27 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-30 20:06:27 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-01-30 20:06:27 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt
2024-01-30 20:06:27 | INFO | fairseq.trainer | No existing checkpoint found /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_last.pt
2024-01-30 20:06:27 | INFO | fairseq.trainer | loading train data for epoch 1
2024-01-30 20:06:27 | INFO | fairseq.data.data_utils | loaded 289,365 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/train.en-de.en
2024-01-30 20:06:27 | INFO | fairseq.data.data_utils | loaded 289,365 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/train.en-de.de
2024-01-30 20:06:27 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset train en-de 289365 examples
2024-01-30 20:06:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:06:27 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-30 20:06:27 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-30 20:06:27 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-30 20:06:30 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-30 20:06:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:06:30 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-30 20:06:30 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-30 20:06:30 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-30 20:06:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:06:32 | INFO | fairseq.trainer | begin training epoch 1
2024-01-30 20:06:32 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-01-30 20:06:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2024-01-30 20:06:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-01-30 20:06:46 | INFO | train_inner | epoch 001:    102 / 1349 loss=12.045, nll_loss=11.887, ppl=3788.55, wps=60561.9, ups=15.47, wpb=3916.3, bsz=197.7, num_updates=100, lr=1.25e-05, gnorm=4.255, loss_scale=32, train_wall=14, gb_free=29.8, wall=19
2024-01-30 20:06:52 | INFO | train_inner | epoch 001:    202 / 1349 loss=10.698, nll_loss=10.38, ppl=1332.74, wps=64340.2, ups=16.33, wpb=3940.8, bsz=220.9, num_updates=200, lr=2.5e-05, gnorm=2.203, loss_scale=32, train_wall=6, gb_free=29.8, wall=25
2024-01-30 20:06:58 | INFO | train_inner | epoch 001:    302 / 1349 loss=9.818, nll_loss=9.37, ppl=661.72, wps=63676.9, ups=16.48, wpb=3863.3, bsz=219.8, num_updates=300, lr=3.75e-05, gnorm=2.672, loss_scale=32, train_wall=6, gb_free=29.8, wall=31
2024-01-30 20:07:04 | INFO | train_inner | epoch 001:    402 / 1349 loss=9.368, nll_loss=8.827, ppl=454.21, wps=64572.5, ups=16.57, wpb=3896.1, bsz=201.9, num_updates=400, lr=5e-05, gnorm=2.35, loss_scale=32, train_wall=6, gb_free=29.8, wall=37
2024-01-30 20:07:10 | INFO | train_inner | epoch 001:    502 / 1349 loss=9.141, nll_loss=8.553, ppl=375.49, wps=63496.5, ups=16.48, wpb=3852.6, bsz=220.5, num_updates=500, lr=6.25e-05, gnorm=2.176, loss_scale=32, train_wall=6, gb_free=29.8, wall=44
2024-01-30 20:07:16 | INFO | train_inner | epoch 001:    602 / 1349 loss=8.989, nll_loss=8.374, ppl=331.67, wps=64548.1, ups=16.43, wpb=3929.3, bsz=211.8, num_updates=600, lr=7.5e-05, gnorm=2.126, loss_scale=32, train_wall=6, gb_free=29.7, wall=50
2024-01-30 20:07:23 | INFO | train_inner | epoch 001:    702 / 1349 loss=8.768, nll_loss=8.122, ppl=278.52, wps=64771.6, ups=16.31, wpb=3971.9, bsz=227.9, num_updates=700, lr=8.75e-05, gnorm=2.079, loss_scale=32, train_wall=6, gb_free=29.8, wall=56
2024-01-30 20:07:29 | INFO | train_inner | epoch 001:    802 / 1349 loss=8.534, nll_loss=7.851, ppl=230.95, wps=65102.6, ups=16.47, wpb=3952, bsz=223.4, num_updates=800, lr=0.0001, gnorm=2.525, loss_scale=32, train_wall=6, gb_free=29.7, wall=62
2024-01-30 20:07:35 | INFO | train_inner | epoch 001:    902 / 1349 loss=8.221, nll_loss=7.499, ppl=180.88, wps=64419.9, ups=16.37, wpb=3936.2, bsz=215.8, num_updates=900, lr=0.0001125, gnorm=2.059, loss_scale=32, train_wall=6, gb_free=29.8, wall=68
2024-01-30 20:07:41 | INFO | train_inner | epoch 001:   1002 / 1349 loss=7.904, nll_loss=7.134, ppl=140.49, wps=65021.9, ups=16.45, wpb=3951.7, bsz=223, num_updates=1000, lr=0.000125, gnorm=2.614, loss_scale=32, train_wall=6, gb_free=29.8, wall=74
2024-01-30 20:07:47 | INFO | train_inner | epoch 001:   1102 / 1349 loss=7.478, nll_loss=6.653, ppl=100.63, wps=64615.8, ups=16.43, wpb=3932.3, bsz=210.6, num_updates=1100, lr=0.0001375, gnorm=2.408, loss_scale=32, train_wall=6, gb_free=29.7, wall=80
2024-01-30 20:07:53 | INFO | train_inner | epoch 001:   1202 / 1349 loss=7.137, nll_loss=6.26, ppl=76.62, wps=64580.5, ups=16.39, wpb=3941, bsz=219.8, num_updates=1200, lr=0.00015, gnorm=2.893, loss_scale=32, train_wall=6, gb_free=29.7, wall=86
2024-01-30 20:07:59 | INFO | train_inner | epoch 001:   1302 / 1349 loss=6.754, nll_loss=5.823, ppl=56.61, wps=64290.9, ups=16.53, wpb=3888.3, bsz=203.8, num_updates=1300, lr=0.0001625, gnorm=2.763, loss_scale=32, train_wall=6, gb_free=29.8, wall=92
2024-01-30 20:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:08:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:08:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.695 | nll_loss 4.481 | ppl 22.33 | wps 168325 | wpb 3275.2 | bsz 176.5 | num_updates 1347
2024-01-30 20:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1347 updates
2024-01-30 20:08:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint1.pt
2024-01-30 20:08:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint1.pt
2024-01-30 20:08:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint1.pt (epoch 1 @ 1347 updates, score 5.695) (writing took 5.000004268949851 seconds)
2024-01-30 20:08:09 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-01-30 20:08:09 | INFO | train | epoch 001 | loss 8.753 | nll_loss 8.117 | ppl 277.54 | wps 59332 | ups 15.13 | wpb 3921.1 | bsz 214.6 | num_updates 1347 | lr 0.000168375 | gnorm 2.568 | loss_scale 32 | train_wall 87 | gb_free 29.8 | wall 102
2024-01-30 20:08:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:08:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:08:09 | INFO | fairseq.trainer | begin training epoch 2
2024-01-30 20:08:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:08:12 | INFO | train_inner | epoch 002:     53 / 1349 loss=6.392, nll_loss=5.406, ppl=42.4, wps=30469, ups=7.77, wpb=3923.8, bsz=209.8, num_updates=1400, lr=0.000175, gnorm=3.157, loss_scale=32, train_wall=6, gb_free=29.8, wall=105
2024-01-30 20:08:18 | INFO | train_inner | epoch 002:    153 / 1349 loss=5.83, nll_loss=4.761, ppl=27.12, wps=64182.3, ups=16.4, wpb=3912.6, bsz=222.1, num_updates=1500, lr=0.0001875, gnorm=3.118, loss_scale=32, train_wall=6, gb_free=29.8, wall=111
2024-01-30 20:08:24 | INFO | train_inner | epoch 002:    253 / 1349 loss=5.32, nll_loss=4.173, ppl=18.04, wps=65043.2, ups=16.44, wpb=3955.7, bsz=221.9, num_updates=1600, lr=0.0002, gnorm=3.053, loss_scale=32, train_wall=6, gb_free=29.8, wall=117
2024-01-30 20:08:30 | INFO | train_inner | epoch 002:    353 / 1349 loss=5.063, nll_loss=3.869, ppl=14.62, wps=64891, ups=16.42, wpb=3952.2, bsz=217.1, num_updates=1700, lr=0.0002125, gnorm=3.414, loss_scale=32, train_wall=6, gb_free=29.8, wall=123
2024-01-30 20:08:36 | INFO | train_inner | epoch 002:    453 / 1349 loss=4.803, nll_loss=3.564, ppl=11.82, wps=63341.6, ups=16.46, wpb=3848.8, bsz=210.2, num_updates=1800, lr=0.000225, gnorm=3.192, loss_scale=32, train_wall=6, gb_free=29.8, wall=130
2024-01-30 20:08:42 | INFO | train_inner | epoch 002:    553 / 1349 loss=4.495, nll_loss=3.203, ppl=9.21, wps=64483.8, ups=16.27, wpb=3963.1, bsz=219.5, num_updates=1900, lr=0.0002375, gnorm=3.124, loss_scale=32, train_wall=6, gb_free=29.8, wall=136
2024-01-30 20:08:48 | INFO | train_inner | epoch 002:    653 / 1349 loss=4.297, nll_loss=2.964, ppl=7.8, wps=63234.4, ups=16.59, wpb=3811.2, bsz=206.7, num_updates=2000, lr=0.00025, gnorm=3.067, loss_scale=32, train_wall=6, gb_free=29.8, wall=142
2024-01-30 20:08:55 | INFO | train_inner | epoch 002:    753 / 1349 loss=4.046, nll_loss=2.667, ppl=6.35, wps=65086.8, ups=16.5, wpb=3944.6, bsz=196.6, num_updates=2100, lr=0.0002625, gnorm=3.025, loss_scale=32, train_wall=6, gb_free=29.8, wall=148
2024-01-30 20:09:01 | INFO | train_inner | epoch 002:    853 / 1349 loss=3.752, nll_loss=2.321, ppl=5, wps=64823.5, ups=16.48, wpb=3934.5, bsz=212, num_updates=2200, lr=0.000275, gnorm=2.686, loss_scale=32, train_wall=6, gb_free=29.8, wall=154
2024-01-30 20:09:07 | INFO | train_inner | epoch 002:    953 / 1349 loss=3.627, nll_loss=2.171, ppl=4.5, wps=64610.5, ups=16.46, wpb=3926.5, bsz=218.2, num_updates=2300, lr=0.0002875, gnorm=2.608, loss_scale=32, train_wall=6, gb_free=29.8, wall=160
2024-01-30 20:09:13 | INFO | train_inner | epoch 002:   1053 / 1349 loss=3.418, nll_loss=1.923, ppl=3.79, wps=64522.4, ups=16.36, wpb=3943.8, bsz=224.3, num_updates=2400, lr=0.0003, gnorm=2.312, loss_scale=32, train_wall=6, gb_free=29.7, wall=166
2024-01-30 20:09:19 | INFO | train_inner | epoch 002:   1153 / 1349 loss=3.395, nll_loss=1.892, ppl=3.71, wps=64395.6, ups=16.44, wpb=3917.2, bsz=209.7, num_updates=2500, lr=0.0003125, gnorm=2.22, loss_scale=32, train_wall=6, gb_free=29.7, wall=172
2024-01-30 20:09:25 | INFO | train_inner | epoch 002:   1253 / 1349 loss=3.299, nll_loss=1.779, ppl=3.43, wps=65124.3, ups=16.51, wpb=3944.9, bsz=220.3, num_updates=2600, lr=0.000325, gnorm=2.014, loss_scale=32, train_wall=6, gb_free=29.9, wall=178
2024-01-30 20:09:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:09:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:09:32 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 2.977 | nll_loss 1.083 | ppl 2.12 | wps 167555 | wpb 3275.2 | bsz 176.5 | num_updates 2696 | best_loss 2.977
2024-01-30 20:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2696 updates
2024-01-30 20:09:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint2.pt
2024-01-30 20:09:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint2.pt
2024-01-30 20:09:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint2.pt (epoch 2 @ 2696 updates, score 2.977) (writing took 4.842628309037536 seconds)
2024-01-30 20:09:37 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-01-30 20:09:37 | INFO | train | epoch 002 | loss 4.274 | nll_loss 2.934 | ppl 7.64 | wps 59590.2 | ups 15.2 | wpb 3921.2 | bsz 214.5 | num_updates 2696 | lr 0.000337 | gnorm 2.769 | loss_scale 32 | train_wall 79 | gb_free 29.7 | wall 191
2024-01-30 20:09:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:09:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:09:37 | INFO | fairseq.trainer | begin training epoch 3
2024-01-30 20:09:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:09:38 | INFO | train_inner | epoch 003:      4 / 1349 loss=3.12, nll_loss=1.572, ppl=2.97, wps=30697.5, ups=7.83, wpb=3918.8, bsz=207.7, num_updates=2700, lr=0.0003375, gnorm=1.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=191
2024-01-30 20:09:44 | INFO | train_inner | epoch 003:    104 / 1349 loss=3.181, nll_loss=1.64, ppl=3.12, wps=64801.6, ups=16.55, wpb=3914.4, bsz=213, num_updates=2800, lr=0.00035, gnorm=1.953, loss_scale=32, train_wall=6, gb_free=29.8, wall=197
2024-01-30 20:09:50 | INFO | train_inner | epoch 003:    204 / 1349 loss=2.949, nll_loss=1.374, ppl=2.59, wps=64384.2, ups=16.45, wpb=3913.2, bsz=226.4, num_updates=2900, lr=0.0003625, gnorm=1.656, loss_scale=32, train_wall=6, gb_free=29.8, wall=203
2024-01-30 20:09:56 | INFO | train_inner | epoch 003:    304 / 1349 loss=3.022, nll_loss=1.459, ppl=2.75, wps=64998.9, ups=16.55, wpb=3928.6, bsz=206.2, num_updates=3000, lr=0.000375, gnorm=1.738, loss_scale=32, train_wall=6, gb_free=29.7, wall=209
2024-01-30 20:10:02 | INFO | train_inner | epoch 003:    404 / 1349 loss=2.897, nll_loss=1.317, ppl=2.49, wps=64547.3, ups=16.48, wpb=3916.1, bsz=216.8, num_updates=3100, lr=0.0003875, gnorm=1.612, loss_scale=32, train_wall=6, gb_free=29.8, wall=215
2024-01-30 20:10:08 | INFO | train_inner | epoch 003:    504 / 1349 loss=2.854, nll_loss=1.269, ppl=2.41, wps=64821.8, ups=16.48, wpb=3933, bsz=222.2, num_updates=3200, lr=0.0004, gnorm=1.572, loss_scale=32, train_wall=6, gb_free=29.8, wall=221
2024-01-30 20:10:14 | INFO | train_inner | epoch 003:    604 / 1349 loss=2.792, nll_loss=1.199, ppl=2.3, wps=64557.3, ups=16.58, wpb=3894.4, bsz=208.6, num_updates=3300, lr=0.0004125, gnorm=1.454, loss_scale=32, train_wall=6, gb_free=29.7, wall=227
2024-01-30 20:10:20 | INFO | train_inner | epoch 003:    704 / 1349 loss=2.907, nll_loss=1.332, ppl=2.52, wps=64923.8, ups=16.63, wpb=3903.3, bsz=197.7, num_updates=3400, lr=0.000425, gnorm=1.696, loss_scale=32, train_wall=6, gb_free=29.8, wall=233
2024-01-30 20:10:26 | INFO | train_inner | epoch 003:    804 / 1349 loss=2.782, nll_loss=1.186, ppl=2.27, wps=65006.9, ups=16.66, wpb=3903, bsz=212, num_updates=3500, lr=0.0004375, gnorm=1.472, loss_scale=32, train_wall=6, gb_free=29.8, wall=239
2024-01-30 20:10:32 | INFO | train_inner | epoch 003:    904 / 1349 loss=2.811, nll_loss=1.224, ppl=2.34, wps=64825.8, ups=16.49, wpb=3932.1, bsz=219.4, num_updates=3600, lr=0.00045, gnorm=1.619, loss_scale=32, train_wall=6, gb_free=29.8, wall=245
2024-01-30 20:10:38 | INFO | train_inner | epoch 003:   1004 / 1349 loss=2.66, nll_loss=1.052, ppl=2.07, wps=65198.7, ups=16.44, wpb=3965.9, bsz=228.4, num_updates=3700, lr=0.0004625, gnorm=1.322, loss_scale=32, train_wall=6, gb_free=29.8, wall=251
2024-01-30 20:10:44 | INFO | train_inner | epoch 003:   1104 / 1349 loss=2.735, nll_loss=1.142, ppl=2.21, wps=64762.1, ups=16.42, wpb=3943.3, bsz=224.6, num_updates=3800, lr=0.000475, gnorm=1.563, loss_scale=32, train_wall=6, gb_free=29.7, wall=257
2024-01-30 20:10:50 | INFO | train_inner | epoch 003:   1204 / 1349 loss=2.665, nll_loss=1.062, ppl=2.09, wps=65257.1, ups=16.54, wpb=3945.1, bsz=206.5, num_updates=3900, lr=0.0004875, gnorm=1.372, loss_scale=32, train_wall=6, gb_free=29.8, wall=264
2024-01-30 20:10:56 | INFO | train_inner | epoch 003:   1304 / 1349 loss=2.733, nll_loss=1.141, ppl=2.21, wps=64597, ups=16.66, wpb=3878.1, bsz=202.8, num_updates=4000, lr=0.0005, gnorm=1.604, loss_scale=32, train_wall=6, gb_free=29.8, wall=270
2024-01-30 20:10:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:10:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:11:01 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 2.791 | nll_loss 0.89 | ppl 1.85 | wps 169285 | wpb 3275.2 | bsz 176.5 | num_updates 4045 | best_loss 2.791
2024-01-30 20:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4045 updates
2024-01-30 20:11:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint3.pt
2024-01-30 20:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint3.pt
2024-01-30 20:11:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint3.pt (epoch 3 @ 4045 updates, score 2.791) (writing took 4.837657885975204 seconds)
2024-01-30 20:11:06 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-01-30 20:11:06 | INFO | train | epoch 003 | loss 2.836 | nll_loss 1.251 | ppl 2.38 | wps 59899.9 | ups 15.28 | wpb 3921.2 | bsz 214.5 | num_updates 4045 | lr 0.000497211 | gnorm 1.58 | loss_scale 32 | train_wall 79 | gb_free 29.8 | wall 279
2024-01-30 20:11:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:11:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:11:06 | INFO | fairseq.trainer | begin training epoch 4
2024-01-30 20:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:11:09 | INFO | train_inner | epoch 004:     55 / 1349 loss=2.661, nll_loss=1.059, ppl=2.08, wps=30523, ups=7.86, wpb=3881, bsz=208, num_updates=4100, lr=0.000493865, gnorm=1.468, loss_scale=32, train_wall=6, gb_free=29.8, wall=282
2024-01-30 20:11:15 | INFO | train_inner | epoch 004:    155 / 1349 loss=2.67, nll_loss=1.069, ppl=2.1, wps=65330.6, ups=16.55, wpb=3947.1, bsz=220.8, num_updates=4200, lr=0.00048795, gnorm=1.464, loss_scale=32, train_wall=6, gb_free=29.8, wall=288
2024-01-30 20:11:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 20:11:21 | INFO | train_inner | epoch 004:    256 / 1349 loss=2.556, nll_loss=0.943, ppl=1.92, wps=63993.9, ups=16.37, wpb=3908.3, bsz=209.5, num_updates=4300, lr=0.000482243, gnorm=1.389, loss_scale=16, train_wall=6, gb_free=29.8, wall=294
2024-01-30 20:11:27 | INFO | train_inner | epoch 004:    356 / 1349 loss=2.576, nll_loss=0.968, ppl=1.96, wps=65907.5, ups=16.62, wpb=3964.8, bsz=208.9, num_updates=4400, lr=0.000476731, gnorm=1.263, loss_scale=16, train_wall=6, gb_free=29.8, wall=300
2024-01-30 20:11:33 | INFO | train_inner | epoch 004:    456 / 1349 loss=2.497, nll_loss=0.883, ppl=1.84, wps=64686.5, ups=16.5, wpb=3921.6, bsz=206.4, num_updates=4500, lr=0.000471405, gnorm=1.289, loss_scale=16, train_wall=6, gb_free=29.8, wall=306
2024-01-30 20:11:39 | INFO | train_inner | epoch 004:    556 / 1349 loss=2.584, nll_loss=0.982, ppl=1.98, wps=64905.1, ups=16.58, wpb=3913.9, bsz=202.6, num_updates=4600, lr=0.000466252, gnorm=1.488, loss_scale=16, train_wall=6, gb_free=29.8, wall=313
2024-01-30 20:11:45 | INFO | train_inner | epoch 004:    656 / 1349 loss=2.508, nll_loss=0.898, ppl=1.86, wps=65783.7, ups=16.63, wpb=3955.9, bsz=212, num_updates=4700, lr=0.000461266, gnorm=1.204, loss_scale=16, train_wall=6, gb_free=29.9, wall=319
2024-01-30 20:11:51 | INFO | train_inner | epoch 004:    756 / 1349 loss=2.491, nll_loss=0.882, ppl=1.84, wps=64900.1, ups=16.64, wpb=3900.7, bsz=214.9, num_updates=4800, lr=0.000456435, gnorm=1.216, loss_scale=16, train_wall=6, gb_free=29.8, wall=325
2024-01-30 20:11:57 | INFO | train_inner | epoch 004:    856 / 1349 loss=2.454, nll_loss=0.841, ppl=1.79, wps=64691.2, ups=16.55, wpb=3908.9, bsz=223.4, num_updates=4900, lr=0.000451754, gnorm=1.195, loss_scale=16, train_wall=6, gb_free=29.9, wall=331
2024-01-30 20:12:03 | INFO | train_inner | epoch 004:    956 / 1349 loss=2.47, nll_loss=0.861, ppl=1.82, wps=64467.7, ups=16.55, wpb=3894.8, bsz=229.9, num_updates=5000, lr=0.000447214, gnorm=1.26, loss_scale=16, train_wall=6, gb_free=29.8, wall=337
2024-01-30 20:12:09 | INFO | train_inner | epoch 004:   1056 / 1349 loss=2.383, nll_loss=0.764, ppl=1.7, wps=64808.7, ups=16.56, wpb=3913.6, bsz=210.3, num_updates=5100, lr=0.000442807, gnorm=1.146, loss_scale=16, train_wall=6, gb_free=30, wall=343
2024-01-30 20:12:15 | INFO | train_inner | epoch 004:   1156 / 1349 loss=2.465, nll_loss=0.858, ppl=1.81, wps=64981.5, ups=16.67, wpb=3898.6, bsz=214.5, num_updates=5200, lr=0.000438529, gnorm=1.219, loss_scale=16, train_wall=6, gb_free=29.7, wall=349
2024-01-30 20:12:22 | INFO | train_inner | epoch 004:   1256 / 1349 loss=2.395, nll_loss=0.783, ppl=1.72, wps=64926.2, ups=16.5, wpb=3935, bsz=218.2, num_updates=5300, lr=0.000434372, gnorm=1.089, loss_scale=16, train_wall=6, gb_free=29.7, wall=355
2024-01-30 20:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:12:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:12:29 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 2.413 | nll_loss 0.541 | ppl 1.45 | wps 169285 | wpb 3275.2 | bsz 176.5 | num_updates 5393 | best_loss 2.413
2024-01-30 20:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5393 updates
2024-01-30 20:12:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint4.pt
2024-01-30 20:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint4.pt
2024-01-30 20:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint4.pt (epoch 4 @ 5393 updates, score 2.413) (writing took 5.009674201020971 seconds)
2024-01-30 20:12:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-01-30 20:12:34 | INFO | train | epoch 004 | loss 2.5 | nll_loss 0.89 | ppl 1.85 | wps 59875.7 | ups 15.27 | wpb 3921.1 | bsz 214.5 | num_updates 5393 | lr 0.000430611 | gnorm 1.264 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 367
2024-01-30 20:12:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:12:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:12:34 | INFO | fairseq.trainer | begin training epoch 5
2024-01-30 20:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:12:34 | INFO | train_inner | epoch 005:      7 / 1349 loss=2.32, nll_loss=0.7, ppl=1.62, wps=30545.3, ups=7.72, wpb=3958.7, bsz=231.7, num_updates=5400, lr=0.000430331, gnorm=1.074, loss_scale=16, train_wall=6, gb_free=29.8, wall=368
2024-01-30 20:12:40 | INFO | train_inner | epoch 005:    107 / 1349 loss=2.375, nll_loss=0.763, ppl=1.7, wps=65481.7, ups=16.63, wpb=3937.2, bsz=210.3, num_updates=5500, lr=0.000426401, gnorm=1.081, loss_scale=16, train_wall=6, gb_free=29.8, wall=374
2024-01-30 20:12:47 | INFO | train_inner | epoch 005:    207 / 1349 loss=2.375, nll_loss=0.764, ppl=1.7, wps=64185.2, ups=16.59, wpb=3868.8, bsz=213.5, num_updates=5600, lr=0.000422577, gnorm=1.14, loss_scale=16, train_wall=6, gb_free=29.7, wall=380
2024-01-30 20:12:53 | INFO | train_inner | epoch 005:    307 / 1349 loss=2.37, nll_loss=0.759, ppl=1.69, wps=64713.6, ups=16.57, wpb=3904.6, bsz=204.5, num_updates=5700, lr=0.000418854, gnorm=1.096, loss_scale=16, train_wall=6, gb_free=29.7, wall=386
2024-01-30 20:12:59 | INFO | train_inner | epoch 005:    407 / 1349 loss=2.329, nll_loss=0.716, ppl=1.64, wps=64872.8, ups=16.7, wpb=3884.2, bsz=213, num_updates=5800, lr=0.000415227, gnorm=1.009, loss_scale=16, train_wall=6, gb_free=29.8, wall=392
2024-01-30 20:13:05 | INFO | train_inner | epoch 005:    507 / 1349 loss=2.288, nll_loss=0.67, ppl=1.59, wps=65118.4, ups=16.46, wpb=3955.6, bsz=220.2, num_updates=5900, lr=0.000411693, gnorm=1.081, loss_scale=16, train_wall=6, gb_free=29.8, wall=398
2024-01-30 20:13:11 | INFO | train_inner | epoch 005:    607 / 1349 loss=2.292, nll_loss=0.676, ppl=1.6, wps=64832.3, ups=16.55, wpb=3918.2, bsz=214.6, num_updates=6000, lr=0.000408248, gnorm=1.038, loss_scale=16, train_wall=6, gb_free=29.8, wall=404
2024-01-30 20:13:17 | INFO | train_inner | epoch 005:    707 / 1349 loss=2.38, nll_loss=0.778, ppl=1.71, wps=65196.4, ups=16.58, wpb=3932.5, bsz=221, num_updates=6100, lr=0.000404888, gnorm=1.209, loss_scale=16, train_wall=6, gb_free=30, wall=410
2024-01-30 20:13:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-30 20:13:23 | INFO | train_inner | epoch 005:    808 / 1349 loss=2.316, nll_loss=0.704, ppl=1.63, wps=64535.6, ups=16.48, wpb=3916.7, bsz=204.3, num_updates=6200, lr=0.00040161, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.8, wall=416
2024-01-30 20:13:29 | INFO | train_inner | epoch 005:    908 / 1349 loss=2.269, nll_loss=0.654, ppl=1.57, wps=64937.4, ups=16.53, wpb=3927.5, bsz=214.4, num_updates=6300, lr=0.00039841, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.8, wall=422
2024-01-30 20:13:35 | INFO | train_inner | epoch 005:   1008 / 1349 loss=2.288, nll_loss=0.677, ppl=1.6, wps=65181.4, ups=16.59, wpb=3927.9, bsz=222.3, num_updates=6400, lr=0.000395285, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.8, wall=428
2024-01-30 20:13:41 | INFO | train_inner | epoch 005:   1108 / 1349 loss=2.327, nll_loss=0.724, ppl=1.65, wps=65699.7, ups=16.62, wpb=3953.5, bsz=209.5, num_updates=6500, lr=0.000392232, gnorm=1.04, loss_scale=8, train_wall=6, gb_free=29.7, wall=434
2024-01-30 20:13:47 | INFO | train_inner | epoch 005:   1208 / 1349 loss=2.279, nll_loss=0.667, ppl=1.59, wps=65091.5, ups=16.64, wpb=3912.3, bsz=221, num_updates=6600, lr=0.000389249, gnorm=1.097, loss_scale=8, train_wall=6, gb_free=29.8, wall=440
2024-01-30 20:13:53 | INFO | train_inner | epoch 005:   1308 / 1349 loss=2.252, nll_loss=0.639, ppl=1.56, wps=65552.3, ups=16.65, wpb=3936.6, bsz=214.7, num_updates=6700, lr=0.000386334, gnorm=0.913, loss_scale=8, train_wall=6, gb_free=29.8, wall=446
2024-01-30 20:13:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:13:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:13:57 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.34 | nll_loss 0.477 | ppl 1.39 | wps 170080 | wpb 3275.2 | bsz 176.5 | num_updates 6741 | best_loss 2.34
2024-01-30 20:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6741 updates
2024-01-30 20:13:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint5.pt
2024-01-30 20:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint5.pt
2024-01-30 20:14:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint5.pt (epoch 5 @ 6741 updates, score 2.34) (writing took 4.920944090001285 seconds)
2024-01-30 20:14:02 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-01-30 20:14:02 | INFO | train | epoch 005 | loss 2.315 | nll_loss 0.704 | ppl 1.63 | wps 60018.2 | ups 15.31 | wpb 3921 | bsz 214.5 | num_updates 6741 | lr 0.000385157 | gnorm 1.049 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 455
2024-01-30 20:14:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:14:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:14:02 | INFO | fairseq.trainer | begin training epoch 6
2024-01-30 20:14:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:14:06 | INFO | train_inner | epoch 006:     59 / 1349 loss=2.249, nll_loss=0.639, ppl=1.56, wps=30589.2, ups=7.81, wpb=3917.3, bsz=219.6, num_updates=6800, lr=0.000383482, gnorm=0.935, loss_scale=8, train_wall=6, gb_free=29.9, wall=459
2024-01-30 20:14:12 | INFO | train_inner | epoch 006:    159 / 1349 loss=2.234, nll_loss=0.622, ppl=1.54, wps=65359.8, ups=16.54, wpb=3951.8, bsz=219.7, num_updates=6900, lr=0.000380693, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.8, wall=465
2024-01-30 20:14:18 | INFO | train_inner | epoch 006:    259 / 1349 loss=2.266, nll_loss=0.659, ppl=1.58, wps=65025.6, ups=16.73, wpb=3887.1, bsz=206.7, num_updates=7000, lr=0.000377964, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.8, wall=471
2024-01-30 20:14:24 | INFO | train_inner | epoch 006:    359 / 1349 loss=2.224, nll_loss=0.612, ppl=1.53, wps=64977.8, ups=16.63, wpb=3907.9, bsz=213, num_updates=7100, lr=0.000375293, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.8, wall=477
2024-01-30 20:14:30 | INFO | train_inner | epoch 006:    459 / 1349 loss=2.243, nll_loss=0.635, ppl=1.55, wps=65247.1, ups=16.57, wpb=3938.8, bsz=222.7, num_updates=7200, lr=0.000372678, gnorm=0.912, loss_scale=8, train_wall=6, gb_free=29.9, wall=483
2024-01-30 20:14:36 | INFO | train_inner | epoch 006:    559 / 1349 loss=2.218, nll_loss=0.607, ppl=1.52, wps=64595.3, ups=16.61, wpb=3888.8, bsz=219.8, num_updates=7300, lr=0.000370117, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=489
2024-01-30 20:14:42 | INFO | train_inner | epoch 006:    659 / 1349 loss=2.183, nll_loss=0.568, ppl=1.48, wps=65129.6, ups=16.55, wpb=3934.3, bsz=208.5, num_updates=7400, lr=0.000367607, gnorm=0.813, loss_scale=8, train_wall=6, gb_free=29.8, wall=495
2024-01-30 20:14:48 | INFO | train_inner | epoch 006:    759 / 1349 loss=2.176, nll_loss=0.562, ppl=1.48, wps=64951.5, ups=16.54, wpb=3928.1, bsz=207.4, num_updates=7500, lr=0.000365148, gnorm=0.846, loss_scale=8, train_wall=6, gb_free=29.8, wall=501
2024-01-30 20:14:54 | INFO | train_inner | epoch 006:    859 / 1349 loss=2.217, nll_loss=0.609, ppl=1.52, wps=65630.3, ups=16.71, wpb=3927.4, bsz=205, num_updates=7600, lr=0.000362738, gnorm=0.877, loss_scale=8, train_wall=6, gb_free=29.7, wall=507
2024-01-30 20:15:00 | INFO | train_inner | epoch 006:    959 / 1349 loss=2.23, nll_loss=0.625, ppl=1.54, wps=64673.2, ups=16.74, wpb=3862.9, bsz=211.4, num_updates=7700, lr=0.000360375, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.7, wall=513
2024-01-30 20:15:06 | INFO | train_inner | epoch 006:   1059 / 1349 loss=2.183, nll_loss=0.571, ppl=1.49, wps=64603.5, ups=16.51, wpb=3912.8, bsz=219.5, num_updates=7800, lr=0.000358057, gnorm=0.873, loss_scale=8, train_wall=6, gb_free=29.7, wall=519
2024-01-30 20:15:12 | INFO | train_inner | epoch 006:   1159 / 1349 loss=2.202, nll_loss=0.596, ppl=1.51, wps=65343.7, ups=16.61, wpb=3934.3, bsz=209.4, num_updates=7900, lr=0.000355784, gnorm=0.858, loss_scale=8, train_wall=6, gb_free=29.7, wall=525
2024-01-30 20:15:18 | INFO | train_inner | epoch 006:   1259 / 1349 loss=2.17, nll_loss=0.559, ppl=1.47, wps=65313.6, ups=16.62, wpb=3929, bsz=225.7, num_updates=8000, lr=0.000353553, gnorm=0.868, loss_scale=8, train_wall=6, gb_free=29.7, wall=531
2024-01-30 20:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:15:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:15:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 2.255 | nll_loss 0.406 | ppl 1.32 | wps 170477 | wpb 3275.2 | bsz 176.5 | num_updates 8090 | best_loss 2.255
2024-01-30 20:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8090 updates
2024-01-30 20:15:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint6.pt
2024-01-30 20:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint6.pt
2024-01-30 20:15:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint6.pt (epoch 6 @ 8090 updates, score 2.255) (writing took 5.02356703102123 seconds)
2024-01-30 20:15:30 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-01-30 20:15:30 | INFO | train | epoch 006 | loss 2.211 | nll_loss 0.601 | ppl 1.52 | wps 59986.4 | ups 15.3 | wpb 3921.2 | bsz 214.5 | num_updates 8090 | lr 0.000351581 | gnorm 0.906 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 543
2024-01-30 20:15:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:15:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:15:30 | INFO | fairseq.trainer | begin training epoch 7
2024-01-30 20:15:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:15:31 | INFO | train_inner | epoch 007:     10 / 1349 loss=2.15, nll_loss=0.539, ppl=1.45, wps=30652.4, ups=7.7, wpb=3981.9, bsz=214, num_updates=8100, lr=0.000351364, gnorm=0.849, loss_scale=8, train_wall=6, gb_free=29.7, wall=544
2024-01-30 20:15:37 | INFO | train_inner | epoch 007:    110 / 1349 loss=2.144, nll_loss=0.532, ppl=1.45, wps=65142.6, ups=16.45, wpb=3959.1, bsz=227, num_updates=8200, lr=0.000349215, gnorm=0.786, loss_scale=8, train_wall=6, gb_free=29.8, wall=550
2024-01-30 20:15:43 | INFO | train_inner | epoch 007:    210 / 1349 loss=2.187, nll_loss=0.582, ppl=1.5, wps=64074.5, ups=16.6, wpb=3859.6, bsz=209, num_updates=8300, lr=0.000347105, gnorm=0.909, loss_scale=8, train_wall=6, gb_free=29.8, wall=556
2024-01-30 20:15:49 | INFO | train_inner | epoch 007:    310 / 1349 loss=2.166, nll_loss=0.558, ppl=1.47, wps=65331.2, ups=16.68, wpb=3916.4, bsz=214.7, num_updates=8400, lr=0.000345033, gnorm=0.858, loss_scale=8, train_wall=6, gb_free=29.8, wall=562
2024-01-30 20:15:55 | INFO | train_inner | epoch 007:    410 / 1349 loss=2.138, nll_loss=0.526, ppl=1.44, wps=64067.4, ups=16.39, wpb=3908.5, bsz=216.6, num_updates=8500, lr=0.000342997, gnorm=0.859, loss_scale=8, train_wall=6, gb_free=29.7, wall=568
2024-01-30 20:16:01 | INFO | train_inner | epoch 007:    510 / 1349 loss=2.137, nll_loss=0.527, ppl=1.44, wps=65763.7, ups=16.51, wpb=3982.1, bsz=224.9, num_updates=8600, lr=0.000340997, gnorm=0.761, loss_scale=8, train_wall=6, gb_free=29.7, wall=574
2024-01-30 20:16:07 | INFO | train_inner | epoch 007:    610 / 1349 loss=2.107, nll_loss=0.494, ppl=1.41, wps=64903, ups=16.41, wpb=3954.9, bsz=215.3, num_updates=8700, lr=0.000339032, gnorm=0.789, loss_scale=8, train_wall=6, gb_free=29.7, wall=580
2024-01-30 20:16:13 | INFO | train_inner | epoch 007:    710 / 1349 loss=2.153, nll_loss=0.546, ppl=1.46, wps=65639.2, ups=16.68, wpb=3934.7, bsz=208.2, num_updates=8800, lr=0.0003371, gnorm=0.797, loss_scale=8, train_wall=6, gb_free=29.8, wall=586
2024-01-30 20:16:19 | INFO | train_inner | epoch 007:    810 / 1349 loss=2.172, nll_loss=0.567, ppl=1.48, wps=64376.3, ups=16.73, wpb=3847.9, bsz=209, num_updates=8900, lr=0.000335201, gnorm=0.906, loss_scale=8, train_wall=6, gb_free=29.8, wall=592
2024-01-30 20:16:25 | INFO | train_inner | epoch 007:    910 / 1349 loss=2.146, nll_loss=0.539, ppl=1.45, wps=65017.6, ups=16.61, wpb=3915.1, bsz=199.9, num_updates=9000, lr=0.000333333, gnorm=0.861, loss_scale=8, train_wall=6, gb_free=29.8, wall=598
2024-01-30 20:16:31 | INFO | train_inner | epoch 007:   1010 / 1349 loss=2.164, nll_loss=0.561, ppl=1.48, wps=64988.5, ups=16.6, wpb=3915.9, bsz=221.6, num_updates=9100, lr=0.000331497, gnorm=0.852, loss_scale=8, train_wall=6, gb_free=29.7, wall=604
2024-01-30 20:16:37 | INFO | train_inner | epoch 007:   1110 / 1349 loss=2.129, nll_loss=0.52, ppl=1.43, wps=64599, ups=16.51, wpb=3912.1, bsz=221.6, num_updates=9200, lr=0.00032969, gnorm=0.834, loss_scale=8, train_wall=6, gb_free=29.8, wall=611
2024-01-30 20:16:43 | INFO | train_inner | epoch 007:   1210 / 1349 loss=2.139, nll_loss=0.533, ppl=1.45, wps=65435.6, ups=16.44, wpb=3980.2, bsz=207.2, num_updates=9300, lr=0.000327913, gnorm=0.742, loss_scale=8, train_wall=6, gb_free=29.7, wall=617
2024-01-30 20:16:49 | INFO | train_inner | epoch 007:   1310 / 1349 loss=2.146, nll_loss=0.541, ppl=1.45, wps=64014.5, ups=16.6, wpb=3856.6, bsz=211.4, num_updates=9400, lr=0.000326164, gnorm=0.814, loss_scale=8, train_wall=6, gb_free=29.7, wall=623
2024-01-30 20:16:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:16:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:16:54 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 2.216 | nll_loss 0.383 | ppl 1.3 | wps 164693 | wpb 3275.2 | bsz 176.5 | num_updates 9439 | best_loss 2.216
2024-01-30 20:16:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 9439 updates
2024-01-30 20:16:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint7.pt
2024-01-30 20:16:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint7.pt
2024-01-30 20:16:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint7.pt (epoch 7 @ 9439 updates, score 2.216) (writing took 5.062397876987234 seconds)
2024-01-30 20:16:59 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-01-30 20:16:59 | INFO | train | epoch 007 | loss 2.146 | nll_loss 0.538 | ppl 1.45 | wps 59724 | ups 15.23 | wpb 3921.2 | bsz 214.5 | num_updates 9439 | lr 0.00032549 | gnorm 0.825 | loss_scale 8 | train_wall 79 | gb_free 29.7 | wall 632
2024-01-30 20:16:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:16:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:16:59 | INFO | fairseq.trainer | begin training epoch 8
2024-01-30 20:16:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:17:03 | INFO | train_inner | epoch 008:     61 / 1349 loss=2.098, nll_loss=0.489, ppl=1.4, wps=30119.3, ups=7.61, wpb=3959.6, bsz=215.8, num_updates=9500, lr=0.000324443, gnorm=0.751, loss_scale=8, train_wall=6, gb_free=29.8, wall=636
2024-01-30 20:17:09 | INFO | train_inner | epoch 008:    161 / 1349 loss=2.146, nll_loss=0.542, ppl=1.46, wps=64076.8, ups=16.51, wpb=3882.1, bsz=208.1, num_updates=9600, lr=0.000322749, gnorm=0.825, loss_scale=8, train_wall=6, gb_free=29.8, wall=642
2024-01-30 20:17:15 | INFO | train_inner | epoch 008:    261 / 1349 loss=2.071, nll_loss=0.459, ppl=1.37, wps=64858.1, ups=16.32, wpb=3974.7, bsz=226.3, num_updates=9700, lr=0.000321081, gnorm=0.729, loss_scale=8, train_wall=6, gb_free=29.8, wall=648
2024-01-30 20:17:21 | INFO | train_inner | epoch 008:    361 / 1349 loss=2.156, nll_loss=0.554, ppl=1.47, wps=64402.1, ups=16.66, wpb=3866, bsz=204.5, num_updates=9800, lr=0.000319438, gnorm=0.817, loss_scale=8, train_wall=6, gb_free=29.8, wall=654
2024-01-30 20:17:27 | INFO | train_inner | epoch 008:    461 / 1349 loss=2.122, nll_loss=0.516, ppl=1.43, wps=65354.6, ups=16.69, wpb=3916.3, bsz=202.2, num_updates=9900, lr=0.000317821, gnorm=0.79, loss_scale=8, train_wall=6, gb_free=29.7, wall=660
2024-01-30 20:17:33 | INFO | train_inner | epoch 008:    561 / 1349 loss=2.117, nll_loss=0.51, ppl=1.42, wps=65361.5, ups=16.63, wpb=3929.2, bsz=207.3, num_updates=10000, lr=0.000316228, gnorm=0.765, loss_scale=8, train_wall=6, gb_free=29.8, wall=666
2024-01-30 20:17:39 | INFO | train_inner | epoch 008:    661 / 1349 loss=2.101, nll_loss=0.492, ppl=1.41, wps=64233.2, ups=16.25, wpb=3954, bsz=211.8, num_updates=10100, lr=0.000314658, gnorm=0.768, loss_scale=8, train_wall=6, gb_free=29.8, wall=672
2024-01-30 20:17:45 | INFO | train_inner | epoch 008:    761 / 1349 loss=2.082, nll_loss=0.473, ppl=1.39, wps=63055.6, ups=16.2, wpb=3891.2, bsz=240.9, num_updates=10200, lr=0.000313112, gnorm=0.718, loss_scale=8, train_wall=6, gb_free=29.8, wall=678
2024-01-30 20:17:51 | INFO | train_inner | epoch 008:    861 / 1349 loss=2.072, nll_loss=0.462, ppl=1.38, wps=64919.2, ups=16.31, wpb=3980, bsz=222.3, num_updates=10300, lr=0.000311588, gnorm=0.691, loss_scale=8, train_wall=6, gb_free=29.7, wall=684
2024-01-30 20:17:57 | INFO | train_inner | epoch 008:    961 / 1349 loss=2.061, nll_loss=0.451, ppl=1.37, wps=64883.2, ups=16.48, wpb=3937.2, bsz=219.3, num_updates=10400, lr=0.000310087, gnorm=0.684, loss_scale=8, train_wall=6, gb_free=29.8, wall=691
2024-01-30 20:18:03 | INFO | train_inner | epoch 008:   1061 / 1349 loss=2.114, nll_loss=0.511, ppl=1.43, wps=64413.4, ups=16.59, wpb=3882.2, bsz=211.6, num_updates=10500, lr=0.000308607, gnorm=0.788, loss_scale=8, train_wall=6, gb_free=29.7, wall=697
2024-01-30 20:18:09 | INFO | train_inner | epoch 008:   1161 / 1349 loss=2.095, nll_loss=0.49, ppl=1.4, wps=65498.9, ups=16.5, wpb=3969.6, bsz=219.8, num_updates=10600, lr=0.000307148, gnorm=0.716, loss_scale=8, train_wall=6, gb_free=29.8, wall=703
2024-01-30 20:18:15 | INFO | train_inner | epoch 008:   1261 / 1349 loss=2.103, nll_loss=0.499, ppl=1.41, wps=64019.9, ups=16.62, wpb=3851.2, bsz=203.4, num_updates=10700, lr=0.000305709, gnorm=0.826, loss_scale=8, train_wall=6, gb_free=29.8, wall=709
2024-01-30 20:18:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:18:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:18:22 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 2.169 | nll_loss 0.343 | ppl 1.27 | wps 170958 | wpb 3275.2 | bsz 176.5 | num_updates 10788 | best_loss 2.169
2024-01-30 20:18:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10788 updates
2024-01-30 20:18:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint8.pt
2024-01-30 20:18:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint8.pt
2024-01-30 20:18:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint8.pt (epoch 8 @ 10788 updates, score 2.169) (writing took 4.786109710927121 seconds)
2024-01-30 20:18:27 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-01-30 20:18:27 | INFO | train | epoch 008 | loss 2.103 | nll_loss 0.496 | ppl 1.41 | wps 59750.4 | ups 15.24 | wpb 3921.2 | bsz 214.5 | num_updates 10788 | lr 0.00030446 | gnorm 0.755 | loss_scale 8 | train_wall 79 | gb_free 29.7 | wall 721
2024-01-30 20:18:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:18:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:18:27 | INFO | fairseq.trainer | begin training epoch 9
2024-01-30 20:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:18:28 | INFO | train_inner | epoch 009:     12 / 1349 loss=2.101, nll_loss=0.497, ppl=1.41, wps=30641.5, ups=7.87, wpb=3891.9, bsz=213.4, num_updates=10800, lr=0.00030429, gnorm=0.733, loss_scale=8, train_wall=6, gb_free=29.7, wall=721
2024-01-30 20:18:34 | INFO | train_inner | epoch 009:    112 / 1349 loss=2.084, nll_loss=0.477, ppl=1.39, wps=64031.4, ups=16.55, wpb=3869, bsz=212.3, num_updates=10900, lr=0.000302891, gnorm=0.736, loss_scale=8, train_wall=6, gb_free=29.8, wall=727
2024-01-30 20:18:40 | INFO | train_inner | epoch 009:    212 / 1349 loss=2.075, nll_loss=0.467, ppl=1.38, wps=65879.3, ups=16.56, wpb=3978.5, bsz=203.8, num_updates=11000, lr=0.000301511, gnorm=0.65, loss_scale=8, train_wall=6, gb_free=29.7, wall=733
2024-01-30 20:18:46 | INFO | train_inner | epoch 009:    312 / 1349 loss=2.068, nll_loss=0.461, ppl=1.38, wps=65087.2, ups=16.62, wpb=3916.8, bsz=229.1, num_updates=11100, lr=0.00030015, gnorm=0.684, loss_scale=8, train_wall=6, gb_free=29.8, wall=739
2024-01-30 20:18:52 | INFO | train_inner | epoch 009:    412 / 1349 loss=2.096, nll_loss=0.492, ppl=1.41, wps=65456, ups=16.8, wpb=3896.5, bsz=205.6, num_updates=11200, lr=0.000298807, gnorm=0.793, loss_scale=8, train_wall=6, gb_free=29.8, wall=745
2024-01-30 20:18:58 | INFO | train_inner | epoch 009:    512 / 1349 loss=2.059, nll_loss=0.452, ppl=1.37, wps=65166.2, ups=16.61, wpb=3922.4, bsz=212.1, num_updates=11300, lr=0.000297482, gnorm=0.672, loss_scale=8, train_wall=6, gb_free=29.7, wall=751
2024-01-30 20:19:04 | INFO | train_inner | epoch 009:    612 / 1349 loss=2.084, nll_loss=0.48, ppl=1.39, wps=65810.2, ups=16.67, wpb=3948.3, bsz=217, num_updates=11400, lr=0.000296174, gnorm=0.71, loss_scale=8, train_wall=6, gb_free=29.8, wall=757
2024-01-30 20:19:10 | INFO | train_inner | epoch 009:    712 / 1349 loss=2.055, nll_loss=0.448, ppl=1.36, wps=65785.8, ups=16.6, wpb=3962.9, bsz=216.6, num_updates=11500, lr=0.000294884, gnorm=0.671, loss_scale=8, train_wall=6, gb_free=29.7, wall=763
2024-01-30 20:19:16 | INFO | train_inner | epoch 009:    812 / 1349 loss=2.076, nll_loss=0.472, ppl=1.39, wps=65729, ups=16.7, wpb=3935.8, bsz=207.3, num_updates=11600, lr=0.00029361, gnorm=0.678, loss_scale=8, train_wall=6, gb_free=29.9, wall=769
2024-01-30 20:19:22 | INFO | train_inner | epoch 009:    912 / 1349 loss=2.07, nll_loss=0.465, ppl=1.38, wps=65239.9, ups=16.68, wpb=3911.6, bsz=215, num_updates=11700, lr=0.000292353, gnorm=0.821, loss_scale=8, train_wall=6, gb_free=29.8, wall=775
2024-01-30 20:19:28 | INFO | train_inner | epoch 009:   1012 / 1349 loss=2.054, nll_loss=0.448, ppl=1.36, wps=65282.3, ups=16.68, wpb=3913.3, bsz=209, num_updates=11800, lr=0.000291111, gnorm=0.688, loss_scale=8, train_wall=6, gb_free=29.8, wall=781
2024-01-30 20:19:34 | INFO | train_inner | epoch 009:   1112 / 1349 loss=2.049, nll_loss=0.443, ppl=1.36, wps=65014.9, ups=16.55, wpb=3928.2, bsz=224.8, num_updates=11900, lr=0.000289886, gnorm=0.626, loss_scale=8, train_wall=6, gb_free=29.8, wall=787
2024-01-30 20:19:40 | INFO | train_inner | epoch 009:   1212 / 1349 loss=2.062, nll_loss=0.459, ppl=1.37, wps=65158, ups=16.62, wpb=3919.8, bsz=223.8, num_updates=12000, lr=0.000288675, gnorm=0.679, loss_scale=8, train_wall=6, gb_free=29.7, wall=793
2024-01-30 20:19:46 | INFO | train_inner | epoch 009:   1312 / 1349 loss=2.067, nll_loss=0.464, ppl=1.38, wps=64769.7, ups=16.62, wpb=3897.1, bsz=208.7, num_updates=12100, lr=0.00028748, gnorm=0.72, loss_scale=8, train_wall=6, gb_free=29.7, wall=799
2024-01-30 20:19:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:19:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:19:50 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 2.145 | nll_loss 0.329 | ppl 1.26 | wps 171654 | wpb 3275.2 | bsz 176.5 | num_updates 12137 | best_loss 2.145
2024-01-30 20:19:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12137 updates
2024-01-30 20:19:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint9.pt
2024-01-30 20:19:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint9.pt
2024-01-30 20:19:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint9.pt (epoch 9 @ 12137 updates, score 2.145) (writing took 4.885071160970256 seconds)
2024-01-30 20:19:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-01-30 20:19:55 | INFO | train | epoch 009 | loss 2.07 | nll_loss 0.464 | ppl 1.38 | wps 60212.9 | ups 15.36 | wpb 3921.2 | bsz 214.5 | num_updates 12137 | lr 0.000287041 | gnorm 0.704 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 808
2024-01-30 20:19:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:19:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:19:55 | INFO | fairseq.trainer | begin training epoch 10
2024-01-30 20:19:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:19:59 | INFO | train_inner | epoch 010:     63 / 1349 loss=2.033, nll_loss=0.425, ppl=1.34, wps=30807.2, ups=7.81, wpb=3945, bsz=228.6, num_updates=12200, lr=0.000286299, gnorm=0.687, loss_scale=8, train_wall=6, gb_free=29.7, wall=812
2024-01-30 20:20:05 | INFO | train_inner | epoch 010:    163 / 1349 loss=2.061, nll_loss=0.457, ppl=1.37, wps=65372.3, ups=16.77, wpb=3897.1, bsz=216.6, num_updates=12300, lr=0.000285133, gnorm=0.779, loss_scale=8, train_wall=6, gb_free=29.7, wall=818
2024-01-30 20:20:11 | INFO | train_inner | epoch 010:    263 / 1349 loss=2.049, nll_loss=0.444, ppl=1.36, wps=64086.2, ups=16.67, wpb=3845.1, bsz=210.9, num_updates=12400, lr=0.000283981, gnorm=0.649, loss_scale=8, train_wall=6, gb_free=29.8, wall=824
2024-01-30 20:20:17 | INFO | train_inner | epoch 010:    363 / 1349 loss=2.047, nll_loss=0.442, ppl=1.36, wps=65105.8, ups=16.71, wpb=3897.3, bsz=200.2, num_updates=12500, lr=0.000282843, gnorm=0.695, loss_scale=8, train_wall=6, gb_free=29.8, wall=830
2024-01-30 20:20:23 | INFO | train_inner | epoch 010:    463 / 1349 loss=2.041, nll_loss=0.436, ppl=1.35, wps=66154.6, ups=16.67, wpb=3969.2, bsz=205.8, num_updates=12600, lr=0.000281718, gnorm=0.674, loss_scale=8, train_wall=6, gb_free=29.7, wall=836
2024-01-30 20:20:29 | INFO | train_inner | epoch 010:    563 / 1349 loss=2.067, nll_loss=0.465, ppl=1.38, wps=66263.7, ups=16.72, wpb=3963.2, bsz=214, num_updates=12700, lr=0.000280607, gnorm=0.699, loss_scale=8, train_wall=6, gb_free=29.7, wall=842
2024-01-30 20:20:35 | INFO | train_inner | epoch 010:    663 / 1349 loss=2.063, nll_loss=0.461, ppl=1.38, wps=65459.9, ups=16.7, wpb=3920.6, bsz=214.8, num_updates=12800, lr=0.000279508, gnorm=0.714, loss_scale=8, train_wall=6, gb_free=29.7, wall=848
2024-01-30 20:20:41 | INFO | train_inner | epoch 010:    763 / 1349 loss=2.046, nll_loss=0.442, ppl=1.36, wps=65256, ups=16.59, wpb=3934.4, bsz=218.8, num_updates=12900, lr=0.000278423, gnorm=0.68, loss_scale=8, train_wall=6, gb_free=29.8, wall=854
2024-01-30 20:20:47 | INFO | train_inner | epoch 010:    863 / 1349 loss=2.016, nll_loss=0.409, ppl=1.33, wps=65116.3, ups=16.53, wpb=3939.1, bsz=215.6, num_updates=13000, lr=0.00027735, gnorm=0.604, loss_scale=8, train_wall=6, gb_free=29.8, wall=860
2024-01-30 20:20:53 | INFO | train_inner | epoch 010:    963 / 1349 loss=2.05, nll_loss=0.447, ppl=1.36, wps=64886.2, ups=16.69, wpb=3887.8, bsz=211.2, num_updates=13100, lr=0.000276289, gnorm=0.671, loss_scale=8, train_wall=6, gb_free=29.9, wall=866
2024-01-30 20:20:59 | INFO | train_inner | epoch 010:   1063 / 1349 loss=2.057, nll_loss=0.455, ppl=1.37, wps=65409.7, ups=16.68, wpb=3922.2, bsz=222, num_updates=13200, lr=0.000275241, gnorm=0.688, loss_scale=8, train_wall=6, gb_free=29.8, wall=872
2024-01-30 20:21:05 | INFO | train_inner | epoch 010:   1163 / 1349 loss=2.024, nll_loss=0.419, ppl=1.34, wps=65019.6, ups=16.63, wpb=3909.8, bsz=214.3, num_updates=13300, lr=0.000274204, gnorm=0.694, loss_scale=8, train_wall=6, gb_free=29.7, wall=878
2024-01-30 20:21:11 | INFO | train_inner | epoch 010:   1263 / 1349 loss=2.033, nll_loss=0.429, ppl=1.35, wps=65387.5, ups=16.56, wpb=3949.7, bsz=217.8, num_updates=13400, lr=0.000273179, gnorm=0.596, loss_scale=8, train_wall=6, gb_free=29.8, wall=884
2024-01-30 20:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:21:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:21:18 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 2.146 | nll_loss 0.336 | ppl 1.26 | wps 173072 | wpb 3275.2 | bsz 176.5 | num_updates 13486 | best_loss 2.145
2024-01-30 20:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 13486 updates
2024-01-30 20:21:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint10.pt
2024-01-30 20:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint10.pt
2024-01-30 20:21:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint10.pt (epoch 10 @ 13486 updates, score 2.146) (writing took 3.539856093004346 seconds)
2024-01-30 20:21:22 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-01-30 20:21:22 | INFO | train | epoch 010 | loss 2.044 | nll_loss 0.44 | ppl 1.36 | wps 61187 | ups 15.6 | wpb 3921.2 | bsz 214.5 | num_updates 13486 | lr 0.000272307 | gnorm 0.679 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 895
2024-01-30 20:21:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:21:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:21:22 | INFO | fairseq.trainer | begin training epoch 11
2024-01-30 20:21:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:21:23 | INFO | train_inner | epoch 011:     14 / 1349 loss=2.039, nll_loss=0.436, ppl=1.35, wps=34264, ups=8.74, wpb=3920.9, bsz=216.6, num_updates=13500, lr=0.000272166, gnorm=0.679, loss_scale=8, train_wall=6, gb_free=29.8, wall=896
2024-01-30 20:21:29 | INFO | train_inner | epoch 011:    114 / 1349 loss=2.03, nll_loss=0.425, ppl=1.34, wps=66110.2, ups=16.78, wpb=3940.9, bsz=216.6, num_updates=13600, lr=0.000271163, gnorm=0.686, loss_scale=8, train_wall=6, gb_free=29.7, wall=902
2024-01-30 20:21:35 | INFO | train_inner | epoch 011:    214 / 1349 loss=2.023, nll_loss=0.418, ppl=1.34, wps=64792.4, ups=16.59, wpb=3905.3, bsz=211.9, num_updates=13700, lr=0.000270172, gnorm=0.645, loss_scale=8, train_wall=6, gb_free=29.8, wall=908
2024-01-30 20:21:41 | INFO | train_inner | epoch 011:    314 / 1349 loss=2.004, nll_loss=0.398, ppl=1.32, wps=64923.7, ups=16.46, wpb=3944.5, bsz=223.8, num_updates=13800, lr=0.000269191, gnorm=0.678, loss_scale=8, train_wall=6, gb_free=29.7, wall=914
2024-01-30 20:21:47 | INFO | train_inner | epoch 011:    414 / 1349 loss=2.042, nll_loss=0.44, ppl=1.36, wps=65880.1, ups=16.75, wpb=3934.2, bsz=212.3, num_updates=13900, lr=0.000268221, gnorm=0.609, loss_scale=8, train_wall=6, gb_free=29.8, wall=920
2024-01-30 20:21:53 | INFO | train_inner | epoch 011:    514 / 1349 loss=2.023, nll_loss=0.418, ppl=1.34, wps=64360.9, ups=16.73, wpb=3847, bsz=196.2, num_updates=14000, lr=0.000267261, gnorm=0.665, loss_scale=8, train_wall=6, gb_free=29.8, wall=926
2024-01-30 20:21:59 | INFO | train_inner | epoch 011:    614 / 1349 loss=2.028, nll_loss=0.424, ppl=1.34, wps=65211.1, ups=16.6, wpb=3928.1, bsz=226.5, num_updates=14100, lr=0.000266312, gnorm=0.636, loss_scale=8, train_wall=6, gb_free=29.8, wall=932
2024-01-30 20:22:05 | INFO | train_inner | epoch 011:    714 / 1349 loss=2.003, nll_loss=0.397, ppl=1.32, wps=65228.2, ups=16.44, wpb=3967.9, bsz=228.5, num_updates=14200, lr=0.000265372, gnorm=0.644, loss_scale=8, train_wall=6, gb_free=29.8, wall=938
2024-01-30 20:22:11 | INFO | train_inner | epoch 011:    814 / 1349 loss=2.022, nll_loss=0.419, ppl=1.34, wps=65795, ups=16.63, wpb=3956.4, bsz=223.6, num_updates=14300, lr=0.000264443, gnorm=0.614, loss_scale=8, train_wall=6, gb_free=29.8, wall=944
2024-01-30 20:22:17 | INFO | train_inner | epoch 011:    914 / 1349 loss=2.04, nll_loss=0.439, ppl=1.36, wps=65463.1, ups=16.71, wpb=3917.6, bsz=208.6, num_updates=14400, lr=0.000263523, gnorm=0.593, loss_scale=8, train_wall=6, gb_free=29.7, wall=950
2024-01-30 20:22:23 | INFO | train_inner | epoch 011:   1014 / 1349 loss=2.05, nll_loss=0.45, ppl=1.37, wps=65332.8, ups=16.77, wpb=3897, bsz=208.4, num_updates=14500, lr=0.000262613, gnorm=0.653, loss_scale=8, train_wall=6, gb_free=29.8, wall=956
2024-01-30 20:22:29 | INFO | train_inner | epoch 011:   1114 / 1349 loss=2.014, nll_loss=0.41, ppl=1.33, wps=64965.4, ups=16.65, wpb=3900.8, bsz=219.3, num_updates=14600, lr=0.000261712, gnorm=0.617, loss_scale=8, train_wall=6, gb_free=29.9, wall=962
2024-01-30 20:22:35 | INFO | train_inner | epoch 011:   1214 / 1349 loss=2.002, nll_loss=0.397, ppl=1.32, wps=65468.6, ups=16.64, wpb=3935, bsz=214.6, num_updates=14700, lr=0.00026082, gnorm=0.628, loss_scale=8, train_wall=6, gb_free=29.8, wall=968
2024-01-30 20:22:41 | INFO | train_inner | epoch 011:   1314 / 1349 loss=2.018, nll_loss=0.415, ppl=1.33, wps=65204.5, ups=16.73, wpb=3897.4, bsz=202.7, num_updates=14800, lr=0.000259938, gnorm=0.666, loss_scale=8, train_wall=6, gb_free=29.9, wall=974
2024-01-30 20:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:22:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:22:44 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 2.117 | nll_loss 0.31 | ppl 1.24 | wps 172271 | wpb 3275.2 | bsz 176.5 | num_updates 14835 | best_loss 2.117
2024-01-30 20:22:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 14835 updates
2024-01-30 20:22:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint11.pt
2024-01-30 20:22:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint11.pt
2024-01-30 20:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint11.pt (epoch 11 @ 14835 updates, score 2.117) (writing took 4.810409397003241 seconds)
2024-01-30 20:22:49 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-01-30 20:22:49 | INFO | train | epoch 011 | loss 2.023 | nll_loss 0.42 | ppl 1.34 | wps 60305.8 | ups 15.38 | wpb 3921.2 | bsz 214.5 | num_updates 14835 | lr 0.000259631 | gnorm 0.642 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 983
2024-01-30 20:22:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:22:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:22:49 | INFO | fairseq.trainer | begin training epoch 12
2024-01-30 20:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:22:53 | INFO | train_inner | epoch 012:     65 / 1349 loss=1.996, nll_loss=0.39, ppl=1.31, wps=30897.5, ups=7.87, wpb=3927.2, bsz=215.2, num_updates=14900, lr=0.000259064, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.8, wall=987
2024-01-30 20:22:59 | INFO | train_inner | epoch 012:    165 / 1349 loss=1.995, nll_loss=0.39, ppl=1.31, wps=64681.4, ups=16.34, wpb=3959, bsz=222.6, num_updates=15000, lr=0.000258199, gnorm=0.619, loss_scale=8, train_wall=6, gb_free=29.8, wall=993
2024-01-30 20:23:05 | INFO | train_inner | epoch 012:    265 / 1349 loss=2.016, nll_loss=0.413, ppl=1.33, wps=64542.9, ups=16.74, wpb=3854.6, bsz=222.9, num_updates=15100, lr=0.000257343, gnorm=0.66, loss_scale=8, train_wall=6, gb_free=29.8, wall=999
2024-01-30 20:23:11 | INFO | train_inner | epoch 012:    365 / 1349 loss=2.027, nll_loss=0.426, ppl=1.34, wps=66108.3, ups=16.82, wpb=3930.8, bsz=207.4, num_updates=15200, lr=0.000256495, gnorm=0.573, loss_scale=8, train_wall=6, gb_free=29.8, wall=1005
2024-01-30 20:23:17 | INFO | train_inner | epoch 012:    465 / 1349 loss=2.013, nll_loss=0.411, ppl=1.33, wps=65335.4, ups=16.81, wpb=3887.2, bsz=206, num_updates=15300, lr=0.000255655, gnorm=0.637, loss_scale=8, train_wall=6, gb_free=29.8, wall=1011
2024-01-30 20:23:23 | INFO | train_inner | epoch 012:    565 / 1349 loss=1.988, nll_loss=0.382, ppl=1.3, wps=65562.5, ups=16.6, wpb=3949.5, bsz=218, num_updates=15400, lr=0.000254824, gnorm=0.571, loss_scale=8, train_wall=6, gb_free=29.7, wall=1017
2024-01-30 20:23:29 | INFO | train_inner | epoch 012:    665 / 1349 loss=2.025, nll_loss=0.424, ppl=1.34, wps=64981.8, ups=16.71, wpb=3889.6, bsz=207.4, num_updates=15500, lr=0.000254, gnorm=0.642, loss_scale=8, train_wall=6, gb_free=29.7, wall=1023
2024-01-30 20:23:35 | INFO | train_inner | epoch 012:    765 / 1349 loss=1.989, nll_loss=0.385, ppl=1.31, wps=64830.1, ups=16.57, wpb=3913.2, bsz=220.6, num_updates=15600, lr=0.000253185, gnorm=0.562, loss_scale=8, train_wall=6, gb_free=29.7, wall=1029
2024-01-30 20:23:41 | INFO | train_inner | epoch 012:    865 / 1349 loss=2.014, nll_loss=0.413, ppl=1.33, wps=65216, ups=16.72, wpb=3899.8, bsz=208.6, num_updates=15700, lr=0.000252377, gnorm=0.642, loss_scale=8, train_wall=6, gb_free=29.7, wall=1035
2024-01-30 20:23:47 | INFO | train_inner | epoch 012:    965 / 1349 loss=2.008, nll_loss=0.406, ppl=1.33, wps=66169, ups=16.75, wpb=3950.2, bsz=219.4, num_updates=15800, lr=0.000251577, gnorm=0.638, loss_scale=8, train_wall=6, gb_free=29.7, wall=1041
2024-01-30 20:23:53 | INFO | train_inner | epoch 012:   1065 / 1349 loss=2.006, nll_loss=0.404, ppl=1.32, wps=64783.8, ups=16.65, wpb=3891.3, bsz=214.3, num_updates=15900, lr=0.000250785, gnorm=0.606, loss_scale=8, train_wall=6, gb_free=29.8, wall=1047
2024-01-30 20:23:59 | INFO | train_inner | epoch 012:   1165 / 1349 loss=1.988, nll_loss=0.384, ppl=1.3, wps=65715.9, ups=16.52, wpb=3977.4, bsz=207.6, num_updates=16000, lr=0.00025, gnorm=0.587, loss_scale=8, train_wall=6, gb_free=29.8, wall=1053
2024-01-30 20:24:05 | INFO | train_inner | epoch 012:   1265 / 1349 loss=2.03, nll_loss=0.431, ppl=1.35, wps=65865.9, ups=16.73, wpb=3936.3, bsz=217.6, num_updates=16100, lr=0.000249222, gnorm=0.63, loss_scale=8, train_wall=6, gb_free=29.9, wall=1059
2024-01-30 20:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:24:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:24:12 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 2.109 | nll_loss 0.308 | ppl 1.24 | wps 171402 | wpb 3275.2 | bsz 176.5 | num_updates 16184 | best_loss 2.109
2024-01-30 20:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 16184 updates
2024-01-30 20:24:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint12.pt
2024-01-30 20:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint12.pt
2024-01-30 20:24:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint12.pt (epoch 12 @ 16184 updates, score 2.109) (writing took 4.799202751950361 seconds)
2024-01-30 20:24:17 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-01-30 20:24:17 | INFO | train | epoch 012 | loss 2.007 | nll_loss 0.404 | ppl 1.32 | wps 60314.3 | ups 15.38 | wpb 3921.2 | bsz 214.5 | num_updates 16184 | lr 0.000248575 | gnorm 0.61 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 1070
2024-01-30 20:24:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:24:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:24:17 | INFO | fairseq.trainer | begin training epoch 13
2024-01-30 20:24:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:24:18 | INFO | train_inner | epoch 013:     16 / 1349 loss=2.008, nll_loss=0.407, ppl=1.33, wps=30945, ups=7.87, wpb=3930.9, bsz=209.1, num_updates=16200, lr=0.000248452, gnorm=0.629, loss_scale=8, train_wall=6, gb_free=29.8, wall=1071
2024-01-30 20:24:24 | INFO | train_inner | epoch 013:    116 / 1349 loss=1.998, nll_loss=0.395, ppl=1.31, wps=65862.9, ups=16.74, wpb=3933.5, bsz=197.6, num_updates=16300, lr=0.000247689, gnorm=0.667, loss_scale=8, train_wall=6, gb_free=29.8, wall=1077
2024-01-30 20:24:30 | INFO | train_inner | epoch 013:    216 / 1349 loss=1.988, nll_loss=0.384, ppl=1.31, wps=65268.5, ups=16.68, wpb=3914.1, bsz=212.9, num_updates=16400, lr=0.000246932, gnorm=0.589, loss_scale=8, train_wall=6, gb_free=29.8, wall=1083
2024-01-30 20:24:36 | INFO | train_inner | epoch 013:    316 / 1349 loss=1.996, nll_loss=0.393, ppl=1.31, wps=64907.4, ups=16.74, wpb=3877.9, bsz=208.1, num_updates=16500, lr=0.000246183, gnorm=0.594, loss_scale=8, train_wall=6, gb_free=29.7, wall=1089
2024-01-30 20:24:42 | INFO | train_inner | epoch 013:    416 / 1349 loss=1.994, nll_loss=0.391, ppl=1.31, wps=66583.3, ups=16.67, wpb=3995.1, bsz=216.1, num_updates=16600, lr=0.00024544, gnorm=0.571, loss_scale=8, train_wall=6, gb_free=29.8, wall=1095
2024-01-30 20:24:48 | INFO | train_inner | epoch 013:    516 / 1349 loss=1.983, nll_loss=0.379, ppl=1.3, wps=65094.8, ups=16.59, wpb=3924, bsz=213.5, num_updates=16700, lr=0.000244704, gnorm=0.554, loss_scale=8, train_wall=6, gb_free=29.8, wall=1101
2024-01-30 20:24:54 | INFO | train_inner | epoch 013:    616 / 1349 loss=2.013, nll_loss=0.413, ppl=1.33, wps=65262, ups=16.73, wpb=3901.8, bsz=217.9, num_updates=16800, lr=0.000243975, gnorm=0.65, loss_scale=8, train_wall=6, gb_free=29.7, wall=1107
2024-01-30 20:25:00 | INFO | train_inner | epoch 013:    716 / 1349 loss=1.992, nll_loss=0.389, ppl=1.31, wps=65151, ups=16.63, wpb=3918.2, bsz=217, num_updates=16900, lr=0.000243252, gnorm=0.603, loss_scale=8, train_wall=6, gb_free=29.8, wall=1113
2024-01-30 20:25:06 | INFO | train_inner | epoch 013:    816 / 1349 loss=2.017, nll_loss=0.417, ppl=1.34, wps=65232, ups=16.78, wpb=3887.9, bsz=206.6, num_updates=17000, lr=0.000242536, gnorm=0.656, loss_scale=8, train_wall=6, gb_free=29.8, wall=1119
2024-01-30 20:25:12 | INFO | train_inner | epoch 013:    916 / 1349 loss=2.007, nll_loss=0.407, ppl=1.33, wps=65382.8, ups=16.7, wpb=3914.1, bsz=215.4, num_updates=17100, lr=0.000241825, gnorm=0.588, loss_scale=8, train_wall=6, gb_free=29.7, wall=1125
2024-01-30 20:25:18 | INFO | train_inner | epoch 013:   1016 / 1349 loss=1.974, nll_loss=0.369, ppl=1.29, wps=64913.3, ups=16.52, wpb=3928.3, bsz=223.5, num_updates=17200, lr=0.000241121, gnorm=0.569, loss_scale=8, train_wall=6, gb_free=29.7, wall=1131
2024-01-30 20:25:24 | INFO | train_inner | epoch 013:   1116 / 1349 loss=1.992, nll_loss=0.391, ppl=1.31, wps=65535.6, ups=16.72, wpb=3920.2, bsz=215, num_updates=17300, lr=0.000240424, gnorm=0.607, loss_scale=8, train_wall=6, gb_free=29.9, wall=1137
2024-01-30 20:25:30 | INFO | train_inner | epoch 013:   1216 / 1349 loss=1.966, nll_loss=0.362, ppl=1.28, wps=65917.1, ups=16.57, wpb=3977, bsz=226.4, num_updates=17400, lr=0.000239732, gnorm=0.571, loss_scale=8, train_wall=6, gb_free=29.8, wall=1143
2024-01-30 20:25:36 | INFO | train_inner | epoch 013:   1316 / 1349 loss=1.993, nll_loss=0.392, ppl=1.31, wps=65236.7, ups=16.71, wpb=3902.9, bsz=218.9, num_updates=17500, lr=0.000239046, gnorm=0.649, loss_scale=8, train_wall=6, gb_free=29.9, wall=1149
2024-01-30 20:25:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:25:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:25:40 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 2.106 | nll_loss 0.307 | ppl 1.24 | wps 171802 | wpb 3275.2 | bsz 176.5 | num_updates 17533 | best_loss 2.106
2024-01-30 20:25:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 17533 updates
2024-01-30 20:25:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint13.pt
2024-01-30 20:25:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint13.pt
2024-01-30 20:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint13.pt (epoch 13 @ 17533 updates, score 2.106) (writing took 4.818752152961679 seconds)
2024-01-30 20:25:45 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-01-30 20:25:45 | INFO | train | epoch 013 | loss 1.993 | nll_loss 0.391 | ppl 1.31 | wps 60383.5 | ups 15.4 | wpb 3921.2 | bsz 214.5 | num_updates 17533 | lr 0.000238821 | gnorm 0.607 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 1158
2024-01-30 20:25:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:25:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:25:45 | INFO | fairseq.trainer | begin training epoch 14
2024-01-30 20:25:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:25:49 | INFO | train_inner | epoch 014:     67 / 1349 loss=1.984, nll_loss=0.381, ppl=1.3, wps=30359.6, ups=7.89, wpb=3849.3, bsz=214.4, num_updates=17600, lr=0.000238366, gnorm=0.727, loss_scale=8, train_wall=6, gb_free=29.8, wall=1162
2024-01-30 20:25:55 | INFO | train_inner | epoch 014:    167 / 1349 loss=1.969, nll_loss=0.365, ppl=1.29, wps=65495.9, ups=16.68, wpb=3926, bsz=211.4, num_updates=17700, lr=0.000237691, gnorm=0.588, loss_scale=8, train_wall=6, gb_free=29.8, wall=1168
2024-01-30 20:26:01 | INFO | train_inner | epoch 014:    267 / 1349 loss=1.988, nll_loss=0.386, ppl=1.31, wps=65258.3, ups=16.77, wpb=3891.7, bsz=202.6, num_updates=17800, lr=0.000237023, gnorm=0.615, loss_scale=8, train_wall=6, gb_free=29.8, wall=1174
2024-01-30 20:26:07 | INFO | train_inner | epoch 014:    367 / 1349 loss=1.949, nll_loss=0.343, ppl=1.27, wps=65633.1, ups=16.52, wpb=3972.6, bsz=228.2, num_updates=17900, lr=0.00023636, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=1180
2024-01-30 20:26:13 | INFO | train_inner | epoch 014:    467 / 1349 loss=1.976, nll_loss=0.373, ppl=1.3, wps=65439.3, ups=16.58, wpb=3946.6, bsz=216.2, num_updates=18000, lr=0.000235702, gnorm=0.603, loss_scale=8, train_wall=6, gb_free=29.8, wall=1186
2024-01-30 20:26:19 | INFO | train_inner | epoch 014:    567 / 1349 loss=1.995, nll_loss=0.395, ppl=1.31, wps=65781.2, ups=16.81, wpb=3913.4, bsz=208, num_updates=18100, lr=0.00023505, gnorm=0.617, loss_scale=8, train_wall=6, gb_free=29.7, wall=1192
2024-01-30 20:26:25 | INFO | train_inner | epoch 014:    667 / 1349 loss=1.996, nll_loss=0.396, ppl=1.32, wps=65430, ups=16.7, wpb=3918.6, bsz=211, num_updates=18200, lr=0.000234404, gnorm=0.638, loss_scale=8, train_wall=6, gb_free=29.9, wall=1198
2024-01-30 20:26:31 | INFO | train_inner | epoch 014:    767 / 1349 loss=2.001, nll_loss=0.401, ppl=1.32, wps=65139.7, ups=16.76, wpb=3887.7, bsz=200.2, num_updates=18300, lr=0.000233762, gnorm=0.657, loss_scale=8, train_wall=6, gb_free=29.7, wall=1204
2024-01-30 20:26:37 | INFO | train_inner | epoch 014:    867 / 1349 loss=1.966, nll_loss=0.363, ppl=1.29, wps=65192.7, ups=16.53, wpb=3943.1, bsz=219.4, num_updates=18400, lr=0.000233126, gnorm=0.56, loss_scale=8, train_wall=6, gb_free=29.8, wall=1210
2024-01-30 20:26:43 | INFO | train_inner | epoch 014:    967 / 1349 loss=1.974, nll_loss=0.372, ppl=1.29, wps=65535.7, ups=16.64, wpb=3937.8, bsz=220.6, num_updates=18500, lr=0.000232495, gnorm=0.557, loss_scale=8, train_wall=6, gb_free=29.7, wall=1216
2024-01-30 20:26:49 | INFO | train_inner | epoch 014:   1067 / 1349 loss=1.968, nll_loss=0.365, ppl=1.29, wps=65564.5, ups=16.6, wpb=3949.1, bsz=221.5, num_updates=18600, lr=0.000231869, gnorm=0.61, loss_scale=8, train_wall=6, gb_free=29.7, wall=1222
2024-01-30 20:26:55 | INFO | train_inner | epoch 014:   1167 / 1349 loss=1.987, nll_loss=0.386, ppl=1.31, wps=65311.4, ups=16.73, wpb=3904, bsz=213.4, num_updates=18700, lr=0.000231249, gnorm=0.587, loss_scale=8, train_wall=6, gb_free=29.8, wall=1228
2024-01-30 20:27:01 | INFO | train_inner | epoch 014:   1267 / 1349 loss=1.981, nll_loss=0.38, ppl=1.3, wps=66145, ups=16.74, wpb=3950.5, bsz=214.9, num_updates=18800, lr=0.000230633, gnorm=0.584, loss_scale=8, train_wall=6, gb_free=29.9, wall=1234
2024-01-30 20:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:27:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:27:07 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 2.097 | nll_loss 0.302 | ppl 1.23 | wps 173828 | wpb 3275.2 | bsz 176.5 | num_updates 18882 | best_loss 2.097
2024-01-30 20:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 18882 updates
2024-01-30 20:27:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint14.pt
2024-01-30 20:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint14.pt
2024-01-30 20:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint14.pt (epoch 14 @ 18882 updates, score 2.097) (writing took 4.820548700983636 seconds)
2024-01-30 20:27:12 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-01-30 20:27:12 | INFO | train | epoch 014 | loss 1.98 | nll_loss 0.378 | ppl 1.3 | wps 60384.5 | ups 15.4 | wpb 3921.2 | bsz 214.5 | num_updates 18882 | lr 0.000230131 | gnorm 0.608 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 1245
2024-01-30 20:27:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:27:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:27:12 | INFO | fairseq.trainer | begin training epoch 15
2024-01-30 20:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:27:13 | INFO | train_inner | epoch 015:     18 / 1349 loss=1.977, nll_loss=0.376, ppl=1.3, wps=30672.5, ups=7.86, wpb=3900.2, bsz=224.9, num_updates=18900, lr=0.000230022, gnorm=0.651, loss_scale=8, train_wall=6, gb_free=29.7, wall=1247
2024-01-30 20:27:19 | INFO | train_inner | epoch 015:    118 / 1349 loss=1.969, nll_loss=0.366, ppl=1.29, wps=65791.6, ups=16.69, wpb=3941.8, bsz=215.4, num_updates=19000, lr=0.000229416, gnorm=0.554, loss_scale=8, train_wall=6, gb_free=29.7, wall=1253
2024-01-30 20:27:25 | INFO | train_inner | epoch 015:    218 / 1349 loss=1.964, nll_loss=0.361, ppl=1.28, wps=63856.5, ups=16.7, wpb=3823.1, bsz=209.1, num_updates=19100, lr=0.000228814, gnorm=0.607, loss_scale=8, train_wall=6, gb_free=29.8, wall=1259
2024-01-30 20:27:31 | INFO | train_inner | epoch 015:    318 / 1349 loss=1.977, nll_loss=0.376, ppl=1.3, wps=66439.4, ups=16.76, wpb=3963.6, bsz=207.9, num_updates=19200, lr=0.000228218, gnorm=0.595, loss_scale=8, train_wall=6, gb_free=29.8, wall=1265
2024-01-30 20:27:37 | INFO | train_inner | epoch 015:    418 / 1349 loss=1.979, nll_loss=0.378, ppl=1.3, wps=64786.3, ups=16.76, wpb=3865.8, bsz=217.4, num_updates=19300, lr=0.000227626, gnorm=0.625, loss_scale=8, train_wall=6, gb_free=29.7, wall=1271
2024-01-30 20:27:43 | INFO | train_inner | epoch 015:    518 / 1349 loss=1.978, nll_loss=0.377, ppl=1.3, wps=65469.7, ups=16.76, wpb=3907, bsz=209.5, num_updates=19400, lr=0.000227038, gnorm=0.585, loss_scale=8, train_wall=6, gb_free=29.7, wall=1276
2024-01-30 20:27:49 | INFO | train_inner | epoch 015:    618 / 1349 loss=1.968, nll_loss=0.367, ppl=1.29, wps=65282.5, ups=16.65, wpb=3921.1, bsz=215.2, num_updates=19500, lr=0.000226455, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.8, wall=1282
2024-01-30 20:27:55 | INFO | train_inner | epoch 015:    718 / 1349 loss=1.966, nll_loss=0.365, ppl=1.29, wps=65071.7, ups=16.67, wpb=3904.6, bsz=226, num_updates=19600, lr=0.000225877, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=29.8, wall=1288
2024-01-30 20:28:01 | INFO | train_inner | epoch 015:    818 / 1349 loss=1.964, nll_loss=0.363, ppl=1.29, wps=65777.9, ups=16.61, wpb=3960.9, bsz=215.6, num_updates=19700, lr=0.000225303, gnorm=0.583, loss_scale=8, train_wall=6, gb_free=29.8, wall=1295
2024-01-30 20:28:07 | INFO | train_inner | epoch 015:    918 / 1349 loss=1.971, nll_loss=0.37, ppl=1.29, wps=65041.3, ups=16.62, wpb=3912.6, bsz=218, num_updates=19800, lr=0.000224733, gnorm=0.619, loss_scale=8, train_wall=6, gb_free=29.7, wall=1301
2024-01-30 20:28:13 | INFO | train_inner | epoch 015:   1018 / 1349 loss=1.975, nll_loss=0.375, ppl=1.3, wps=65984.8, ups=16.68, wpb=3957, bsz=209.5, num_updates=19900, lr=0.000224168, gnorm=0.613, loss_scale=8, train_wall=6, gb_free=29.8, wall=1307
2024-01-30 20:28:19 | INFO | train_inner | epoch 015:   1118 / 1349 loss=1.976, nll_loss=0.376, ppl=1.3, wps=65660, ups=16.74, wpb=3922.1, bsz=206, num_updates=20000, lr=0.000223607, gnorm=0.604, loss_scale=8, train_wall=6, gb_free=29.8, wall=1313
2024-01-30 20:28:25 | INFO | train_inner | epoch 015:   1218 / 1349 loss=1.958, nll_loss=0.355, ppl=1.28, wps=65766.1, ups=16.64, wpb=3953, bsz=216.6, num_updates=20100, lr=0.00022305, gnorm=0.593, loss_scale=8, train_wall=6, gb_free=29.8, wall=1319
2024-01-30 20:28:31 | INFO | train_inner | epoch 015:   1318 / 1349 loss=1.976, nll_loss=0.377, ppl=1.3, wps=65524.7, ups=16.65, wpb=3934.5, bsz=214.2, num_updates=20200, lr=0.000222497, gnorm=0.575, loss_scale=8, train_wall=6, gb_free=29.7, wall=1325
2024-01-30 20:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:28:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:28:35 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 2.088 | nll_loss 0.3 | ppl 1.23 | wps 173863 | wpb 3275.2 | bsz 176.5 | num_updates 20231 | best_loss 2.088
2024-01-30 20:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 20231 updates
2024-01-30 20:28:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint15.pt
2024-01-30 20:28:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint15.pt
2024-01-30 20:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint15.pt (epoch 15 @ 20231 updates, score 2.088) (writing took 4.8726990630384535 seconds)
2024-01-30 20:28:40 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-01-30 20:28:40 | INFO | train | epoch 015 | loss 1.97 | nll_loss 0.369 | ppl 1.29 | wps 60355.8 | ups 15.39 | wpb 3921.2 | bsz 214.5 | num_updates 20231 | lr 0.000222327 | gnorm 0.59 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 1333
2024-01-30 20:28:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:28:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:28:40 | INFO | fairseq.trainer | begin training epoch 16
2024-01-30 20:28:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:28:44 | INFO | train_inner | epoch 016:     69 / 1349 loss=1.951, nll_loss=0.348, ppl=1.27, wps=30714.5, ups=7.83, wpb=3924.8, bsz=221.5, num_updates=20300, lr=0.000221948, gnorm=0.54, loss_scale=8, train_wall=6, gb_free=29.7, wall=1337
2024-01-30 20:28:50 | INFO | train_inner | epoch 016:    169 / 1349 loss=1.947, nll_loss=0.343, ppl=1.27, wps=65768.9, ups=16.66, wpb=3946.9, bsz=219.1, num_updates=20400, lr=0.000221404, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.9, wall=1343
2024-01-30 20:28:56 | INFO | train_inner | epoch 016:    269 / 1349 loss=1.948, nll_loss=0.345, ppl=1.27, wps=65230.6, ups=16.61, wpb=3926.7, bsz=214.5, num_updates=20500, lr=0.000220863, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.7, wall=1349
2024-01-30 20:29:02 | INFO | train_inner | epoch 016:    369 / 1349 loss=1.966, nll_loss=0.365, ppl=1.29, wps=64871.3, ups=16.82, wpb=3855.9, bsz=216.7, num_updates=20600, lr=0.000220326, gnorm=0.639, loss_scale=8, train_wall=6, gb_free=29.7, wall=1355
2024-01-30 20:29:08 | INFO | train_inner | epoch 016:    469 / 1349 loss=1.955, nll_loss=0.353, ppl=1.28, wps=65883.8, ups=16.64, wpb=3959.6, bsz=218.2, num_updates=20700, lr=0.000219793, gnorm=0.599, loss_scale=8, train_wall=6, gb_free=29.8, wall=1361
2024-01-30 20:29:14 | INFO | train_inner | epoch 016:    569 / 1349 loss=1.965, nll_loss=0.365, ppl=1.29, wps=65194.6, ups=16.64, wpb=3917, bsz=214.6, num_updates=20800, lr=0.000219265, gnorm=0.565, loss_scale=8, train_wall=6, gb_free=29.8, wall=1367
2024-01-30 20:29:20 | INFO | train_inner | epoch 016:    669 / 1349 loss=1.955, nll_loss=0.353, ppl=1.28, wps=64754.3, ups=16.64, wpb=3892.3, bsz=208.2, num_updates=20900, lr=0.000218739, gnorm=0.557, loss_scale=8, train_wall=6, gb_free=29.7, wall=1373
2024-01-30 20:29:26 | INFO | train_inner | epoch 016:    769 / 1349 loss=1.968, nll_loss=0.368, ppl=1.29, wps=64914.4, ups=16.71, wpb=3885, bsz=211, num_updates=21000, lr=0.000218218, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.8, wall=1379
2024-01-30 20:29:32 | INFO | train_inner | epoch 016:    869 / 1349 loss=1.962, nll_loss=0.361, ppl=1.28, wps=65850.6, ups=16.67, wpb=3949.2, bsz=221, num_updates=21100, lr=0.0002177, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.7, wall=1385
2024-01-30 20:29:38 | INFO | train_inner | epoch 016:    969 / 1349 loss=1.947, nll_loss=0.345, ppl=1.27, wps=64858.2, ups=16.6, wpb=3906.2, bsz=215.7, num_updates=21200, lr=0.000217186, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=1391
2024-01-30 20:29:44 | INFO | train_inner | epoch 016:   1069 / 1349 loss=1.961, nll_loss=0.361, ppl=1.28, wps=65138.2, ups=16.59, wpb=3926.5, bsz=218.7, num_updates=21300, lr=0.000216676, gnorm=0.61, loss_scale=8, train_wall=6, gb_free=29.8, wall=1397
2024-01-30 20:29:50 | INFO | train_inner | epoch 016:   1169 / 1349 loss=1.954, nll_loss=0.353, ppl=1.28, wps=65558.7, ups=16.62, wpb=3945, bsz=214.6, num_updates=21400, lr=0.000216169, gnorm=0.56, loss_scale=8, train_wall=6, gb_free=29.8, wall=1403
2024-01-30 20:29:56 | INFO | train_inner | epoch 016:   1269 / 1349 loss=1.977, nll_loss=0.378, ppl=1.3, wps=65741.3, ups=16.76, wpb=3921.4, bsz=205.7, num_updates=21500, lr=0.000215666, gnorm=0.563, loss_scale=8, train_wall=6, gb_free=29.7, wall=1409
2024-01-30 20:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:30:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:30:03 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 2.084 | nll_loss 0.296 | ppl 1.23 | wps 173040 | wpb 3275.2 | bsz 176.5 | num_updates 21580 | best_loss 2.084
2024-01-30 20:30:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 21580 updates
2024-01-30 20:30:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint16.pt
2024-01-30 20:30:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint16.pt
2024-01-30 20:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint16.pt (epoch 16 @ 21580 updates, score 2.084) (writing took 4.903927905950695 seconds)
2024-01-30 20:30:08 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-01-30 20:30:08 | INFO | train | epoch 016 | loss 1.96 | nll_loss 0.358 | ppl 1.28 | wps 60296.2 | ups 15.38 | wpb 3921.2 | bsz 214.5 | num_updates 21580 | lr 0.000215265 | gnorm 0.572 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 1421
2024-01-30 20:30:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:30:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:30:08 | INFO | fairseq.trainer | begin training epoch 17
2024-01-30 20:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:30:09 | INFO | train_inner | epoch 017:     20 / 1349 loss=1.972, nll_loss=0.373, ppl=1.29, wps=30787.9, ups=7.83, wpb=3933.7, bsz=207.4, num_updates=21600, lr=0.000215166, gnorm=0.603, loss_scale=8, train_wall=6, gb_free=29.8, wall=1422
2024-01-30 20:30:15 | INFO | train_inner | epoch 017:    120 / 1349 loss=1.934, nll_loss=0.33, ppl=1.26, wps=65015.2, ups=16.67, wpb=3899.6, bsz=206.1, num_updates=21700, lr=0.000214669, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=1428
2024-01-30 20:30:21 | INFO | train_inner | epoch 017:    220 / 1349 loss=1.939, nll_loss=0.336, ppl=1.26, wps=65131.3, ups=16.6, wpb=3922.6, bsz=214.6, num_updates=21800, lr=0.000214176, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=1434
2024-01-30 20:30:27 | INFO | train_inner | epoch 017:    320 / 1349 loss=1.943, nll_loss=0.34, ppl=1.27, wps=65379.3, ups=16.7, wpb=3915.9, bsz=220.1, num_updates=21900, lr=0.000213687, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=1440
2024-01-30 20:30:33 | INFO | train_inner | epoch 017:    420 / 1349 loss=1.961, nll_loss=0.361, ppl=1.28, wps=65988.3, ups=16.75, wpb=3938.4, bsz=217.4, num_updates=22000, lr=0.000213201, gnorm=0.559, loss_scale=8, train_wall=6, gb_free=29.7, wall=1446
2024-01-30 20:30:39 | INFO | train_inner | epoch 017:    520 / 1349 loss=1.96, nll_loss=0.36, ppl=1.28, wps=65576.4, ups=16.75, wpb=3916, bsz=202.6, num_updates=22100, lr=0.000212718, gnorm=0.588, loss_scale=8, train_wall=6, gb_free=29.8, wall=1452
2024-01-30 20:30:45 | INFO | train_inner | epoch 017:    620 / 1349 loss=1.942, nll_loss=0.34, ppl=1.27, wps=65673.9, ups=16.62, wpb=3952.7, bsz=219.3, num_updates=22200, lr=0.000212238, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.7, wall=1458
2024-01-30 20:30:51 | INFO | train_inner | epoch 017:    720 / 1349 loss=1.936, nll_loss=0.334, ppl=1.26, wps=65894.8, ups=16.53, wpb=3986, bsz=227.9, num_updates=22300, lr=0.000211762, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=1464
2024-01-30 20:30:57 | INFO | train_inner | epoch 017:    820 / 1349 loss=1.956, nll_loss=0.356, ppl=1.28, wps=65094.9, ups=16.65, wpb=3910.4, bsz=212.1, num_updates=22400, lr=0.000211289, gnorm=0.609, loss_scale=8, train_wall=6, gb_free=29.8, wall=1470
2024-01-30 20:31:03 | INFO | train_inner | epoch 017:    920 / 1349 loss=1.946, nll_loss=0.345, ppl=1.27, wps=65279.5, ups=16.64, wpb=3923.6, bsz=224.9, num_updates=22500, lr=0.000210819, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.7, wall=1476
2024-01-30 20:31:09 | INFO | train_inner | epoch 017:   1020 / 1349 loss=1.95, nll_loss=0.349, ppl=1.27, wps=65436.4, ups=16.56, wpb=3950.4, bsz=222.1, num_updates=22600, lr=0.000210352, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.9, wall=1482
2024-01-30 20:31:15 | INFO | train_inner | epoch 017:   1120 / 1349 loss=1.955, nll_loss=0.355, ppl=1.28, wps=65877.1, ups=16.69, wpb=3946.5, bsz=211.8, num_updates=22700, lr=0.000209888, gnorm=0.567, loss_scale=16, train_wall=6, gb_free=29.8, wall=1488
2024-01-30 20:31:21 | INFO | train_inner | epoch 017:   1220 / 1349 loss=1.955, nll_loss=0.354, ppl=1.28, wps=65069.3, ups=16.67, wpb=3903, bsz=210.3, num_updates=22800, lr=0.000209427, gnorm=0.576, loss_scale=16, train_wall=6, gb_free=29.8, wall=1494
2024-01-30 20:31:27 | INFO | train_inner | epoch 017:   1320 / 1349 loss=1.984, nll_loss=0.387, ppl=1.31, wps=64748.5, ups=16.92, wpb=3827.4, bsz=206, num_updates=22900, lr=0.000208969, gnorm=0.625, loss_scale=16, train_wall=6, gb_free=29.9, wall=1500
2024-01-30 20:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:31:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:31:30 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 2.073 | nll_loss 0.289 | ppl 1.22 | wps 172436 | wpb 3275.2 | bsz 176.5 | num_updates 22929 | best_loss 2.073
2024-01-30 20:31:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 22929 updates
2024-01-30 20:31:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint17.pt
2024-01-30 20:31:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint17.pt
2024-01-30 20:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint17.pt (epoch 17 @ 22929 updates, score 2.073) (writing took 4.842545832972974 seconds)
2024-01-30 20:31:35 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-01-30 20:31:35 | INFO | train | epoch 017 | loss 1.951 | nll_loss 0.35 | ppl 1.27 | wps 60350.9 | ups 15.39 | wpb 3921.2 | bsz 214.5 | num_updates 22929 | lr 0.000208837 | gnorm 0.551 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 1508
2024-01-30 20:31:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:31:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:31:35 | INFO | fairseq.trainer | begin training epoch 18
2024-01-30 20:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:31:40 | INFO | train_inner | epoch 018:     71 / 1349 loss=1.945, nll_loss=0.343, ppl=1.27, wps=30873.6, ups=7.84, wpb=3937.8, bsz=214.2, num_updates=23000, lr=0.000208514, gnorm=0.549, loss_scale=16, train_wall=6, gb_free=29.7, wall=1513
2024-01-30 20:31:46 | INFO | train_inner | epoch 018:    171 / 1349 loss=1.954, nll_loss=0.352, ppl=1.28, wps=65233.8, ups=16.78, wpb=3887.7, bsz=189.8, num_updates=23100, lr=0.000208063, gnorm=0.572, loss_scale=16, train_wall=6, gb_free=29.7, wall=1519
2024-01-30 20:31:52 | INFO | train_inner | epoch 018:    271 / 1349 loss=1.943, nll_loss=0.342, ppl=1.27, wps=65901.4, ups=16.77, wpb=3930.8, bsz=200.5, num_updates=23200, lr=0.000207614, gnorm=0.578, loss_scale=16, train_wall=6, gb_free=29.8, wall=1525
2024-01-30 20:31:58 | INFO | train_inner | epoch 018:    371 / 1349 loss=1.945, nll_loss=0.344, ppl=1.27, wps=65275, ups=16.72, wpb=3905.1, bsz=206.8, num_updates=23300, lr=0.000207168, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.8, wall=1531
2024-01-30 20:32:04 | INFO | train_inner | epoch 018:    471 / 1349 loss=1.934, nll_loss=0.332, ppl=1.26, wps=65472.6, ups=16.63, wpb=3938.2, bsz=219, num_updates=23400, lr=0.000206725, gnorm=0.519, loss_scale=16, train_wall=6, gb_free=29.7, wall=1537
2024-01-30 20:32:10 | INFO | train_inner | epoch 018:    571 / 1349 loss=1.926, nll_loss=0.323, ppl=1.25, wps=64915.5, ups=16.55, wpb=3922.6, bsz=219.4, num_updates=23500, lr=0.000206284, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.8, wall=1543
2024-01-30 20:32:16 | INFO | train_inner | epoch 018:    671 / 1349 loss=1.953, nll_loss=0.353, ppl=1.28, wps=65548.4, ups=16.73, wpb=3918.6, bsz=215.8, num_updates=23600, lr=0.000205847, gnorm=0.643, loss_scale=16, train_wall=6, gb_free=29.8, wall=1549
2024-01-30 20:32:22 | INFO | train_inner | epoch 018:    771 / 1349 loss=1.952, nll_loss=0.352, ppl=1.28, wps=65661.4, ups=16.68, wpb=3935.6, bsz=225, num_updates=23700, lr=0.000205412, gnorm=0.554, loss_scale=16, train_wall=6, gb_free=29.8, wall=1555
2024-01-30 20:32:28 | INFO | train_inner | epoch 018:    871 / 1349 loss=1.946, nll_loss=0.345, ppl=1.27, wps=65214.6, ups=16.71, wpb=3902, bsz=214.6, num_updates=23800, lr=0.00020498, gnorm=0.617, loss_scale=16, train_wall=6, gb_free=29.8, wall=1561
2024-01-30 20:32:34 | INFO | train_inner | epoch 018:    971 / 1349 loss=1.942, nll_loss=0.341, ppl=1.27, wps=65227, ups=16.67, wpb=3911.9, bsz=221, num_updates=23900, lr=0.000204551, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=29.8, wall=1567
2024-01-30 20:32:40 | INFO | train_inner | epoch 018:   1071 / 1349 loss=1.936, nll_loss=0.334, ppl=1.26, wps=65174, ups=16.7, wpb=3903.5, bsz=214.1, num_updates=24000, lr=0.000204124, gnorm=0.538, loss_scale=16, train_wall=6, gb_free=29.8, wall=1573
2024-01-30 20:32:46 | INFO | train_inner | epoch 018:   1171 / 1349 loss=1.942, nll_loss=0.342, ppl=1.27, wps=65040.6, ups=16.63, wpb=3910.7, bsz=214.6, num_updates=24100, lr=0.0002037, gnorm=0.584, loss_scale=16, train_wall=6, gb_free=29.8, wall=1579
2024-01-30 20:32:52 | INFO | train_inner | epoch 018:   1271 / 1349 loss=1.946, nll_loss=0.346, ppl=1.27, wps=65609.6, ups=16.6, wpb=3951.3, bsz=232.2, num_updates=24200, lr=0.000203279, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.7, wall=1585
2024-01-30 20:32:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:32:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:32:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 2.083 | nll_loss 0.295 | ppl 1.23 | wps 163485 | wpb 3275.2 | bsz 176.5 | num_updates 24278 | best_loss 2.073
2024-01-30 20:32:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 24278 updates
2024-01-30 20:32:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint18.pt
2024-01-30 20:32:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint18.pt
2024-01-30 20:33:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint18.pt (epoch 18 @ 24278 updates, score 2.083) (writing took 3.549300992046483 seconds)
2024-01-30 20:33:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-01-30 20:33:02 | INFO | train | epoch 018 | loss 1.943 | nll_loss 0.342 | ppl 1.27 | wps 61191.9 | ups 15.61 | wpb 3921.2 | bsz 214.5 | num_updates 24278 | lr 0.000202952 | gnorm 0.556 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 1595
2024-01-30 20:33:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:33:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:33:02 | INFO | fairseq.trainer | begin training epoch 19
2024-01-30 20:33:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:33:03 | INFO | train_inner | epoch 019:     22 / 1349 loss=1.948, nll_loss=0.348, ppl=1.27, wps=34171.9, ups=8.68, wpb=3938.2, bsz=205.3, num_updates=24300, lr=0.00020286, gnorm=0.557, loss_scale=16, train_wall=6, gb_free=29.8, wall=1596
2024-01-30 20:33:09 | INFO | train_inner | epoch 019:    122 / 1349 loss=1.927, nll_loss=0.325, ppl=1.25, wps=65333.4, ups=16.65, wpb=3923, bsz=216.2, num_updates=24400, lr=0.000202444, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.7, wall=1602
2024-01-30 20:33:15 | INFO | train_inner | epoch 019:    222 / 1349 loss=1.929, nll_loss=0.326, ppl=1.25, wps=65657.9, ups=16.6, wpb=3954.5, bsz=218.2, num_updates=24500, lr=0.000202031, gnorm=0.553, loss_scale=16, train_wall=6, gb_free=29.8, wall=1608
2024-01-30 20:33:21 | INFO | train_inner | epoch 019:    322 / 1349 loss=1.921, nll_loss=0.319, ppl=1.25, wps=65474.2, ups=16.61, wpb=3941.9, bsz=217, num_updates=24600, lr=0.000201619, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=1614
2024-01-30 20:33:27 | INFO | train_inner | epoch 019:    422 / 1349 loss=1.952, nll_loss=0.353, ppl=1.28, wps=65188.8, ups=16.76, wpb=3889.6, bsz=204.5, num_updates=24700, lr=0.000201211, gnorm=0.568, loss_scale=16, train_wall=6, gb_free=29.8, wall=1620
2024-01-30 20:33:33 | INFO | train_inner | epoch 019:    522 / 1349 loss=1.936, nll_loss=0.335, ppl=1.26, wps=65603.6, ups=16.63, wpb=3944.8, bsz=223.4, num_updates=24800, lr=0.000200805, gnorm=0.604, loss_scale=16, train_wall=6, gb_free=29.7, wall=1626
2024-01-30 20:33:39 | INFO | train_inner | epoch 019:    622 / 1349 loss=1.934, nll_loss=0.333, ppl=1.26, wps=65467.5, ups=16.68, wpb=3924.4, bsz=204, num_updates=24900, lr=0.000200401, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.8, wall=1632
2024-01-30 20:33:45 | INFO | train_inner | epoch 019:    722 / 1349 loss=1.933, nll_loss=0.332, ppl=1.26, wps=65700.6, ups=16.71, wpb=3931.8, bsz=226.3, num_updates=25000, lr=0.0002, gnorm=0.564, loss_scale=16, train_wall=6, gb_free=29.9, wall=1638
2024-01-30 20:33:51 | INFO | train_inner | epoch 019:    822 / 1349 loss=1.946, nll_loss=0.346, ppl=1.27, wps=65621.1, ups=16.68, wpb=3935.1, bsz=211.5, num_updates=25100, lr=0.000199601, gnorm=0.583, loss_scale=16, train_wall=6, gb_free=29.8, wall=1644
2024-01-30 20:33:57 | INFO | train_inner | epoch 019:    922 / 1349 loss=1.939, nll_loss=0.339, ppl=1.26, wps=65188.4, ups=16.72, wpb=3898.8, bsz=223.4, num_updates=25200, lr=0.000199205, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=1650
2024-01-30 20:34:03 | INFO | train_inner | epoch 019:   1022 / 1349 loss=1.942, nll_loss=0.342, ppl=1.27, wps=65095.8, ups=16.77, wpb=3882.4, bsz=209.5, num_updates=25300, lr=0.000198811, gnorm=0.55, loss_scale=16, train_wall=6, gb_free=29.7, wall=1656
2024-01-30 20:34:09 | INFO | train_inner | epoch 019:   1122 / 1349 loss=1.944, nll_loss=0.344, ppl=1.27, wps=65329.1, ups=16.66, wpb=3921.6, bsz=210, num_updates=25400, lr=0.000198419, gnorm=0.539, loss_scale=16, train_wall=6, gb_free=29.7, wall=1662
2024-01-30 20:34:15 | INFO | train_inner | epoch 019:   1222 / 1349 loss=1.925, nll_loss=0.324, ppl=1.25, wps=65787.6, ups=16.56, wpb=3973.1, bsz=220.7, num_updates=25500, lr=0.00019803, gnorm=0.53, loss_scale=16, train_wall=6, gb_free=29.7, wall=1668
2024-01-30 20:34:21 | INFO | train_inner | epoch 019:   1322 / 1349 loss=1.935, nll_loss=0.335, ppl=1.26, wps=64708.2, ups=16.75, wpb=3863.7, bsz=215.8, num_updates=25600, lr=0.000197642, gnorm=0.603, loss_scale=16, train_wall=6, gb_free=29.8, wall=1674
2024-01-30 20:34:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:34:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:34:24 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 2.083 | nll_loss 0.297 | ppl 1.23 | wps 172550 | wpb 3275.2 | bsz 176.5 | num_updates 25627 | best_loss 2.073
2024-01-30 20:34:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 25627 updates
2024-01-30 20:34:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint19.pt
2024-01-30 20:34:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint19.pt
2024-01-30 20:34:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint19.pt (epoch 19 @ 25627 updates, score 2.083) (writing took 3.641499532968737 seconds)
2024-01-30 20:34:28 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-01-30 20:34:28 | INFO | train | epoch 019 | loss 1.936 | nll_loss 0.335 | ppl 1.26 | wps 61214.8 | ups 15.61 | wpb 3921.2 | bsz 214.5 | num_updates 25627 | lr 0.000197538 | gnorm 0.554 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 1681
2024-01-30 20:34:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:34:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:34:28 | INFO | fairseq.trainer | begin training epoch 20
2024-01-30 20:34:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:34:33 | INFO | train_inner | epoch 020:     73 / 1349 loss=1.917, nll_loss=0.314, ppl=1.24, wps=33661.9, ups=8.69, wpb=3872, bsz=203.1, num_updates=25700, lr=0.000197257, gnorm=0.551, loss_scale=16, train_wall=6, gb_free=29.7, wall=1686
2024-01-30 20:34:39 | INFO | train_inner | epoch 020:    173 / 1349 loss=1.931, nll_loss=0.329, ppl=1.26, wps=65876.4, ups=16.65, wpb=3955.7, bsz=212.2, num_updates=25800, lr=0.000196875, gnorm=0.546, loss_scale=16, train_wall=6, gb_free=29.9, wall=1692
2024-01-30 20:34:45 | INFO | train_inner | epoch 020:    273 / 1349 loss=1.931, nll_loss=0.33, ppl=1.26, wps=65572.8, ups=16.64, wpb=3941, bsz=219.8, num_updates=25900, lr=0.000196494, gnorm=0.528, loss_scale=16, train_wall=6, gb_free=29.8, wall=1698
2024-01-30 20:34:51 | INFO | train_inner | epoch 020:    373 / 1349 loss=1.943, nll_loss=0.343, ppl=1.27, wps=65675.7, ups=16.73, wpb=3925.8, bsz=217.8, num_updates=26000, lr=0.000196116, gnorm=0.527, loss_scale=16, train_wall=6, gb_free=29.8, wall=1704
2024-01-30 20:34:57 | INFO | train_inner | epoch 020:    473 / 1349 loss=1.929, nll_loss=0.328, ppl=1.26, wps=65511.6, ups=16.71, wpb=3919.8, bsz=214.9, num_updates=26100, lr=0.00019574, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.7, wall=1710
2024-01-30 20:35:03 | INFO | train_inner | epoch 020:    573 / 1349 loss=1.921, nll_loss=0.32, ppl=1.25, wps=65479.2, ups=16.66, wpb=3929.6, bsz=207.4, num_updates=26200, lr=0.000195366, gnorm=0.524, loss_scale=16, train_wall=6, gb_free=29.8, wall=1716
2024-01-30 20:35:09 | INFO | train_inner | epoch 020:    673 / 1349 loss=1.923, nll_loss=0.322, ppl=1.25, wps=65832.2, ups=16.56, wpb=3974.3, bsz=216.5, num_updates=26300, lr=0.000194994, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=1722
2024-01-30 20:35:15 | INFO | train_inner | epoch 020:    773 / 1349 loss=1.92, nll_loss=0.318, ppl=1.25, wps=64210, ups=16.64, wpb=3858.9, bsz=214.6, num_updates=26400, lr=0.000194625, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.8, wall=1728
2024-01-30 20:35:21 | INFO | train_inner | epoch 020:    873 / 1349 loss=1.935, nll_loss=0.336, ppl=1.26, wps=65954.4, ups=16.69, wpb=3950.7, bsz=217.4, num_updates=26500, lr=0.000194257, gnorm=0.575, loss_scale=16, train_wall=6, gb_free=29.7, wall=1734
2024-01-30 20:35:27 | INFO | train_inner | epoch 020:    973 / 1349 loss=1.928, nll_loss=0.328, ppl=1.26, wps=65610.3, ups=16.74, wpb=3920.1, bsz=217, num_updates=26600, lr=0.000193892, gnorm=0.54, loss_scale=16, train_wall=6, gb_free=29.8, wall=1740
2024-01-30 20:35:33 | INFO | train_inner | epoch 020:   1073 / 1349 loss=1.927, nll_loss=0.327, ppl=1.25, wps=65003.6, ups=16.58, wpb=3920.3, bsz=219, num_updates=26700, lr=0.000193528, gnorm=0.552, loss_scale=16, train_wall=6, gb_free=29.8, wall=1746
2024-01-30 20:35:39 | INFO | train_inner | epoch 020:   1173 / 1349 loss=1.931, nll_loss=0.33, ppl=1.26, wps=64919.2, ups=16.67, wpb=3894.6, bsz=215.6, num_updates=26800, lr=0.000193167, gnorm=0.608, loss_scale=16, train_wall=6, gb_free=29.8, wall=1752
2024-01-30 20:35:45 | INFO | train_inner | epoch 020:   1273 / 1349 loss=1.936, nll_loss=0.336, ppl=1.26, wps=65183.9, ups=16.75, wpb=3890.8, bsz=210.6, num_updates=26900, lr=0.000192807, gnorm=0.574, loss_scale=16, train_wall=6, gb_free=29.9, wall=1758
2024-01-30 20:35:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:35:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:35:51 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 2.063 | nll_loss 0.282 | ppl 1.22 | wps 173228 | wpb 3275.2 | bsz 176.5 | num_updates 26976 | best_loss 2.063
2024-01-30 20:35:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 26976 updates
2024-01-30 20:35:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint20.pt
2024-01-30 20:35:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint20.pt
2024-01-30 20:35:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint20.pt (epoch 20 @ 26976 updates, score 2.063) (writing took 4.838923827977851 seconds)
2024-01-30 20:35:56 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-01-30 20:35:56 | INFO | train | epoch 020 | loss 1.929 | nll_loss 0.329 | ppl 1.26 | wps 60359.6 | ups 15.39 | wpb 3921.2 | bsz 214.5 | num_updates 26976 | lr 0.000192536 | gnorm 0.544 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 1769
2024-01-30 20:35:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:35:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:35:56 | INFO | fairseq.trainer | begin training epoch 21
2024-01-30 20:35:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:35:57 | INFO | train_inner | epoch 021:     24 / 1349 loss=1.945, nll_loss=0.346, ppl=1.27, wps=30682.5, ups=7.88, wpb=3894.2, bsz=201.2, num_updates=27000, lr=0.00019245, gnorm=0.624, loss_scale=16, train_wall=6, gb_free=29.8, wall=1770
2024-01-30 20:36:03 | INFO | train_inner | epoch 021:    124 / 1349 loss=1.9, nll_loss=0.296, ppl=1.23, wps=64818.8, ups=16.61, wpb=3902.6, bsz=214.4, num_updates=27100, lr=0.000192095, gnorm=0.578, loss_scale=16, train_wall=6, gb_free=29.8, wall=1776
2024-01-30 20:36:09 | INFO | train_inner | epoch 021:    224 / 1349 loss=1.91, nll_loss=0.308, ppl=1.24, wps=64803.1, ups=16.6, wpb=3904.7, bsz=228.2, num_updates=27200, lr=0.000191741, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.8, wall=1782
2024-01-30 20:36:15 | INFO | train_inner | epoch 021:    324 / 1349 loss=1.911, nll_loss=0.309, ppl=1.24, wps=65937.1, ups=16.55, wpb=3984.7, bsz=222, num_updates=27300, lr=0.00019139, gnorm=0.561, loss_scale=16, train_wall=6, gb_free=29.8, wall=1789
2024-01-30 20:36:21 | INFO | train_inner | epoch 021:    424 / 1349 loss=1.927, nll_loss=0.327, ppl=1.25, wps=65905.6, ups=16.77, wpb=3929, bsz=204.6, num_updates=27400, lr=0.00019104, gnorm=0.53, loss_scale=16, train_wall=6, gb_free=29.8, wall=1794
2024-01-30 20:36:27 | INFO | train_inner | epoch 021:    524 / 1349 loss=1.93, nll_loss=0.329, ppl=1.26, wps=65887.4, ups=16.74, wpb=3935.1, bsz=206, num_updates=27500, lr=0.000190693, gnorm=0.547, loss_scale=16, train_wall=6, gb_free=29.7, wall=1800
2024-01-30 20:36:33 | INFO | train_inner | epoch 021:    624 / 1349 loss=1.928, nll_loss=0.327, ppl=1.25, wps=64934, ups=16.82, wpb=3859.5, bsz=206.6, num_updates=27600, lr=0.000190347, gnorm=0.57, loss_scale=16, train_wall=6, gb_free=29.8, wall=1806
2024-01-30 20:36:39 | INFO | train_inner | epoch 021:    724 / 1349 loss=1.929, nll_loss=0.329, ppl=1.26, wps=65574.3, ups=16.67, wpb=3933.6, bsz=215.3, num_updates=27700, lr=0.000190003, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.8, wall=1812
2024-01-30 20:36:45 | INFO | train_inner | epoch 021:    824 / 1349 loss=1.922, nll_loss=0.322, ppl=1.25, wps=65001.4, ups=16.73, wpb=3885.5, bsz=218.1, num_updates=27800, lr=0.000189661, gnorm=0.472, loss_scale=16, train_wall=6, gb_free=29.8, wall=1818
2024-01-30 20:36:51 | INFO | train_inner | epoch 021:    924 / 1349 loss=1.923, nll_loss=0.322, ppl=1.25, wps=65038.8, ups=16.59, wpb=3921.4, bsz=218.7, num_updates=27900, lr=0.000189321, gnorm=0.589, loss_scale=16, train_wall=6, gb_free=29.9, wall=1824
2024-01-30 20:36:57 | INFO | train_inner | epoch 021:   1024 / 1349 loss=1.929, nll_loss=0.329, ppl=1.26, wps=65297.9, ups=16.73, wpb=3902.6, bsz=216.4, num_updates=28000, lr=0.000188982, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.8, wall=1830
2024-01-30 20:37:03 | INFO | train_inner | epoch 021:   1124 / 1349 loss=1.919, nll_loss=0.319, ppl=1.25, wps=65679.2, ups=16.67, wpb=3940.7, bsz=217.2, num_updates=28100, lr=0.000188646, gnorm=0.57, loss_scale=16, train_wall=6, gb_free=29.9, wall=1836
2024-01-30 20:37:09 | INFO | train_inner | epoch 021:   1224 / 1349 loss=1.929, nll_loss=0.33, ppl=1.26, wps=65663.8, ups=16.56, wpb=3965.4, bsz=219.3, num_updates=28200, lr=0.000188311, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.8, wall=1842
2024-01-30 20:37:15 | INFO | train_inner | epoch 021:   1324 / 1349 loss=1.924, nll_loss=0.325, ppl=1.25, wps=65395.8, ups=16.66, wpb=3926, bsz=216.6, num_updates=28300, lr=0.000187978, gnorm=0.578, loss_scale=16, train_wall=6, gb_free=29.8, wall=1848
2024-01-30 20:37:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:37:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:37:18 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 2.063 | nll_loss 0.286 | ppl 1.22 | wps 173296 | wpb 3275.2 | bsz 176.5 | num_updates 28325 | best_loss 2.063
2024-01-30 20:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 28325 updates
2024-01-30 20:37:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint21.pt
2024-01-30 20:37:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint21.pt
2024-01-30 20:37:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint21.pt (epoch 21 @ 28325 updates, score 2.063) (writing took 4.879735675058328 seconds)
2024-01-30 20:37:23 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-01-30 20:37:23 | INFO | train | epoch 021 | loss 1.923 | nll_loss 0.322 | ppl 1.25 | wps 60341.9 | ups 15.39 | wpb 3921.2 | bsz 214.5 | num_updates 28325 | lr 0.000187895 | gnorm 0.552 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 1857
2024-01-30 20:37:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:37:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:37:23 | INFO | fairseq.trainer | begin training epoch 22
2024-01-30 20:37:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:37:28 | INFO | train_inner | epoch 022:     75 / 1349 loss=1.923, nll_loss=0.322, ppl=1.25, wps=30792.4, ups=7.85, wpb=3921.9, bsz=203.5, num_updates=28400, lr=0.000187647, gnorm=0.568, loss_scale=16, train_wall=6, gb_free=29.9, wall=1861
2024-01-30 20:37:34 | INFO | train_inner | epoch 022:    175 / 1349 loss=1.907, nll_loss=0.306, ppl=1.24, wps=64919.5, ups=16.56, wpb=3921.4, bsz=226.1, num_updates=28500, lr=0.000187317, gnorm=0.485, loss_scale=16, train_wall=6, gb_free=29.8, wall=1867
2024-01-30 20:37:40 | INFO | train_inner | epoch 022:    275 / 1349 loss=1.904, nll_loss=0.301, ppl=1.23, wps=65561, ups=16.63, wpb=3942.5, bsz=220.8, num_updates=28600, lr=0.000186989, gnorm=0.527, loss_scale=16, train_wall=6, gb_free=29.7, wall=1873
2024-01-30 20:37:46 | INFO | train_inner | epoch 022:    375 / 1349 loss=1.921, nll_loss=0.321, ppl=1.25, wps=65698.6, ups=16.66, wpb=3943.7, bsz=210.1, num_updates=28700, lr=0.000186663, gnorm=0.56, loss_scale=16, train_wall=6, gb_free=29.8, wall=1879
2024-01-30 20:37:52 | INFO | train_inner | epoch 022:    475 / 1349 loss=1.903, nll_loss=0.301, ppl=1.23, wps=65290.2, ups=16.62, wpb=3927.6, bsz=221.5, num_updates=28800, lr=0.000186339, gnorm=0.531, loss_scale=16, train_wall=6, gb_free=29.8, wall=1885
2024-01-30 20:37:58 | INFO | train_inner | epoch 022:    575 / 1349 loss=1.929, nll_loss=0.329, ppl=1.26, wps=65093.9, ups=16.72, wpb=3892.2, bsz=207.4, num_updates=28900, lr=0.000186016, gnorm=0.566, loss_scale=16, train_wall=6, gb_free=29.7, wall=1891
2024-01-30 20:38:04 | INFO | train_inner | epoch 022:    675 / 1349 loss=1.918, nll_loss=0.318, ppl=1.25, wps=65476, ups=16.6, wpb=3943.7, bsz=218.1, num_updates=29000, lr=0.000185695, gnorm=0.54, loss_scale=16, train_wall=6, gb_free=29.8, wall=1897
2024-01-30 20:38:10 | INFO | train_inner | epoch 022:    775 / 1349 loss=1.92, nll_loss=0.32, ppl=1.25, wps=66516.8, ups=16.73, wpb=3975.9, bsz=211.5, num_updates=29100, lr=0.000185376, gnorm=0.521, loss_scale=16, train_wall=6, gb_free=29.8, wall=1903
2024-01-30 20:38:16 | INFO | train_inner | epoch 022:    875 / 1349 loss=1.913, nll_loss=0.313, ppl=1.24, wps=65659.2, ups=16.65, wpb=3944.2, bsz=214.6, num_updates=29200, lr=0.000185058, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=1909
2024-01-30 20:38:22 | INFO | train_inner | epoch 022:    975 / 1349 loss=1.929, nll_loss=0.329, ppl=1.26, wps=65483.3, ups=16.79, wpb=3899.8, bsz=211, num_updates=29300, lr=0.000184742, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.7, wall=1915
2024-01-30 20:38:28 | INFO | train_inner | epoch 022:   1075 / 1349 loss=1.934, nll_loss=0.335, ppl=1.26, wps=65421.5, ups=16.79, wpb=3896.5, bsz=201, num_updates=29400, lr=0.000184428, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.7, wall=1921
2024-01-30 20:38:34 | INFO | train_inner | epoch 022:   1175 / 1349 loss=1.908, nll_loss=0.307, ppl=1.24, wps=65342.2, ups=16.53, wpb=3953.3, bsz=218.6, num_updates=29500, lr=0.000184115, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=1927
2024-01-30 20:38:40 | INFO | train_inner | epoch 022:   1275 / 1349 loss=1.914, nll_loss=0.314, ppl=1.24, wps=63910, ups=16.66, wpb=3836.9, bsz=218.2, num_updates=29600, lr=0.000183804, gnorm=0.528, loss_scale=16, train_wall=6, gb_free=29.8, wall=1933
2024-01-30 20:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:38:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:38:46 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 2.067 | nll_loss 0.285 | ppl 1.22 | wps 171592 | wpb 3275.2 | bsz 176.5 | num_updates 29674 | best_loss 2.063
2024-01-30 20:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 29674 updates
2024-01-30 20:38:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint22.pt
2024-01-30 20:38:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint22.pt
2024-01-30 20:38:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint22.pt (epoch 22 @ 29674 updates, score 2.067) (writing took 3.5489431039895862 seconds)
2024-01-30 20:38:50 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-01-30 20:38:50 | INFO | train | epoch 022 | loss 1.917 | nll_loss 0.316 | ppl 1.25 | wps 61230.5 | ups 15.62 | wpb 3921.2 | bsz 214.5 | num_updates 29674 | lr 0.000183574 | gnorm 0.529 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 1943
2024-01-30 20:38:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:38:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:38:50 | INFO | fairseq.trainer | begin training epoch 23
2024-01-30 20:38:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:38:51 | INFO | train_inner | epoch 023:     26 / 1349 loss=1.919, nll_loss=0.319, ppl=1.25, wps=34353.5, ups=8.75, wpb=3927.2, bsz=211.1, num_updates=29700, lr=0.000183494, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.8, wall=1945
2024-01-30 20:38:57 | INFO | train_inner | epoch 023:    126 / 1349 loss=1.927, nll_loss=0.327, ppl=1.25, wps=65716.4, ups=16.86, wpb=3898.2, bsz=216.3, num_updates=29800, lr=0.000183186, gnorm=0.561, loss_scale=16, train_wall=6, gb_free=29.9, wall=1951
2024-01-30 20:39:03 | INFO | train_inner | epoch 023:    226 / 1349 loss=1.907, nll_loss=0.304, ppl=1.23, wps=66072, ups=16.7, wpb=3957.4, bsz=204.4, num_updates=29900, lr=0.000182879, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.7, wall=1957
2024-01-30 20:39:09 | INFO | train_inner | epoch 023:    326 / 1349 loss=1.909, nll_loss=0.308, ppl=1.24, wps=65529.3, ups=16.59, wpb=3951, bsz=217.4, num_updates=30000, lr=0.000182574, gnorm=0.56, loss_scale=16, train_wall=6, gb_free=29.8, wall=1963
2024-01-30 20:39:15 | INFO | train_inner | epoch 023:    426 / 1349 loss=1.9, nll_loss=0.298, ppl=1.23, wps=65030.1, ups=16.55, wpb=3929.6, bsz=222, num_updates=30100, lr=0.000182271, gnorm=0.473, loss_scale=16, train_wall=6, gb_free=29.7, wall=1969
2024-01-30 20:39:21 | INFO | train_inner | epoch 023:    526 / 1349 loss=1.901, nll_loss=0.299, ppl=1.23, wps=65578.6, ups=16.58, wpb=3954.4, bsz=212.7, num_updates=30200, lr=0.000181969, gnorm=0.508, loss_scale=16, train_wall=6, gb_free=29.7, wall=1975
2024-01-30 20:39:27 | INFO | train_inner | epoch 023:    626 / 1349 loss=1.92, nll_loss=0.32, ppl=1.25, wps=64982.3, ups=16.72, wpb=3885.8, bsz=210.6, num_updates=30300, lr=0.000181668, gnorm=0.549, loss_scale=16, train_wall=6, gb_free=29.7, wall=1981
2024-01-30 20:39:33 | INFO | train_inner | epoch 023:    726 / 1349 loss=1.918, nll_loss=0.318, ppl=1.25, wps=65551, ups=16.72, wpb=3920.2, bsz=217.5, num_updates=30400, lr=0.000181369, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.8, wall=1987
2024-01-30 20:39:39 | INFO | train_inner | epoch 023:    826 / 1349 loss=1.916, nll_loss=0.316, ppl=1.24, wps=65162.9, ups=16.72, wpb=3898.4, bsz=201.9, num_updates=30500, lr=0.000181071, gnorm=0.526, loss_scale=16, train_wall=6, gb_free=29.8, wall=1993
2024-01-30 20:39:45 | INFO | train_inner | epoch 023:    926 / 1349 loss=1.923, nll_loss=0.323, ppl=1.25, wps=64841.6, ups=16.76, wpb=3869.1, bsz=204.4, num_updates=30600, lr=0.000180775, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.7, wall=1999
2024-01-30 20:39:51 | INFO | train_inner | epoch 023:   1026 / 1349 loss=1.906, nll_loss=0.305, ppl=1.24, wps=66008.8, ups=16.67, wpb=3960, bsz=212.2, num_updates=30700, lr=0.000180481, gnorm=0.481, loss_scale=16, train_wall=6, gb_free=29.8, wall=2005
2024-01-30 20:39:57 | INFO | train_inner | epoch 023:   1126 / 1349 loss=1.901, nll_loss=0.299, ppl=1.23, wps=65334, ups=16.6, wpb=3934.7, bsz=216, num_updates=30800, lr=0.000180187, gnorm=0.472, loss_scale=16, train_wall=6, gb_free=29.8, wall=2011
2024-01-30 20:40:03 | INFO | train_inner | epoch 023:   1226 / 1349 loss=1.897, nll_loss=0.296, ppl=1.23, wps=64610.7, ups=16.51, wpb=3912.5, bsz=235.7, num_updates=30900, lr=0.000179896, gnorm=0.534, loss_scale=16, train_wall=6, gb_free=30, wall=2017
2024-01-30 20:40:09 | INFO | train_inner | epoch 023:   1326 / 1349 loss=1.926, nll_loss=0.328, ppl=1.25, wps=64915.7, ups=16.75, wpb=3875.1, bsz=214.1, num_updates=31000, lr=0.000179605, gnorm=0.53, loss_scale=16, train_wall=6, gb_free=29.8, wall=2023
2024-01-30 20:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:40:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:40:12 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 2.063 | nll_loss 0.289 | ppl 1.22 | wps 173510 | wpb 3275.2 | bsz 176.5 | num_updates 31023 | best_loss 2.063
2024-01-30 20:40:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 31023 updates
2024-01-30 20:40:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint23.pt
2024-01-30 20:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint23.pt
2024-01-30 20:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint23.pt (epoch 23 @ 31023 updates, score 2.063) (writing took 4.838948416989297 seconds)
2024-01-30 20:40:17 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-01-30 20:40:17 | INFO | train | epoch 023 | loss 1.911 | nll_loss 0.311 | ppl 1.24 | wps 60337.6 | ups 15.39 | wpb 3921.2 | bsz 214.5 | num_updates 31023 | lr 0.000179539 | gnorm 0.516 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 2031
2024-01-30 20:40:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:40:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:40:17 | INFO | fairseq.trainer | begin training epoch 24
2024-01-30 20:40:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:40:22 | INFO | train_inner | epoch 024:     77 / 1349 loss=1.92, nll_loss=0.32, ppl=1.25, wps=30797.6, ups=7.88, wpb=3908.2, bsz=210.3, num_updates=31100, lr=0.000179316, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.9, wall=2035
2024-01-30 20:40:28 | INFO | train_inner | epoch 024:    177 / 1349 loss=1.916, nll_loss=0.315, ppl=1.24, wps=65655.5, ups=16.81, wpb=3905.9, bsz=204.5, num_updates=31200, lr=0.000179029, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=2041
2024-01-30 20:40:34 | INFO | train_inner | epoch 024:    277 / 1349 loss=1.903, nll_loss=0.302, ppl=1.23, wps=65505.2, ups=16.56, wpb=3955.9, bsz=215.4, num_updates=31300, lr=0.000178743, gnorm=0.508, loss_scale=16, train_wall=6, gb_free=29.7, wall=2047
2024-01-30 20:40:40 | INFO | train_inner | epoch 024:    377 / 1349 loss=1.903, nll_loss=0.302, ppl=1.23, wps=65641.7, ups=16.66, wpb=3939.7, bsz=213, num_updates=31400, lr=0.000178458, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.8, wall=2053
2024-01-30 20:40:46 | INFO | train_inner | epoch 024:    477 / 1349 loss=1.897, nll_loss=0.296, ppl=1.23, wps=65561.6, ups=16.62, wpb=3944.3, bsz=214.9, num_updates=31500, lr=0.000178174, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.7, wall=2059
2024-01-30 20:40:52 | INFO | train_inner | epoch 024:    577 / 1349 loss=1.904, nll_loss=0.303, ppl=1.23, wps=65277.9, ups=16.74, wpb=3898.7, bsz=209, num_updates=31600, lr=0.000177892, gnorm=0.549, loss_scale=16, train_wall=6, gb_free=29.7, wall=2065
2024-01-30 20:40:58 | INFO | train_inner | epoch 024:    677 / 1349 loss=1.91, nll_loss=0.31, ppl=1.24, wps=64883.8, ups=16.67, wpb=3892.8, bsz=219.4, num_updates=31700, lr=0.000177611, gnorm=0.572, loss_scale=16, train_wall=6, gb_free=29.8, wall=2071
2024-01-30 20:41:04 | INFO | train_inner | epoch 024:    777 / 1349 loss=1.909, nll_loss=0.308, ppl=1.24, wps=64788.2, ups=16.73, wpb=3872.2, bsz=210.8, num_updates=31800, lr=0.000177332, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=2077
2024-01-30 20:41:10 | INFO | train_inner | epoch 024:    877 / 1349 loss=1.907, nll_loss=0.307, ppl=1.24, wps=64688.3, ups=16.7, wpb=3872.7, bsz=211.8, num_updates=31900, lr=0.000177054, gnorm=0.568, loss_scale=16, train_wall=6, gb_free=29.8, wall=2083
2024-01-30 20:41:16 | INFO | train_inner | epoch 024:    977 / 1349 loss=1.895, nll_loss=0.294, ppl=1.23, wps=65463.1, ups=16.55, wpb=3955.2, bsz=216.1, num_updates=32000, lr=0.000176777, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=2089
2024-01-30 20:41:22 | INFO | train_inner | epoch 024:   1077 / 1349 loss=1.903, nll_loss=0.303, ppl=1.23, wps=65299.2, ups=16.51, wpb=3953.9, bsz=224.2, num_updates=32100, lr=0.000176501, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.8, wall=2095
2024-01-30 20:41:28 | INFO | train_inner | epoch 024:   1177 / 1349 loss=1.902, nll_loss=0.301, ppl=1.23, wps=65385.1, ups=16.58, wpb=3944.4, bsz=233.8, num_updates=32200, lr=0.000176227, gnorm=0.519, loss_scale=16, train_wall=6, gb_free=29.7, wall=2101
2024-01-30 20:41:34 | INFO | train_inner | epoch 024:   1277 / 1349 loss=1.91, nll_loss=0.31, ppl=1.24, wps=65685.5, ups=16.71, wpb=3930.4, bsz=223.5, num_updates=32300, lr=0.000175954, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=2107
2024-01-30 20:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:41:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:41:40 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 2.069 | nll_loss 0.293 | ppl 1.23 | wps 172027 | wpb 3275.2 | bsz 176.5 | num_updates 32372 | best_loss 2.063
2024-01-30 20:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 32372 updates
2024-01-30 20:41:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint24.pt
2024-01-30 20:41:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint24.pt
2024-01-30 20:41:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint24.pt (epoch 24 @ 32372 updates, score 2.069) (writing took 3.522900680080056 seconds)
2024-01-30 20:41:44 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-01-30 20:41:44 | INFO | train | epoch 024 | loss 1.907 | nll_loss 0.306 | ppl 1.24 | wps 61258.1 | ups 15.62 | wpb 3921.2 | bsz 214.5 | num_updates 32372 | lr 0.000175758 | gnorm 0.525 | loss_scale 16 | train_wall 78 | gb_free 29.9 | wall 2117
2024-01-30 20:41:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:41:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:41:44 | INFO | fairseq.trainer | begin training epoch 25
2024-01-30 20:41:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:41:46 | INFO | train_inner | epoch 025:     28 / 1349 loss=1.91, nll_loss=0.31, ppl=1.24, wps=34479.9, ups=8.77, wpb=3929.5, bsz=201.7, num_updates=32400, lr=0.000175682, gnorm=0.546, loss_scale=16, train_wall=6, gb_free=29.8, wall=2119
2024-01-30 20:41:52 | INFO | train_inner | epoch 025:    128 / 1349 loss=1.905, nll_loss=0.304, ppl=1.23, wps=65504.9, ups=16.66, wpb=3931.6, bsz=208.7, num_updates=32500, lr=0.000175412, gnorm=0.557, loss_scale=16, train_wall=6, gb_free=29.7, wall=2125
2024-01-30 20:41:58 | INFO | train_inner | epoch 025:    228 / 1349 loss=1.897, nll_loss=0.295, ppl=1.23, wps=65736.6, ups=16.67, wpb=3944.2, bsz=209, num_updates=32600, lr=0.000175142, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.8, wall=2131
2024-01-30 20:42:04 | INFO | train_inner | epoch 025:    328 / 1349 loss=1.906, nll_loss=0.306, ppl=1.24, wps=65570.9, ups=16.71, wpb=3924.3, bsz=217.2, num_updates=32700, lr=0.000174874, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=2137
2024-01-30 20:42:10 | INFO | train_inner | epoch 025:    428 / 1349 loss=1.902, nll_loss=0.301, ppl=1.23, wps=65318.8, ups=16.74, wpb=3902.2, bsz=205.9, num_updates=32800, lr=0.000174608, gnorm=0.524, loss_scale=16, train_wall=6, gb_free=29.8, wall=2143
2024-01-30 20:42:16 | INFO | train_inner | epoch 025:    528 / 1349 loss=1.891, nll_loss=0.29, ppl=1.22, wps=65059.2, ups=16.53, wpb=3936, bsz=236.2, num_updates=32900, lr=0.000174342, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=2149
2024-01-30 20:42:22 | INFO | train_inner | epoch 025:    628 / 1349 loss=1.912, nll_loss=0.312, ppl=1.24, wps=65986.1, ups=16.71, wpb=3949.9, bsz=202.1, num_updates=33000, lr=0.000174078, gnorm=0.567, loss_scale=16, train_wall=6, gb_free=29.8, wall=2155
2024-01-30 20:42:28 | INFO | train_inner | epoch 025:    728 / 1349 loss=1.907, nll_loss=0.307, ppl=1.24, wps=66139.2, ups=16.64, wpb=3974.9, bsz=217, num_updates=33100, lr=0.000173814, gnorm=0.549, loss_scale=16, train_wall=6, gb_free=29.8, wall=2161
2024-01-30 20:42:34 | INFO | train_inner | epoch 025:    828 / 1349 loss=1.895, nll_loss=0.294, ppl=1.23, wps=65398.8, ups=16.59, wpb=3941.8, bsz=213, num_updates=33200, lr=0.000173553, gnorm=0.567, loss_scale=16, train_wall=6, gb_free=29.7, wall=2167
2024-01-30 20:42:40 | INFO | train_inner | epoch 025:    928 / 1349 loss=1.9, nll_loss=0.3, ppl=1.23, wps=65644.2, ups=16.6, wpb=3955.1, bsz=211.1, num_updates=33300, lr=0.000173292, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=2173
2024-01-30 20:42:46 | INFO | train_inner | epoch 025:   1028 / 1349 loss=1.885, nll_loss=0.283, ppl=1.22, wps=65665, ups=16.52, wpb=3973.8, bsz=225.6, num_updates=33400, lr=0.000173032, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.8, wall=2179
2024-01-30 20:42:52 | INFO | train_inner | epoch 025:   1128 / 1349 loss=1.917, nll_loss=0.318, ppl=1.25, wps=63951.7, ups=16.84, wpb=3798.5, bsz=204.9, num_updates=33500, lr=0.000172774, gnorm=0.556, loss_scale=16, train_wall=6, gb_free=29.7, wall=2185
2024-01-30 20:42:58 | INFO | train_inner | epoch 025:   1228 / 1349 loss=1.916, nll_loss=0.317, ppl=1.25, wps=64804.3, ups=16.72, wpb=3876.3, bsz=219.9, num_updates=33600, lr=0.000172516, gnorm=0.594, loss_scale=16, train_wall=6, gb_free=29.8, wall=2191
2024-01-30 20:43:04 | INFO | train_inner | epoch 025:   1328 / 1349 loss=1.899, nll_loss=0.298, ppl=1.23, wps=64478.4, ups=16.67, wpb=3868.8, bsz=223, num_updates=33700, lr=0.00017226, gnorm=0.521, loss_scale=16, train_wall=6, gb_free=29.7, wall=2197
2024-01-30 20:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:43:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:43:06 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 2.066 | nll_loss 0.286 | ppl 1.22 | wps 172525 | wpb 3275.2 | bsz 176.5 | num_updates 33721 | best_loss 2.063
2024-01-30 20:43:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 33721 updates
2024-01-30 20:43:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint25.pt
2024-01-30 20:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint25.pt
2024-01-30 20:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint25.pt (epoch 25 @ 33721 updates, score 2.066) (writing took 3.619824890047312 seconds)
2024-01-30 20:43:10 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-01-30 20:43:10 | INFO | train | epoch 025 | loss 1.902 | nll_loss 0.302 | ppl 1.23 | wps 61179.1 | ups 15.6 | wpb 3921.2 | bsz 214.5 | num_updates 33721 | lr 0.000172207 | gnorm 0.531 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 2203
2024-01-30 20:43:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:43:10 | INFO | fairseq.trainer | begin training epoch 26
2024-01-30 20:43:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:43:15 | INFO | train_inner | epoch 026:     79 / 1349 loss=1.892, nll_loss=0.291, ppl=1.22, wps=34210.7, ups=8.69, wpb=3935.8, bsz=219.1, num_updates=33800, lr=0.000172005, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.9, wall=2208
2024-01-30 20:43:21 | INFO | train_inner | epoch 026:    179 / 1349 loss=1.911, nll_loss=0.312, ppl=1.24, wps=65440.1, ups=16.84, wpb=3885.7, bsz=202.6, num_updates=33900, lr=0.000171751, gnorm=0.547, loss_scale=16, train_wall=6, gb_free=29.7, wall=2214
2024-01-30 20:43:27 | INFO | train_inner | epoch 026:    279 / 1349 loss=1.893, nll_loss=0.291, ppl=1.22, wps=65746.6, ups=16.6, wpb=3960.2, bsz=232, num_updates=34000, lr=0.000171499, gnorm=0.478, loss_scale=16, train_wall=6, gb_free=29.8, wall=2220
2024-01-30 20:43:33 | INFO | train_inner | epoch 026:    379 / 1349 loss=1.891, nll_loss=0.29, ppl=1.22, wps=65736, ups=16.66, wpb=3946.4, bsz=218.2, num_updates=34100, lr=0.000171247, gnorm=0.495, loss_scale=16, train_wall=6, gb_free=29.8, wall=2226
2024-01-30 20:43:39 | INFO | train_inner | epoch 026:    479 / 1349 loss=1.893, nll_loss=0.292, ppl=1.22, wps=65240.8, ups=16.57, wpb=3936.4, bsz=214.6, num_updates=34200, lr=0.000170996, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.8, wall=2232
2024-01-30 20:43:45 | INFO | train_inner | epoch 026:    579 / 1349 loss=1.896, nll_loss=0.295, ppl=1.23, wps=65717.3, ups=16.72, wpb=3929.8, bsz=213.6, num_updates=34300, lr=0.000170747, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=2238
2024-01-30 20:43:51 | INFO | train_inner | epoch 026:    679 / 1349 loss=1.902, nll_loss=0.302, ppl=1.23, wps=65224.1, ups=16.72, wpb=3901, bsz=211, num_updates=34400, lr=0.000170499, gnorm=0.56, loss_scale=16, train_wall=6, gb_free=29.8, wall=2244
2024-01-30 20:43:57 | INFO | train_inner | epoch 026:    779 / 1349 loss=1.898, nll_loss=0.297, ppl=1.23, wps=64200.2, ups=16.66, wpb=3854.4, bsz=206.3, num_updates=34500, lr=0.000170251, gnorm=0.556, loss_scale=16, train_wall=6, gb_free=29.7, wall=2250
2024-01-30 20:44:06 | INFO | train_inner | epoch 026:    879 / 1349 loss=1.904, nll_loss=0.304, ppl=1.23, wps=45208.8, ups=11.49, wpb=3933.6, bsz=216.6, num_updates=34600, lr=0.000170005, gnorm=0.529, loss_scale=16, train_wall=8, gb_free=29.8, wall=2259
2024-01-30 20:44:12 | INFO | train_inner | epoch 026:    979 / 1349 loss=1.899, nll_loss=0.299, ppl=1.23, wps=65626.4, ups=16.63, wpb=3945.3, bsz=215.9, num_updates=34700, lr=0.00016976, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.7, wall=2265
2024-01-30 20:44:18 | INFO | train_inner | epoch 026:   1079 / 1349 loss=1.899, nll_loss=0.299, ppl=1.23, wps=65168.1, ups=16.61, wpb=3922.4, bsz=215, num_updates=34800, lr=0.000169516, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=2271
2024-01-30 20:44:24 | INFO | train_inner | epoch 026:   1179 / 1349 loss=1.893, nll_loss=0.292, ppl=1.22, wps=65274.5, ups=16.67, wpb=3915.1, bsz=211.3, num_updates=34900, lr=0.000169273, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=2277
2024-01-30 20:44:30 | INFO | train_inner | epoch 026:   1279 / 1349 loss=1.894, nll_loss=0.294, ppl=1.23, wps=65766.6, ups=16.61, wpb=3959.4, bsz=219.6, num_updates=35000, lr=0.000169031, gnorm=0.514, loss_scale=16, train_wall=6, gb_free=29.8, wall=2283
2024-01-30 20:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:44:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:44:36 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 2.067 | nll_loss 0.289 | ppl 1.22 | wps 171187 | wpb 3275.2 | bsz 176.5 | num_updates 35070 | best_loss 2.063
2024-01-30 20:44:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 35070 updates
2024-01-30 20:44:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint26.pt
2024-01-30 20:44:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint26.pt
2024-01-30 20:44:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint26.pt (epoch 26 @ 35070 updates, score 2.067) (writing took 3.5523588450159878 seconds)
2024-01-30 20:44:39 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-01-30 20:44:39 | INFO | train | epoch 026 | loss 1.898 | nll_loss 0.297 | ppl 1.23 | wps 59380.1 | ups 15.14 | wpb 3921.2 | bsz 214.5 | num_updates 35070 | lr 0.000168862 | gnorm 0.516 | loss_scale 16 | train_wall 81 | gb_free 29.7 | wall 2293
2024-01-30 20:44:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:44:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:44:39 | INFO | fairseq.trainer | begin training epoch 27
2024-01-30 20:44:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:44:41 | INFO | train_inner | epoch 027:     30 / 1349 loss=1.903, nll_loss=0.303, ppl=1.23, wps=33937.5, ups=8.74, wpb=3883.5, bsz=203.9, num_updates=35100, lr=0.00016879, gnorm=0.578, loss_scale=16, train_wall=6, gb_free=29.8, wall=2294
2024-01-30 20:44:47 | INFO | train_inner | epoch 027:    130 / 1349 loss=1.887, nll_loss=0.286, ppl=1.22, wps=65720.3, ups=16.61, wpb=3956.5, bsz=223.1, num_updates=35200, lr=0.00016855, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.9, wall=2300
2024-01-30 20:44:53 | INFO | train_inner | epoch 027:    230 / 1349 loss=1.89, nll_loss=0.289, ppl=1.22, wps=65712.5, ups=16.77, wpb=3918.1, bsz=205.4, num_updates=35300, lr=0.000168311, gnorm=0.579, loss_scale=16, train_wall=6, gb_free=29.7, wall=2306
2024-01-30 20:44:59 | INFO | train_inner | epoch 027:    330 / 1349 loss=1.896, nll_loss=0.296, ppl=1.23, wps=64719, ups=16.7, wpb=3875.9, bsz=213.8, num_updates=35400, lr=0.000168073, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=2312
2024-01-30 20:45:05 | INFO | train_inner | epoch 027:    430 / 1349 loss=1.884, nll_loss=0.282, ppl=1.22, wps=65530.8, ups=16.57, wpb=3955.8, bsz=219.9, num_updates=35500, lr=0.000167836, gnorm=0.509, loss_scale=16, train_wall=6, gb_free=29.7, wall=2318
2024-01-30 20:45:11 | INFO | train_inner | epoch 027:    530 / 1349 loss=1.888, nll_loss=0.287, ppl=1.22, wps=64820.2, ups=16.64, wpb=3894.3, bsz=213.6, num_updates=35600, lr=0.0001676, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.8, wall=2324
2024-01-30 20:45:17 | INFO | train_inner | epoch 027:    630 / 1349 loss=1.888, nll_loss=0.288, ppl=1.22, wps=65389.5, ups=16.68, wpb=3919.7, bsz=210.3, num_updates=35700, lr=0.000167365, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=29.8, wall=2330
2024-01-30 20:45:23 | INFO | train_inner | epoch 027:    730 / 1349 loss=1.886, nll_loss=0.286, ppl=1.22, wps=65195.5, ups=16.63, wpb=3921.1, bsz=214.3, num_updates=35800, lr=0.000167132, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.7, wall=2336
2024-01-30 20:45:29 | INFO | train_inner | epoch 027:    830 / 1349 loss=1.895, nll_loss=0.294, ppl=1.23, wps=65272.1, ups=16.59, wpb=3935.5, bsz=229.8, num_updates=35900, lr=0.000166899, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.8, wall=2343
2024-01-30 20:45:35 | INFO | train_inner | epoch 027:    930 / 1349 loss=1.901, nll_loss=0.302, ppl=1.23, wps=66292.9, ups=16.74, wpb=3961.2, bsz=220.5, num_updates=36000, lr=0.000166667, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=2348
2024-01-30 20:45:41 | INFO | train_inner | epoch 027:   1030 / 1349 loss=1.899, nll_loss=0.299, ppl=1.23, wps=65151.2, ups=16.71, wpb=3898.2, bsz=198.5, num_updates=36100, lr=0.000166436, gnorm=0.548, loss_scale=16, train_wall=6, gb_free=29.8, wall=2354
2024-01-30 20:45:47 | INFO | train_inner | epoch 027:   1130 / 1349 loss=1.904, nll_loss=0.305, ppl=1.24, wps=65101.5, ups=16.78, wpb=3880.2, bsz=212.3, num_updates=36200, lr=0.000166206, gnorm=0.521, loss_scale=16, train_wall=6, gb_free=29.8, wall=2360
2024-01-30 20:45:53 | INFO | train_inner | epoch 027:   1230 / 1349 loss=1.904, nll_loss=0.305, ppl=1.24, wps=64962, ups=16.65, wpb=3902.7, bsz=211.4, num_updates=36300, lr=0.000165977, gnorm=0.556, loss_scale=16, train_wall=6, gb_free=29.7, wall=2366
2024-01-30 20:45:59 | INFO | train_inner | epoch 027:   1330 / 1349 loss=1.904, nll_loss=0.305, ppl=1.24, wps=65713.2, ups=16.72, wpb=3930.7, bsz=209.8, num_updates=36400, lr=0.000165748, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.8, wall=2372
2024-01-30 20:46:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:46:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:46:02 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 2.07 | nll_loss 0.293 | ppl 1.22 | wps 172146 | wpb 3275.2 | bsz 176.5 | num_updates 36419 | best_loss 2.063
2024-01-30 20:46:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 36419 updates
2024-01-30 20:46:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint27.pt
2024-01-30 20:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint27.pt
2024-01-30 20:46:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint27.pt (epoch 27 @ 36419 updates, score 2.07) (writing took 2.3215003280201927 seconds)
2024-01-30 20:46:04 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-01-30 20:46:04 | INFO | train | epoch 027 | loss 1.894 | nll_loss 0.293 | ppl 1.23 | wps 62167.3 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 36419 | lr 0.000165705 | gnorm 0.531 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 2378
2024-01-30 20:46:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:46:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:46:05 | INFO | fairseq.trainer | begin training epoch 28
2024-01-30 20:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:46:09 | INFO | train_inner | epoch 028:     81 / 1349 loss=1.889, nll_loss=0.288, ppl=1.22, wps=38364.8, ups=9.79, wpb=3917.2, bsz=208.9, num_updates=36500, lr=0.000165521, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=2383
2024-01-30 20:46:15 | INFO | train_inner | epoch 028:    181 / 1349 loss=1.891, nll_loss=0.29, ppl=1.22, wps=64987, ups=16.74, wpb=3881.9, bsz=208.6, num_updates=36600, lr=0.000165295, gnorm=0.544, loss_scale=16, train_wall=6, gb_free=29.7, wall=2389
2024-01-30 20:46:21 | INFO | train_inner | epoch 028:    281 / 1349 loss=1.884, nll_loss=0.283, ppl=1.22, wps=66079.5, ups=16.69, wpb=3960.1, bsz=224.2, num_updates=36700, lr=0.00016507, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.9, wall=2395
2024-01-30 20:46:27 | INFO | train_inner | epoch 028:    381 / 1349 loss=1.89, nll_loss=0.29, ppl=1.22, wps=65367.8, ups=16.8, wpb=3891.1, bsz=212.1, num_updates=36800, lr=0.000164845, gnorm=0.56, loss_scale=16, train_wall=6, gb_free=29.8, wall=2401
2024-01-30 20:46:33 | INFO | train_inner | epoch 028:    481 / 1349 loss=1.879, nll_loss=0.277, ppl=1.21, wps=65043.7, ups=16.65, wpb=3905.4, bsz=203.8, num_updates=36900, lr=0.000164622, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.7, wall=2407
2024-01-30 20:46:39 | INFO | train_inner | epoch 028:    581 / 1349 loss=1.895, nll_loss=0.295, ppl=1.23, wps=64806.5, ups=16.76, wpb=3866.9, bsz=213.4, num_updates=37000, lr=0.000164399, gnorm=0.574, loss_scale=16, train_wall=6, gb_free=29.7, wall=2413
2024-01-30 20:46:45 | INFO | train_inner | epoch 028:    681 / 1349 loss=1.888, nll_loss=0.288, ppl=1.22, wps=65529.5, ups=16.72, wpb=3918.4, bsz=209.3, num_updates=37100, lr=0.000164177, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.8, wall=2419
2024-01-30 20:46:51 | INFO | train_inner | epoch 028:    781 / 1349 loss=1.897, nll_loss=0.297, ppl=1.23, wps=64645, ups=16.7, wpb=3871.6, bsz=204.1, num_updates=37200, lr=0.000163956, gnorm=0.553, loss_scale=16, train_wall=6, gb_free=29.8, wall=2424
2024-01-30 20:46:57 | INFO | train_inner | epoch 028:    881 / 1349 loss=1.892, nll_loss=0.291, ppl=1.22, wps=66065, ups=16.74, wpb=3947.3, bsz=214.2, num_updates=37300, lr=0.000163737, gnorm=0.526, loss_scale=16, train_wall=6, gb_free=29.8, wall=2430
2024-01-30 20:47:03 | INFO | train_inner | epoch 028:    981 / 1349 loss=1.889, nll_loss=0.289, ppl=1.22, wps=65277.7, ups=16.6, wpb=3932.3, bsz=213.3, num_updates=37400, lr=0.000163517, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.8, wall=2436
2024-01-30 20:47:09 | INFO | train_inner | epoch 028:   1081 / 1349 loss=1.888, nll_loss=0.287, ppl=1.22, wps=65637.3, ups=16.54, wpb=3968.6, bsz=227.6, num_updates=37500, lr=0.000163299, gnorm=0.483, loss_scale=16, train_wall=6, gb_free=29.8, wall=2443
2024-01-30 20:47:15 | INFO | train_inner | epoch 028:   1181 / 1349 loss=1.88, nll_loss=0.279, ppl=1.21, wps=65684.3, ups=16.49, wpb=3983.9, bsz=234.3, num_updates=37600, lr=0.000163082, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.8, wall=2449
2024-01-30 20:47:21 | INFO | train_inner | epoch 028:   1281 / 1349 loss=1.902, nll_loss=0.303, ppl=1.23, wps=65473.4, ups=16.63, wpb=3937.8, bsz=214, num_updates=37700, lr=0.000162866, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.8, wall=2455
2024-01-30 20:47:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:47:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:47:27 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 2.067 | nll_loss 0.299 | ppl 1.23 | wps 171416 | wpb 3275.2 | bsz 176.5 | num_updates 37768 | best_loss 2.063
2024-01-30 20:47:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 37768 updates
2024-01-30 20:47:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint28.pt
2024-01-30 20:47:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint28.pt
2024-01-30 20:47:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint28.pt (epoch 28 @ 37768 updates, score 2.067) (writing took 3.550300266011618 seconds)
2024-01-30 20:47:31 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-01-30 20:47:31 | INFO | train | epoch 028 | loss 1.889 | nll_loss 0.289 | ppl 1.22 | wps 61190.1 | ups 15.61 | wpb 3921.2 | bsz 214.5 | num_updates 37768 | lr 0.000162719 | gnorm 0.524 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 2464
2024-01-30 20:47:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:47:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:47:31 | INFO | fairseq.trainer | begin training epoch 29
2024-01-30 20:47:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:47:33 | INFO | train_inner | epoch 029:     32 / 1349 loss=1.882, nll_loss=0.281, ppl=1.22, wps=34136.1, ups=8.7, wpb=3922.5, bsz=220.5, num_updates=37800, lr=0.00016265, gnorm=0.495, loss_scale=16, train_wall=6, gb_free=29.8, wall=2466
2024-01-30 20:47:39 | INFO | train_inner | epoch 029:    132 / 1349 loss=1.886, nll_loss=0.285, ppl=1.22, wps=65232.7, ups=16.61, wpb=3926.5, bsz=221.4, num_updates=37900, lr=0.000162435, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=2472
2024-01-30 20:47:45 | INFO | train_inner | epoch 029:    232 / 1349 loss=1.887, nll_loss=0.286, ppl=1.22, wps=65496.4, ups=16.68, wpb=3927.6, bsz=212.7, num_updates=38000, lr=0.000162221, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.8, wall=2478
2024-01-30 20:47:51 | INFO | train_inner | epoch 029:    332 / 1349 loss=1.893, nll_loss=0.293, ppl=1.22, wps=65557.7, ups=16.8, wpb=3901.7, bsz=198.6, num_updates=38100, lr=0.000162008, gnorm=0.618, loss_scale=16, train_wall=6, gb_free=29.8, wall=2484
2024-01-30 20:47:57 | INFO | train_inner | epoch 029:    432 / 1349 loss=1.885, nll_loss=0.284, ppl=1.22, wps=65655, ups=16.71, wpb=3929.3, bsz=215.8, num_updates=38200, lr=0.000161796, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=2490
2024-01-30 20:48:03 | INFO | train_inner | epoch 029:    532 / 1349 loss=1.884, nll_loss=0.283, ppl=1.22, wps=66170.2, ups=16.62, wpb=3981.6, bsz=218.2, num_updates=38300, lr=0.000161585, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.7, wall=2496
2024-01-30 20:48:09 | INFO | train_inner | epoch 029:    632 / 1349 loss=1.884, nll_loss=0.284, ppl=1.22, wps=64502, ups=16.59, wpb=3888.3, bsz=222.6, num_updates=38400, lr=0.000161374, gnorm=0.566, loss_scale=16, train_wall=6, gb_free=29.8, wall=2502
2024-01-30 20:48:15 | INFO | train_inner | epoch 029:    732 / 1349 loss=1.883, nll_loss=0.282, ppl=1.22, wps=65392.5, ups=16.64, wpb=3929.4, bsz=222.7, num_updates=38500, lr=0.000161165, gnorm=0.555, loss_scale=16, train_wall=6, gb_free=29.7, wall=2508
2024-01-30 20:48:21 | INFO | train_inner | epoch 029:    832 / 1349 loss=1.886, nll_loss=0.286, ppl=1.22, wps=65553.8, ups=16.68, wpb=3930.7, bsz=220.6, num_updates=38600, lr=0.000160956, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=2514
2024-01-30 20:48:27 | INFO | train_inner | epoch 029:    932 / 1349 loss=1.885, nll_loss=0.285, ppl=1.22, wps=65079.9, ups=16.73, wpb=3890.9, bsz=214.5, num_updates=38700, lr=0.000160748, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.8, wall=2520
2024-01-30 20:48:33 | INFO | train_inner | epoch 029:   1032 / 1349 loss=1.892, nll_loss=0.292, ppl=1.22, wps=65142.2, ups=16.75, wpb=3888.8, bsz=199.4, num_updates=38800, lr=0.00016054, gnorm=0.563, loss_scale=16, train_wall=6, gb_free=29.7, wall=2526
2024-01-30 20:48:39 | INFO | train_inner | epoch 029:   1132 / 1349 loss=1.889, nll_loss=0.289, ppl=1.22, wps=64926.1, ups=16.71, wpb=3886.3, bsz=206.9, num_updates=38900, lr=0.000160334, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=2532
2024-01-30 20:48:45 | INFO | train_inner | epoch 029:   1232 / 1349 loss=1.882, nll_loss=0.282, ppl=1.22, wps=65931.8, ups=16.6, wpb=3973, bsz=225.4, num_updates=39000, lr=0.000160128, gnorm=0.508, loss_scale=32, train_wall=6, gb_free=29.7, wall=2538
2024-01-30 20:48:51 | INFO | train_inner | epoch 029:   1332 / 1349 loss=1.888, nll_loss=0.288, ppl=1.22, wps=65119.7, ups=16.63, wpb=3915.7, bsz=213.8, num_updates=39100, lr=0.000159923, gnorm=0.576, loss_scale=32, train_wall=6, gb_free=29.7, wall=2544
2024-01-30 20:48:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:48:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:48:54 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 2.068 | nll_loss 0.297 | ppl 1.23 | wps 173335 | wpb 3275.2 | bsz 176.5 | num_updates 39117 | best_loss 2.063
2024-01-30 20:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 39117 updates
2024-01-30 20:48:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint29.pt
2024-01-30 20:48:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint29.pt
2024-01-30 20:48:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint29.pt (epoch 29 @ 39117 updates, score 2.068) (writing took 2.2228531549917534 seconds)
2024-01-30 20:48:56 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-01-30 20:48:56 | INFO | train | epoch 029 | loss 1.886 | nll_loss 0.286 | ppl 1.22 | wps 62247.1 | ups 15.87 | wpb 3921.2 | bsz 214.5 | num_updates 39117 | lr 0.000159889 | gnorm 0.533 | loss_scale 32 | train_wall 78 | gb_free 29.8 | wall 2549
2024-01-30 20:48:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:48:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:48:56 | INFO | fairseq.trainer | begin training epoch 30
2024-01-30 20:48:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:49:01 | INFO | train_inner | epoch 030:     83 / 1349 loss=1.885, nll_loss=0.284, ppl=1.22, wps=38623.8, ups=9.99, wpb=3867.7, bsz=211.2, num_updates=39200, lr=0.000159719, gnorm=0.512, loss_scale=32, train_wall=6, gb_free=29.8, wall=2554
2024-01-30 20:49:07 | INFO | train_inner | epoch 030:    183 / 1349 loss=1.87, nll_loss=0.268, ppl=1.2, wps=65595, ups=16.53, wpb=3967.3, bsz=223.4, num_updates=39300, lr=0.000159516, gnorm=0.512, loss_scale=32, train_wall=6, gb_free=29.8, wall=2560
2024-01-30 20:49:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 20:49:13 | INFO | train_inner | epoch 030:    284 / 1349 loss=1.88, nll_loss=0.279, ppl=1.21, wps=65061.4, ups=16.58, wpb=3923.7, bsz=211.6, num_updates=39400, lr=0.000159313, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.8, wall=2566
2024-01-30 20:49:19 | INFO | train_inner | epoch 030:    384 / 1349 loss=1.874, nll_loss=0.272, ppl=1.21, wps=65011, ups=16.63, wpb=3908.4, bsz=218.5, num_updates=39500, lr=0.000159111, gnorm=0.468, loss_scale=16, train_wall=6, gb_free=29.7, wall=2572
2024-01-30 20:49:25 | INFO | train_inner | epoch 030:    484 / 1349 loss=1.883, nll_loss=0.282, ppl=1.22, wps=65552.4, ups=16.73, wpb=3918.4, bsz=208.8, num_updates=39600, lr=0.00015891, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=2578
2024-01-30 20:49:31 | INFO | train_inner | epoch 030:    584 / 1349 loss=1.884, nll_loss=0.284, ppl=1.22, wps=64901.7, ups=16.68, wpb=3891.6, bsz=218.8, num_updates=39700, lr=0.00015871, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=2584
2024-01-30 20:49:37 | INFO | train_inner | epoch 030:    684 / 1349 loss=1.876, nll_loss=0.275, ppl=1.21, wps=65114.3, ups=16.64, wpb=3912.3, bsz=221.4, num_updates=39800, lr=0.000158511, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.7, wall=2590
2024-01-30 20:49:43 | INFO | train_inner | epoch 030:    784 / 1349 loss=1.892, nll_loss=0.293, ppl=1.22, wps=65576.6, ups=16.73, wpb=3919.3, bsz=209.6, num_updates=39900, lr=0.000158312, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=2596
2024-01-30 20:49:49 | INFO | train_inner | epoch 030:    884 / 1349 loss=1.876, nll_loss=0.275, ppl=1.21, wps=65366.3, ups=16.58, wpb=3942.1, bsz=214.4, num_updates=40000, lr=0.000158114, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=2602
2024-01-30 20:49:55 | INFO | train_inner | epoch 030:    984 / 1349 loss=1.883, nll_loss=0.283, ppl=1.22, wps=66241.9, ups=16.7, wpb=3965.6, bsz=213.8, num_updates=40100, lr=0.000157917, gnorm=0.568, loss_scale=16, train_wall=6, gb_free=29.7, wall=2608
2024-01-30 20:50:01 | INFO | train_inner | epoch 030:   1084 / 1349 loss=1.887, nll_loss=0.288, ppl=1.22, wps=65638.9, ups=16.77, wpb=3913, bsz=205, num_updates=40200, lr=0.00015772, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=2614
2024-01-30 20:50:07 | INFO | train_inner | epoch 030:   1184 / 1349 loss=1.885, nll_loss=0.285, ppl=1.22, wps=65370, ups=16.6, wpb=3938.5, bsz=214.6, num_updates=40300, lr=0.000157524, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=2620
2024-01-30 20:50:13 | INFO | train_inner | epoch 030:   1284 / 1349 loss=1.888, nll_loss=0.288, ppl=1.22, wps=65583.5, ups=16.7, wpb=3926.7, bsz=212.8, num_updates=40400, lr=0.000157329, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=2626
2024-01-30 20:50:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:50:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:50:18 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 2.068 | nll_loss 0.301 | ppl 1.23 | wps 173482 | wpb 3275.2 | bsz 176.5 | num_updates 40465 | best_loss 2.063
2024-01-30 20:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 40465 updates
2024-01-30 20:50:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint30.pt
2024-01-30 20:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint30.pt
2024-01-30 20:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint30.pt (epoch 30 @ 40465 updates, score 2.068) (writing took 2.273860401008278 seconds)
2024-01-30 20:50:21 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-01-30 20:50:21 | INFO | train | epoch 030 | loss 1.882 | nll_loss 0.282 | ppl 1.22 | wps 62217.7 | ups 15.87 | wpb 3921.4 | bsz 214.5 | num_updates 40465 | lr 0.000157203 | gnorm 0.51 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 2634
2024-01-30 20:50:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:50:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:50:21 | INFO | fairseq.trainer | begin training epoch 31
2024-01-30 20:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:50:23 | INFO | train_inner | epoch 031:     35 / 1349 loss=1.886, nll_loss=0.287, ppl=1.22, wps=38964.8, ups=9.92, wpb=3926.1, bsz=217.6, num_updates=40500, lr=0.000157135, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.7, wall=2636
2024-01-30 20:50:29 | INFO | train_inner | epoch 031:    135 / 1349 loss=1.864, nll_loss=0.261, ppl=1.2, wps=65353.6, ups=16.64, wpb=3927.4, bsz=223.4, num_updates=40600, lr=0.000156941, gnorm=0.464, loss_scale=16, train_wall=6, gb_free=29.8, wall=2642
2024-01-30 20:50:35 | INFO | train_inner | epoch 031:    235 / 1349 loss=1.879, nll_loss=0.278, ppl=1.21, wps=65428.9, ups=16.65, wpb=3928.6, bsz=210.8, num_updates=40700, lr=0.000156748, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.9, wall=2648
2024-01-30 20:50:41 | INFO | train_inner | epoch 031:    335 / 1349 loss=1.883, nll_loss=0.282, ppl=1.22, wps=65869.3, ups=16.7, wpb=3945.3, bsz=215.1, num_updates=40800, lr=0.000156556, gnorm=0.559, loss_scale=16, train_wall=6, gb_free=29.7, wall=2654
2024-01-30 20:50:47 | INFO | train_inner | epoch 031:    435 / 1349 loss=1.886, nll_loss=0.286, ppl=1.22, wps=65411.4, ups=16.81, wpb=3890.9, bsz=215.1, num_updates=40900, lr=0.000156365, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.8, wall=2660
2024-01-30 20:50:53 | INFO | train_inner | epoch 031:    535 / 1349 loss=1.877, nll_loss=0.277, ppl=1.21, wps=65641.5, ups=16.64, wpb=3945.6, bsz=217.3, num_updates=41000, lr=0.000156174, gnorm=0.519, loss_scale=16, train_wall=6, gb_free=29.8, wall=2666
2024-01-30 20:50:59 | INFO | train_inner | epoch 031:    635 / 1349 loss=1.88, nll_loss=0.28, ppl=1.21, wps=65337.4, ups=16.6, wpb=3937, bsz=217, num_updates=41100, lr=0.000155984, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=2672
2024-01-30 20:51:05 | INFO | train_inner | epoch 031:    735 / 1349 loss=1.876, nll_loss=0.275, ppl=1.21, wps=64830.6, ups=16.67, wpb=3888.8, bsz=204.3, num_updates=41200, lr=0.000155794, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.8, wall=2678
2024-01-30 20:51:11 | INFO | train_inner | epoch 031:    835 / 1349 loss=1.879, nll_loss=0.278, ppl=1.21, wps=64695.1, ups=16.69, wpb=3875.4, bsz=218, num_updates=41300, lr=0.000155606, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.8, wall=2684
2024-01-30 20:51:17 | INFO | train_inner | epoch 031:    935 / 1349 loss=1.879, nll_loss=0.279, ppl=1.21, wps=65511.4, ups=16.71, wpb=3920.2, bsz=207.4, num_updates=41400, lr=0.000155417, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.7, wall=2690
2024-01-30 20:51:23 | INFO | train_inner | epoch 031:   1035 / 1349 loss=1.885, nll_loss=0.286, ppl=1.22, wps=65887, ups=16.74, wpb=3935.7, bsz=209.4, num_updates=41500, lr=0.00015523, gnorm=0.531, loss_scale=16, train_wall=6, gb_free=29.7, wall=2696
2024-01-30 20:51:29 | INFO | train_inner | epoch 031:   1135 / 1349 loss=1.884, nll_loss=0.285, ppl=1.22, wps=65303.5, ups=16.7, wpb=3911.5, bsz=213.4, num_updates=41600, lr=0.000155043, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.8, wall=2702
2024-01-30 20:51:35 | INFO | train_inner | epoch 031:   1235 / 1349 loss=1.878, nll_loss=0.278, ppl=1.21, wps=65252.2, ups=16.61, wpb=3927.7, bsz=211.5, num_updates=41700, lr=0.000154857, gnorm=0.474, loss_scale=16, train_wall=6, gb_free=29.8, wall=2708
2024-01-30 20:51:41 | INFO | train_inner | epoch 031:   1335 / 1349 loss=1.874, nll_loss=0.274, ppl=1.21, wps=65388.3, ups=16.62, wpb=3935.2, bsz=223.1, num_updates=41800, lr=0.000154672, gnorm=0.459, loss_scale=16, train_wall=6, gb_free=29.8, wall=2714
2024-01-30 20:51:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:51:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:51:43 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 2.07 | nll_loss 0.298 | ppl 1.23 | wps 172063 | wpb 3275.2 | bsz 176.5 | num_updates 41814 | best_loss 2.063
2024-01-30 20:51:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 41814 updates
2024-01-30 20:51:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint31.pt
2024-01-30 20:51:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint31.pt
2024-01-30 20:51:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint31.pt (epoch 31 @ 41814 updates, score 2.07) (writing took 2.260376666090451 seconds)
2024-01-30 20:51:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-01-30 20:51:46 | INFO | train | epoch 031 | loss 1.879 | nll_loss 0.278 | ppl 1.21 | wps 62227.2 | ups 15.87 | wpb 3921.2 | bsz 214.5 | num_updates 41814 | lr 0.000154646 | gnorm 0.508 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 2719
2024-01-30 20:51:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:51:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:51:46 | INFO | fairseq.trainer | begin training epoch 32
2024-01-30 20:51:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:51:51 | INFO | train_inner | epoch 032:     86 / 1349 loss=1.869, nll_loss=0.268, ppl=1.2, wps=38975.9, ups=9.91, wpb=3932.3, bsz=223.8, num_updates=41900, lr=0.000154487, gnorm=0.468, loss_scale=16, train_wall=6, gb_free=29.7, wall=2724
2024-01-30 20:51:59 | INFO | train_inner | epoch 032:    186 / 1349 loss=1.883, nll_loss=0.283, ppl=1.22, wps=46746.5, ups=12.06, wpb=3877.5, bsz=207, num_updates=42000, lr=0.000154303, gnorm=0.545, loss_scale=16, train_wall=8, gb_free=29.8, wall=2733
2024-01-30 20:52:06 | INFO | train_inner | epoch 032:    286 / 1349 loss=1.869, nll_loss=0.268, ppl=1.2, wps=62091.8, ups=15.81, wpb=3926.6, bsz=217.4, num_updates=42100, lr=0.00015412, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=2739
2024-01-30 20:52:12 | INFO | train_inner | epoch 032:    386 / 1349 loss=1.876, nll_loss=0.275, ppl=1.21, wps=65748.9, ups=16.7, wpb=3938.1, bsz=210.3, num_updates=42200, lr=0.000153937, gnorm=0.524, loss_scale=16, train_wall=6, gb_free=29.8, wall=2745
2024-01-30 20:52:18 | INFO | train_inner | epoch 032:    486 / 1349 loss=1.863, nll_loss=0.262, ppl=1.2, wps=65663.9, ups=16.67, wpb=3939.6, bsz=212.5, num_updates=42300, lr=0.000153755, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=2751
2024-01-30 20:52:24 | INFO | train_inner | epoch 032:    586 / 1349 loss=1.883, nll_loss=0.283, ppl=1.22, wps=65617.3, ups=16.78, wpb=3910.9, bsz=208, num_updates=42400, lr=0.000153574, gnorm=0.582, loss_scale=16, train_wall=6, gb_free=29.8, wall=2757
2024-01-30 20:52:30 | INFO | train_inner | epoch 032:    686 / 1349 loss=1.881, nll_loss=0.281, ppl=1.22, wps=65449.1, ups=16.64, wpb=3933.8, bsz=209.6, num_updates=42500, lr=0.000153393, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.7, wall=2763
2024-01-30 20:52:36 | INFO | train_inner | epoch 032:    786 / 1349 loss=1.869, nll_loss=0.268, ppl=1.2, wps=64661.4, ups=16.58, wpb=3900.9, bsz=215.3, num_updates=42600, lr=0.000153213, gnorm=0.551, loss_scale=16, train_wall=6, gb_free=29.7, wall=2769
2024-01-30 20:52:42 | INFO | train_inner | epoch 032:    886 / 1349 loss=1.883, nll_loss=0.283, ppl=1.22, wps=65819.9, ups=16.69, wpb=3943.7, bsz=201.9, num_updates=42700, lr=0.000153033, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.8, wall=2775
2024-01-30 20:52:48 | INFO | train_inner | epoch 032:    986 / 1349 loss=1.881, nll_loss=0.281, ppl=1.21, wps=65377.2, ups=16.73, wpb=3907.3, bsz=208.7, num_updates=42800, lr=0.000152854, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=2781
2024-01-30 20:52:54 | INFO | train_inner | epoch 032:   1086 / 1349 loss=1.879, nll_loss=0.279, ppl=1.21, wps=65413.5, ups=16.63, wpb=3932.8, bsz=219.4, num_updates=42900, lr=0.000152676, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.8, wall=2787
2024-01-30 20:53:00 | INFO | train_inner | epoch 032:   1186 / 1349 loss=1.885, nll_loss=0.286, ppl=1.22, wps=65476.3, ups=16.79, wpb=3900.3, bsz=224, num_updates=43000, lr=0.000152499, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=2793
2024-01-30 20:53:06 | INFO | train_inner | epoch 032:   1286 / 1349 loss=1.874, nll_loss=0.273, ppl=1.21, wps=65137.2, ups=16.64, wpb=3915.6, bsz=215.5, num_updates=43100, lr=0.000152322, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=2799
2024-01-30 20:53:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:53:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:53:11 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 2.076 | nll_loss 0.309 | ppl 1.24 | wps 173415 | wpb 3275.2 | bsz 176.5 | num_updates 43163 | best_loss 2.063
2024-01-30 20:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 43163 updates
2024-01-30 20:53:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint32.pt
2024-01-30 20:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint32.pt
2024-01-30 20:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint32.pt (epoch 32 @ 43163 updates, score 2.076) (writing took 2.486173066077754 seconds)
2024-01-30 20:53:14 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-01-30 20:53:14 | INFO | train | epoch 032 | loss 1.876 | nll_loss 0.275 | ppl 1.21 | wps 60168.3 | ups 15.34 | wpb 3921.2 | bsz 214.5 | num_updates 43163 | lr 0.00015221 | gnorm 0.514 | loss_scale 16 | train_wall 80 | gb_free 29.7 | wall 2807
2024-01-30 20:53:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:53:14 | INFO | fairseq.trainer | begin training epoch 33
2024-01-30 20:53:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:53:16 | INFO | train_inner | epoch 033:     37 / 1349 loss=1.866, nll_loss=0.265, ppl=1.2, wps=37895.1, ups=9.58, wpb=3954.4, bsz=228.8, num_updates=43200, lr=0.000152145, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.8, wall=2809
2024-01-30 20:53:22 | INFO | train_inner | epoch 033:    137 / 1349 loss=1.86, nll_loss=0.259, ppl=1.2, wps=64943.4, ups=16.56, wpb=3922.3, bsz=227, num_updates=43300, lr=0.000151969, gnorm=0.435, loss_scale=16, train_wall=6, gb_free=29.8, wall=2815
2024-01-30 20:53:28 | INFO | train_inner | epoch 033:    237 / 1349 loss=1.867, nll_loss=0.265, ppl=1.2, wps=64547.7, ups=16.74, wpb=3855.3, bsz=203.8, num_updates=43400, lr=0.000151794, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.8, wall=2821
2024-01-30 20:53:34 | INFO | train_inner | epoch 033:    337 / 1349 loss=1.875, nll_loss=0.274, ppl=1.21, wps=65513.1, ups=16.69, wpb=3925.9, bsz=202.6, num_updates=43500, lr=0.00015162, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=2827
2024-01-30 20:53:40 | INFO | train_inner | epoch 033:    437 / 1349 loss=1.881, nll_loss=0.281, ppl=1.21, wps=65104.9, ups=16.7, wpb=3899.3, bsz=220.6, num_updates=43600, lr=0.000151446, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=2833
2024-01-30 20:53:46 | INFO | train_inner | epoch 033:    537 / 1349 loss=1.86, nll_loss=0.259, ppl=1.2, wps=65756.9, ups=16.54, wpb=3974.5, bsz=219.1, num_updates=43700, lr=0.000151272, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.8, wall=2839
2024-01-30 20:53:52 | INFO | train_inner | epoch 033:    637 / 1349 loss=1.877, nll_loss=0.278, ppl=1.21, wps=65856.4, ups=16.64, wpb=3958.1, bsz=212, num_updates=43800, lr=0.000151099, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.9, wall=2845
2024-01-30 20:53:58 | INFO | train_inner | epoch 033:    737 / 1349 loss=1.861, nll_loss=0.26, ppl=1.2, wps=66050.7, ups=16.6, wpb=3979.5, bsz=219.3, num_updates=43900, lr=0.000150927, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=2851
2024-01-30 20:54:04 | INFO | train_inner | epoch 033:    837 / 1349 loss=1.882, nll_loss=0.283, ppl=1.22, wps=66047, ups=16.79, wpb=3932.8, bsz=202.6, num_updates=44000, lr=0.000150756, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=2857
2024-01-30 20:54:10 | INFO | train_inner | epoch 033:    937 / 1349 loss=1.868, nll_loss=0.268, ppl=1.2, wps=64141.7, ups=16.5, wpb=3886.5, bsz=217.6, num_updates=44100, lr=0.000150585, gnorm=0.442, loss_scale=16, train_wall=6, gb_free=29.7, wall=2863
2024-01-30 20:54:16 | INFO | train_inner | epoch 033:   1037 / 1349 loss=1.879, nll_loss=0.28, ppl=1.21, wps=65192.7, ups=16.74, wpb=3893.9, bsz=222.2, num_updates=44200, lr=0.000150414, gnorm=0.514, loss_scale=16, train_wall=6, gb_free=29.8, wall=2869
2024-01-30 20:54:22 | INFO | train_inner | epoch 033:   1137 / 1349 loss=1.88, nll_loss=0.28, ppl=1.21, wps=65024, ups=16.7, wpb=3893, bsz=214.6, num_updates=44300, lr=0.000150244, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=2875
2024-01-30 20:54:28 | INFO | train_inner | epoch 033:   1237 / 1349 loss=1.873, nll_loss=0.274, ppl=1.21, wps=65512.4, ups=16.71, wpb=3921.3, bsz=206.4, num_updates=44400, lr=0.000150075, gnorm=0.551, loss_scale=16, train_wall=6, gb_free=29.9, wall=2881
2024-01-30 20:54:34 | INFO | train_inner | epoch 033:   1337 / 1349 loss=1.882, nll_loss=0.283, ppl=1.22, wps=65341.5, ups=16.71, wpb=3910.3, bsz=222.2, num_updates=44500, lr=0.000149906, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.9, wall=2887
2024-01-30 20:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:54:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:54:36 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 2.071 | nll_loss 0.303 | ppl 1.23 | wps 173176 | wpb 3275.2 | bsz 176.5 | num_updates 44512 | best_loss 2.063
2024-01-30 20:54:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 44512 updates
2024-01-30 20:54:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint33.pt
2024-01-30 20:54:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint33.pt
2024-01-30 20:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint33.pt (epoch 33 @ 44512 updates, score 2.071) (writing took 2.2934544489253312 seconds)
2024-01-30 20:54:39 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-01-30 20:54:39 | INFO | train | epoch 033 | loss 1.873 | nll_loss 0.272 | ppl 1.21 | wps 62149.1 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 44512 | lr 0.000149886 | gnorm 0.5 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 2892
2024-01-30 20:54:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:54:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:54:39 | INFO | fairseq.trainer | begin training epoch 34
2024-01-30 20:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:54:44 | INFO | train_inner | epoch 034:     88 / 1349 loss=1.858, nll_loss=0.256, ppl=1.19, wps=39116.8, ups=9.88, wpb=3957.6, bsz=212.2, num_updates=44600, lr=0.000149738, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.8, wall=2897
2024-01-30 20:54:50 | INFO | train_inner | epoch 034:    188 / 1349 loss=1.872, nll_loss=0.271, ppl=1.21, wps=66075.4, ups=16.72, wpb=3950.9, bsz=208.2, num_updates=44700, lr=0.000149571, gnorm=0.561, loss_scale=16, train_wall=6, gb_free=29.9, wall=2903
2024-01-30 20:54:56 | INFO | train_inner | epoch 034:    288 / 1349 loss=1.861, nll_loss=0.26, ppl=1.2, wps=65224.4, ups=16.56, wpb=3938.9, bsz=226.8, num_updates=44800, lr=0.000149404, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.7, wall=2909
2024-01-30 20:55:02 | INFO | train_inner | epoch 034:    388 / 1349 loss=1.865, nll_loss=0.264, ppl=1.2, wps=65338.2, ups=16.64, wpb=3927.6, bsz=218.9, num_updates=44900, lr=0.000149237, gnorm=0.469, loss_scale=16, train_wall=6, gb_free=29.7, wall=2915
2024-01-30 20:55:08 | INFO | train_inner | epoch 034:    488 / 1349 loss=1.859, nll_loss=0.257, ppl=1.2, wps=65377.4, ups=16.53, wpb=3954.5, bsz=219, num_updates=45000, lr=0.000149071, gnorm=0.46, loss_scale=16, train_wall=6, gb_free=29.8, wall=2921
2024-01-30 20:55:14 | INFO | train_inner | epoch 034:    588 / 1349 loss=1.869, nll_loss=0.269, ppl=1.2, wps=65241.4, ups=16.63, wpb=3922.9, bsz=217.8, num_updates=45100, lr=0.000148906, gnorm=0.539, loss_scale=16, train_wall=6, gb_free=29.8, wall=2928
2024-01-30 20:55:20 | INFO | train_inner | epoch 034:    688 / 1349 loss=1.879, nll_loss=0.28, ppl=1.21, wps=64819.6, ups=16.73, wpb=3874.4, bsz=214.2, num_updates=45200, lr=0.000148741, gnorm=0.517, loss_scale=16, train_wall=6, gb_free=29.7, wall=2933
2024-01-30 20:55:26 | INFO | train_inner | epoch 034:    788 / 1349 loss=1.875, nll_loss=0.275, ppl=1.21, wps=65828.8, ups=16.69, wpb=3944.1, bsz=200.6, num_updates=45300, lr=0.000148577, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.7, wall=2939
2024-01-30 20:55:32 | INFO | train_inner | epoch 034:    888 / 1349 loss=1.875, nll_loss=0.275, ppl=1.21, wps=63899.9, ups=16.62, wpb=3843.8, bsz=221.9, num_updates=45400, lr=0.000148413, gnorm=0.537, loss_scale=16, train_wall=6, gb_free=29.8, wall=2945
2024-01-30 20:55:38 | INFO | train_inner | epoch 034:    988 / 1349 loss=1.868, nll_loss=0.268, ppl=1.2, wps=65444.1, ups=16.49, wpb=3967.7, bsz=231.5, num_updates=45500, lr=0.00014825, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=2952
2024-01-30 20:55:44 | INFO | train_inner | epoch 034:   1088 / 1349 loss=1.868, nll_loss=0.268, ppl=1.2, wps=64831.7, ups=16.7, wpb=3881.5, bsz=220.1, num_updates=45600, lr=0.000148087, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=2958
2024-01-30 20:55:50 | INFO | train_inner | epoch 034:   1188 / 1349 loss=1.873, nll_loss=0.273, ppl=1.21, wps=64637.1, ups=16.73, wpb=3864.3, bsz=208.8, num_updates=45700, lr=0.000147925, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=29.8, wall=2964
2024-01-30 20:55:56 | INFO | train_inner | epoch 034:   1288 / 1349 loss=1.875, nll_loss=0.276, ppl=1.21, wps=65804.5, ups=16.69, wpb=3943.3, bsz=206.5, num_updates=45800, lr=0.000147764, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.8, wall=2970
2024-01-30 20:56:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:56:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:56:02 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 2.072 | nll_loss 0.307 | ppl 1.24 | wps 172601 | wpb 3275.2 | bsz 176.5 | num_updates 45861 | best_loss 2.063
2024-01-30 20:56:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 45861 updates
2024-01-30 20:56:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint34.pt
2024-01-30 20:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint34.pt
2024-01-30 20:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint34.pt (epoch 34 @ 45861 updates, score 2.072) (writing took 2.2382788229733706 seconds)
2024-01-30 20:56:04 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-01-30 20:56:04 | INFO | train | epoch 034 | loss 1.87 | nll_loss 0.27 | ppl 1.21 | wps 62177.2 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 45861 | lr 0.000147665 | gnorm 0.513 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 2977
2024-01-30 20:56:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:56:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:56:04 | INFO | fairseq.trainer | begin training epoch 35
2024-01-30 20:56:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:56:06 | INFO | train_inner | epoch 035:     39 / 1349 loss=1.878, nll_loss=0.278, ppl=1.21, wps=38975.5, ups=9.95, wpb=3915.5, bsz=203.4, num_updates=45900, lr=0.000147602, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.7, wall=2980
2024-01-30 20:56:12 | INFO | train_inner | epoch 035:    139 / 1349 loss=1.854, nll_loss=0.252, ppl=1.19, wps=65711.2, ups=16.53, wpb=3975.2, bsz=219.4, num_updates=46000, lr=0.000147442, gnorm=0.434, loss_scale=16, train_wall=6, gb_free=29.7, wall=2986
2024-01-30 20:56:18 | INFO | train_inner | epoch 035:    239 / 1349 loss=1.867, nll_loss=0.266, ppl=1.2, wps=65335.6, ups=16.67, wpb=3920.4, bsz=205.8, num_updates=46100, lr=0.000147282, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.9, wall=2992
2024-01-30 20:56:24 | INFO | train_inner | epoch 035:    339 / 1349 loss=1.861, nll_loss=0.26, ppl=1.2, wps=64649.7, ups=16.66, wpb=3881, bsz=213.4, num_updates=46200, lr=0.000147122, gnorm=0.552, loss_scale=16, train_wall=6, gb_free=29.8, wall=2998
2024-01-30 20:56:30 | INFO | train_inner | epoch 035:    439 / 1349 loss=1.873, nll_loss=0.273, ppl=1.21, wps=66111.2, ups=16.74, wpb=3949.2, bsz=215, num_updates=46300, lr=0.000146964, gnorm=0.517, loss_scale=16, train_wall=6, gb_free=29.8, wall=3004
2024-01-30 20:56:36 | INFO | train_inner | epoch 035:    539 / 1349 loss=1.859, nll_loss=0.258, ppl=1.2, wps=65095.9, ups=16.49, wpb=3947, bsz=222.9, num_updates=46400, lr=0.000146805, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=3010
2024-01-30 20:56:42 | INFO | train_inner | epoch 035:    639 / 1349 loss=1.876, nll_loss=0.276, ppl=1.21, wps=65084.7, ups=16.73, wpb=3889.1, bsz=212.3, num_updates=46500, lr=0.000146647, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.8, wall=3016
2024-01-30 20:56:48 | INFO | train_inner | epoch 035:    739 / 1349 loss=1.868, nll_loss=0.268, ppl=1.2, wps=65769.6, ups=16.65, wpb=3949.8, bsz=209.5, num_updates=46600, lr=0.00014649, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=3022
2024-01-30 20:56:54 | INFO | train_inner | epoch 035:    839 / 1349 loss=1.864, nll_loss=0.264, ppl=1.2, wps=65203.8, ups=16.66, wpb=3914.9, bsz=214.6, num_updates=46700, lr=0.000146333, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=3028
2024-01-30 20:57:00 | INFO | train_inner | epoch 035:    939 / 1349 loss=1.866, nll_loss=0.265, ppl=1.2, wps=65708.2, ups=16.57, wpb=3964.5, bsz=223.3, num_updates=46800, lr=0.000146176, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.8, wall=3034
2024-01-30 20:57:06 | INFO | train_inner | epoch 035:   1039 / 1349 loss=1.864, nll_loss=0.264, ppl=1.2, wps=64448.5, ups=16.63, wpb=3874.3, bsz=215.8, num_updates=46900, lr=0.00014602, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.8, wall=3040
2024-01-30 20:57:12 | INFO | train_inner | epoch 035:   1139 / 1349 loss=1.872, nll_loss=0.273, ppl=1.21, wps=64971, ups=16.7, wpb=3890.4, bsz=207.7, num_updates=47000, lr=0.000145865, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.8, wall=3046
2024-01-30 20:57:18 | INFO | train_inner | epoch 035:   1239 / 1349 loss=1.876, nll_loss=0.276, ppl=1.21, wps=65231.5, ups=16.78, wpb=3888, bsz=211.7, num_updates=47100, lr=0.00014571, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.8, wall=3052
2024-01-30 20:57:24 | INFO | train_inner | epoch 035:   1339 / 1349 loss=1.871, nll_loss=0.272, ppl=1.21, wps=65948.4, ups=16.75, wpb=3938.2, bsz=211.8, num_updates=47200, lr=0.000145556, gnorm=0.544, loss_scale=16, train_wall=6, gb_free=29.7, wall=3058
2024-01-30 20:57:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:57:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:57:27 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 2.077 | nll_loss 0.315 | ppl 1.24 | wps 172747 | wpb 3275.2 | bsz 176.5 | num_updates 47210 | best_loss 2.063
2024-01-30 20:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 47210 updates
2024-01-30 20:57:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint35.pt
2024-01-30 20:57:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint35.pt
2024-01-30 20:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint35.pt (epoch 35 @ 47210 updates, score 2.077) (writing took 2.339885000954382 seconds)
2024-01-30 20:57:29 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-01-30 20:57:29 | INFO | train | epoch 035 | loss 1.867 | nll_loss 0.267 | ppl 1.2 | wps 62107.8 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 47210 | lr 0.00014554 | gnorm 0.51 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 3062
2024-01-30 20:57:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:57:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:57:29 | INFO | fairseq.trainer | begin training epoch 36
2024-01-30 20:57:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:57:35 | INFO | train_inner | epoch 036:     90 / 1349 loss=1.87, nll_loss=0.269, ppl=1.21, wps=38671.6, ups=9.87, wpb=3917.2, bsz=215.7, num_updates=47300, lr=0.000145402, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=3068
2024-01-30 20:57:41 | INFO | train_inner | epoch 036:    190 / 1349 loss=1.859, nll_loss=0.258, ppl=1.2, wps=65008.5, ups=16.52, wpb=3934.4, bsz=220.5, num_updates=47400, lr=0.000145248, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=3074
2024-01-30 20:57:47 | INFO | train_inner | epoch 036:    290 / 1349 loss=1.857, nll_loss=0.255, ppl=1.19, wps=65224.2, ups=16.49, wpb=3955.8, bsz=208.3, num_updates=47500, lr=0.000145095, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=3080
2024-01-30 20:57:53 | INFO | train_inner | epoch 036:    390 / 1349 loss=1.852, nll_loss=0.25, ppl=1.19, wps=65135.2, ups=16.65, wpb=3912.9, bsz=205.4, num_updates=47600, lr=0.000144943, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=3086
2024-01-30 20:57:59 | INFO | train_inner | epoch 036:    490 / 1349 loss=1.871, nll_loss=0.271, ppl=1.21, wps=65260.1, ups=16.61, wpb=3928.4, bsz=227.5, num_updates=47700, lr=0.000144791, gnorm=0.555, loss_scale=16, train_wall=6, gb_free=29.7, wall=3092
2024-01-30 20:58:05 | INFO | train_inner | epoch 036:    590 / 1349 loss=1.864, nll_loss=0.264, ppl=1.2, wps=65176.8, ups=16.66, wpb=3912.8, bsz=222.8, num_updates=47800, lr=0.000144639, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.7, wall=3098
2024-01-30 20:58:11 | INFO | train_inner | epoch 036:    690 / 1349 loss=1.864, nll_loss=0.263, ppl=1.2, wps=64978.7, ups=16.6, wpb=3915.3, bsz=218.5, num_updates=47900, lr=0.000144488, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=3104
2024-01-30 20:58:17 | INFO | train_inner | epoch 036:    790 / 1349 loss=1.873, nll_loss=0.274, ppl=1.21, wps=65535.3, ups=16.83, wpb=3894.8, bsz=195, num_updates=48000, lr=0.000144338, gnorm=0.552, loss_scale=16, train_wall=6, gb_free=29.8, wall=3110
2024-01-30 20:58:23 | INFO | train_inner | epoch 036:    890 / 1349 loss=1.867, nll_loss=0.267, ppl=1.2, wps=65271.1, ups=16.62, wpb=3927.8, bsz=214.2, num_updates=48100, lr=0.000144187, gnorm=0.509, loss_scale=16, train_wall=6, gb_free=29.7, wall=3116
2024-01-30 20:58:29 | INFO | train_inner | epoch 036:    990 / 1349 loss=1.863, nll_loss=0.262, ppl=1.2, wps=65792.3, ups=16.67, wpb=3945.8, bsz=215.4, num_updates=48200, lr=0.000144038, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.7, wall=3122
2024-01-30 20:58:35 | INFO | train_inner | epoch 036:   1090 / 1349 loss=1.869, nll_loss=0.269, ppl=1.21, wps=65308.2, ups=16.76, wpb=3896.4, bsz=203, num_updates=48300, lr=0.000143889, gnorm=0.551, loss_scale=16, train_wall=6, gb_free=29.7, wall=3128
2024-01-30 20:58:41 | INFO | train_inner | epoch 036:   1190 / 1349 loss=1.859, nll_loss=0.259, ppl=1.2, wps=65048, ups=16.56, wpb=3928.3, bsz=218.2, num_updates=48400, lr=0.00014374, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=3134
2024-01-30 20:58:47 | INFO | train_inner | epoch 036:   1290 / 1349 loss=1.864, nll_loss=0.264, ppl=1.2, wps=65003.6, ups=16.67, wpb=3900.3, bsz=224, num_updates=48500, lr=0.000143592, gnorm=0.534, loss_scale=16, train_wall=6, gb_free=29.7, wall=3140
2024-01-30 20:58:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 20:58:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:58:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 2.077 | nll_loss 0.309 | ppl 1.24 | wps 173254 | wpb 3275.2 | bsz 176.5 | num_updates 48559 | best_loss 2.063
2024-01-30 20:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 48559 updates
2024-01-30 20:58:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint36.pt
2024-01-30 20:58:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint36.pt
2024-01-30 20:58:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint36.pt (epoch 36 @ 48559 updates, score 2.077) (writing took 2.3660446419380605 seconds)
2024-01-30 20:58:54 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-01-30 20:58:54 | INFO | train | epoch 036 | loss 1.864 | nll_loss 0.264 | ppl 1.2 | wps 62091.1 | ups 15.83 | wpb 3921.2 | bsz 214.5 | num_updates 48559 | lr 0.000143504 | gnorm 0.513 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 3147
2024-01-30 20:58:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 20:58:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 20:58:54 | INFO | fairseq.trainer | begin training epoch 37
2024-01-30 20:58:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 20:58:57 | INFO | train_inner | epoch 037:     41 / 1349 loss=1.868, nll_loss=0.268, ppl=1.2, wps=38615.4, ups=9.87, wpb=3913.4, bsz=211.8, num_updates=48600, lr=0.000143444, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.9, wall=3150
2024-01-30 20:59:03 | INFO | train_inner | epoch 037:    141 / 1349 loss=1.866, nll_loss=0.265, ppl=1.2, wps=64864.9, ups=16.79, wpb=3862.3, bsz=202.6, num_updates=48700, lr=0.000143296, gnorm=0.539, loss_scale=16, train_wall=6, gb_free=29.8, wall=3156
2024-01-30 20:59:09 | INFO | train_inner | epoch 037:    241 / 1349 loss=1.861, nll_loss=0.26, ppl=1.2, wps=65061.5, ups=16.64, wpb=3909.6, bsz=207.7, num_updates=48800, lr=0.00014315, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.8, wall=3162
2024-01-30 20:59:15 | INFO | train_inner | epoch 037:    341 / 1349 loss=1.862, nll_loss=0.261, ppl=1.2, wps=65208.4, ups=16.72, wpb=3901.1, bsz=216.6, num_updates=48900, lr=0.000143003, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.8, wall=3168
2024-01-30 20:59:21 | INFO | train_inner | epoch 037:    441 / 1349 loss=1.851, nll_loss=0.249, ppl=1.19, wps=65405.7, ups=16.44, wpb=3977.6, bsz=233.5, num_updates=49000, lr=0.000142857, gnorm=0.479, loss_scale=16, train_wall=6, gb_free=29.8, wall=3174
2024-01-30 20:59:27 | INFO | train_inner | epoch 037:    541 / 1349 loss=1.861, nll_loss=0.26, ppl=1.2, wps=65811.9, ups=16.66, wpb=3949.3, bsz=204.8, num_updates=49100, lr=0.000142712, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.8, wall=3180
2024-01-30 20:59:33 | INFO | train_inner | epoch 037:    641 / 1349 loss=1.852, nll_loss=0.251, ppl=1.19, wps=64803.4, ups=16.52, wpb=3923.6, bsz=227.7, num_updates=49200, lr=0.000142566, gnorm=0.46, loss_scale=16, train_wall=6, gb_free=29.7, wall=3186
2024-01-30 20:59:39 | INFO | train_inner | epoch 037:    741 / 1349 loss=1.859, nll_loss=0.259, ppl=1.2, wps=65595.1, ups=16.61, wpb=3949.3, bsz=211.1, num_updates=49300, lr=0.000142422, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.9, wall=3192
2024-01-30 20:59:45 | INFO | train_inner | epoch 037:    841 / 1349 loss=1.87, nll_loss=0.27, ppl=1.21, wps=66039.1, ups=16.68, wpb=3959.6, bsz=212.8, num_updates=49400, lr=0.000142278, gnorm=0.531, loss_scale=16, train_wall=6, gb_free=29.8, wall=3198
2024-01-30 20:59:51 | INFO | train_inner | epoch 037:    941 / 1349 loss=1.872, nll_loss=0.274, ppl=1.21, wps=65213.4, ups=16.81, wpb=3880.3, bsz=213.5, num_updates=49500, lr=0.000142134, gnorm=0.579, loss_scale=16, train_wall=6, gb_free=29.8, wall=3204
2024-01-30 20:59:57 | INFO | train_inner | epoch 037:   1041 / 1349 loss=1.857, nll_loss=0.256, ppl=1.19, wps=65770, ups=16.63, wpb=3955.1, bsz=218.6, num_updates=49600, lr=0.00014199, gnorm=0.483, loss_scale=16, train_wall=6, gb_free=29.8, wall=3210
2024-01-30 21:00:03 | INFO | train_inner | epoch 037:   1141 / 1349 loss=1.858, nll_loss=0.258, ppl=1.2, wps=65311.2, ups=16.7, wpb=3910, bsz=214.7, num_updates=49700, lr=0.000141848, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=3216
2024-01-30 21:00:09 | INFO | train_inner | epoch 037:   1241 / 1349 loss=1.871, nll_loss=0.271, ppl=1.21, wps=65411.8, ups=16.67, wpb=3923.4, bsz=208.1, num_updates=49800, lr=0.000141705, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.8, wall=3222
2024-01-30 21:00:15 | INFO | train_inner | epoch 037:   1341 / 1349 loss=1.862, nll_loss=0.262, ppl=1.2, wps=64965.2, ups=16.7, wpb=3889.5, bsz=215.7, num_updates=49900, lr=0.000141563, gnorm=0.456, loss_scale=16, train_wall=6, gb_free=29.8, wall=3228
2024-01-30 21:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:00:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:00:17 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 2.077 | nll_loss 0.312 | ppl 1.24 | wps 171916 | wpb 3275.2 | bsz 176.5 | num_updates 49908 | best_loss 2.063
2024-01-30 21:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 49908 updates
2024-01-30 21:00:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint37.pt
2024-01-30 21:00:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint37.pt
2024-01-30 21:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint37.pt (epoch 37 @ 49908 updates, score 2.077) (writing took 2.244701399002224 seconds)
2024-01-30 21:00:19 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-01-30 21:00:19 | INFO | train | epoch 037 | loss 1.861 | nll_loss 0.261 | ppl 1.2 | wps 62190.7 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 49908 | lr 0.000141552 | gnorm 0.498 | loss_scale 16 | train_wall 78 | gb_free 29.9 | wall 3233
2024-01-30 21:00:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:00:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:00:19 | INFO | fairseq.trainer | begin training epoch 38
2024-01-30 21:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:00:25 | INFO | train_inner | epoch 038:     92 / 1349 loss=1.863, nll_loss=0.263, ppl=1.2, wps=38895.4, ups=9.95, wpb=3908, bsz=215.2, num_updates=50000, lr=0.000141421, gnorm=0.462, loss_scale=16, train_wall=6, gb_free=29.8, wall=3238
2024-01-30 21:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:00:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:00:27 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 2.082 | nll_loss 0.315 | ppl 1.24 | wps 171890 | wpb 3275.2 | bsz 176.5 | num_updates 50000 | best_loss 2.063
2024-01-30 21:00:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 50000 updates
2024-01-30 21:00:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_38_50000.pt
2024-01-30 21:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_38_50000.pt
2024-01-30 21:00:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_38_50000.pt (epoch 38 @ 50000 updates, score 2.082) (writing took 2.23110979096964 seconds)
2024-01-30 21:00:35 | INFO | train_inner | epoch 038:    192 / 1349 loss=1.855, nll_loss=0.254, ppl=1.19, wps=39430.6, ups=9.97, wpb=3953.5, bsz=218.2, num_updates=50100, lr=0.00014128, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.8, wall=3248
2024-01-30 21:00:41 | INFO | train_inner | epoch 038:    292 / 1349 loss=1.855, nll_loss=0.254, ppl=1.19, wps=65379.7, ups=16.58, wpb=3943.7, bsz=218.5, num_updates=50200, lr=0.000141139, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.7, wall=3254
2024-01-30 21:00:47 | INFO | train_inner | epoch 038:    392 / 1349 loss=1.863, nll_loss=0.263, ppl=1.2, wps=64842, ups=16.68, wpb=3887.8, bsz=214.6, num_updates=50300, lr=0.000140999, gnorm=0.544, loss_scale=16, train_wall=6, gb_free=29.8, wall=3260
2024-01-30 21:00:53 | INFO | train_inner | epoch 038:    492 / 1349 loss=1.851, nll_loss=0.25, ppl=1.19, wps=65309.6, ups=16.61, wpb=3931.7, bsz=216.7, num_updates=50400, lr=0.000140859, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.9, wall=3266
2024-01-30 21:00:59 | INFO | train_inner | epoch 038:    592 / 1349 loss=1.853, nll_loss=0.252, ppl=1.19, wps=65563.5, ups=16.55, wpb=3961.9, bsz=220.9, num_updates=50500, lr=0.00014072, gnorm=0.539, loss_scale=16, train_wall=6, gb_free=29.8, wall=3272
2024-01-30 21:01:05 | INFO | train_inner | epoch 038:    692 / 1349 loss=1.852, nll_loss=0.251, ppl=1.19, wps=65020.3, ups=16.68, wpb=3898.3, bsz=220.8, num_updates=50600, lr=0.00014058, gnorm=0.517, loss_scale=16, train_wall=6, gb_free=29.7, wall=3278
2024-01-30 21:01:11 | INFO | train_inner | epoch 038:    792 / 1349 loss=1.863, nll_loss=0.263, ppl=1.2, wps=65445.5, ups=16.61, wpb=3941.1, bsz=214.5, num_updates=50700, lr=0.000140442, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.8, wall=3284
2024-01-30 21:01:17 | INFO | train_inner | epoch 038:    892 / 1349 loss=1.854, nll_loss=0.253, ppl=1.19, wps=64857.2, ups=16.69, wpb=3886.7, bsz=213.2, num_updates=50800, lr=0.000140303, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.7, wall=3290
2024-01-30 21:01:23 | INFO | train_inner | epoch 038:    992 / 1349 loss=1.873, nll_loss=0.274, ppl=1.21, wps=66305, ups=16.76, wpb=3956.2, bsz=199.6, num_updates=50900, lr=0.000140165, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.8, wall=3296
2024-01-30 21:01:29 | INFO | train_inner | epoch 038:   1092 / 1349 loss=1.863, nll_loss=0.263, ppl=1.2, wps=66043.1, ups=16.66, wpb=3964, bsz=220.2, num_updates=51000, lr=0.000140028, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=3302
2024-01-30 21:01:35 | INFO | train_inner | epoch 038:   1192 / 1349 loss=1.866, nll_loss=0.267, ppl=1.2, wps=65437.6, ups=16.68, wpb=3922.1, bsz=210.5, num_updates=51100, lr=0.000139891, gnorm=0.597, loss_scale=16, train_wall=6, gb_free=29.8, wall=3308
2024-01-30 21:01:41 | INFO | train_inner | epoch 038:   1292 / 1349 loss=1.861, nll_loss=0.261, ppl=1.2, wps=64525.6, ups=16.67, wpb=3870.1, bsz=206.2, num_updates=51200, lr=0.000139754, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.8, wall=3314
2024-01-30 21:01:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:01:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:01:46 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 2.079 | nll_loss 0.316 | ppl 1.25 | wps 173548 | wpb 3275.2 | bsz 176.5 | num_updates 51257 | best_loss 2.063
2024-01-30 21:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 51257 updates
2024-01-30 21:01:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint38.pt
2024-01-30 21:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint38.pt
2024-01-30 21:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint38.pt (epoch 38 @ 51257 updates, score 2.079) (writing took 2.260953556979075 seconds)
2024-01-30 21:01:48 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-01-30 21:01:48 | INFO | train | epoch 038 | loss 1.859 | nll_loss 0.259 | ppl 1.2 | wps 59385.7 | ups 15.14 | wpb 3921.2 | bsz 214.5 | num_updates 51257 | lr 0.000139677 | gnorm 0.523 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 3322
2024-01-30 21:01:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:01:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:01:48 | INFO | fairseq.trainer | begin training epoch 39
2024-01-30 21:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:01:51 | INFO | train_inner | epoch 039:     43 / 1349 loss=1.856, nll_loss=0.255, ppl=1.19, wps=37828.2, ups=9.93, wpb=3810.7, bsz=215.7, num_updates=51300, lr=0.000139618, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.7, wall=3324
2024-01-30 21:01:57 | INFO | train_inner | epoch 039:    143 / 1349 loss=1.853, nll_loss=0.252, ppl=1.19, wps=65764.9, ups=16.75, wpb=3925.8, bsz=204.7, num_updates=51400, lr=0.000139482, gnorm=0.466, loss_scale=16, train_wall=6, gb_free=29.7, wall=3330
2024-01-30 21:02:03 | INFO | train_inner | epoch 039:    243 / 1349 loss=1.862, nll_loss=0.261, ppl=1.2, wps=66086.7, ups=16.76, wpb=3943.7, bsz=206.5, num_updates=51500, lr=0.000139347, gnorm=0.58, loss_scale=16, train_wall=6, gb_free=29.7, wall=3336
2024-01-30 21:02:09 | INFO | train_inner | epoch 039:    343 / 1349 loss=1.847, nll_loss=0.246, ppl=1.19, wps=65130.7, ups=16.5, wpb=3948.3, bsz=225.7, num_updates=51600, lr=0.000139212, gnorm=0.467, loss_scale=16, train_wall=6, gb_free=29.8, wall=3342
2024-01-30 21:02:15 | INFO | train_inner | epoch 039:    443 / 1349 loss=1.861, nll_loss=0.26, ppl=1.2, wps=65328.9, ups=16.64, wpb=3926.8, bsz=209, num_updates=51700, lr=0.000139077, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.8, wall=3348
2024-01-30 21:02:21 | INFO | train_inner | epoch 039:    543 / 1349 loss=1.847, nll_loss=0.246, ppl=1.19, wps=64444.9, ups=16.62, wpb=3876.7, bsz=219.5, num_updates=51800, lr=0.000138943, gnorm=0.489, loss_scale=16, train_wall=6, gb_free=29.8, wall=3354
2024-01-30 21:02:27 | INFO | train_inner | epoch 039:    643 / 1349 loss=1.866, nll_loss=0.267, ppl=1.2, wps=65845.6, ups=16.79, wpb=3922.8, bsz=208.7, num_updates=51900, lr=0.000138809, gnorm=0.508, loss_scale=16, train_wall=6, gb_free=29.8, wall=3360
2024-01-30 21:02:33 | INFO | train_inner | epoch 039:    743 / 1349 loss=1.856, nll_loss=0.255, ppl=1.19, wps=64635.3, ups=16.7, wpb=3871.5, bsz=204.7, num_updates=52000, lr=0.000138675, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.8, wall=3366
2024-01-30 21:02:39 | INFO | train_inner | epoch 039:    843 / 1349 loss=1.855, nll_loss=0.255, ppl=1.19, wps=65532.6, ups=16.49, wpb=3974.3, bsz=225.8, num_updates=52100, lr=0.000138542, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.8, wall=3372
2024-01-30 21:02:45 | INFO | train_inner | epoch 039:    943 / 1349 loss=1.855, nll_loss=0.255, ppl=1.19, wps=65412.3, ups=16.57, wpb=3947.6, bsz=218.2, num_updates=52200, lr=0.000138409, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=3378
2024-01-30 21:02:51 | INFO | train_inner | epoch 039:   1043 / 1349 loss=1.865, nll_loss=0.266, ppl=1.2, wps=65352.4, ups=16.7, wpb=3912.8, bsz=207.4, num_updates=52300, lr=0.000138277, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.7, wall=3384
2024-01-30 21:02:57 | INFO | train_inner | epoch 039:   1143 / 1349 loss=1.854, nll_loss=0.254, ppl=1.19, wps=65465.3, ups=16.57, wpb=3951.6, bsz=238.9, num_updates=52400, lr=0.000138145, gnorm=0.467, loss_scale=16, train_wall=6, gb_free=29.7, wall=3390
2024-01-30 21:03:03 | INFO | train_inner | epoch 039:   1243 / 1349 loss=1.86, nll_loss=0.26, ppl=1.2, wps=65411.1, ups=16.7, wpb=3915.9, bsz=203.4, num_updates=52500, lr=0.000138013, gnorm=0.528, loss_scale=16, train_wall=6, gb_free=29.8, wall=3396
2024-01-30 21:03:09 | INFO | train_inner | epoch 039:   1343 / 1349 loss=1.853, nll_loss=0.253, ppl=1.19, wps=65932.2, ups=16.66, wpb=3958, bsz=218.1, num_updates=52600, lr=0.000137882, gnorm=0.476, loss_scale=16, train_wall=6, gb_free=29.7, wall=3402
2024-01-30 21:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:03:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:03:11 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 2.076 | nll_loss 0.317 | ppl 1.25 | wps 172371 | wpb 3275.2 | bsz 176.5 | num_updates 52606 | best_loss 2.063
2024-01-30 21:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 52606 updates
2024-01-30 21:03:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint39.pt
2024-01-30 21:03:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint39.pt
2024-01-30 21:03:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint39.pt (epoch 39 @ 52606 updates, score 2.076) (writing took 2.314860936952755 seconds)
2024-01-30 21:03:14 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-01-30 21:03:14 | INFO | train | epoch 039 | loss 1.857 | nll_loss 0.256 | ppl 1.19 | wps 62112.8 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 52606 | lr 0.000137874 | gnorm 0.501 | loss_scale 16 | train_wall 78 | gb_free 29.9 | wall 3407
2024-01-30 21:03:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:03:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:03:14 | INFO | fairseq.trainer | begin training epoch 40
2024-01-30 21:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:03:19 | INFO | train_inner | epoch 040:     94 / 1349 loss=1.854, nll_loss=0.253, ppl=1.19, wps=38595, ups=9.87, wpb=3912, bsz=205.1, num_updates=52700, lr=0.000137751, gnorm=0.539, loss_scale=16, train_wall=6, gb_free=29.8, wall=3412
2024-01-30 21:03:25 | INFO | train_inner | epoch 040:    194 / 1349 loss=1.842, nll_loss=0.24, ppl=1.18, wps=65449.9, ups=16.53, wpb=3960.6, bsz=217.4, num_updates=52800, lr=0.00013762, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.8, wall=3419
2024-01-30 21:03:31 | INFO | train_inner | epoch 040:    294 / 1349 loss=1.871, nll_loss=0.272, ppl=1.21, wps=65724.6, ups=16.88, wpb=3892.9, bsz=194.4, num_updates=52900, lr=0.00013749, gnorm=0.579, loss_scale=16, train_wall=6, gb_free=29.8, wall=3424
2024-01-30 21:03:37 | INFO | train_inner | epoch 040:    394 / 1349 loss=1.853, nll_loss=0.253, ppl=1.19, wps=65147, ups=16.66, wpb=3911, bsz=210.2, num_updates=53000, lr=0.000137361, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.7, wall=3430
2024-01-30 21:03:43 | INFO | train_inner | epoch 040:    494 / 1349 loss=1.848, nll_loss=0.247, ppl=1.19, wps=64146.8, ups=16.46, wpb=3897.6, bsz=224.3, num_updates=53100, lr=0.000137231, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=3437
2024-01-30 21:03:49 | INFO | train_inner | epoch 040:    594 / 1349 loss=1.849, nll_loss=0.248, ppl=1.19, wps=66072.2, ups=16.63, wpb=3972.6, bsz=214.2, num_updates=53200, lr=0.000137102, gnorm=0.446, loss_scale=16, train_wall=6, gb_free=29.8, wall=3443
2024-01-30 21:03:55 | INFO | train_inner | epoch 040:    694 / 1349 loss=1.858, nll_loss=0.258, ppl=1.2, wps=64584.4, ups=16.7, wpb=3866.9, bsz=212.6, num_updates=53300, lr=0.000136973, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.7, wall=3449
2024-01-30 21:04:01 | INFO | train_inner | epoch 040:    794 / 1349 loss=1.856, nll_loss=0.255, ppl=1.19, wps=64511.5, ups=16.69, wpb=3864.1, bsz=214.2, num_updates=53400, lr=0.000136845, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.7, wall=3455
2024-01-30 21:04:07 | INFO | train_inner | epoch 040:    894 / 1349 loss=1.851, nll_loss=0.251, ppl=1.19, wps=65340.9, ups=16.63, wpb=3929.3, bsz=219.4, num_updates=53500, lr=0.000136717, gnorm=0.478, loss_scale=16, train_wall=6, gb_free=29.8, wall=3461
2024-01-30 21:04:13 | INFO | train_inner | epoch 040:    994 / 1349 loss=1.845, nll_loss=0.245, ppl=1.19, wps=65303.4, ups=16.52, wpb=3954, bsz=216, num_updates=53600, lr=0.00013659, gnorm=0.46, loss_scale=16, train_wall=6, gb_free=29.7, wall=3467
2024-01-30 21:04:19 | INFO | train_inner | epoch 040:   1094 / 1349 loss=1.86, nll_loss=0.261, ppl=1.2, wps=65463.6, ups=16.7, wpb=3920.7, bsz=226.6, num_updates=53700, lr=0.000136462, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.8, wall=3473
2024-01-30 21:04:25 | INFO | train_inner | epoch 040:   1194 / 1349 loss=1.855, nll_loss=0.255, ppl=1.19, wps=65514.3, ups=16.7, wpb=3923.1, bsz=215.4, num_updates=53800, lr=0.000136335, gnorm=0.545, loss_scale=16, train_wall=6, gb_free=29.7, wall=3479
2024-01-30 21:04:31 | INFO | train_inner | epoch 040:   1294 / 1349 loss=1.859, nll_loss=0.259, ppl=1.2, wps=65411.7, ups=16.64, wpb=3930.7, bsz=223.4, num_updates=53900, lr=0.000136209, gnorm=0.556, loss_scale=16, train_wall=6, gb_free=29.7, wall=3485
2024-01-30 21:04:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:04:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:04:36 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 2.074 | nll_loss 0.312 | ppl 1.24 | wps 171498 | wpb 3275.2 | bsz 176.5 | num_updates 53955 | best_loss 2.063
2024-01-30 21:04:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 53955 updates
2024-01-30 21:04:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint40.pt
2024-01-30 21:04:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint40.pt
2024-01-30 21:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint40.pt (epoch 40 @ 53955 updates, score 2.074) (writing took 2.384754610946402 seconds)
2024-01-30 21:04:39 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-01-30 21:04:39 | INFO | train | epoch 040 | loss 1.854 | nll_loss 0.254 | ppl 1.19 | wps 62089.5 | ups 15.83 | wpb 3921.2 | bsz 214.5 | num_updates 53955 | lr 0.00013614 | gnorm 0.511 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 3492
2024-01-30 21:04:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:04:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:04:39 | INFO | fairseq.trainer | begin training epoch 41
2024-01-30 21:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:04:42 | INFO | train_inner | epoch 041:     45 / 1349 loss=1.857, nll_loss=0.256, ppl=1.19, wps=38454.6, ups=9.82, wpb=3916.2, bsz=213, num_updates=54000, lr=0.000136083, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.7, wall=3495
2024-01-30 21:04:48 | INFO | train_inner | epoch 041:    145 / 1349 loss=1.85, nll_loss=0.25, ppl=1.19, wps=65952.9, ups=16.65, wpb=3961.9, bsz=215.4, num_updates=54100, lr=0.000135957, gnorm=0.476, loss_scale=16, train_wall=6, gb_free=29.8, wall=3501
2024-01-30 21:04:54 | INFO | train_inner | epoch 041:    245 / 1349 loss=1.854, nll_loss=0.254, ppl=1.19, wps=65488.4, ups=16.73, wpb=3915.2, bsz=201, num_updates=54200, lr=0.000135831, gnorm=0.54, loss_scale=16, train_wall=6, gb_free=29.8, wall=3507
2024-01-30 21:05:00 | INFO | train_inner | epoch 041:    345 / 1349 loss=1.854, nll_loss=0.253, ppl=1.19, wps=64992.7, ups=16.72, wpb=3887.7, bsz=207.6, num_updates=54300, lr=0.000135706, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=29.8, wall=3513
2024-01-30 21:05:06 | INFO | train_inner | epoch 041:    445 / 1349 loss=1.848, nll_loss=0.247, ppl=1.19, wps=65069, ups=16.56, wpb=3928.2, bsz=213.1, num_updates=54400, lr=0.000135582, gnorm=0.564, loss_scale=16, train_wall=6, gb_free=29.8, wall=3519
2024-01-30 21:05:12 | INFO | train_inner | epoch 041:    545 / 1349 loss=1.848, nll_loss=0.247, ppl=1.19, wps=64512.9, ups=16.53, wpb=3901.6, bsz=213.7, num_updates=54500, lr=0.000135457, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.7, wall=3525
2024-01-30 21:05:18 | INFO | train_inner | epoch 041:    645 / 1349 loss=1.851, nll_loss=0.251, ppl=1.19, wps=64491.6, ups=16.68, wpb=3865.9, bsz=226.5, num_updates=54600, lr=0.000135333, gnorm=0.465, loss_scale=16, train_wall=6, gb_free=29.9, wall=3531
2024-01-30 21:05:24 | INFO | train_inner | epoch 041:    745 / 1349 loss=1.852, nll_loss=0.252, ppl=1.19, wps=64353.6, ups=16.69, wpb=3855.9, bsz=209.4, num_updates=54700, lr=0.000135209, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.7, wall=3537
2024-01-30 21:05:30 | INFO | train_inner | epoch 041:    845 / 1349 loss=1.848, nll_loss=0.248, ppl=1.19, wps=65449.2, ups=16.49, wpb=3968.9, bsz=231.4, num_updates=54800, lr=0.000135086, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.7, wall=3543
2024-01-30 21:05:36 | INFO | train_inner | epoch 041:    945 / 1349 loss=1.861, nll_loss=0.262, ppl=1.2, wps=66252.7, ups=16.7, wpb=3966.5, bsz=208.4, num_updates=54900, lr=0.000134963, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=3549
2024-01-30 21:05:42 | INFO | train_inner | epoch 041:   1045 / 1349 loss=1.851, nll_loss=0.251, ppl=1.19, wps=65925.6, ups=16.66, wpb=3956.8, bsz=222.4, num_updates=55000, lr=0.00013484, gnorm=0.527, loss_scale=16, train_wall=6, gb_free=29.8, wall=3555
2024-01-30 21:05:48 | INFO | train_inner | epoch 041:   1145 / 1349 loss=1.855, nll_loss=0.256, ppl=1.19, wps=65021.3, ups=16.72, wpb=3889.6, bsz=211.4, num_updates=55100, lr=0.000134718, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.7, wall=3561
2024-01-30 21:05:54 | INFO | train_inner | epoch 041:   1245 / 1349 loss=1.859, nll_loss=0.26, ppl=1.2, wps=65316.9, ups=16.72, wpb=3906.2, bsz=201.7, num_updates=55200, lr=0.000134595, gnorm=0.545, loss_scale=16, train_wall=6, gb_free=29.8, wall=3567
2024-01-30 21:06:00 | INFO | train_inner | epoch 041:   1345 / 1349 loss=1.857, nll_loss=0.258, ppl=1.2, wps=66902.4, ups=16.84, wpb=3973.8, bsz=223.3, num_updates=55300, lr=0.000134474, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=3573
2024-01-30 21:06:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:06:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:06:01 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 2.077 | nll_loss 0.314 | ppl 1.24 | wps 171937 | wpb 3275.2 | bsz 176.5 | num_updates 55304 | best_loss 2.063
2024-01-30 21:06:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 55304 updates
2024-01-30 21:06:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint41.pt
2024-01-30 21:06:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint41.pt
2024-01-30 21:06:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint41.pt (epoch 41 @ 55304 updates, score 2.077) (writing took 2.2523226350313053 seconds)
2024-01-30 21:06:04 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-01-30 21:06:04 | INFO | train | epoch 041 | loss 1.853 | nll_loss 0.252 | ppl 1.19 | wps 62184.7 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 55304 | lr 0.000134469 | gnorm 0.514 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 3577
2024-01-30 21:06:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:06:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:06:04 | INFO | fairseq.trainer | begin training epoch 42
2024-01-30 21:06:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:06:10 | INFO | train_inner | epoch 042:     96 / 1349 loss=1.846, nll_loss=0.245, ppl=1.19, wps=38795, ups=9.89, wpb=3922.1, bsz=213.4, num_updates=55400, lr=0.000134352, gnorm=0.448, loss_scale=16, train_wall=6, gb_free=29.8, wall=3583
2024-01-30 21:06:16 | INFO | train_inner | epoch 042:    196 / 1349 loss=1.843, nll_loss=0.242, ppl=1.18, wps=65756.9, ups=16.61, wpb=3958.4, bsz=214.3, num_updates=55500, lr=0.000134231, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.7, wall=3589
2024-01-30 21:06:22 | INFO | train_inner | epoch 042:    296 / 1349 loss=1.844, nll_loss=0.243, ppl=1.18, wps=64977.9, ups=16.64, wpb=3905.6, bsz=213.1, num_updates=55600, lr=0.00013411, gnorm=0.465, loss_scale=16, train_wall=6, gb_free=29.7, wall=3595
2024-01-30 21:06:28 | INFO | train_inner | epoch 042:    396 / 1349 loss=1.856, nll_loss=0.256, ppl=1.19, wps=65971.5, ups=16.67, wpb=3957.4, bsz=203.4, num_updates=55700, lr=0.00013399, gnorm=0.538, loss_scale=16, train_wall=6, gb_free=29.8, wall=3601
2024-01-30 21:06:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 21:06:34 | INFO | train_inner | epoch 042:    497 / 1349 loss=1.847, nll_loss=0.246, ppl=1.19, wps=64109, ups=16.44, wpb=3898.8, bsz=216.6, num_updates=55800, lr=0.00013387, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.8, wall=3607
2024-01-30 21:06:40 | INFO | train_inner | epoch 042:    597 / 1349 loss=1.85, nll_loss=0.249, ppl=1.19, wps=65016.8, ups=16.7, wpb=3893.8, bsz=216.1, num_updates=55900, lr=0.00013375, gnorm=0.528, loss_scale=16, train_wall=6, gb_free=29.8, wall=3613
2024-01-30 21:06:46 | INFO | train_inner | epoch 042:    697 / 1349 loss=1.851, nll_loss=0.251, ppl=1.19, wps=65160.1, ups=16.62, wpb=3920.3, bsz=217.2, num_updates=56000, lr=0.000133631, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.8, wall=3619
2024-01-30 21:06:52 | INFO | train_inner | epoch 042:    797 / 1349 loss=1.849, nll_loss=0.249, ppl=1.19, wps=65008.3, ups=16.61, wpb=3912.7, bsz=217.5, num_updates=56100, lr=0.000133511, gnorm=0.509, loss_scale=16, train_wall=6, gb_free=29.8, wall=3625
2024-01-30 21:06:58 | INFO | train_inner | epoch 042:    897 / 1349 loss=1.854, nll_loss=0.254, ppl=1.19, wps=65830.3, ups=16.78, wpb=3922.5, bsz=215, num_updates=56200, lr=0.000133393, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.8, wall=3631
2024-01-30 21:07:04 | INFO | train_inner | epoch 042:    997 / 1349 loss=1.845, nll_loss=0.244, ppl=1.18, wps=64756.3, ups=16.56, wpb=3911.3, bsz=220.2, num_updates=56300, lr=0.000133274, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=3637
2024-01-30 21:07:10 | INFO | train_inner | epoch 042:   1097 / 1349 loss=1.852, nll_loss=0.252, ppl=1.19, wps=65244.6, ups=16.57, wpb=3938.1, bsz=216.2, num_updates=56400, lr=0.000133156, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.7, wall=3643
2024-01-30 21:07:16 | INFO | train_inner | epoch 042:   1197 / 1349 loss=1.856, nll_loss=0.257, ppl=1.19, wps=66007.1, ups=16.73, wpb=3944.6, bsz=205.2, num_updates=56500, lr=0.000133038, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.9, wall=3649
2024-01-30 21:07:22 | INFO | train_inner | epoch 042:   1297 / 1349 loss=1.859, nll_loss=0.26, ppl=1.2, wps=65527.7, ups=16.77, wpb=3907.3, bsz=208.8, num_updates=56600, lr=0.00013292, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.7, wall=3655
2024-01-30 21:07:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:07:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:07:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 2.082 | nll_loss 0.32 | ppl 1.25 | wps 171493 | wpb 3275.2 | bsz 176.5 | num_updates 56652 | best_loss 2.063
2024-01-30 21:07:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 56652 updates
2024-01-30 21:07:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint42.pt
2024-01-30 21:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint42.pt
2024-01-30 21:07:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint42.pt (epoch 42 @ 56652 updates, score 2.082) (writing took 2.415694477967918 seconds)
2024-01-30 21:07:29 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-01-30 21:07:29 | INFO | train | epoch 042 | loss 1.85 | nll_loss 0.25 | ppl 1.19 | wps 62004.5 | ups 15.81 | wpb 3921.1 | bsz 214.5 | num_updates 56652 | lr 0.000132859 | gnorm 0.503 | loss_scale 16 | train_wall 78 | gb_free 29.9 | wall 3662
2024-01-30 21:07:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:07:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:07:29 | INFO | fairseq.trainer | begin training epoch 43
2024-01-30 21:07:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:07:32 | INFO | train_inner | epoch 043:     48 / 1349 loss=1.842, nll_loss=0.241, ppl=1.18, wps=38270.7, ups=9.74, wpb=3929.7, bsz=225.8, num_updates=56700, lr=0.000132803, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.7, wall=3665
2024-01-30 21:07:38 | INFO | train_inner | epoch 043:    148 / 1349 loss=1.84, nll_loss=0.239, ppl=1.18, wps=65046.1, ups=16.62, wpb=3914.2, bsz=224.5, num_updates=56800, lr=0.000132686, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.9, wall=3671
2024-01-30 21:07:44 | INFO | train_inner | epoch 043:    248 / 1349 loss=1.847, nll_loss=0.247, ppl=1.19, wps=65472.4, ups=16.68, wpb=3926.3, bsz=204.6, num_updates=56900, lr=0.00013257, gnorm=0.534, loss_scale=16, train_wall=6, gb_free=29.7, wall=3677
2024-01-30 21:07:50 | INFO | train_inner | epoch 043:    348 / 1349 loss=1.848, nll_loss=0.247, ppl=1.19, wps=65055, ups=16.62, wpb=3913.6, bsz=213.5, num_updates=57000, lr=0.000132453, gnorm=0.425, loss_scale=16, train_wall=6, gb_free=29.7, wall=3683
2024-01-30 21:07:56 | INFO | train_inner | epoch 043:    448 / 1349 loss=1.844, nll_loss=0.244, ppl=1.18, wps=64553, ups=16.67, wpb=3872.2, bsz=217.2, num_updates=57100, lr=0.000132337, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=3689
2024-01-30 21:08:03 | INFO | train_inner | epoch 043:    548 / 1349 loss=1.85, nll_loss=0.25, ppl=1.19, wps=54300.6, ups=13.92, wpb=3902.2, bsz=205.4, num_updates=57200, lr=0.000132221, gnorm=0.526, loss_scale=16, train_wall=7, gb_free=29.7, wall=3696
2024-01-30 21:08:10 | INFO | train_inner | epoch 043:    648 / 1349 loss=1.85, nll_loss=0.25, ppl=1.19, wps=62719.6, ups=15.89, wpb=3948.3, bsz=206.4, num_updates=57300, lr=0.000132106, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=3703
2024-01-30 21:08:16 | INFO | train_inner | epoch 043:    748 / 1349 loss=1.842, nll_loss=0.241, ppl=1.18, wps=65422.8, ups=16.57, wpb=3948.8, bsz=222.1, num_updates=57400, lr=0.000131991, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=3709
2024-01-30 21:08:22 | INFO | train_inner | epoch 043:    848 / 1349 loss=1.846, nll_loss=0.246, ppl=1.19, wps=65035.1, ups=16.57, wpb=3923.8, bsz=222.4, num_updates=57500, lr=0.000131876, gnorm=0.561, loss_scale=16, train_wall=6, gb_free=29.7, wall=3715
2024-01-30 21:08:28 | INFO | train_inner | epoch 043:    948 / 1349 loss=1.85, nll_loss=0.25, ppl=1.19, wps=65510, ups=16.79, wpb=3902.4, bsz=216.6, num_updates=57600, lr=0.000131762, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.8, wall=3721
2024-01-30 21:08:34 | INFO | train_inner | epoch 043:   1048 / 1349 loss=1.852, nll_loss=0.252, ppl=1.19, wps=65035, ups=16.62, wpb=3912.4, bsz=213.3, num_updates=57700, lr=0.000131647, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=3727
2024-01-30 21:08:40 | INFO | train_inner | epoch 043:   1148 / 1349 loss=1.856, nll_loss=0.257, ppl=1.19, wps=65834.6, ups=16.61, wpb=3963.6, bsz=210.3, num_updates=57800, lr=0.000131533, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=3733
2024-01-30 21:08:46 | INFO | train_inner | epoch 043:   1248 / 1349 loss=1.849, nll_loss=0.25, ppl=1.19, wps=66064.6, ups=16.7, wpb=3957.1, bsz=215.3, num_updates=57900, lr=0.00013142, gnorm=0.495, loss_scale=16, train_wall=6, gb_free=29.7, wall=3739
2024-01-30 21:08:52 | INFO | train_inner | epoch 043:   1348 / 1349 loss=1.854, nll_loss=0.254, ppl=1.19, wps=65268.5, ups=16.9, wpb=3862.6, bsz=215.1, num_updates=58000, lr=0.000131306, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=3745
2024-01-30 21:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:08:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:08:53 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 2.084 | nll_loss 0.326 | ppl 1.25 | wps 171980 | wpb 3275.2 | bsz 176.5 | num_updates 58001 | best_loss 2.063
2024-01-30 21:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 58001 updates
2024-01-30 21:08:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint43.pt
2024-01-30 21:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint43.pt
2024-01-30 21:08:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint43.pt (epoch 43 @ 58001 updates, score 2.084) (writing took 2.290105971042067 seconds)
2024-01-30 21:08:56 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-01-30 21:08:56 | INFO | train | epoch 043 | loss 1.848 | nll_loss 0.248 | ppl 1.19 | wps 61066.4 | ups 15.57 | wpb 3921.2 | bsz 214.5 | num_updates 58001 | lr 0.000131305 | gnorm 0.499 | loss_scale 16 | train_wall 80 | gb_free 29.8 | wall 3749
2024-01-30 21:08:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:08:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:08:56 | INFO | fairseq.trainer | begin training epoch 44
2024-01-30 21:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:09:02 | INFO | train_inner | epoch 044:     99 / 1349 loss=1.843, nll_loss=0.242, ppl=1.18, wps=38395.1, ups=9.84, wpb=3902.5, bsz=219, num_updates=58100, lr=0.000131193, gnorm=0.468, loss_scale=16, train_wall=6, gb_free=29.8, wall=3755
2024-01-30 21:09:08 | INFO | train_inner | epoch 044:    199 / 1349 loss=1.846, nll_loss=0.246, ppl=1.19, wps=65176.1, ups=16.68, wpb=3908.2, bsz=213, num_updates=58200, lr=0.000131081, gnorm=0.568, loss_scale=16, train_wall=6, gb_free=29.8, wall=3761
2024-01-30 21:09:14 | INFO | train_inner | epoch 044:    299 / 1349 loss=1.844, nll_loss=0.244, ppl=1.18, wps=59154.9, ups=15.11, wpb=3915.4, bsz=222.8, num_updates=58300, lr=0.000130968, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.7, wall=3768
2024-01-30 21:09:20 | INFO | train_inner | epoch 044:    399 / 1349 loss=1.845, nll_loss=0.244, ppl=1.18, wps=63166.8, ups=16.15, wpb=3911.8, bsz=217.8, num_updates=58400, lr=0.000130856, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=3774
2024-01-30 21:09:26 | INFO | train_inner | epoch 044:    499 / 1349 loss=1.848, nll_loss=0.249, ppl=1.19, wps=65604.2, ups=16.66, wpb=3937.2, bsz=215.7, num_updates=58500, lr=0.000130744, gnorm=0.524, loss_scale=16, train_wall=6, gb_free=29.7, wall=3780
2024-01-30 21:09:41 | INFO | train_inner | epoch 044:    599 / 1349 loss=1.845, nll_loss=0.245, ppl=1.19, wps=26723, ups=6.78, wpb=3942.5, bsz=223, num_updates=58600, lr=0.000130632, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.7, wall=3794
2024-01-30 21:09:47 | INFO | train_inner | epoch 044:    699 / 1349 loss=1.837, nll_loss=0.236, ppl=1.18, wps=63988.6, ups=16.55, wpb=3866.4, bsz=219.8, num_updates=58700, lr=0.000130521, gnorm=0.416, loss_scale=16, train_wall=6, gb_free=29.8, wall=3801
2024-01-30 21:09:53 | INFO | train_inner | epoch 044:    799 / 1349 loss=1.85, nll_loss=0.251, ppl=1.19, wps=65525.7, ups=16.66, wpb=3933, bsz=211, num_updates=58800, lr=0.00013041, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.8, wall=3807
2024-01-30 21:09:59 | INFO | train_inner | epoch 044:    899 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=65152.2, ups=16.6, wpb=3925.7, bsz=217.8, num_updates=58900, lr=0.000130299, gnorm=0.558, loss_scale=16, train_wall=6, gb_free=29.7, wall=3813
2024-01-30 21:10:05 | INFO | train_inner | epoch 044:    999 / 1349 loss=1.847, nll_loss=0.247, ppl=1.19, wps=65495.6, ups=16.64, wpb=3935.2, bsz=212.9, num_updates=59000, lr=0.000130189, gnorm=0.537, loss_scale=16, train_wall=6, gb_free=29.8, wall=3819
2024-01-30 21:10:11 | INFO | train_inner | epoch 044:   1099 / 1349 loss=1.85, nll_loss=0.25, ppl=1.19, wps=65496.1, ups=16.66, wpb=3931.2, bsz=212.5, num_updates=59100, lr=0.000130079, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.9, wall=3825
2024-01-30 21:10:17 | INFO | train_inner | epoch 044:   1199 / 1349 loss=1.859, nll_loss=0.26, ppl=1.2, wps=66015.2, ups=16.79, wpb=3931.7, bsz=192.8, num_updates=59200, lr=0.000129969, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.8, wall=3831
2024-01-30 21:10:23 | INFO | train_inner | epoch 044:   1299 / 1349 loss=1.85, nll_loss=0.25, ppl=1.19, wps=65610.2, ups=16.7, wpb=3929.6, bsz=202.1, num_updates=59300, lr=0.000129859, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=3836
2024-01-30 21:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:10:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:10:28 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 2.083 | nll_loss 0.327 | ppl 1.25 | wps 163383 | wpb 3275.2 | bsz 176.5 | num_updates 59350 | best_loss 2.063
2024-01-30 21:10:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 59350 updates
2024-01-30 21:10:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint44.pt
2024-01-30 21:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint44.pt
2024-01-30 21:10:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint44.pt (epoch 44 @ 59350 updates, score 2.083) (writing took 2.2826743309851736 seconds)
2024-01-30 21:10:30 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-01-30 21:10:30 | INFO | train | epoch 044 | loss 1.846 | nll_loss 0.246 | ppl 1.19 | wps 55862.2 | ups 14.25 | wpb 3921.2 | bsz 214.5 | num_updates 59350 | lr 0.000129804 | gnorm 0.508 | loss_scale 16 | train_wall 79 | gb_free 29.7 | wall 3844
2024-01-30 21:10:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:10:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:10:30 | INFO | fairseq.trainer | begin training epoch 45
2024-01-30 21:10:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:10:33 | INFO | train_inner | epoch 045:     50 / 1349 loss=1.833, nll_loss=0.232, ppl=1.17, wps=38374.7, ups=9.8, wpb=3915.2, bsz=232.5, num_updates=59400, lr=0.00012975, gnorm=0.481, loss_scale=16, train_wall=6, gb_free=29.8, wall=3847
2024-01-30 21:10:40 | INFO | train_inner | epoch 045:    150 / 1349 loss=1.842, nll_loss=0.242, ppl=1.18, wps=65732.4, ups=16.62, wpb=3955.1, bsz=222.4, num_updates=59500, lr=0.000129641, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=3853
2024-01-30 21:10:45 | INFO | train_inner | epoch 045:    250 / 1349 loss=1.843, nll_loss=0.242, ppl=1.18, wps=64851.5, ups=16.71, wpb=3881.9, bsz=205.7, num_updates=59600, lr=0.000129532, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.7, wall=3859
2024-01-30 21:10:51 | INFO | train_inner | epoch 045:    350 / 1349 loss=1.845, nll_loss=0.245, ppl=1.19, wps=65623.5, ups=16.65, wpb=3941.4, bsz=214.2, num_updates=59700, lr=0.000129423, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=3865
2024-01-30 21:10:57 | INFO | train_inner | epoch 045:    450 / 1349 loss=1.844, nll_loss=0.243, ppl=1.18, wps=65403.3, ups=16.72, wpb=3912.2, bsz=216.1, num_updates=59800, lr=0.000129315, gnorm=0.478, loss_scale=16, train_wall=6, gb_free=29.9, wall=3871
2024-01-30 21:11:03 | INFO | train_inner | epoch 045:    550 / 1349 loss=1.842, nll_loss=0.241, ppl=1.18, wps=65392, ups=16.64, wpb=3928.8, bsz=208.6, num_updates=59900, lr=0.000129207, gnorm=0.457, loss_scale=16, train_wall=6, gb_free=29.9, wall=3877
2024-01-30 21:11:09 | INFO | train_inner | epoch 045:    650 / 1349 loss=1.845, nll_loss=0.245, ppl=1.19, wps=64637.4, ups=16.71, wpb=3868.9, bsz=212.5, num_updates=60000, lr=0.000129099, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.8, wall=3883
2024-01-30 21:11:17 | INFO | train_inner | epoch 045:    750 / 1349 loss=1.847, nll_loss=0.247, ppl=1.19, wps=50202, ups=12.71, wpb=3948.9, bsz=212.3, num_updates=60100, lr=0.000128992, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.8, wall=3891
2024-01-30 21:11:24 | INFO | train_inner | epoch 045:    850 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=63828.4, ups=16.19, wpb=3941.5, bsz=226.1, num_updates=60200, lr=0.000128885, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.8, wall=3897
2024-01-30 21:11:29 | INFO | train_inner | epoch 045:    950 / 1349 loss=1.848, nll_loss=0.248, ppl=1.19, wps=65214.8, ups=16.74, wpb=3896, bsz=204, num_updates=60300, lr=0.000128778, gnorm=0.58, loss_scale=16, train_wall=6, gb_free=29.7, wall=3903
2024-01-30 21:11:44 | INFO | train_inner | epoch 045:   1050 / 1349 loss=1.851, nll_loss=0.252, ppl=1.19, wps=26524.5, ups=6.74, wpb=3935.9, bsz=217.3, num_updates=60400, lr=0.000128671, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=3918
2024-01-30 21:11:50 | INFO | train_inner | epoch 045:   1150 / 1349 loss=1.845, nll_loss=0.245, ppl=1.18, wps=65164.4, ups=16.63, wpb=3917.3, bsz=212.5, num_updates=60500, lr=0.000128565, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=3924
2024-01-30 21:11:56 | INFO | train_inner | epoch 045:   1250 / 1349 loss=1.845, nll_loss=0.245, ppl=1.19, wps=65597.6, ups=16.67, wpb=3934, bsz=217.1, num_updates=60600, lr=0.000128459, gnorm=0.457, loss_scale=16, train_wall=6, gb_free=29.8, wall=3930
2024-01-30 21:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:12:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:12:04 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 2.089 | nll_loss 0.33 | ppl 1.26 | wps 172037 | wpb 3275.2 | bsz 176.5 | num_updates 60699 | best_loss 2.063
2024-01-30 21:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 60699 updates
2024-01-30 21:12:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint45.pt
2024-01-30 21:12:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint45.pt
2024-01-30 21:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint45.pt (epoch 45 @ 60699 updates, score 2.089) (writing took 2.3367486820789054 seconds)
2024-01-30 21:12:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-01-30 21:12:07 | INFO | train | epoch 045 | loss 1.844 | nll_loss 0.244 | ppl 1.18 | wps 54837.9 | ups 13.99 | wpb 3921.2 | bsz 214.5 | num_updates 60699 | lr 0.000128354 | gnorm 0.502 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 3940
2024-01-30 21:12:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:12:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:12:07 | INFO | fairseq.trainer | begin training epoch 46
2024-01-30 21:12:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:12:07 | INFO | train_inner | epoch 046:      1 / 1349 loss=1.843, nll_loss=0.244, ppl=1.18, wps=36950.4, ups=9.41, wpb=3925.6, bsz=210.8, num_updates=60700, lr=0.000128353, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.8, wall=3940
2024-01-30 21:12:13 | INFO | train_inner | epoch 046:    101 / 1349 loss=1.836, nll_loss=0.235, ppl=1.18, wps=65209.7, ups=16.6, wpb=3927.5, bsz=207.4, num_updates=60800, lr=0.000128247, gnorm=0.474, loss_scale=16, train_wall=6, gb_free=29.8, wall=3946
2024-01-30 21:12:19 | INFO | train_inner | epoch 046:    201 / 1349 loss=1.841, nll_loss=0.24, ppl=1.18, wps=64970, ups=16.71, wpb=3889.1, bsz=217.9, num_updates=60900, lr=0.000128142, gnorm=0.461, loss_scale=16, train_wall=6, gb_free=29.8, wall=3952
2024-01-30 21:12:25 | INFO | train_inner | epoch 046:    301 / 1349 loss=1.836, nll_loss=0.236, ppl=1.18, wps=65185.3, ups=16.67, wpb=3909.9, bsz=220, num_updates=61000, lr=0.000128037, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.8, wall=3958
2024-01-30 21:12:31 | INFO | train_inner | epoch 046:    401 / 1349 loss=1.841, nll_loss=0.241, ppl=1.18, wps=65142.2, ups=16.76, wpb=3887.4, bsz=210.7, num_updates=61100, lr=0.000127932, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=3964
2024-01-30 21:12:37 | INFO | train_inner | epoch 046:    501 / 1349 loss=1.842, nll_loss=0.242, ppl=1.18, wps=64705.6, ups=16.63, wpb=3890.3, bsz=217.8, num_updates=61200, lr=0.000127827, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=3970
2024-01-30 21:12:43 | INFO | train_inner | epoch 046:    601 / 1349 loss=1.843, nll_loss=0.243, ppl=1.18, wps=65694.8, ups=16.69, wpb=3935.7, bsz=207.6, num_updates=61300, lr=0.000127723, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.8, wall=3976
2024-01-30 21:12:49 | INFO | train_inner | epoch 046:    701 / 1349 loss=1.853, nll_loss=0.253, ppl=1.19, wps=65472.5, ups=16.8, wpb=3896.8, bsz=197.7, num_updates=61400, lr=0.000127619, gnorm=0.552, loss_scale=16, train_wall=6, gb_free=29.7, wall=3982
2024-01-30 21:12:55 | INFO | train_inner | epoch 046:    801 / 1349 loss=1.835, nll_loss=0.234, ppl=1.18, wps=65068.6, ups=16.54, wpb=3934.2, bsz=222.2, num_updates=61500, lr=0.000127515, gnorm=0.452, loss_scale=16, train_wall=6, gb_free=29.7, wall=3988
2024-01-30 21:13:01 | INFO | train_inner | epoch 046:    901 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=65970.7, ups=16.72, wpb=3945, bsz=220.4, num_updates=61600, lr=0.000127412, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=3994
2024-01-30 21:13:07 | INFO | train_inner | epoch 046:   1001 / 1349 loss=1.841, nll_loss=0.242, ppl=1.18, wps=65475.4, ups=16.54, wpb=3958.2, bsz=219.9, num_updates=61700, lr=0.000127309, gnorm=0.472, loss_scale=16, train_wall=6, gb_free=29.7, wall=4000
2024-01-30 21:13:13 | INFO | train_inner | epoch 046:   1101 / 1349 loss=1.847, nll_loss=0.248, ppl=1.19, wps=65721, ups=16.7, wpb=3935.7, bsz=217, num_updates=61800, lr=0.000127205, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.8, wall=4006
2024-01-30 21:13:19 | INFO | train_inner | epoch 046:   1201 / 1349 loss=1.845, nll_loss=0.245, ppl=1.19, wps=65387.4, ups=16.61, wpb=3937.5, bsz=229.9, num_updates=61900, lr=0.000127103, gnorm=0.463, loss_scale=16, train_wall=6, gb_free=29.8, wall=4012
2024-01-30 21:13:25 | INFO | train_inner | epoch 046:   1301 / 1349 loss=1.841, nll_loss=0.241, ppl=1.18, wps=65199.7, ups=16.63, wpb=3920.4, bsz=204.4, num_updates=62000, lr=0.000127, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.7, wall=4018
2024-01-30 21:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:13:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:13:30 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 2.091 | nll_loss 0.332 | ppl 1.26 | wps 171504 | wpb 3275.2 | bsz 176.5 | num_updates 62048 | best_loss 2.063
2024-01-30 21:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 62048 updates
2024-01-30 21:13:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint46.pt
2024-01-30 21:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint46.pt
2024-01-30 21:13:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint46.pt (epoch 46 @ 62048 updates, score 2.091) (writing took 2.2946825310355052 seconds)
2024-01-30 21:13:32 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-01-30 21:13:32 | INFO | train | epoch 046 | loss 1.842 | nll_loss 0.242 | ppl 1.18 | wps 62174.5 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 62048 | lr 0.000126951 | gnorm 0.493 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 4025
2024-01-30 21:13:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:13:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:13:32 | INFO | fairseq.trainer | begin training epoch 47
2024-01-30 21:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:13:35 | INFO | train_inner | epoch 047:     52 / 1349 loss=1.844, nll_loss=0.243, ppl=1.18, wps=38517.8, ups=9.93, wpb=3879.8, bsz=198.6, num_updates=62100, lr=0.000126898, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.9, wall=4028
2024-01-30 21:13:41 | INFO | train_inner | epoch 047:    152 / 1349 loss=1.837, nll_loss=0.237, ppl=1.18, wps=66004.1, ups=16.67, wpb=3959.1, bsz=220.6, num_updates=62200, lr=0.000126796, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=4034
2024-01-30 21:13:47 | INFO | train_inner | epoch 047:    252 / 1349 loss=1.841, nll_loss=0.241, ppl=1.18, wps=65332.4, ups=16.74, wpb=3903.3, bsz=213, num_updates=62300, lr=0.000126694, gnorm=0.462, loss_scale=16, train_wall=6, gb_free=29.8, wall=4040
2024-01-30 21:13:53 | INFO | train_inner | epoch 047:    352 / 1349 loss=1.841, nll_loss=0.24, ppl=1.18, wps=65285.4, ups=16.74, wpb=3900.2, bsz=213.4, num_updates=62400, lr=0.000126592, gnorm=0.492, loss_scale=16, train_wall=6, gb_free=29.8, wall=4046
2024-01-30 21:13:59 | INFO | train_inner | epoch 047:    452 / 1349 loss=1.842, nll_loss=0.242, ppl=1.18, wps=66017.5, ups=16.69, wpb=3955.1, bsz=197.5, num_updates=62500, lr=0.000126491, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.8, wall=4052
2024-01-30 21:14:05 | INFO | train_inner | epoch 047:    552 / 1349 loss=1.829, nll_loss=0.228, ppl=1.17, wps=64739.4, ups=16.51, wpb=3922.3, bsz=226.1, num_updates=62600, lr=0.00012639, gnorm=0.453, loss_scale=16, train_wall=6, gb_free=29.7, wall=4058
2024-01-30 21:14:11 | INFO | train_inner | epoch 047:    652 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=64983.4, ups=16.64, wpb=3905.1, bsz=216.4, num_updates=62700, lr=0.000126289, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=4064
2024-01-30 21:14:17 | INFO | train_inner | epoch 047:    752 / 1349 loss=1.841, nll_loss=0.241, ppl=1.18, wps=65379.4, ups=16.66, wpb=3924.7, bsz=217.9, num_updates=62800, lr=0.000126189, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.9, wall=4070
2024-01-30 21:14:23 | INFO | train_inner | epoch 047:    852 / 1349 loss=1.842, nll_loss=0.243, ppl=1.18, wps=65630.7, ups=16.63, wpb=3946.8, bsz=200.7, num_updates=62900, lr=0.000126088, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.7, wall=4076
2024-01-30 21:14:29 | INFO | train_inner | epoch 047:    952 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=65538.4, ups=16.61, wpb=3945.6, bsz=225.1, num_updates=63000, lr=0.000125988, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.7, wall=4082
2024-01-30 21:14:35 | INFO | train_inner | epoch 047:   1052 / 1349 loss=1.839, nll_loss=0.239, ppl=1.18, wps=65405.9, ups=16.68, wpb=3921.5, bsz=219.4, num_updates=63100, lr=0.000125888, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.8, wall=4088
2024-01-30 21:14:41 | INFO | train_inner | epoch 047:   1152 / 1349 loss=1.842, nll_loss=0.243, ppl=1.18, wps=64677.7, ups=16.47, wpb=3927.4, bsz=225, num_updates=63200, lr=0.000125789, gnorm=0.481, loss_scale=16, train_wall=6, gb_free=29.7, wall=4094
2024-01-30 21:14:47 | INFO | train_inner | epoch 047:   1252 / 1349 loss=1.846, nll_loss=0.246, ppl=1.19, wps=65030.9, ups=16.68, wpb=3897.9, bsz=214.2, num_updates=63300, lr=0.000125689, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.7, wall=4100
2024-01-30 21:14:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:14:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:14:55 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 2.091 | nll_loss 0.332 | ppl 1.26 | wps 172256 | wpb 3275.2 | bsz 176.5 | num_updates 63397 | best_loss 2.063
2024-01-30 21:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 63397 updates
2024-01-30 21:14:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint47.pt
2024-01-30 21:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint47.pt
2024-01-30 21:14:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint47.pt (epoch 47 @ 63397 updates, score 2.091) (writing took 2.2910440230043605 seconds)
2024-01-30 21:14:57 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-01-30 21:14:57 | INFO | train | epoch 047 | loss 1.84 | nll_loss 0.24 | ppl 1.18 | wps 62180.3 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 63397 | lr 0.000125593 | gnorm 0.491 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 4110
2024-01-30 21:14:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:14:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:14:57 | INFO | fairseq.trainer | begin training epoch 48
2024-01-30 21:14:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:14:57 | INFO | train_inner | epoch 048:      3 / 1349 loss=1.846, nll_loss=0.246, ppl=1.19, wps=38782.3, ups=9.94, wpb=3902.6, bsz=207.4, num_updates=63400, lr=0.00012559, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=30, wall=4110
2024-01-30 21:15:03 | INFO | train_inner | epoch 048:    103 / 1349 loss=1.826, nll_loss=0.225, ppl=1.17, wps=65674.9, ups=16.57, wpb=3964.3, bsz=226.3, num_updates=63500, lr=0.000125491, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=4116
2024-01-30 21:15:09 | INFO | train_inner | epoch 048:    203 / 1349 loss=1.834, nll_loss=0.233, ppl=1.18, wps=65134.9, ups=16.57, wpb=3929.7, bsz=217.2, num_updates=63600, lr=0.000125392, gnorm=0.456, loss_scale=16, train_wall=6, gb_free=29.8, wall=4123
2024-01-30 21:15:15 | INFO | train_inner | epoch 048:    303 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=64604.9, ups=16.7, wpb=3867.4, bsz=208.2, num_updates=63700, lr=0.000125294, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.7, wall=4129
2024-01-30 21:15:21 | INFO | train_inner | epoch 048:    403 / 1349 loss=1.842, nll_loss=0.242, ppl=1.18, wps=66234.9, ups=16.73, wpb=3958.3, bsz=213.3, num_updates=63800, lr=0.000125196, gnorm=0.547, loss_scale=16, train_wall=6, gb_free=29.8, wall=4134
2024-01-30 21:15:27 | INFO | train_inner | epoch 048:    503 / 1349 loss=1.837, nll_loss=0.237, ppl=1.18, wps=65251.7, ups=16.59, wpb=3933.1, bsz=209.2, num_updates=63900, lr=0.000125098, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=4141
2024-01-30 21:15:33 | INFO | train_inner | epoch 048:    603 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=65730.4, ups=16.6, wpb=3959, bsz=207, num_updates=64000, lr=0.000125, gnorm=0.527, loss_scale=16, train_wall=6, gb_free=29.8, wall=4147
2024-01-30 21:15:39 | INFO | train_inner | epoch 048:    703 / 1349 loss=1.835, nll_loss=0.235, ppl=1.18, wps=64510.7, ups=16.65, wpb=3874, bsz=224, num_updates=64100, lr=0.000124902, gnorm=0.485, loss_scale=16, train_wall=6, gb_free=29.9, wall=4153
2024-01-30 21:15:45 | INFO | train_inner | epoch 048:    803 / 1349 loss=1.851, nll_loss=0.251, ppl=1.19, wps=66154.4, ups=16.86, wpb=3923, bsz=199.9, num_updates=64200, lr=0.000124805, gnorm=0.581, loss_scale=16, train_wall=6, gb_free=29.8, wall=4158
2024-01-30 21:15:51 | INFO | train_inner | epoch 048:    903 / 1349 loss=1.847, nll_loss=0.247, ppl=1.19, wps=65752.8, ups=16.74, wpb=3926.9, bsz=209.9, num_updates=64300, lr=0.000124708, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.8, wall=4164
2024-01-30 21:15:57 | INFO | train_inner | epoch 048:   1003 / 1349 loss=1.837, nll_loss=0.237, ppl=1.18, wps=64113.1, ups=16.57, wpb=3870.3, bsz=227.7, num_updates=64400, lr=0.000124611, gnorm=0.514, loss_scale=16, train_wall=6, gb_free=29.7, wall=4170
2024-01-30 21:16:03 | INFO | train_inner | epoch 048:   1103 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=65706.7, ups=16.7, wpb=3935.2, bsz=211.5, num_updates=64500, lr=0.000124515, gnorm=0.508, loss_scale=16, train_wall=6, gb_free=29.7, wall=4176
2024-01-30 21:16:09 | INFO | train_inner | epoch 048:   1203 / 1349 loss=1.841, nll_loss=0.242, ppl=1.18, wps=65473.8, ups=16.6, wpb=3943.4, bsz=218.3, num_updates=64600, lr=0.000124418, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=4182
2024-01-30 21:16:15 | INFO | train_inner | epoch 048:   1303 / 1349 loss=1.839, nll_loss=0.238, ppl=1.18, wps=65203.3, ups=16.63, wpb=3921, bsz=217.5, num_updates=64700, lr=0.000124322, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.8, wall=4189
2024-01-30 21:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:16:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:16:20 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 2.091 | nll_loss 0.334 | ppl 1.26 | wps 171490 | wpb 3275.2 | bsz 176.5 | num_updates 64746 | best_loss 2.063
2024-01-30 21:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 64746 updates
2024-01-30 21:16:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint48.pt
2024-01-30 21:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint48.pt
2024-01-30 21:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint48.pt (epoch 48 @ 64746 updates, score 2.091) (writing took 2.5090349559905007 seconds)
2024-01-30 21:16:22 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-01-30 21:16:22 | INFO | train | epoch 048 | loss 1.839 | nll_loss 0.238 | ppl 1.18 | wps 61971.9 | ups 15.8 | wpb 3921.2 | bsz 214.5 | num_updates 64746 | lr 0.000124278 | gnorm 0.506 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 4196
2024-01-30 21:16:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:16:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:16:22 | INFO | fairseq.trainer | begin training epoch 49
2024-01-30 21:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:16:26 | INFO | train_inner | epoch 049:     54 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=36875.6, ups=9.47, wpb=3892, bsz=219, num_updates=64800, lr=0.000124226, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=4199
2024-01-30 21:16:32 | INFO | train_inner | epoch 049:    154 / 1349 loss=1.834, nll_loss=0.233, ppl=1.18, wps=60448, ups=15.38, wpb=3930.6, bsz=215.3, num_updates=64900, lr=0.00012413, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.7, wall=4206
2024-01-30 21:16:38 | INFO | train_inner | epoch 049:    254 / 1349 loss=1.829, nll_loss=0.228, ppl=1.17, wps=64285.1, ups=16.63, wpb=3865.2, bsz=220.6, num_updates=65000, lr=0.000124035, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.9, wall=4212
2024-01-30 21:16:44 | INFO | train_inner | epoch 049:    354 / 1349 loss=1.834, nll_loss=0.234, ppl=1.18, wps=64896.6, ups=16.62, wpb=3904.1, bsz=213.4, num_updates=65100, lr=0.000123939, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.8, wall=4218
2024-01-30 21:16:50 | INFO | train_inner | epoch 049:    454 / 1349 loss=1.846, nll_loss=0.246, ppl=1.19, wps=65756, ups=16.8, wpb=3914.3, bsz=196.5, num_updates=65200, lr=0.000123844, gnorm=0.572, loss_scale=16, train_wall=6, gb_free=29.7, wall=4224
2024-01-30 21:16:56 | INFO | train_inner | epoch 049:    554 / 1349 loss=1.825, nll_loss=0.224, ppl=1.17, wps=65310, ups=16.61, wpb=3933.1, bsz=221.7, num_updates=65300, lr=0.000123749, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.8, wall=4230
2024-01-30 21:17:02 | INFO | train_inner | epoch 049:    654 / 1349 loss=1.836, nll_loss=0.236, ppl=1.18, wps=65881.4, ups=16.61, wpb=3965.4, bsz=217.9, num_updates=65400, lr=0.000123655, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.7, wall=4236
2024-01-30 21:17:08 | INFO | train_inner | epoch 049:    754 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=66012.8, ups=16.65, wpb=3964.4, bsz=214, num_updates=65500, lr=0.00012356, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.7, wall=4242
2024-01-30 21:17:14 | INFO | train_inner | epoch 049:    854 / 1349 loss=1.842, nll_loss=0.242, ppl=1.18, wps=65166.7, ups=16.63, wpb=3918.2, bsz=221.4, num_updates=65600, lr=0.000123466, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.7, wall=4248
2024-01-30 21:17:20 | INFO | train_inner | epoch 049:    954 / 1349 loss=1.832, nll_loss=0.231, ppl=1.17, wps=64525, ups=16.51, wpb=3908, bsz=218.9, num_updates=65700, lr=0.000123372, gnorm=0.459, loss_scale=16, train_wall=6, gb_free=29.8, wall=4254
2024-01-30 21:17:26 | INFO | train_inner | epoch 049:   1054 / 1349 loss=1.848, nll_loss=0.249, ppl=1.19, wps=65730.6, ups=16.83, wpb=3906.7, bsz=201.5, num_updates=65800, lr=0.000123278, gnorm=0.545, loss_scale=16, train_wall=6, gb_free=29.8, wall=4260
2024-01-30 21:17:32 | INFO | train_inner | epoch 049:   1154 / 1349 loss=1.836, nll_loss=0.236, ppl=1.18, wps=65896.4, ups=16.59, wpb=3973.2, bsz=215.4, num_updates=65900, lr=0.000123185, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.8, wall=4266
2024-01-30 21:17:38 | INFO | train_inner | epoch 049:   1254 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=64716.1, ups=16.59, wpb=3900.8, bsz=212.8, num_updates=66000, lr=0.000123091, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=4272
2024-01-30 21:17:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:17:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:17:46 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 2.091 | nll_loss 0.338 | ppl 1.26 | wps 172941 | wpb 3275.2 | bsz 176.5 | num_updates 66095 | best_loss 2.063
2024-01-30 21:17:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 66095 updates
2024-01-30 21:17:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint49.pt
2024-01-30 21:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint49.pt
2024-01-30 21:17:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint49.pt (epoch 49 @ 66095 updates, score 2.091) (writing took 2.2364887830335647 seconds)
2024-01-30 21:17:48 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-01-30 21:17:48 | INFO | train | epoch 049 | loss 1.837 | nll_loss 0.236 | ppl 1.18 | wps 61687.4 | ups 15.73 | wpb 3921.2 | bsz 214.5 | num_updates 66095 | lr 0.000123003 | gnorm 0.499 | loss_scale 16 | train_wall 79 | gb_free 29.8 | wall 4281
2024-01-30 21:17:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:17:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:17:48 | INFO | fairseq.trainer | begin training epoch 50
2024-01-30 21:17:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:17:48 | INFO | train_inner | epoch 050:      5 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=38970.6, ups=9.98, wpb=3905.3, bsz=212.7, num_updates=66100, lr=0.000122998, gnorm=0.489, loss_scale=16, train_wall=6, gb_free=29.7, wall=4282
2024-01-30 21:17:54 | INFO | train_inner | epoch 050:    105 / 1349 loss=1.833, nll_loss=0.232, ppl=1.17, wps=65389.2, ups=16.65, wpb=3928.2, bsz=217, num_updates=66200, lr=0.000122905, gnorm=0.466, loss_scale=16, train_wall=6, gb_free=29.8, wall=4288
2024-01-30 21:18:00 | INFO | train_inner | epoch 050:    205 / 1349 loss=1.829, nll_loss=0.228, ppl=1.17, wps=65791.6, ups=16.63, wpb=3955.7, bsz=225, num_updates=66300, lr=0.000122813, gnorm=0.485, loss_scale=16, train_wall=6, gb_free=29.8, wall=4294
2024-01-30 21:18:06 | INFO | train_inner | epoch 050:    305 / 1349 loss=1.835, nll_loss=0.234, ppl=1.18, wps=65677.9, ups=16.73, wpb=3926.8, bsz=228.8, num_updates=66400, lr=0.00012272, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=4300
2024-01-30 21:18:13 | INFO | train_inner | epoch 050:    405 / 1349 loss=1.834, nll_loss=0.234, ppl=1.18, wps=63342.6, ups=16.35, wpb=3875.3, bsz=201.5, num_updates=66500, lr=0.000122628, gnorm=0.476, loss_scale=16, train_wall=6, gb_free=29.7, wall=4306
2024-01-30 21:18:19 | INFO | train_inner | epoch 050:    505 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=64837.2, ups=16.54, wpb=3919.8, bsz=193.8, num_updates=66600, lr=0.000122536, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=4312
2024-01-30 21:18:25 | INFO | train_inner | epoch 050:    605 / 1349 loss=1.839, nll_loss=0.239, ppl=1.18, wps=65224.5, ups=16.77, wpb=3890.1, bsz=213, num_updates=66700, lr=0.000122444, gnorm=0.581, loss_scale=16, train_wall=6, gb_free=29.8, wall=4318
2024-01-30 21:18:31 | INFO | train_inner | epoch 050:    705 / 1349 loss=1.835, nll_loss=0.234, ppl=1.18, wps=65839.9, ups=16.56, wpb=3976.6, bsz=213.3, num_updates=66800, lr=0.000122352, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.8, wall=4324
2024-01-30 21:18:37 | INFO | train_inner | epoch 050:    805 / 1349 loss=1.84, nll_loss=0.24, ppl=1.18, wps=64873.8, ups=16.71, wpb=3881.7, bsz=208.5, num_updates=66900, lr=0.000122261, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.7, wall=4330
2024-01-30 21:18:43 | INFO | train_inner | epoch 050:    905 / 1349 loss=1.821, nll_loss=0.22, ppl=1.17, wps=65261.2, ups=16.48, wpb=3959.6, bsz=222.9, num_updates=67000, lr=0.000122169, gnorm=0.449, loss_scale=16, train_wall=6, gb_free=29.7, wall=4336
2024-01-30 21:18:49 | INFO | train_inner | epoch 050:   1005 / 1349 loss=1.834, nll_loss=0.234, ppl=1.18, wps=64907.5, ups=16.56, wpb=3919.1, bsz=222.4, num_updates=67100, lr=0.000122078, gnorm=0.525, loss_scale=16, train_wall=6, gb_free=29.7, wall=4342
2024-01-30 21:18:55 | INFO | train_inner | epoch 050:   1105 / 1349 loss=1.829, nll_loss=0.229, ppl=1.17, wps=64986.6, ups=16.56, wpb=3924.9, bsz=229.4, num_updates=67200, lr=0.000121988, gnorm=0.481, loss_scale=16, train_wall=6, gb_free=29.7, wall=4348
2024-01-30 21:19:01 | INFO | train_inner | epoch 050:   1205 / 1349 loss=1.84, nll_loss=0.241, ppl=1.18, wps=65841.2, ups=16.6, wpb=3966.8, bsz=224.6, num_updates=67300, lr=0.000121897, gnorm=0.558, loss_scale=16, train_wall=6, gb_free=29.7, wall=4354
2024-01-30 21:19:07 | INFO | train_inner | epoch 050:   1305 / 1349 loss=1.849, nll_loss=0.25, ppl=1.19, wps=65157.2, ups=16.73, wpb=3894.6, bsz=198, num_updates=67400, lr=0.000121806, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=4360
2024-01-30 21:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:19:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:19:11 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 2.095 | nll_loss 0.337 | ppl 1.26 | wps 171678 | wpb 3275.2 | bsz 176.5 | num_updates 67444 | best_loss 2.063
2024-01-30 21:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 67444 updates
2024-01-30 21:19:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint50.pt
2024-01-30 21:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint50.pt
2024-01-30 21:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint50.pt (epoch 50 @ 67444 updates, score 2.095) (writing took 2.2978223260724917 seconds)
2024-01-30 21:19:13 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-01-30 21:19:13 | INFO | train | epoch 050 | loss 1.836 | nll_loss 0.235 | ppl 1.18 | wps 61988.2 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 67444 | lr 0.000121767 | gnorm 0.505 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 4367
2024-01-30 21:19:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:19:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:19:13 | INFO | fairseq.trainer | begin training epoch 51
2024-01-30 21:19:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:19:17 | INFO | train_inner | epoch 051:     56 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=38073.8, ups=9.93, wpb=3832.8, bsz=204.3, num_updates=67500, lr=0.000121716, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.8, wall=4370
2024-01-30 21:19:23 | INFO | train_inner | epoch 051:    156 / 1349 loss=1.833, nll_loss=0.233, ppl=1.17, wps=65409.9, ups=16.7, wpb=3917.4, bsz=216, num_updates=67600, lr=0.000121626, gnorm=0.476, loss_scale=16, train_wall=6, gb_free=29.8, wall=4376
2024-01-30 21:19:29 | INFO | train_inner | epoch 051:    256 / 1349 loss=1.827, nll_loss=0.226, ppl=1.17, wps=65046.5, ups=16.6, wpb=3918.8, bsz=217.9, num_updates=67700, lr=0.000121536, gnorm=0.485, loss_scale=16, train_wall=6, gb_free=30, wall=4382
2024-01-30 21:19:35 | INFO | train_inner | epoch 051:    356 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65654.5, ups=16.58, wpb=3959.7, bsz=223.4, num_updates=67800, lr=0.000121447, gnorm=0.452, loss_scale=16, train_wall=6, gb_free=29.8, wall=4388
2024-01-30 21:19:41 | INFO | train_inner | epoch 051:    456 / 1349 loss=1.836, nll_loss=0.237, ppl=1.18, wps=65217.8, ups=16.66, wpb=3914.2, bsz=224.1, num_updates=67900, lr=0.000121357, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.7, wall=4394
2024-01-30 21:19:47 | INFO | train_inner | epoch 051:    556 / 1349 loss=1.826, nll_loss=0.225, ppl=1.17, wps=64749.4, ups=16.53, wpb=3916.3, bsz=226.7, num_updates=68000, lr=0.000121268, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=4400
2024-01-30 21:19:53 | INFO | train_inner | epoch 051:    656 / 1349 loss=1.833, nll_loss=0.232, ppl=1.17, wps=65480.5, ups=16.67, wpb=3928, bsz=213.3, num_updates=68100, lr=0.000121179, gnorm=0.478, loss_scale=16, train_wall=6, gb_free=29.8, wall=4406
2024-01-30 21:19:59 | INFO | train_inner | epoch 051:    756 / 1349 loss=1.839, nll_loss=0.239, ppl=1.18, wps=65118.2, ups=16.8, wpb=3875.3, bsz=203.8, num_updates=68200, lr=0.00012109, gnorm=0.577, loss_scale=16, train_wall=6, gb_free=29.7, wall=4412
2024-01-30 21:20:05 | INFO | train_inner | epoch 051:    856 / 1349 loss=1.839, nll_loss=0.239, ppl=1.18, wps=65758.3, ups=16.75, wpb=3926.4, bsz=205.4, num_updates=68300, lr=0.000121001, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=4418
2024-01-30 21:20:11 | INFO | train_inner | epoch 051:    956 / 1349 loss=1.833, nll_loss=0.233, ppl=1.18, wps=65053, ups=16.65, wpb=3908.2, bsz=212.5, num_updates=68400, lr=0.000120913, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.7, wall=4424
2024-01-30 21:20:17 | INFO | train_inner | epoch 051:   1056 / 1349 loss=1.835, nll_loss=0.236, ppl=1.18, wps=65514.1, ups=16.6, wpb=3946.5, bsz=209.9, num_updates=68500, lr=0.000120824, gnorm=0.498, loss_scale=16, train_wall=6, gb_free=29.7, wall=4430
2024-01-30 21:20:23 | INFO | train_inner | epoch 051:   1156 / 1349 loss=1.836, nll_loss=0.237, ppl=1.18, wps=65530.5, ups=16.63, wpb=3940.2, bsz=217.9, num_updates=68600, lr=0.000120736, gnorm=0.483, loss_scale=16, train_wall=6, gb_free=29.8, wall=4436
2024-01-30 21:20:29 | INFO | train_inner | epoch 051:   1256 / 1349 loss=1.839, nll_loss=0.24, ppl=1.18, wps=65479.7, ups=16.62, wpb=3940.3, bsz=214.4, num_updates=68700, lr=0.000120648, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=4442
2024-01-30 21:20:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:20:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:20:36 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 2.094 | nll_loss 0.338 | ppl 1.26 | wps 171310 | wpb 3275.2 | bsz 176.5 | num_updates 68793 | best_loss 2.063
2024-01-30 21:20:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 68793 updates
2024-01-30 21:20:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint51.pt
2024-01-30 21:20:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint51.pt
2024-01-30 21:20:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint51.pt (epoch 51 @ 68793 updates, score 2.094) (writing took 2.302242752048187 seconds)
2024-01-30 21:20:39 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-01-30 21:20:39 | INFO | train | epoch 051 | loss 1.834 | nll_loss 0.234 | ppl 1.18 | wps 62153.9 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 68793 | lr 0.000120567 | gnorm 0.494 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 4452
2024-01-30 21:20:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:20:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:20:39 | INFO | fairseq.trainer | begin training epoch 52
2024-01-30 21:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:20:39 | INFO | train_inner | epoch 052:      7 / 1349 loss=1.836, nll_loss=0.237, ppl=1.18, wps=38558.3, ups=9.86, wpb=3910, bsz=208.2, num_updates=68800, lr=0.000120561, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=4452
2024-01-30 21:20:45 | INFO | train_inner | epoch 052:    107 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65647.1, ups=16.62, wpb=3951, bsz=218.5, num_updates=68900, lr=0.000120473, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.9, wall=4458
2024-01-30 21:20:51 | INFO | train_inner | epoch 052:    207 / 1349 loss=1.833, nll_loss=0.232, ppl=1.17, wps=64992.1, ups=16.62, wpb=3910.6, bsz=209.5, num_updates=69000, lr=0.000120386, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.8, wall=4464
2024-01-30 21:20:57 | INFO | train_inner | epoch 052:    307 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65050, ups=16.61, wpb=3915.2, bsz=208.9, num_updates=69100, lr=0.000120299, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=4470
2024-01-30 21:21:03 | INFO | train_inner | epoch 052:    407 / 1349 loss=1.833, nll_loss=0.233, ppl=1.18, wps=65033.7, ups=16.71, wpb=3891.1, bsz=201.1, num_updates=69200, lr=0.000120212, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.7, wall=4476
2024-01-30 21:21:09 | INFO | train_inner | epoch 052:    507 / 1349 loss=1.835, nll_loss=0.235, ppl=1.18, wps=65817.2, ups=16.72, wpb=3935.4, bsz=215.5, num_updates=69300, lr=0.000120125, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.8, wall=4482
2024-01-30 21:21:15 | INFO | train_inner | epoch 052:    607 / 1349 loss=1.835, nll_loss=0.235, ppl=1.18, wps=65344.6, ups=16.82, wpb=3884.2, bsz=211.6, num_updates=69400, lr=0.000120038, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.7, wall=4488
2024-01-30 21:21:21 | INFO | train_inner | epoch 052:    707 / 1349 loss=1.834, nll_loss=0.234, ppl=1.18, wps=65207.9, ups=16.7, wpb=3905, bsz=209.2, num_updates=69500, lr=0.000119952, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.7, wall=4494
2024-01-30 21:21:27 | INFO | train_inner | epoch 052:    807 / 1349 loss=1.829, nll_loss=0.228, ppl=1.17, wps=65426.6, ups=16.63, wpb=3935.3, bsz=222.6, num_updates=69600, lr=0.000119866, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.8, wall=4500
2024-01-30 21:21:33 | INFO | train_inner | epoch 052:    907 / 1349 loss=1.838, nll_loss=0.239, ppl=1.18, wps=65073.1, ups=16.65, wpb=3908.7, bsz=209.1, num_updates=69700, lr=0.00011978, gnorm=0.495, loss_scale=16, train_wall=6, gb_free=29.8, wall=4506
2024-01-30 21:21:39 | INFO | train_inner | epoch 052:   1007 / 1349 loss=1.83, nll_loss=0.231, ppl=1.17, wps=66030.6, ups=16.53, wpb=3994.4, bsz=223.3, num_updates=69800, lr=0.000119694, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=4512
2024-01-30 21:21:45 | INFO | train_inner | epoch 052:   1107 / 1349 loss=1.833, nll_loss=0.234, ppl=1.18, wps=65252.6, ups=16.63, wpb=3923.2, bsz=220.6, num_updates=69900, lr=0.000119608, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.8, wall=4518
2024-01-30 21:21:51 | INFO | train_inner | epoch 052:   1207 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=65220.7, ups=16.68, wpb=3909, bsz=217, num_updates=70000, lr=0.000119523, gnorm=0.508, loss_scale=16, train_wall=6, gb_free=29.7, wall=4524
2024-01-30 21:21:57 | INFO | train_inner | epoch 052:   1307 / 1349 loss=1.835, nll_loss=0.236, ppl=1.18, wps=65714.6, ups=16.69, wpb=3938.5, bsz=223.1, num_updates=70100, lr=0.000119438, gnorm=0.479, loss_scale=16, train_wall=6, gb_free=29.8, wall=4530
2024-01-30 21:22:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:22:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:22:01 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 2.099 | nll_loss 0.346 | ppl 1.27 | wps 173018 | wpb 3275.2 | bsz 176.5 | num_updates 70142 | best_loss 2.063
2024-01-30 21:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 70142 updates
2024-01-30 21:22:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint52.pt
2024-01-30 21:22:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint52.pt
2024-01-30 21:22:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint52.pt (epoch 52 @ 70142 updates, score 2.099) (writing took 2.294462743913755 seconds)
2024-01-30 21:22:04 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-01-30 21:22:04 | INFO | train | epoch 052 | loss 1.833 | nll_loss 0.233 | ppl 1.17 | wps 62164.3 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 70142 | lr 0.000119402 | gnorm 0.497 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 4537
2024-01-30 21:22:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:22:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:22:04 | INFO | fairseq.trainer | begin training epoch 53
2024-01-30 21:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:22:07 | INFO | train_inner | epoch 053:     58 / 1349 loss=1.832, nll_loss=0.231, ppl=1.17, wps=38988.4, ups=9.89, wpb=3940.7, bsz=212.5, num_updates=70200, lr=0.000119352, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.7, wall=4540
2024-01-30 21:22:13 | INFO | train_inner | epoch 053:    158 / 1349 loss=1.825, nll_loss=0.224, ppl=1.17, wps=65010.6, ups=16.61, wpb=3913.2, bsz=212.5, num_updates=70300, lr=0.000119268, gnorm=0.463, loss_scale=16, train_wall=6, gb_free=29.8, wall=4546
2024-01-30 21:22:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-30 21:22:19 | INFO | train_inner | epoch 053:    259 / 1349 loss=1.83, nll_loss=0.23, ppl=1.17, wps=65363.9, ups=16.54, wpb=3950.7, bsz=207.7, num_updates=70400, lr=0.000119183, gnorm=0.457, loss_scale=8, train_wall=6, gb_free=29.8, wall=4552
2024-01-30 21:22:25 | INFO | train_inner | epoch 053:    359 / 1349 loss=1.837, nll_loss=0.237, ppl=1.18, wps=65239, ups=16.82, wpb=3877.7, bsz=200.9, num_updates=70500, lr=0.000119098, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.9, wall=4558
2024-01-30 21:22:31 | INFO | train_inner | epoch 053:    459 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=65292.1, ups=16.53, wpb=3949, bsz=228.4, num_updates=70600, lr=0.000119014, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.8, wall=4564
2024-01-30 21:22:37 | INFO | train_inner | epoch 053:    559 / 1349 loss=1.829, nll_loss=0.229, ppl=1.17, wps=65259.8, ups=16.63, wpb=3925.2, bsz=213.3, num_updates=70700, lr=0.00011893, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.8, wall=4570
2024-01-30 21:22:43 | INFO | train_inner | epoch 053:    659 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65021.3, ups=16.59, wpb=3918.5, bsz=216.1, num_updates=70800, lr=0.000118846, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=4576
2024-01-30 21:22:49 | INFO | train_inner | epoch 053:    759 / 1349 loss=1.837, nll_loss=0.237, ppl=1.18, wps=65535.2, ups=16.75, wpb=3912.6, bsz=207.9, num_updates=70900, lr=0.000118762, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.7, wall=4582
2024-01-30 21:22:55 | INFO | train_inner | epoch 053:    859 / 1349 loss=1.835, nll_loss=0.236, ppl=1.18, wps=65731.3, ups=16.7, wpb=3937, bsz=220.6, num_updates=71000, lr=0.000118678, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.7, wall=4588
2024-01-30 21:23:01 | INFO | train_inner | epoch 053:    959 / 1349 loss=1.834, nll_loss=0.235, ppl=1.18, wps=65744.7, ups=16.75, wpb=3926, bsz=211.4, num_updates=71100, lr=0.000118595, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=4594
2024-01-30 21:23:07 | INFO | train_inner | epoch 053:   1059 / 1349 loss=1.833, nll_loss=0.233, ppl=1.18, wps=64643.5, ups=16.74, wpb=3862.1, bsz=213.8, num_updates=71200, lr=0.000118511, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.8, wall=4600
2024-01-30 21:23:13 | INFO | train_inner | epoch 053:   1159 / 1349 loss=1.834, nll_loss=0.235, ppl=1.18, wps=65606.3, ups=16.6, wpb=3952.6, bsz=214.4, num_updates=71300, lr=0.000118428, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=4606
2024-01-30 21:23:19 | INFO | train_inner | epoch 053:   1259 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65630.9, ups=16.49, wpb=3980.1, bsz=230.6, num_updates=71400, lr=0.000118345, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.8, wall=4612
2024-01-30 21:23:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:23:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:23:26 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 2.1 | nll_loss 0.346 | ppl 1.27 | wps 172602 | wpb 3275.2 | bsz 176.5 | num_updates 71490 | best_loss 2.063
2024-01-30 21:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 71490 updates
2024-01-30 21:23:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint53.pt
2024-01-30 21:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint53.pt
2024-01-30 21:23:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint53.pt (epoch 53 @ 71490 updates, score 2.1) (writing took 3.180368868052028 seconds)
2024-01-30 21:23:30 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-01-30 21:23:30 | INFO | train | epoch 053 | loss 1.831 | nll_loss 0.231 | ppl 1.17 | wps 61163.4 | ups 15.6 | wpb 3921.9 | bsz 214.6 | num_updates 71490 | lr 0.000118271 | gnorm 0.502 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 4623
2024-01-30 21:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:23:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:23:31 | INFO | fairseq.trainer | begin training epoch 54
2024-01-30 21:23:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:23:34 | INFO | train_inner | epoch 054:     10 / 1349 loss=1.831, nll_loss=0.231, ppl=1.17, wps=26117.9, ups=6.77, wpb=3856.3, bsz=210, num_updates=71500, lr=0.000118262, gnorm=0.544, loss_scale=8, train_wall=8, gb_free=29.8, wall=4627
2024-01-30 21:23:40 | INFO | train_inner | epoch 054:    110 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=65216.8, ups=16.71, wpb=3903.5, bsz=220.9, num_updates=71600, lr=0.00011818, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.9, wall=4633
2024-01-30 21:23:46 | INFO | train_inner | epoch 054:    210 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=65396.6, ups=16.59, wpb=3942.5, bsz=212.3, num_updates=71700, lr=0.000118097, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.9, wall=4639
2024-01-30 21:23:52 | INFO | train_inner | epoch 054:    310 / 1349 loss=1.831, nll_loss=0.23, ppl=1.17, wps=65197.9, ups=16.68, wpb=3908.8, bsz=205.6, num_updates=71800, lr=0.000118015, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.7, wall=4645
2024-01-30 21:23:58 | INFO | train_inner | epoch 054:    410 / 1349 loss=1.83, nll_loss=0.23, ppl=1.17, wps=65462.5, ups=16.69, wpb=3921.4, bsz=214.4, num_updates=71900, lr=0.000117933, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.7, wall=4651
2024-01-30 21:24:04 | INFO | train_inner | epoch 054:    510 / 1349 loss=1.831, nll_loss=0.231, ppl=1.17, wps=65038, ups=16.65, wpb=3906.4, bsz=207, num_updates=72000, lr=0.000117851, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=4657
2024-01-30 21:24:10 | INFO | train_inner | epoch 054:    610 / 1349 loss=1.834, nll_loss=0.234, ppl=1.18, wps=65515.3, ups=16.72, wpb=3918.5, bsz=207.9, num_updates=72100, lr=0.000117769, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=4663
2024-01-30 21:24:16 | INFO | train_inner | epoch 054:    710 / 1349 loss=1.835, nll_loss=0.235, ppl=1.18, wps=64981.6, ups=16.74, wpb=3881.7, bsz=216, num_updates=72200, lr=0.000117688, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.7, wall=4669
2024-01-30 21:24:22 | INFO | train_inner | epoch 054:    810 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=65178.3, ups=16.54, wpb=3940.4, bsz=222.1, num_updates=72300, lr=0.000117606, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.7, wall=4675
2024-01-30 21:24:28 | INFO | train_inner | epoch 054:    910 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=64322.2, ups=16.64, wpb=3864.6, bsz=229.9, num_updates=72400, lr=0.000117525, gnorm=0.449, loss_scale=8, train_wall=6, gb_free=29.8, wall=4681
2024-01-30 21:24:34 | INFO | train_inner | epoch 054:   1010 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=65785.1, ups=16.72, wpb=3933.5, bsz=213.4, num_updates=72500, lr=0.000117444, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.7, wall=4687
2024-01-30 21:24:40 | INFO | train_inner | epoch 054:   1110 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=65021.4, ups=16.59, wpb=3920.2, bsz=194.6, num_updates=72600, lr=0.000117363, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.7, wall=4693
2024-01-30 21:24:46 | INFO | train_inner | epoch 054:   1210 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65652.3, ups=16.55, wpb=3966.1, bsz=212.3, num_updates=72700, lr=0.000117282, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.7, wall=4699
2024-01-30 21:24:52 | INFO | train_inner | epoch 054:   1310 / 1349 loss=1.833, nll_loss=0.234, ppl=1.18, wps=65725.1, ups=16.57, wpb=3965.6, bsz=229.3, num_updates=72800, lr=0.000117202, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.9, wall=4705
2024-01-30 21:24:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:24:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:24:56 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 2.104 | nll_loss 0.35 | ppl 1.27 | wps 172056 | wpb 3275.2 | bsz 176.5 | num_updates 72839 | best_loss 2.063
2024-01-30 21:24:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 72839 updates
2024-01-30 21:24:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint54.pt
2024-01-30 21:24:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint54.pt
2024-01-30 21:24:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint54.pt (epoch 54 @ 72839 updates, score 2.104) (writing took 2.3275699679506943 seconds)
2024-01-30 21:24:59 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-01-30 21:24:59 | INFO | train | epoch 054 | loss 1.829 | nll_loss 0.229 | ppl 1.17 | wps 59771 | ups 15.24 | wpb 3921.2 | bsz 214.5 | num_updates 72839 | lr 0.00011717 | gnorm 0.498 | loss_scale 8 | train_wall 80 | gb_free 29.7 | wall 4712
2024-01-30 21:24:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:24:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:24:59 | INFO | fairseq.trainer | begin training epoch 55
2024-01-30 21:24:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:25:02 | INFO | train_inner | epoch 055:     61 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=38400.5, ups=9.83, wpb=3905, bsz=221.5, num_updates=72900, lr=0.000117121, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=30, wall=4716
2024-01-30 21:25:08 | INFO | train_inner | epoch 055:    161 / 1349 loss=1.822, nll_loss=0.221, ppl=1.17, wps=65061.9, ups=16.55, wpb=3931.9, bsz=228.8, num_updates=73000, lr=0.000117041, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.7, wall=4722
2024-01-30 21:25:14 | INFO | train_inner | epoch 055:    261 / 1349 loss=1.822, nll_loss=0.221, ppl=1.17, wps=65428.6, ups=16.55, wpb=3952.5, bsz=217.9, num_updates=73100, lr=0.000116961, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=4728
2024-01-30 21:25:20 | INFO | train_inner | epoch 055:    361 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=66115.6, ups=16.75, wpb=3947.1, bsz=206.5, num_updates=73200, lr=0.000116881, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.8, wall=4734
2024-01-30 21:25:26 | INFO | train_inner | epoch 055:    461 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=65527.8, ups=16.6, wpb=3948.2, bsz=222.8, num_updates=73300, lr=0.000116801, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.9, wall=4740
2024-01-30 21:25:32 | INFO | train_inner | epoch 055:    561 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=64534.5, ups=16.67, wpb=3870.8, bsz=205, num_updates=73400, lr=0.000116722, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.7, wall=4746
2024-01-30 21:25:38 | INFO | train_inner | epoch 055:    661 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=64775.2, ups=16.71, wpb=3876.2, bsz=213.5, num_updates=73500, lr=0.000116642, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.7, wall=4752
2024-01-30 21:25:44 | INFO | train_inner | epoch 055:    761 / 1349 loss=1.828, nll_loss=0.228, ppl=1.17, wps=64399, ups=16.63, wpb=3872.6, bsz=205.4, num_updates=73600, lr=0.000116563, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.7, wall=4758
2024-01-30 21:25:50 | INFO | train_inner | epoch 055:    861 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65556.2, ups=16.71, wpb=3922.9, bsz=207.9, num_updates=73700, lr=0.000116484, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.7, wall=4764
2024-01-30 21:25:56 | INFO | train_inner | epoch 055:    961 / 1349 loss=1.838, nll_loss=0.238, ppl=1.18, wps=66469.6, ups=16.83, wpb=3950.3, bsz=212.9, num_updates=73800, lr=0.000116405, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.9, wall=4770
2024-01-30 21:26:02 | INFO | train_inner | epoch 055:   1061 / 1349 loss=1.83, nll_loss=0.231, ppl=1.17, wps=65474, ups=16.68, wpb=3924.3, bsz=218.4, num_updates=73900, lr=0.000116326, gnorm=0.538, loss_scale=8, train_wall=6, gb_free=29.8, wall=4776
2024-01-30 21:26:08 | INFO | train_inner | epoch 055:   1161 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=65433.2, ups=16.57, wpb=3948.6, bsz=224.6, num_updates=74000, lr=0.000116248, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=4782
2024-01-30 21:26:14 | INFO | train_inner | epoch 055:   1261 / 1349 loss=1.828, nll_loss=0.228, ppl=1.17, wps=65076.7, ups=16.61, wpb=3918, bsz=215.5, num_updates=74100, lr=0.000116169, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=4788
2024-01-30 21:26:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:26:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:26:21 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 2.102 | nll_loss 0.347 | ppl 1.27 | wps 172623 | wpb 3275.2 | bsz 176.5 | num_updates 74188 | best_loss 2.063
2024-01-30 21:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 74188 updates
2024-01-30 21:26:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint55.pt
2024-01-30 21:26:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint55.pt
2024-01-30 21:26:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint55.pt (epoch 55 @ 74188 updates, score 2.102) (writing took 2.384504713001661 seconds)
2024-01-30 21:26:24 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-01-30 21:26:24 | INFO | train | epoch 055 | loss 1.828 | nll_loss 0.228 | ppl 1.17 | wps 62106.5 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 74188 | lr 0.0001161 | gnorm 0.509 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 4797
2024-01-30 21:26:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:26:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:26:24 | INFO | fairseq.trainer | begin training epoch 56
2024-01-30 21:26:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:26:25 | INFO | train_inner | epoch 056:     12 / 1349 loss=1.841, nll_loss=0.242, ppl=1.18, wps=38711.2, ups=9.84, wpb=3932.9, bsz=202.9, num_updates=74200, lr=0.000116091, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.7, wall=4798
2024-01-30 21:26:31 | INFO | train_inner | epoch 056:    112 / 1349 loss=1.816, nll_loss=0.215, ppl=1.16, wps=65733.6, ups=16.51, wpb=3981.2, bsz=224.7, num_updates=74300, lr=0.000116013, gnorm=0.442, loss_scale=8, train_wall=6, gb_free=29.7, wall=4804
2024-01-30 21:26:37 | INFO | train_inner | epoch 056:    212 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=64810.1, ups=16.7, wpb=3880, bsz=209.8, num_updates=74400, lr=0.000115935, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=4810
2024-01-30 21:26:43 | INFO | train_inner | epoch 056:    312 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=64763.7, ups=16.61, wpb=3898.1, bsz=222.5, num_updates=74500, lr=0.000115857, gnorm=0.468, loss_scale=8, train_wall=6, gb_free=29.8, wall=4816
2024-01-30 21:26:49 | INFO | train_inner | epoch 056:    412 / 1349 loss=1.823, nll_loss=0.222, ppl=1.17, wps=65250.5, ups=16.65, wpb=3919.6, bsz=230.2, num_updates=74600, lr=0.000115779, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.8, wall=4822
2024-01-30 21:26:55 | INFO | train_inner | epoch 056:    512 / 1349 loss=1.828, nll_loss=0.228, ppl=1.17, wps=65571.4, ups=16.71, wpb=3924.5, bsz=212.7, num_updates=74700, lr=0.000115702, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.9, wall=4828
2024-01-30 21:27:01 | INFO | train_inner | epoch 056:    612 / 1349 loss=1.828, nll_loss=0.228, ppl=1.17, wps=65533.8, ups=16.74, wpb=3914.5, bsz=203.8, num_updates=74800, lr=0.000115624, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=4834
2024-01-30 21:27:07 | INFO | train_inner | epoch 056:    712 / 1349 loss=1.827, nll_loss=0.228, ppl=1.17, wps=65614.4, ups=16.66, wpb=3939.1, bsz=207.9, num_updates=74900, lr=0.000115547, gnorm=0.509, loss_scale=8, train_wall=6, gb_free=29.8, wall=4840
2024-01-30 21:27:13 | INFO | train_inner | epoch 056:    812 / 1349 loss=1.834, nll_loss=0.235, ppl=1.18, wps=64500.7, ups=16.72, wpb=3858.7, bsz=209.5, num_updates=75000, lr=0.00011547, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=4846
2024-01-30 21:27:19 | INFO | train_inner | epoch 056:    912 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=65569.9, ups=16.73, wpb=3920.3, bsz=219.3, num_updates=75100, lr=0.000115393, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.9, wall=4852
2024-01-30 21:27:25 | INFO | train_inner | epoch 056:   1012 / 1349 loss=1.828, nll_loss=0.228, ppl=1.17, wps=64713.4, ups=16.63, wpb=3892.4, bsz=221.5, num_updates=75200, lr=0.000115316, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=4858
2024-01-30 21:27:31 | INFO | train_inner | epoch 056:   1112 / 1349 loss=1.832, nll_loss=0.233, ppl=1.17, wps=65155.8, ups=16.66, wpb=3911.9, bsz=201, num_updates=75300, lr=0.00011524, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=4864
2024-01-30 21:27:37 | INFO | train_inner | epoch 056:   1212 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65558.1, ups=16.66, wpb=3935, bsz=203.8, num_updates=75400, lr=0.000115163, gnorm=0.455, loss_scale=8, train_wall=6, gb_free=29.7, wall=4870
2024-01-30 21:27:43 | INFO | train_inner | epoch 056:   1312 / 1349 loss=1.828, nll_loss=0.228, ppl=1.17, wps=65988.8, ups=16.59, wpb=3977.7, bsz=221.5, num_updates=75500, lr=0.000115087, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.8, wall=4876
2024-01-30 21:27:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:27:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:27:46 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 2.104 | nll_loss 0.351 | ppl 1.28 | wps 172613 | wpb 3275.2 | bsz 176.5 | num_updates 75537 | best_loss 2.063
2024-01-30 21:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 75537 updates
2024-01-30 21:27:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint56.pt
2024-01-30 21:27:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint56.pt
2024-01-30 21:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint56.pt (epoch 56 @ 75537 updates, score 2.104) (writing took 2.3551731690531597 seconds)
2024-01-30 21:27:49 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-01-30 21:27:49 | INFO | train | epoch 056 | loss 1.827 | nll_loss 0.227 | ppl 1.17 | wps 62122.4 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 75537 | lr 0.000115059 | gnorm 0.488 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 4882
2024-01-30 21:27:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:27:49 | INFO | fairseq.trainer | begin training epoch 57
2024-01-30 21:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:27:53 | INFO | train_inner | epoch 057:     63 / 1349 loss=1.822, nll_loss=0.221, ppl=1.17, wps=38353.2, ups=9.82, wpb=3905.7, bsz=217.4, num_updates=75600, lr=0.000115011, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=4886
2024-01-30 21:27:59 | INFO | train_inner | epoch 057:    163 / 1349 loss=1.822, nll_loss=0.221, ppl=1.17, wps=64660.5, ups=16.6, wpb=3895.8, bsz=216.7, num_updates=75700, lr=0.000114935, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=4892
2024-01-30 21:28:05 | INFO | train_inner | epoch 057:    263 / 1349 loss=1.819, nll_loss=0.218, ppl=1.16, wps=65626, ups=16.58, wpb=3958.3, bsz=222, num_updates=75800, lr=0.000114859, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=4898
2024-01-30 21:28:11 | INFO | train_inner | epoch 057:    363 / 1349 loss=1.819, nll_loss=0.218, ppl=1.16, wps=64926.9, ups=16.64, wpb=3903, bsz=211.1, num_updates=75900, lr=0.000114783, gnorm=0.445, loss_scale=8, train_wall=6, gb_free=29.8, wall=4904
2024-01-30 21:28:17 | INFO | train_inner | epoch 057:    463 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=65950.5, ups=16.7, wpb=3948.7, bsz=207.1, num_updates=76000, lr=0.000114708, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.8, wall=4910
2024-01-30 21:28:23 | INFO | train_inner | epoch 057:    563 / 1349 loss=1.826, nll_loss=0.227, ppl=1.17, wps=65517.8, ups=16.6, wpb=3947.5, bsz=219.4, num_updates=76100, lr=0.000114632, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=4916
2024-01-30 21:28:29 | INFO | train_inner | epoch 057:    663 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=64872.1, ups=16.61, wpb=3906.6, bsz=219.9, num_updates=76200, lr=0.000114557, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=4922
2024-01-30 21:28:35 | INFO | train_inner | epoch 057:    763 / 1349 loss=1.829, nll_loss=0.23, ppl=1.17, wps=66054.2, ups=16.75, wpb=3942.6, bsz=204.6, num_updates=76300, lr=0.000114482, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=4928
2024-01-30 21:28:41 | INFO | train_inner | epoch 057:    863 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=64936.2, ups=16.69, wpb=3890.9, bsz=220.3, num_updates=76400, lr=0.000114407, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=4934
2024-01-30 21:28:47 | INFO | train_inner | epoch 057:    963 / 1349 loss=1.826, nll_loss=0.227, ppl=1.17, wps=65450.7, ups=16.72, wpb=3914, bsz=215.4, num_updates=76500, lr=0.000114332, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=4940
2024-01-30 21:28:53 | INFO | train_inner | epoch 057:   1063 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65583.6, ups=16.65, wpb=3938, bsz=217.3, num_updates=76600, lr=0.000114258, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.7, wall=4946
2024-01-30 21:28:59 | INFO | train_inner | epoch 057:   1163 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65054.6, ups=16.59, wpb=3921.8, bsz=220.5, num_updates=76700, lr=0.000114183, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=4952
2024-01-30 21:29:05 | INFO | train_inner | epoch 057:   1263 / 1349 loss=1.832, nll_loss=0.232, ppl=1.17, wps=65716, ups=16.78, wpb=3917.1, bsz=210.9, num_updates=76800, lr=0.000114109, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=4958
2024-01-30 21:29:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:29:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:29:12 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 2.103 | nll_loss 0.35 | ppl 1.27 | wps 171967 | wpb 3275.2 | bsz 176.5 | num_updates 76886 | best_loss 2.063
2024-01-30 21:29:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 76886 updates
2024-01-30 21:29:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint57.pt
2024-01-30 21:29:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint57.pt
2024-01-30 21:29:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint57.pt (epoch 57 @ 76886 updates, score 2.103) (writing took 2.454580601071939 seconds)
2024-01-30 21:29:14 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-01-30 21:29:14 | INFO | train | epoch 057 | loss 1.825 | nll_loss 0.226 | ppl 1.17 | wps 62068.5 | ups 15.83 | wpb 3921.2 | bsz 214.5 | num_updates 76886 | lr 0.000114045 | gnorm 0.503 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 4967
2024-01-30 21:29:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:29:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:29:14 | INFO | fairseq.trainer | begin training epoch 58
2024-01-30 21:29:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:29:15 | INFO | train_inner | epoch 058:     14 / 1349 loss=1.832, nll_loss=0.233, ppl=1.17, wps=38379.2, ups=9.78, wpb=3922.8, bsz=204.6, num_updates=76900, lr=0.000114035, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.9, wall=4968
2024-01-30 21:29:21 | INFO | train_inner | epoch 058:    114 / 1349 loss=1.818, nll_loss=0.217, ppl=1.16, wps=65309.6, ups=16.68, wpb=3915.2, bsz=215.9, num_updates=77000, lr=0.000113961, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=30, wall=4974
2024-01-30 21:29:27 | INFO | train_inner | epoch 058:    214 / 1349 loss=1.82, nll_loss=0.22, ppl=1.16, wps=65378.6, ups=16.7, wpb=3914.5, bsz=217.8, num_updates=77100, lr=0.000113887, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=4980
2024-01-30 21:29:33 | INFO | train_inner | epoch 058:    314 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65132.4, ups=16.68, wpb=3904.8, bsz=208.2, num_updates=77200, lr=0.000113813, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.7, wall=4986
2024-01-30 21:29:39 | INFO | train_inner | epoch 058:    414 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65269.6, ups=16.68, wpb=3913.9, bsz=213.4, num_updates=77300, lr=0.000113739, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=4992
2024-01-30 21:29:45 | INFO | train_inner | epoch 058:    514 / 1349 loss=1.82, nll_loss=0.22, ppl=1.16, wps=65394.9, ups=16.57, wpb=3946.2, bsz=216.1, num_updates=77400, lr=0.000113666, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=4998
2024-01-30 21:29:51 | INFO | train_inner | epoch 058:    614 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65588, ups=16.71, wpb=3926.1, bsz=220.8, num_updates=77500, lr=0.000113592, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=5004
2024-01-30 21:29:57 | INFO | train_inner | epoch 058:    714 / 1349 loss=1.831, nll_loss=0.231, ppl=1.17, wps=65429, ups=16.83, wpb=3888.6, bsz=204.1, num_updates=77600, lr=0.000113519, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=5010
2024-01-30 21:30:03 | INFO | train_inner | epoch 058:    814 / 1349 loss=1.831, nll_loss=0.232, ppl=1.17, wps=64808.6, ups=16.78, wpb=3862.1, bsz=208.4, num_updates=77700, lr=0.000113446, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=5016
2024-01-30 21:30:09 | INFO | train_inner | epoch 058:    914 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=65786.7, ups=16.7, wpb=3938.6, bsz=201.4, num_updates=77800, lr=0.000113373, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=5022
2024-01-30 21:30:15 | INFO | train_inner | epoch 058:   1014 / 1349 loss=1.819, nll_loss=0.219, ppl=1.16, wps=65484.8, ups=16.56, wpb=3954.4, bsz=221.4, num_updates=77900, lr=0.0001133, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=5028
2024-01-30 21:30:21 | INFO | train_inner | epoch 058:   1114 / 1349 loss=1.824, nll_loss=0.225, ppl=1.17, wps=64763, ups=16.53, wpb=3918.6, bsz=214.1, num_updates=78000, lr=0.000113228, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=5034
2024-01-30 21:30:27 | INFO | train_inner | epoch 058:   1214 / 1349 loss=1.825, nll_loss=0.226, ppl=1.17, wps=65445.6, ups=16.66, wpb=3928.3, bsz=214.4, num_updates=78100, lr=0.000113155, gnorm=0.454, loss_scale=8, train_wall=6, gb_free=29.8, wall=5040
2024-01-30 21:30:33 | INFO | train_inner | epoch 058:   1314 / 1349 loss=1.825, nll_loss=0.226, ppl=1.17, wps=65417.4, ups=16.58, wpb=3944.9, bsz=227.8, num_updates=78200, lr=0.000113083, gnorm=0.426, loss_scale=8, train_wall=6, gb_free=29.8, wall=5046
2024-01-30 21:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:30:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:30:37 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 2.109 | nll_loss 0.357 | ppl 1.28 | wps 170953 | wpb 3275.2 | bsz 176.5 | num_updates 78235 | best_loss 2.063
2024-01-30 21:30:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 78235 updates
2024-01-30 21:30:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint58.pt
2024-01-30 21:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint58.pt
2024-01-30 21:30:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint58.pt (epoch 58 @ 78235 updates, score 2.109) (writing took 2.324604868888855 seconds)
2024-01-30 21:30:39 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-01-30 21:30:39 | INFO | train | epoch 058 | loss 1.824 | nll_loss 0.224 | ppl 1.17 | wps 62136.7 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 78235 | lr 0.000113058 | gnorm 0.491 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 5052
2024-01-30 21:30:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:30:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:30:39 | INFO | fairseq.trainer | begin training epoch 59
2024-01-30 21:30:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:30:43 | INFO | train_inner | epoch 059:     65 / 1349 loss=1.82, nll_loss=0.22, ppl=1.16, wps=38074.3, ups=9.58, wpb=3975.5, bsz=219.5, num_updates=78300, lr=0.000113011, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.8, wall=5057
2024-01-30 21:30:50 | INFO | train_inner | epoch 059:    165 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=60815.2, ups=15.49, wpb=3927.2, bsz=212.2, num_updates=78400, lr=0.000112938, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=5063
2024-01-30 21:30:56 | INFO | train_inner | epoch 059:    265 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=64291.1, ups=16.53, wpb=3888.7, bsz=225, num_updates=78500, lr=0.000112867, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.8, wall=5069
2024-01-30 21:31:02 | INFO | train_inner | epoch 059:    365 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=65056.5, ups=16.58, wpb=3923.5, bsz=214.7, num_updates=78600, lr=0.000112795, gnorm=0.464, loss_scale=8, train_wall=6, gb_free=29.7, wall=5075
2024-01-30 21:31:08 | INFO | train_inner | epoch 059:    465 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=65276.4, ups=16.66, wpb=3918.9, bsz=204.6, num_updates=78700, lr=0.000112723, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.9, wall=5081
2024-01-30 21:31:14 | INFO | train_inner | epoch 059:    565 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=64583, ups=16.36, wpb=3947.7, bsz=213, num_updates=78800, lr=0.000112651, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=5087
2024-01-30 21:31:20 | INFO | train_inner | epoch 059:    665 / 1349 loss=1.827, nll_loss=0.228, ppl=1.17, wps=65574.9, ups=16.69, wpb=3930.1, bsz=214.3, num_updates=78900, lr=0.00011258, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.7, wall=5093
2024-01-30 21:31:26 | INFO | train_inner | epoch 059:    765 / 1349 loss=1.83, nll_loss=0.231, ppl=1.17, wps=65402.9, ups=16.8, wpb=3892, bsz=201.7, num_updates=79000, lr=0.000112509, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=5099
2024-01-30 21:31:32 | INFO | train_inner | epoch 059:    865 / 1349 loss=1.82, nll_loss=0.22, ppl=1.16, wps=65026.4, ups=16.55, wpb=3929.9, bsz=211.9, num_updates=79100, lr=0.000112438, gnorm=0.432, loss_scale=8, train_wall=6, gb_free=29.7, wall=5105
2024-01-30 21:31:38 | INFO | train_inner | epoch 059:    965 / 1349 loss=1.828, nll_loss=0.229, ppl=1.17, wps=65502.7, ups=16.81, wpb=3896.1, bsz=213.9, num_updates=79200, lr=0.000112367, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.9, wall=5111
2024-01-30 21:31:44 | INFO | train_inner | epoch 059:   1065 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=65562.1, ups=16.76, wpb=3910.8, bsz=219.5, num_updates=79300, lr=0.000112296, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=5117
2024-01-30 21:31:50 | INFO | train_inner | epoch 059:   1165 / 1349 loss=1.825, nll_loss=0.226, ppl=1.17, wps=65304.9, ups=16.66, wpb=3920.8, bsz=220.7, num_updates=79400, lr=0.000112225, gnorm=0.572, loss_scale=8, train_wall=6, gb_free=29.8, wall=5123
2024-01-30 21:31:56 | INFO | train_inner | epoch 059:   1265 / 1349 loss=1.823, nll_loss=0.224, ppl=1.17, wps=65611.9, ups=16.7, wpb=3928.1, bsz=225.1, num_updates=79500, lr=0.000112154, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=5129
2024-01-30 21:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:32:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:32:05 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 2.109 | nll_loss 0.36 | ppl 1.28 | wps 170244 | wpb 3275.2 | bsz 176.5 | num_updates 79584 | best_loss 2.063
2024-01-30 21:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 79584 updates
2024-01-30 21:32:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint59.pt
2024-01-30 21:32:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint59.pt
2024-01-30 21:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint59.pt (epoch 59 @ 79584 updates, score 2.109) (writing took 2.5387746560154483 seconds)
2024-01-30 21:32:08 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-01-30 21:32:08 | INFO | train | epoch 059 | loss 1.823 | nll_loss 0.223 | ppl 1.17 | wps 59901.9 | ups 15.28 | wpb 3921.2 | bsz 214.5 | num_updates 79584 | lr 0.000112095 | gnorm 0.492 | loss_scale 8 | train_wall 81 | gb_free 29.8 | wall 5141
2024-01-30 21:32:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:32:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:32:08 | INFO | fairseq.trainer | begin training epoch 60
2024-01-30 21:32:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:32:09 | INFO | train_inner | epoch 060:     16 / 1349 loss=1.826, nll_loss=0.227, ppl=1.17, wps=31191.5, ups=7.96, wpb=3918.6, bsz=210.7, num_updates=79600, lr=0.000112084, gnorm=0.544, loss_scale=8, train_wall=8, gb_free=29.8, wall=5142
2024-01-30 21:32:15 | INFO | train_inner | epoch 060:    116 / 1349 loss=1.819, nll_loss=0.218, ppl=1.16, wps=65251.4, ups=16.72, wpb=3902.5, bsz=207.8, num_updates=79700, lr=0.000112014, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=5148
2024-01-30 21:32:21 | INFO | train_inner | epoch 060:    216 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=65410.6, ups=16.63, wpb=3933.8, bsz=215.9, num_updates=79800, lr=0.000111943, gnorm=0.476, loss_scale=8, train_wall=6, gb_free=29.8, wall=5154
2024-01-30 21:32:27 | INFO | train_inner | epoch 060:    316 / 1349 loss=1.821, nll_loss=0.221, ppl=1.17, wps=64881.3, ups=16.58, wpb=3913.7, bsz=210.2, num_updates=79900, lr=0.000111873, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.7, wall=5160
2024-01-30 21:32:33 | INFO | train_inner | epoch 060:    416 / 1349 loss=1.818, nll_loss=0.218, ppl=1.16, wps=64538.6, ups=16.7, wpb=3863.9, bsz=226.6, num_updates=80000, lr=0.000111803, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.7, wall=5166
2024-01-30 21:32:39 | INFO | train_inner | epoch 060:    516 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=65424.5, ups=16.65, wpb=3928.3, bsz=214.6, num_updates=80100, lr=0.000111734, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.7, wall=5172
2024-01-30 21:32:45 | INFO | train_inner | epoch 060:    616 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=60836.4, ups=15.74, wpb=3864.4, bsz=207.8, num_updates=80200, lr=0.000111664, gnorm=0.54, loss_scale=8, train_wall=6, gb_free=29.7, wall=5178
2024-01-30 21:32:51 | INFO | train_inner | epoch 060:    716 / 1349 loss=1.825, nll_loss=0.225, ppl=1.17, wps=65818.1, ups=16.73, wpb=3935.2, bsz=203.6, num_updates=80300, lr=0.000111594, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=5184
2024-01-30 21:32:57 | INFO | train_inner | epoch 060:    816 / 1349 loss=1.827, nll_loss=0.227, ppl=1.17, wps=65453, ups=16.7, wpb=3920.1, bsz=206.6, num_updates=80400, lr=0.000111525, gnorm=0.568, loss_scale=8, train_wall=6, gb_free=29.8, wall=5190
2024-01-30 21:33:03 | INFO | train_inner | epoch 060:    916 / 1349 loss=1.824, nll_loss=0.225, ppl=1.17, wps=65426.1, ups=16.66, wpb=3926.7, bsz=223.2, num_updates=80500, lr=0.000111456, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.7, wall=5196
2024-01-30 21:33:09 | INFO | train_inner | epoch 060:   1016 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=65909.8, ups=16.59, wpb=3973.6, bsz=225, num_updates=80600, lr=0.000111386, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=5202
2024-01-30 21:33:15 | INFO | train_inner | epoch 060:   1116 / 1349 loss=1.824, nll_loss=0.225, ppl=1.17, wps=65631.1, ups=16.61, wpb=3952.4, bsz=217.4, num_updates=80700, lr=0.000111317, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=5208
2024-01-30 21:33:21 | INFO | train_inner | epoch 060:   1216 / 1349 loss=1.82, nll_loss=0.221, ppl=1.17, wps=65396.9, ups=16.48, wpb=3968.5, bsz=220.7, num_updates=80800, lr=0.000111249, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.7, wall=5214
2024-01-30 21:33:27 | INFO | train_inner | epoch 060:   1316 / 1349 loss=1.827, nll_loss=0.228, ppl=1.17, wps=65135, ups=16.72, wpb=3896.2, bsz=213.6, num_updates=80900, lr=0.00011118, gnorm=0.565, loss_scale=8, train_wall=6, gb_free=29.7, wall=5220
2024-01-30 21:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:33:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:33:31 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 2.109 | nll_loss 0.358 | ppl 1.28 | wps 172665 | wpb 3275.2 | bsz 176.5 | num_updates 80933 | best_loss 2.063
2024-01-30 21:33:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 80933 updates
2024-01-30 21:33:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint60.pt
2024-01-30 21:33:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint60.pt
2024-01-30 21:33:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint60.pt (epoch 60 @ 80933 updates, score 2.109) (writing took 2.3885798359988257 seconds)
2024-01-30 21:33:33 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-01-30 21:33:33 | INFO | train | epoch 060 | loss 1.822 | nll_loss 0.222 | ppl 1.17 | wps 61801.2 | ups 15.76 | wpb 3921.2 | bsz 214.5 | num_updates 80933 | lr 0.000111157 | gnorm 0.505 | loss_scale 8 | train_wall 78 | gb_free 30 | wall 5226
2024-01-30 21:33:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:33:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:33:33 | INFO | fairseq.trainer | begin training epoch 61
2024-01-30 21:33:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:33:37 | INFO | train_inner | epoch 061:     67 / 1349 loss=1.818, nll_loss=0.218, ppl=1.16, wps=38229.9, ups=9.75, wpb=3922.2, bsz=212.8, num_updates=81000, lr=0.000111111, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.7, wall=5230
2024-01-30 21:33:43 | INFO | train_inner | epoch 061:    167 / 1349 loss=1.818, nll_loss=0.217, ppl=1.16, wps=64557.7, ups=16.7, wpb=3864.9, bsz=220.5, num_updates=81100, lr=0.000111043, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.7, wall=5236
2024-01-30 21:33:49 | INFO | train_inner | epoch 061:    267 / 1349 loss=1.821, nll_loss=0.221, ppl=1.17, wps=65504.9, ups=16.63, wpb=3938.3, bsz=218.2, num_updates=81200, lr=0.000110974, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=5242
2024-01-30 21:33:55 | INFO | train_inner | epoch 061:    367 / 1349 loss=1.818, nll_loss=0.218, ppl=1.16, wps=65199.8, ups=16.52, wpb=3947.2, bsz=221.2, num_updates=81300, lr=0.000110906, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.7, wall=5249
2024-01-30 21:34:01 | INFO | train_inner | epoch 061:    467 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=64601.5, ups=16.87, wpb=3830, bsz=199.9, num_updates=81400, lr=0.000110838, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=5254
2024-01-30 21:34:07 | INFO | train_inner | epoch 061:    567 / 1349 loss=1.823, nll_loss=0.223, ppl=1.17, wps=65693.8, ups=16.55, wpb=3969.9, bsz=221.8, num_updates=81500, lr=0.00011077, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=5261
2024-01-30 21:34:13 | INFO | train_inner | epoch 061:    667 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=65027.7, ups=16.43, wpb=3958.3, bsz=214.2, num_updates=81600, lr=0.000110702, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=5267
2024-01-30 21:34:19 | INFO | train_inner | epoch 061:    767 / 1349 loss=1.823, nll_loss=0.224, ppl=1.17, wps=65684.2, ups=16.69, wpb=3936.3, bsz=207, num_updates=81700, lr=0.000110634, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=5273
2024-01-30 21:34:25 | INFO | train_inner | epoch 061:    867 / 1349 loss=1.82, nll_loss=0.22, ppl=1.17, wps=65300.3, ups=16.6, wpb=3933.7, bsz=204.2, num_updates=81800, lr=0.000110566, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=5279
2024-01-30 21:34:31 | INFO | train_inner | epoch 061:    967 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=65541.2, ups=16.53, wpb=3966, bsz=221.4, num_updates=81900, lr=0.000110499, gnorm=0.453, loss_scale=8, train_wall=6, gb_free=29.8, wall=5285
2024-01-30 21:34:37 | INFO | train_inner | epoch 061:   1067 / 1349 loss=1.824, nll_loss=0.225, ppl=1.17, wps=64775.1, ups=16.67, wpb=3885.4, bsz=207.3, num_updates=82000, lr=0.000110432, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.7, wall=5291
2024-01-30 21:34:44 | INFO | train_inner | epoch 061:   1167 / 1349 loss=1.818, nll_loss=0.218, ppl=1.16, wps=64793.9, ups=16.49, wpb=3929.9, bsz=221.8, num_updates=82100, lr=0.000110364, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=5297
2024-01-30 21:34:50 | INFO | train_inner | epoch 061:   1267 / 1349 loss=1.826, nll_loss=0.226, ppl=1.17, wps=65375.3, ups=16.69, wpb=3916.7, bsz=215.7, num_updates=82200, lr=0.000110297, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.8, wall=5303
2024-01-30 21:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:34:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:34:56 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 2.11 | nll_loss 0.359 | ppl 1.28 | wps 171694 | wpb 3275.2 | bsz 176.5 | num_updates 82282 | best_loss 2.063
2024-01-30 21:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 82282 updates
2024-01-30 21:34:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint61.pt
2024-01-30 21:34:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint61.pt
2024-01-30 21:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint61.pt (epoch 61 @ 82282 updates, score 2.11) (writing took 2.264498503995128 seconds)
2024-01-30 21:34:58 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-01-30 21:34:58 | INFO | train | epoch 061 | loss 1.821 | nll_loss 0.221 | ppl 1.17 | wps 62012.1 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 82282 | lr 0.000110242 | gnorm 0.492 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 5312
2024-01-30 21:34:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:34:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:34:58 | INFO | fairseq.trainer | begin training epoch 62
2024-01-30 21:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:35:00 | INFO | train_inner | epoch 062:     18 / 1349 loss=1.821, nll_loss=0.221, ppl=1.17, wps=38544.6, ups=9.89, wpb=3895.8, bsz=217.8, num_updates=82300, lr=0.00011023, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.7, wall=5313
2024-01-30 21:35:06 | INFO | train_inner | epoch 062:    118 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=65465.1, ups=16.59, wpb=3945.2, bsz=215.4, num_updates=82400, lr=0.000110163, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.7, wall=5319
2024-01-30 21:35:12 | INFO | train_inner | epoch 062:    218 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=64670.3, ups=16.64, wpb=3887.3, bsz=222.3, num_updates=82500, lr=0.000110096, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.7, wall=5325
2024-01-30 21:35:18 | INFO | train_inner | epoch 062:    318 / 1349 loss=1.82, nll_loss=0.221, ppl=1.17, wps=65202.7, ups=16.69, wpb=3906.5, bsz=218.7, num_updates=82600, lr=0.00011003, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=5331
2024-01-30 21:35:24 | INFO | train_inner | epoch 062:    418 / 1349 loss=1.811, nll_loss=0.21, ppl=1.16, wps=65276.7, ups=16.4, wpb=3980.2, bsz=227.6, num_updates=82700, lr=0.000109963, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=5337
2024-01-30 21:35:30 | INFO | train_inner | epoch 062:    518 / 1349 loss=1.821, nll_loss=0.221, ppl=1.17, wps=65690.4, ups=16.67, wpb=3941.6, bsz=207.4, num_updates=82800, lr=0.000109897, gnorm=0.474, loss_scale=8, train_wall=6, gb_free=29.8, wall=5343
2024-01-30 21:35:36 | INFO | train_inner | epoch 062:    618 / 1349 loss=1.816, nll_loss=0.217, ppl=1.16, wps=65294.7, ups=16.62, wpb=3929.8, bsz=223.6, num_updates=82900, lr=0.00010983, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=5349
2024-01-30 21:35:42 | INFO | train_inner | epoch 062:    718 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65176.8, ups=16.65, wpb=3914, bsz=202.8, num_updates=83000, lr=0.000109764, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.9, wall=5355
2024-01-30 21:35:48 | INFO | train_inner | epoch 062:    818 / 1349 loss=1.822, nll_loss=0.223, ppl=1.17, wps=64969.3, ups=16.66, wpb=3898.7, bsz=206.2, num_updates=83100, lr=0.000109698, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=5361
2024-01-30 21:35:54 | INFO | train_inner | epoch 062:    918 / 1349 loss=1.816, nll_loss=0.217, ppl=1.16, wps=65358.3, ups=16.57, wpb=3944.5, bsz=216.6, num_updates=83200, lr=0.000109632, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.7, wall=5367
2024-01-30 21:36:00 | INFO | train_inner | epoch 062:   1018 / 1349 loss=1.821, nll_loss=0.222, ppl=1.17, wps=64498.3, ups=16.62, wpb=3880.4, bsz=210.7, num_updates=83300, lr=0.000109566, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=5373
2024-01-30 21:36:06 | INFO | train_inner | epoch 062:   1118 / 1349 loss=1.823, nll_loss=0.224, ppl=1.17, wps=64865.1, ups=16.64, wpb=3897.6, bsz=215.6, num_updates=83400, lr=0.000109501, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=5379
2024-01-30 21:36:12 | INFO | train_inner | epoch 062:   1218 / 1349 loss=1.825, nll_loss=0.226, ppl=1.17, wps=65322.3, ups=16.57, wpb=3941.9, bsz=201.5, num_updates=83500, lr=0.000109435, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=5385
2024-01-30 21:36:18 | INFO | train_inner | epoch 062:   1318 / 1349 loss=1.822, nll_loss=0.222, ppl=1.17, wps=65495.4, ups=16.67, wpb=3928.5, bsz=219.8, num_updates=83600, lr=0.00010937, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.9, wall=5391
2024-01-30 21:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:36:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:36:21 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 2.11 | nll_loss 0.361 | ppl 1.28 | wps 171582 | wpb 3275.2 | bsz 176.5 | num_updates 83631 | best_loss 2.063
2024-01-30 21:36:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 83631 updates
2024-01-30 21:36:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint62.pt
2024-01-30 21:36:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint62.pt
2024-01-30 21:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint62.pt (epoch 62 @ 83631 updates, score 2.11) (writing took 2.2944170540431514 seconds)
2024-01-30 21:36:24 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-01-30 21:36:24 | INFO | train | epoch 062 | loss 1.82 | nll_loss 0.22 | ppl 1.16 | wps 61993.3 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 83631 | lr 0.000109349 | gnorm 0.494 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 5397
2024-01-30 21:36:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:36:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:36:24 | INFO | fairseq.trainer | begin training epoch 63
2024-01-30 21:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:36:28 | INFO | train_inner | epoch 063:     69 / 1349 loss=1.817, nll_loss=0.216, ppl=1.16, wps=38657.4, ups=9.87, wpb=3918.5, bsz=209.2, num_updates=83700, lr=0.000109304, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=5401
2024-01-30 21:36:34 | INFO | train_inner | epoch 063:    169 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=65648.7, ups=16.65, wpb=3943.2, bsz=217.4, num_updates=83800, lr=0.000109239, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=5407
2024-01-30 21:36:40 | INFO | train_inner | epoch 063:    269 / 1349 loss=1.824, nll_loss=0.224, ppl=1.17, wps=65311.3, ups=16.71, wpb=3908.5, bsz=200.2, num_updates=83900, lr=0.000109174, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.7, wall=5413
2024-01-30 21:36:46 | INFO | train_inner | epoch 063:    369 / 1349 loss=1.819, nll_loss=0.219, ppl=1.16, wps=65572.3, ups=16.73, wpb=3919.3, bsz=201.8, num_updates=84000, lr=0.000109109, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.9, wall=5419
2024-01-30 21:36:52 | INFO | train_inner | epoch 063:    469 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=64660.9, ups=16.56, wpb=3905.5, bsz=214, num_updates=84100, lr=0.000109044, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=5425
2024-01-30 21:36:58 | INFO | train_inner | epoch 063:    569 / 1349 loss=1.821, nll_loss=0.222, ppl=1.17, wps=64944.5, ups=16.71, wpb=3886, bsz=221, num_updates=84200, lr=0.000108979, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=30.4, wall=5431
2024-01-30 21:37:04 | INFO | train_inner | epoch 063:    669 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=64995, ups=16.53, wpb=3930.8, bsz=223, num_updates=84300, lr=0.000108915, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.9, wall=5437
2024-01-30 21:37:10 | INFO | train_inner | epoch 063:    769 / 1349 loss=1.817, nll_loss=0.217, ppl=1.16, wps=65311.9, ups=16.49, wpb=3960.3, bsz=210.2, num_updates=84400, lr=0.00010885, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=5443
2024-01-30 21:37:16 | INFO | train_inner | epoch 063:    869 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=64640.2, ups=16.59, wpb=3897.1, bsz=224.8, num_updates=84500, lr=0.000108786, gnorm=0.454, loss_scale=8, train_wall=6, gb_free=29.7, wall=5449
2024-01-30 21:37:22 | INFO | train_inner | epoch 063:    969 / 1349 loss=1.82, nll_loss=0.221, ppl=1.17, wps=65373, ups=16.61, wpb=3936.2, bsz=215.3, num_updates=84600, lr=0.000108721, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.9, wall=5455
2024-01-30 21:37:28 | INFO | train_inner | epoch 063:   1069 / 1349 loss=1.821, nll_loss=0.222, ppl=1.17, wps=64651.6, ups=16.66, wpb=3881.2, bsz=214.6, num_updates=84700, lr=0.000108657, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=5461
2024-01-30 21:37:34 | INFO | train_inner | epoch 063:   1169 / 1349 loss=1.82, nll_loss=0.22, ppl=1.16, wps=65267.3, ups=16.65, wpb=3919.5, bsz=223.7, num_updates=84800, lr=0.000108593, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=5467
2024-01-30 21:37:40 | INFO | train_inner | epoch 063:   1269 / 1349 loss=1.822, nll_loss=0.223, ppl=1.17, wps=65285.1, ups=16.61, wpb=3931.3, bsz=222.2, num_updates=84900, lr=0.000108529, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.9, wall=5473
2024-01-30 21:37:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:37:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:37:47 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 2.111 | nll_loss 0.364 | ppl 1.29 | wps 172045 | wpb 3275.2 | bsz 176.5 | num_updates 84980 | best_loss 2.063
2024-01-30 21:37:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 84980 updates
2024-01-30 21:37:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint63.pt
2024-01-30 21:37:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint63.pt
2024-01-30 21:37:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint63.pt (epoch 63 @ 84980 updates, score 2.111) (writing took 2.316119449096732 seconds)
2024-01-30 21:37:49 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-01-30 21:37:49 | INFO | train | epoch 063 | loss 1.818 | nll_loss 0.219 | ppl 1.16 | wps 61997.3 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 84980 | lr 0.000108478 | gnorm 0.499 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 5482
2024-01-30 21:37:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:37:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:37:49 | INFO | fairseq.trainer | begin training epoch 64
2024-01-30 21:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:37:50 | INFO | train_inner | epoch 064:     20 / 1349 loss=1.822, nll_loss=0.223, ppl=1.17, wps=38811.3, ups=9.85, wpb=3942.1, bsz=201.4, num_updates=85000, lr=0.000108465, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=5484
2024-01-30 21:37:56 | INFO | train_inner | epoch 064:    120 / 1349 loss=1.812, nll_loss=0.211, ppl=1.16, wps=65697.4, ups=16.56, wpb=3966.4, bsz=225.7, num_updates=85100, lr=0.000108401, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=5490
2024-01-30 21:38:02 | INFO | train_inner | epoch 064:    220 / 1349 loss=1.816, nll_loss=0.217, ppl=1.16, wps=65091, ups=16.66, wpb=3905.9, bsz=205.3, num_updates=85200, lr=0.000108338, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=5496
2024-01-30 21:38:08 | INFO | train_inner | epoch 064:    320 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=65306.1, ups=16.55, wpb=3946.3, bsz=215.9, num_updates=85300, lr=0.000108274, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=5502
2024-01-30 21:38:15 | INFO | train_inner | epoch 064:    420 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=65071.1, ups=16.47, wpb=3950.9, bsz=229, num_updates=85400, lr=0.000108211, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=5508
2024-01-30 21:38:21 | INFO | train_inner | epoch 064:    520 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=64993.9, ups=16.63, wpb=3908, bsz=211.2, num_updates=85500, lr=0.000108148, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=5514
2024-01-30 21:38:26 | INFO | train_inner | epoch 064:    620 / 1349 loss=1.819, nll_loss=0.22, ppl=1.16, wps=64078.1, ups=16.73, wpb=3830.9, bsz=210.6, num_updates=85600, lr=0.000108084, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=5520
2024-01-30 21:38:33 | INFO | train_inner | epoch 064:    720 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=65592.3, ups=16.55, wpb=3963.7, bsz=217.8, num_updates=85700, lr=0.000108021, gnorm=0.461, loss_scale=8, train_wall=6, gb_free=29.7, wall=5526
2024-01-30 21:38:39 | INFO | train_inner | epoch 064:    820 / 1349 loss=1.822, nll_loss=0.222, ppl=1.17, wps=64955.9, ups=16.58, wpb=3917.6, bsz=213.6, num_updates=85800, lr=0.000107958, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.7, wall=5532
2024-01-30 21:38:45 | INFO | train_inner | epoch 064:    920 / 1349 loss=1.819, nll_loss=0.219, ppl=1.16, wps=65369.6, ups=16.59, wpb=3939.9, bsz=213.1, num_updates=85900, lr=0.000107896, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.8, wall=5538
2024-01-30 21:38:51 | INFO | train_inner | epoch 064:   1020 / 1349 loss=1.819, nll_loss=0.219, ppl=1.16, wps=65205.4, ups=16.69, wpb=3906.9, bsz=212.2, num_updates=86000, lr=0.000107833, gnorm=0.559, loss_scale=8, train_wall=6, gb_free=29.7, wall=5544
2024-01-30 21:38:57 | INFO | train_inner | epoch 064:   1120 / 1349 loss=1.819, nll_loss=0.22, ppl=1.16, wps=64406.7, ups=16.67, wpb=3863.2, bsz=207.3, num_updates=86100, lr=0.00010777, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.9, wall=5550
2024-01-30 21:39:03 | INFO | train_inner | epoch 064:   1220 / 1349 loss=1.823, nll_loss=0.224, ppl=1.17, wps=65743.4, ups=16.8, wpb=3913.4, bsz=209.4, num_updates=86200, lr=0.000107708, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.8, wall=5556
2024-01-30 21:39:09 | INFO | train_inner | epoch 064:   1320 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=65510.3, ups=16.58, wpb=3950.5, bsz=213.5, num_updates=86300, lr=0.000107645, gnorm=0.445, loss_scale=8, train_wall=6, gb_free=29.7, wall=5562
2024-01-30 21:39:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:39:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:39:12 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 2.115 | nll_loss 0.364 | ppl 1.29 | wps 172294 | wpb 3275.2 | bsz 176.5 | num_updates 86329 | best_loss 2.063
2024-01-30 21:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 86329 updates
2024-01-30 21:39:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint64.pt
2024-01-30 21:39:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint64.pt
2024-01-30 21:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint64.pt (epoch 64 @ 86329 updates, score 2.115) (writing took 2.2585159150185063 seconds)
2024-01-30 21:39:14 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-01-30 21:39:14 | INFO | train | epoch 064 | loss 1.817 | nll_loss 0.218 | ppl 1.16 | wps 62050.4 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 86329 | lr 0.000107627 | gnorm 0.485 | loss_scale 8 | train_wall 78 | gb_free 29.9 | wall 5568
2024-01-30 21:39:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:39:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:39:14 | INFO | fairseq.trainer | begin training epoch 65
2024-01-30 21:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:39:19 | INFO | train_inner | epoch 065:     71 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=38545.8, ups=9.91, wpb=3888.1, bsz=213.8, num_updates=86400, lr=0.000107583, gnorm=0.471, loss_scale=8, train_wall=6, gb_free=29.9, wall=5572
2024-01-30 21:39:25 | INFO | train_inner | epoch 065:    171 / 1349 loss=1.812, nll_loss=0.212, ppl=1.16, wps=65104.6, ups=16.64, wpb=3912.8, bsz=225, num_updates=86500, lr=0.000107521, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.8, wall=5578
2024-01-30 21:39:31 | INFO | train_inner | epoch 065:    271 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=65394.2, ups=16.68, wpb=3920.4, bsz=199.2, num_updates=86600, lr=0.000107459, gnorm=0.568, loss_scale=8, train_wall=6, gb_free=29.8, wall=5584
2024-01-30 21:39:37 | INFO | train_inner | epoch 065:    371 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=66148.8, ups=16.57, wpb=3991.2, bsz=216.6, num_updates=86700, lr=0.000107397, gnorm=0.462, loss_scale=16, train_wall=6, gb_free=29.7, wall=5590
2024-01-30 21:39:43 | INFO | train_inner | epoch 065:    471 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=64695.8, ups=16.56, wpb=3907.8, bsz=215.5, num_updates=86800, lr=0.000107335, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=5596
2024-01-30 21:39:49 | INFO | train_inner | epoch 065:    571 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=65515.6, ups=16.56, wpb=3955.1, bsz=218.5, num_updates=86900, lr=0.000107273, gnorm=0.553, loss_scale=16, train_wall=6, gb_free=29.7, wall=5602
2024-01-30 21:39:55 | INFO | train_inner | epoch 065:    671 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=65658.8, ups=16.6, wpb=3954.2, bsz=222.1, num_updates=87000, lr=0.000107211, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.7, wall=5608
2024-01-30 21:40:01 | INFO | train_inner | epoch 065:    771 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=64904.6, ups=16.61, wpb=3908.2, bsz=214.3, num_updates=87100, lr=0.00010715, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.8, wall=5614
2024-01-30 21:40:07 | INFO | train_inner | epoch 065:    871 / 1349 loss=1.819, nll_loss=0.22, ppl=1.16, wps=65166.8, ups=16.67, wpb=3908.1, bsz=214.5, num_updates=87200, lr=0.000107088, gnorm=0.526, loss_scale=16, train_wall=6, gb_free=29.7, wall=5620
2024-01-30 21:40:13 | INFO | train_inner | epoch 065:    971 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=64748.5, ups=16.5, wpb=3923, bsz=220.2, num_updates=87300, lr=0.000107027, gnorm=0.534, loss_scale=16, train_wall=6, gb_free=29.7, wall=5626
2024-01-30 21:40:19 | INFO | train_inner | epoch 065:   1071 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=64638.8, ups=16.36, wpb=3952.1, bsz=221.8, num_updates=87400, lr=0.000106966, gnorm=0.483, loss_scale=16, train_wall=6, gb_free=29.8, wall=5632
2024-01-30 21:40:25 | INFO | train_inner | epoch 065:   1171 / 1349 loss=1.818, nll_loss=0.218, ppl=1.16, wps=65281, ups=16.6, wpb=3932, bsz=214.2, num_updates=87500, lr=0.000106904, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.8, wall=5638
2024-01-30 21:40:31 | INFO | train_inner | epoch 065:   1271 / 1349 loss=1.828, nll_loss=0.229, ppl=1.17, wps=64647.9, ups=16.76, wpb=3856.5, bsz=201.4, num_updates=87600, lr=0.000106843, gnorm=0.571, loss_scale=16, train_wall=6, gb_free=30, wall=5644
2024-01-30 21:40:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:40:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:40:37 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 2.114 | nll_loss 0.365 | ppl 1.29 | wps 170999 | wpb 3275.2 | bsz 176.5 | num_updates 87678 | best_loss 2.063
2024-01-30 21:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 87678 updates
2024-01-30 21:40:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint65.pt
2024-01-30 21:40:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint65.pt
2024-01-30 21:40:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint65.pt (epoch 65 @ 87678 updates, score 2.114) (writing took 2.2908651820616797 seconds)
2024-01-30 21:40:40 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-01-30 21:40:40 | INFO | train | epoch 065 | loss 1.817 | nll_loss 0.217 | ppl 1.16 | wps 61949 | ups 15.8 | wpb 3921.2 | bsz 214.5 | num_updates 87678 | lr 0.000106796 | gnorm 0.51 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 5653
2024-01-30 21:40:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:40:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:40:40 | INFO | fairseq.trainer | begin training epoch 66
2024-01-30 21:40:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:40:41 | INFO | train_inner | epoch 066:     22 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=38636.7, ups=9.88, wpb=3908.8, bsz=209.7, num_updates=87700, lr=0.000106783, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.9, wall=5654
2024-01-30 21:40:47 | INFO | train_inner | epoch 066:    122 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=65164.1, ups=16.63, wpb=3917.8, bsz=214.9, num_updates=87800, lr=0.000106722, gnorm=0.487, loss_scale=16, train_wall=6, gb_free=29.8, wall=5660
2024-01-30 21:40:53 | INFO | train_inner | epoch 066:    222 / 1349 loss=1.81, nll_loss=0.21, ppl=1.16, wps=65004.2, ups=16.53, wpb=3932.9, bsz=224.4, num_updates=87900, lr=0.000106661, gnorm=0.481, loss_scale=16, train_wall=6, gb_free=29.8, wall=5666
2024-01-30 21:40:59 | INFO | train_inner | epoch 066:    322 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=64767.9, ups=16.72, wpb=3873.4, bsz=221, num_updates=88000, lr=0.0001066, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.8, wall=5672
2024-01-30 21:41:05 | INFO | train_inner | epoch 066:    422 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=65463.9, ups=16.53, wpb=3960.2, bsz=217.5, num_updates=88100, lr=0.00010654, gnorm=0.478, loss_scale=16, train_wall=6, gb_free=29.8, wall=5678
2024-01-30 21:41:11 | INFO | train_inner | epoch 066:    522 / 1349 loss=1.811, nll_loss=0.211, ppl=1.16, wps=65103.3, ups=16.63, wpb=3915.5, bsz=215.4, num_updates=88200, lr=0.000106479, gnorm=0.463, loss_scale=16, train_wall=6, gb_free=29.8, wall=5684
2024-01-30 21:41:17 | INFO | train_inner | epoch 066:    622 / 1349 loss=1.812, nll_loss=0.212, ppl=1.16, wps=65218.7, ups=16.64, wpb=3919.3, bsz=209, num_updates=88300, lr=0.000106419, gnorm=0.474, loss_scale=16, train_wall=6, gb_free=29.7, wall=5690
2024-01-30 21:41:23 | INFO | train_inner | epoch 066:    722 / 1349 loss=1.825, nll_loss=0.226, ppl=1.17, wps=65638.2, ups=16.69, wpb=3931.8, bsz=203, num_updates=88400, lr=0.000106359, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=5696
2024-01-30 21:41:29 | INFO | train_inner | epoch 066:    822 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=65233.2, ups=16.63, wpb=3923.8, bsz=203.1, num_updates=88500, lr=0.000106299, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.8, wall=5702
2024-01-30 21:41:35 | INFO | train_inner | epoch 066:    922 / 1349 loss=1.82, nll_loss=0.221, ppl=1.17, wps=65344.4, ups=16.66, wpb=3921.2, bsz=213.2, num_updates=88600, lr=0.000106239, gnorm=0.521, loss_scale=16, train_wall=6, gb_free=29.8, wall=5708
2024-01-30 21:41:41 | INFO | train_inner | epoch 066:   1022 / 1349 loss=1.818, nll_loss=0.219, ppl=1.16, wps=64750.6, ups=16.66, wpb=3886.3, bsz=215.3, num_updates=88700, lr=0.000106179, gnorm=0.486, loss_scale=16, train_wall=6, gb_free=29.8, wall=5714
2024-01-30 21:41:47 | INFO | train_inner | epoch 066:   1122 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=65077.4, ups=16.51, wpb=3942.4, bsz=220.2, num_updates=88800, lr=0.000106119, gnorm=0.455, loss_scale=16, train_wall=6, gb_free=29.8, wall=5721
2024-01-30 21:41:53 | INFO | train_inner | epoch 066:   1222 / 1349 loss=1.821, nll_loss=0.222, ppl=1.17, wps=65610.4, ups=16.71, wpb=3926.8, bsz=211.6, num_updates=88900, lr=0.000106059, gnorm=0.464, loss_scale=16, train_wall=6, gb_free=29.8, wall=5726
2024-01-30 21:41:59 | INFO | train_inner | epoch 066:   1322 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=64702.9, ups=16.53, wpb=3914.2, bsz=218.3, num_updates=89000, lr=0.000106, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=30, wall=5733
2024-01-30 21:42:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:42:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:42:03 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 2.126 | nll_loss 0.377 | ppl 1.3 | wps 171101 | wpb 3275.2 | bsz 176.5 | num_updates 89027 | best_loss 2.063
2024-01-30 21:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 89027 updates
2024-01-30 21:42:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint66.pt
2024-01-30 21:42:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint66.pt
2024-01-30 21:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint66.pt (epoch 66 @ 89027 updates, score 2.126) (writing took 2.3686747590545565 seconds)
2024-01-30 21:42:05 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-01-30 21:42:05 | INFO | train | epoch 066 | loss 1.815 | nll_loss 0.216 | ppl 1.16 | wps 61977.9 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 89027 | lr 0.000105984 | gnorm 0.483 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 5738
2024-01-30 21:42:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:42:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:42:05 | INFO | fairseq.trainer | begin training epoch 67
2024-01-30 21:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:42:10 | INFO | train_inner | epoch 067:     73 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=38089.8, ups=9.82, wpb=3880.4, bsz=212, num_updates=89100, lr=0.00010594, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=5743
2024-01-30 21:42:15 | INFO | train_inner | epoch 067:    173 / 1349 loss=1.81, nll_loss=0.21, ppl=1.16, wps=65197.8, ups=16.72, wpb=3899.3, bsz=204.6, num_updates=89200, lr=0.000105881, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=5749
2024-01-30 21:42:22 | INFO | train_inner | epoch 067:    273 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=65038.1, ups=16.63, wpb=3911.6, bsz=216.4, num_updates=89300, lr=0.000105822, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=5755
2024-01-30 21:42:28 | INFO | train_inner | epoch 067:    373 / 1349 loss=1.812, nll_loss=0.211, ppl=1.16, wps=65099.2, ups=16.66, wpb=3907.8, bsz=207, num_updates=89400, lr=0.000105762, gnorm=0.477, loss_scale=16, train_wall=6, gb_free=29.8, wall=5761
2024-01-30 21:42:34 | INFO | train_inner | epoch 067:    473 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=65733, ups=16.63, wpb=3953.3, bsz=217.2, num_updates=89500, lr=0.000105703, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.8, wall=5767
2024-01-30 21:42:40 | INFO | train_inner | epoch 067:    573 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=65745.1, ups=16.5, wpb=3985.6, bsz=224.2, num_updates=89600, lr=0.000105644, gnorm=0.535, loss_scale=16, train_wall=6, gb_free=29.7, wall=5773
2024-01-30 21:42:46 | INFO | train_inner | epoch 067:    673 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=65561.1, ups=16.57, wpb=3956.6, bsz=212.3, num_updates=89700, lr=0.000105585, gnorm=0.542, loss_scale=16, train_wall=6, gb_free=29.7, wall=5779
2024-01-30 21:42:52 | INFO | train_inner | epoch 067:    773 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=64704.3, ups=16.64, wpb=3888.8, bsz=222.6, num_updates=89800, lr=0.000105527, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=30.1, wall=5785
2024-01-30 21:42:58 | INFO | train_inner | epoch 067:    873 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=65097.7, ups=16.67, wpb=3904.2, bsz=203.8, num_updates=89900, lr=0.000105468, gnorm=0.509, loss_scale=16, train_wall=6, gb_free=29.7, wall=5791
2024-01-30 21:43:04 | INFO | train_inner | epoch 067:    973 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=64587.8, ups=16.5, wpb=3914.9, bsz=230.2, num_updates=90000, lr=0.000105409, gnorm=0.47, loss_scale=16, train_wall=6, gb_free=29.8, wall=5797
2024-01-30 21:43:10 | INFO | train_inner | epoch 067:   1073 / 1349 loss=1.819, nll_loss=0.22, ppl=1.16, wps=64767.6, ups=16.63, wpb=3894.8, bsz=212.2, num_updates=90100, lr=0.000105351, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=5803
2024-01-30 21:43:16 | INFO | train_inner | epoch 067:   1173 / 1349 loss=1.816, nll_loss=0.217, ppl=1.16, wps=64799.3, ups=16.69, wpb=3882.3, bsz=209, num_updates=90200, lr=0.000105292, gnorm=0.528, loss_scale=16, train_wall=6, gb_free=29.8, wall=5809
2024-01-30 21:43:22 | INFO | train_inner | epoch 067:   1273 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=64935.9, ups=16.51, wpb=3934.2, bsz=220.3, num_updates=90300, lr=0.000105234, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.7, wall=5815
2024-01-30 21:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:43:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:43:28 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 2.119 | nll_loss 0.371 | ppl 1.29 | wps 170218 | wpb 3275.2 | bsz 176.5 | num_updates 90376 | best_loss 2.063
2024-01-30 21:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 90376 updates
2024-01-30 21:43:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint67.pt
2024-01-30 21:43:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint67.pt
2024-01-30 21:43:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint67.pt (epoch 67 @ 90376 updates, score 2.119) (writing took 2.2910373979248106 seconds)
2024-01-30 21:43:30 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-01-30 21:43:30 | INFO | train | epoch 067 | loss 1.815 | nll_loss 0.215 | ppl 1.16 | wps 61994.4 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 90376 | lr 0.00010519 | gnorm 0.51 | loss_scale 16 | train_wall 78 | gb_free 29.7 | wall 5824
2024-01-30 21:43:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:43:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:43:30 | INFO | fairseq.trainer | begin training epoch 68
2024-01-30 21:43:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:43:32 | INFO | train_inner | epoch 068:     24 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=39188.8, ups=9.86, wpb=3973.4, bsz=211.8, num_updates=90400, lr=0.000105176, gnorm=0.512, loss_scale=16, train_wall=6, gb_free=29.8, wall=5825
2024-01-30 21:43:38 | INFO | train_inner | epoch 068:    124 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=65455.4, ups=16.39, wpb=3994.4, bsz=234.3, num_updates=90500, lr=0.000105118, gnorm=0.441, loss_scale=16, train_wall=6, gb_free=29.8, wall=5831
2024-01-30 21:43:44 | INFO | train_inner | epoch 068:    224 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=64927.1, ups=16.6, wpb=3911.4, bsz=218.3, num_updates=90600, lr=0.00010506, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.7, wall=5837
2024-01-30 21:43:50 | INFO | train_inner | epoch 068:    324 / 1349 loss=1.819, nll_loss=0.219, ppl=1.16, wps=65539.2, ups=16.73, wpb=3917.9, bsz=210.2, num_updates=90700, lr=0.000105002, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.7, wall=5843
2024-01-30 21:43:56 | INFO | train_inner | epoch 068:    424 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=64982.2, ups=16.63, wpb=3908.2, bsz=211.4, num_updates=90800, lr=0.000104944, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.9, wall=5849
2024-01-30 21:44:02 | INFO | train_inner | epoch 068:    524 / 1349 loss=1.812, nll_loss=0.213, ppl=1.16, wps=64476.2, ups=16.55, wpb=3895.2, bsz=220.6, num_updates=90900, lr=0.000104886, gnorm=0.479, loss_scale=16, train_wall=6, gb_free=29.7, wall=5855
2024-01-30 21:44:08 | INFO | train_inner | epoch 068:    624 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=65165.5, ups=16.54, wpb=3939.4, bsz=221.5, num_updates=91000, lr=0.000104828, gnorm=0.501, loss_scale=16, train_wall=6, gb_free=29.7, wall=5861
2024-01-30 21:44:14 | INFO | train_inner | epoch 068:    724 / 1349 loss=1.816, nll_loss=0.216, ppl=1.16, wps=64816.6, ups=16.7, wpb=3880.8, bsz=209.1, num_updates=91100, lr=0.000104771, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.7, wall=5867
2024-01-30 21:44:20 | INFO | train_inner | epoch 068:    824 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=64653.7, ups=16.6, wpb=3895.2, bsz=203.4, num_updates=91200, lr=0.000104713, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=5873
2024-01-30 21:44:26 | INFO | train_inner | epoch 068:    924 / 1349 loss=1.818, nll_loss=0.219, ppl=1.16, wps=65876.5, ups=16.75, wpb=3932.4, bsz=202.6, num_updates=91300, lr=0.000104656, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=5879
2024-01-30 21:44:32 | INFO | train_inner | epoch 068:   1024 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=65687.6, ups=16.54, wpb=3972, bsz=233.1, num_updates=91400, lr=0.000104599, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.8, wall=5885
2024-01-30 21:44:38 | INFO | train_inner | epoch 068:   1124 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=64271.8, ups=16.66, wpb=3857.6, bsz=206.9, num_updates=91500, lr=0.000104542, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.7, wall=5891
2024-01-30 21:44:44 | INFO | train_inner | epoch 068:   1224 / 1349 loss=1.819, nll_loss=0.22, ppl=1.16, wps=65597.5, ups=16.69, wpb=3930.9, bsz=206.6, num_updates=91600, lr=0.000104485, gnorm=0.505, loss_scale=16, train_wall=6, gb_free=29.8, wall=5897
2024-01-30 21:44:50 | INFO | train_inner | epoch 068:   1324 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=65236.2, ups=16.6, wpb=3930.5, bsz=214.8, num_updates=91700, lr=0.000104428, gnorm=0.559, loss_scale=16, train_wall=6, gb_free=29.8, wall=5903
2024-01-30 21:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:44:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:44:53 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 2.118 | nll_loss 0.369 | ppl 1.29 | wps 171972 | wpb 3275.2 | bsz 176.5 | num_updates 91725 | best_loss 2.063
2024-01-30 21:44:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 91725 updates
2024-01-30 21:44:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint68.pt
2024-01-30 21:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint68.pt
2024-01-30 21:44:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint68.pt (epoch 68 @ 91725 updates, score 2.118) (writing took 2.287460033898242 seconds)
2024-01-30 21:44:56 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-01-30 21:44:56 | INFO | train | epoch 068 | loss 1.814 | nll_loss 0.215 | ppl 1.16 | wps 62016.1 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 91725 | lr 0.000104413 | gnorm 0.501 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 5909
2024-01-30 21:44:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:44:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:44:56 | INFO | fairseq.trainer | begin training epoch 69
2024-01-30 21:44:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:45:00 | INFO | train_inner | epoch 069:     75 / 1349 loss=1.811, nll_loss=0.211, ppl=1.16, wps=38729.6, ups=9.89, wpb=3915.9, bsz=210.3, num_updates=91800, lr=0.000104371, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.8, wall=5913
2024-01-30 21:45:06 | INFO | train_inner | epoch 069:    175 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=66151.8, ups=16.7, wpb=3960.9, bsz=211, num_updates=91900, lr=0.000104314, gnorm=0.538, loss_scale=16, train_wall=6, gb_free=29.7, wall=5919
2024-01-30 21:45:12 | INFO | train_inner | epoch 069:    275 / 1349 loss=1.811, nll_loss=0.211, ppl=1.16, wps=64963.9, ups=16.66, wpb=3900.3, bsz=208.7, num_updates=92000, lr=0.000104257, gnorm=0.527, loss_scale=16, train_wall=6, gb_free=29.8, wall=5925
2024-01-30 21:45:18 | INFO | train_inner | epoch 069:    375 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=65535.1, ups=16.68, wpb=3929.7, bsz=205.4, num_updates=92100, lr=0.000104201, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=5931
2024-01-30 21:45:24 | INFO | train_inner | epoch 069:    475 / 1349 loss=1.812, nll_loss=0.212, ppl=1.16, wps=64448.1, ups=16.61, wpb=3879.4, bsz=211.8, num_updates=92200, lr=0.000104144, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=5937
2024-01-30 21:45:30 | INFO | train_inner | epoch 069:    575 / 1349 loss=1.81, nll_loss=0.21, ppl=1.16, wps=64751.9, ups=16.55, wpb=3911.6, bsz=225.2, num_updates=92300, lr=0.000104088, gnorm=0.489, loss_scale=16, train_wall=6, gb_free=29.9, wall=5944
2024-01-30 21:45:36 | INFO | train_inner | epoch 069:    675 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=65191.9, ups=16.7, wpb=3902.8, bsz=202.8, num_updates=92400, lr=0.000104031, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.8, wall=5950
2024-01-30 21:45:42 | INFO | train_inner | epoch 069:    775 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=65627.1, ups=16.64, wpb=3943.2, bsz=223, num_updates=92500, lr=0.000103975, gnorm=0.588, loss_scale=16, train_wall=6, gb_free=29.9, wall=5956
2024-01-30 21:45:48 | INFO | train_inner | epoch 069:    875 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=65812.3, ups=16.61, wpb=3962.2, bsz=215.7, num_updates=92600, lr=0.000103919, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=5962
2024-01-30 21:45:54 | INFO | train_inner | epoch 069:    975 / 1349 loss=1.813, nll_loss=0.213, ppl=1.16, wps=64719.6, ups=16.56, wpb=3908.6, bsz=212.6, num_updates=92700, lr=0.000103863, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=5968
2024-01-30 21:46:00 | INFO | train_inner | epoch 069:   1075 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=64311.5, ups=16.45, wpb=3908.6, bsz=219.8, num_updates=92800, lr=0.000103807, gnorm=0.588, loss_scale=16, train_wall=6, gb_free=29.7, wall=5974
2024-01-30 21:46:06 | INFO | train_inner | epoch 069:   1175 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=64778.9, ups=16.62, wpb=3897.7, bsz=214.6, num_updates=92900, lr=0.000103751, gnorm=0.474, loss_scale=16, train_wall=6, gb_free=29.8, wall=5980
2024-01-30 21:46:13 | INFO | train_inner | epoch 069:   1275 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65212.2, ups=16.49, wpb=3954, bsz=221, num_updates=93000, lr=0.000103695, gnorm=0.49, loss_scale=16, train_wall=6, gb_free=29.7, wall=5986
2024-01-30 21:46:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:46:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:46:19 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 2.122 | nll_loss 0.373 | ppl 1.29 | wps 172076 | wpb 3275.2 | bsz 176.5 | num_updates 93074 | best_loss 2.063
2024-01-30 21:46:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 93074 updates
2024-01-30 21:46:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint69.pt
2024-01-30 21:46:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint69.pt
2024-01-30 21:46:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint69.pt (epoch 69 @ 93074 updates, score 2.122) (writing took 2.2454051269451156 seconds)
2024-01-30 21:46:21 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-01-30 21:46:21 | INFO | train | epoch 069 | loss 1.813 | nll_loss 0.213 | ppl 1.16 | wps 62014.4 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 93074 | lr 0.000103654 | gnorm 0.517 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 5994
2024-01-30 21:46:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:46:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:46:21 | INFO | fairseq.trainer | begin training epoch 70
2024-01-30 21:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:46:23 | INFO | train_inner | epoch 070:     26 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=38814.2, ups=9.92, wpb=3914.2, bsz=216.2, num_updates=93100, lr=0.000103639, gnorm=0.617, loss_scale=16, train_wall=6, gb_free=29.8, wall=5996
2024-01-30 21:46:29 | INFO | train_inner | epoch 070:    126 / 1349 loss=1.808, nll_loss=0.208, ppl=1.15, wps=66112.4, ups=16.57, wpb=3990.2, bsz=215.7, num_updates=93200, lr=0.000103584, gnorm=0.448, loss_scale=16, train_wall=6, gb_free=29.7, wall=6002
2024-01-30 21:46:35 | INFO | train_inner | epoch 070:    226 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65089.7, ups=16.51, wpb=3943.6, bsz=220.9, num_updates=93300, lr=0.000103528, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=6008
2024-01-30 21:46:41 | INFO | train_inner | epoch 070:    326 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=65245.6, ups=16.59, wpb=3932.1, bsz=210.7, num_updates=93400, lr=0.000103473, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=6014
2024-01-30 21:46:47 | INFO | train_inner | epoch 070:    426 / 1349 loss=1.819, nll_loss=0.22, ppl=1.16, wps=64756.7, ups=16.82, wpb=3849.3, bsz=202.4, num_updates=93500, lr=0.000103418, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=6020
2024-01-30 21:46:53 | INFO | train_inner | epoch 070:    526 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=65461.6, ups=16.51, wpb=3964.9, bsz=220.2, num_updates=93600, lr=0.000103362, gnorm=0.489, loss_scale=16, train_wall=6, gb_free=29.8, wall=6026
2024-01-30 21:46:59 | INFO | train_inner | epoch 070:    626 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=64390.7, ups=16.61, wpb=3877.7, bsz=213.3, num_updates=93700, lr=0.000103307, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.8, wall=6032
2024-01-30 21:47:05 | INFO | train_inner | epoch 070:    726 / 1349 loss=1.81, nll_loss=0.21, ppl=1.16, wps=65433.8, ups=16.61, wpb=3939.2, bsz=215.5, num_updates=93800, lr=0.000103252, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.8, wall=6038
2024-01-30 21:47:11 | INFO | train_inner | epoch 070:    826 / 1349 loss=1.812, nll_loss=0.213, ppl=1.16, wps=65622.3, ups=16.64, wpb=3943.6, bsz=205, num_updates=93900, lr=0.000103197, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=6044
2024-01-30 21:47:17 | INFO | train_inner | epoch 070:    926 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=65094.8, ups=16.54, wpb=3936.2, bsz=230.7, num_updates=94000, lr=0.000103142, gnorm=0.471, loss_scale=16, train_wall=6, gb_free=29.8, wall=6050
2024-01-30 21:47:23 | INFO | train_inner | epoch 070:   1026 / 1349 loss=1.816, nll_loss=0.217, ppl=1.16, wps=64979.9, ups=16.73, wpb=3884.3, bsz=209, num_updates=94100, lr=0.000103087, gnorm=0.521, loss_scale=16, train_wall=6, gb_free=30, wall=6056
2024-01-30 21:47:29 | INFO | train_inner | epoch 070:   1126 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=65008.5, ups=16.58, wpb=3920.1, bsz=212.8, num_updates=94200, lr=0.000103033, gnorm=0.538, loss_scale=16, train_wall=6, gb_free=29.8, wall=6062
2024-01-30 21:47:35 | INFO | train_inner | epoch 070:   1226 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=65341.1, ups=16.55, wpb=3947.2, bsz=220.2, num_updates=94300, lr=0.000102978, gnorm=0.469, loss_scale=16, train_wall=6, gb_free=29.8, wall=6068
2024-01-30 21:47:41 | INFO | train_inner | epoch 070:   1326 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=64367.8, ups=16.57, wpb=3885.7, bsz=213, num_updates=94400, lr=0.000102923, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.7, wall=6074
2024-01-30 21:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:47:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:47:44 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 2.125 | nll_loss 0.377 | ppl 1.3 | wps 172026 | wpb 3275.2 | bsz 176.5 | num_updates 94423 | best_loss 2.063
2024-01-30 21:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 94423 updates
2024-01-30 21:47:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint70.pt
2024-01-30 21:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint70.pt
2024-01-30 21:47:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint70.pt (epoch 70 @ 94423 updates, score 2.125) (writing took 2.259240674902685 seconds)
2024-01-30 21:47:46 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-01-30 21:47:46 | INFO | train | epoch 070 | loss 1.812 | nll_loss 0.213 | ppl 1.16 | wps 62005 | ups 15.81 | wpb 3921.2 | bsz 214.5 | num_updates 94423 | lr 0.000102911 | gnorm 0.51 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 6080
2024-01-30 21:47:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:47:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:47:46 | INFO | fairseq.trainer | begin training epoch 71
2024-01-30 21:47:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:47:51 | INFO | train_inner | epoch 071:     77 / 1349 loss=1.811, nll_loss=0.211, ppl=1.16, wps=38712.8, ups=9.95, wpb=3889.9, bsz=206.6, num_updates=94500, lr=0.000102869, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=6084
2024-01-30 21:47:57 | INFO | train_inner | epoch 071:    177 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=66206.4, ups=16.61, wpb=3986.8, bsz=216.7, num_updates=94600, lr=0.000102815, gnorm=0.464, loss_scale=16, train_wall=6, gb_free=29.7, wall=6090
2024-01-30 21:48:03 | INFO | train_inner | epoch 071:    277 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=65326.4, ups=16.62, wpb=3931.7, bsz=216.2, num_updates=94700, lr=0.00010276, gnorm=0.456, loss_scale=16, train_wall=6, gb_free=29.8, wall=6096
2024-01-30 21:48:09 | INFO | train_inner | epoch 071:    377 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=64312.1, ups=16.59, wpb=3877.3, bsz=215.5, num_updates=94800, lr=0.000102706, gnorm=0.511, loss_scale=16, train_wall=6, gb_free=29.8, wall=6102
2024-01-30 21:48:15 | INFO | train_inner | epoch 071:    477 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=64897.8, ups=16.7, wpb=3885.1, bsz=217.2, num_updates=94900, lr=0.000102652, gnorm=0.55, loss_scale=16, train_wall=6, gb_free=29.7, wall=6108
2024-01-30 21:48:21 | INFO | train_inner | epoch 071:    577 / 1349 loss=1.81, nll_loss=0.21, ppl=1.16, wps=65100.3, ups=16.64, wpb=3911.2, bsz=220.6, num_updates=95000, lr=0.000102598, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.8, wall=6114
2024-01-30 21:48:27 | INFO | train_inner | epoch 071:    677 / 1349 loss=1.809, nll_loss=0.209, ppl=1.16, wps=65837.7, ups=16.62, wpb=3960.6, bsz=209.5, num_updates=95100, lr=0.000102544, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.9, wall=6120
2024-01-30 21:48:33 | INFO | train_inner | epoch 071:    777 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65353.9, ups=16.6, wpb=3936.3, bsz=225.5, num_updates=95200, lr=0.00010249, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=6126
2024-01-30 21:48:39 | INFO | train_inner | epoch 071:    877 / 1349 loss=1.818, nll_loss=0.219, ppl=1.16, wps=64773.5, ups=16.74, wpb=3868.7, bsz=201.1, num_updates=95300, lr=0.000102436, gnorm=0.566, loss_scale=16, train_wall=6, gb_free=29.7, wall=6132
2024-01-30 21:48:45 | INFO | train_inner | epoch 071:    977 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=65988.4, ups=16.76, wpb=3937.2, bsz=207.8, num_updates=95400, lr=0.000102383, gnorm=0.513, loss_scale=16, train_wall=6, gb_free=29.8, wall=6138
2024-01-30 21:48:51 | INFO | train_inner | epoch 071:   1077 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=65384.3, ups=16.71, wpb=3912, bsz=207, num_updates=95500, lr=0.000102329, gnorm=0.509, loss_scale=16, train_wall=6, gb_free=29.8, wall=6144
2024-01-30 21:48:57 | INFO | train_inner | epoch 071:   1177 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=64747.3, ups=16.48, wpb=3927.7, bsz=236.1, num_updates=95600, lr=0.000102275, gnorm=0.503, loss_scale=16, train_wall=6, gb_free=29.9, wall=6150
2024-01-30 21:49:03 | INFO | train_inner | epoch 071:   1277 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=65029.7, ups=16.54, wpb=3931.9, bsz=222.9, num_updates=95700, lr=0.000102222, gnorm=0.472, loss_scale=16, train_wall=6, gb_free=29.8, wall=6156
2024-01-30 21:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-30 21:49:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:49:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:49:09 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 2.123 | nll_loss 0.377 | ppl 1.3 | wps 159250 | wpb 3275.2 | bsz 176.5 | num_updates 95771 | best_loss 2.063
2024-01-30 21:49:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 95771 updates
2024-01-30 21:49:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint71.pt
2024-01-30 21:49:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint71.pt
2024-01-30 21:49:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint71.pt (epoch 71 @ 95771 updates, score 2.123) (writing took 2.2753844279795885 seconds)
2024-01-30 21:49:12 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-01-30 21:49:12 | INFO | train | epoch 071 | loss 1.811 | nll_loss 0.212 | ppl 1.16 | wps 61584.8 | ups 15.71 | wpb 3921 | bsz 214.5 | num_updates 95771 | lr 0.000102184 | gnorm 0.497 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6165
2024-01-30 21:49:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:49:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:49:12 | INFO | fairseq.trainer | begin training epoch 72
2024-01-30 21:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:49:17 | INFO | train_inner | epoch 072:     29 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=27822.2, ups=7.12, wpb=3908.6, bsz=199.4, num_updates=95800, lr=0.000102169, gnorm=0.456, loss_scale=8, train_wall=9, gb_free=29.8, wall=6170
2024-01-30 21:49:23 | INFO | train_inner | epoch 072:    129 / 1349 loss=1.811, nll_loss=0.211, ppl=1.16, wps=65297.2, ups=16.81, wpb=3884.4, bsz=202.8, num_updates=95900, lr=0.000102115, gnorm=0.468, loss_scale=8, train_wall=6, gb_free=29.8, wall=6176
2024-01-30 21:49:29 | INFO | train_inner | epoch 072:    229 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65337.8, ups=16.63, wpb=3928.2, bsz=234.5, num_updates=96000, lr=0.000102062, gnorm=0.442, loss_scale=8, train_wall=6, gb_free=29.8, wall=6182
2024-01-30 21:49:35 | INFO | train_inner | epoch 072:    329 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=66234.5, ups=16.67, wpb=3974.1, bsz=206.2, num_updates=96100, lr=0.000102009, gnorm=0.474, loss_scale=8, train_wall=6, gb_free=29.8, wall=6188
2024-01-30 21:49:41 | INFO | train_inner | epoch 072:    429 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=65277.8, ups=16.51, wpb=3954.8, bsz=215.1, num_updates=96200, lr=0.000101956, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.7, wall=6194
2024-01-30 21:49:47 | INFO | train_inner | epoch 072:    529 / 1349 loss=1.812, nll_loss=0.213, ppl=1.16, wps=64956.3, ups=16.72, wpb=3884, bsz=205.9, num_updates=96300, lr=0.000101903, gnorm=0.542, loss_scale=8, train_wall=6, gb_free=29.8, wall=6200
2024-01-30 21:49:53 | INFO | train_inner | epoch 072:    629 / 1349 loss=1.815, nll_loss=0.215, ppl=1.16, wps=66009.4, ups=16.72, wpb=3948.3, bsz=209.4, num_updates=96400, lr=0.00010185, gnorm=0.476, loss_scale=8, train_wall=6, gb_free=29.8, wall=6206
2024-01-30 21:49:59 | INFO | train_inner | epoch 072:    729 / 1349 loss=1.807, nll_loss=0.208, ppl=1.15, wps=65161.7, ups=16.56, wpb=3934.4, bsz=209.8, num_updates=96500, lr=0.000101797, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.7, wall=6212
2024-01-30 21:50:05 | INFO | train_inner | epoch 072:    829 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=64626.8, ups=16.54, wpb=3906.6, bsz=230.7, num_updates=96600, lr=0.000101745, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=6218
2024-01-30 21:50:11 | INFO | train_inner | epoch 072:    929 / 1349 loss=1.812, nll_loss=0.213, ppl=1.16, wps=65270.5, ups=16.6, wpb=3931.2, bsz=217.8, num_updates=96700, lr=0.000101692, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=29.8, wall=6224
2024-01-30 21:50:17 | INFO | train_inner | epoch 072:   1029 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=64925.1, ups=16.57, wpb=3918.1, bsz=223.5, num_updates=96800, lr=0.000101639, gnorm=0.514, loss_scale=8, train_wall=6, gb_free=29.8, wall=6230
2024-01-30 21:50:23 | INFO | train_inner | epoch 072:   1129 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65079.6, ups=16.59, wpb=3922.4, bsz=227.4, num_updates=96900, lr=0.000101587, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.7, wall=6237
2024-01-30 21:50:29 | INFO | train_inner | epoch 072:   1229 / 1349 loss=1.816, nll_loss=0.218, ppl=1.16, wps=65209.8, ups=16.85, wpb=3869.4, bsz=201.6, num_updates=97000, lr=0.000101535, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.7, wall=6242
2024-01-30 21:50:35 | INFO | train_inner | epoch 072:   1329 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=65749.4, ups=16.74, wpb=3928.1, bsz=202, num_updates=97100, lr=0.000101482, gnorm=0.566, loss_scale=8, train_wall=6, gb_free=29.7, wall=6248
2024-01-30 21:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:50:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:50:38 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 2.126 | nll_loss 0.38 | ppl 1.3 | wps 172294 | wpb 3275.2 | bsz 176.5 | num_updates 97120 | best_loss 2.063
2024-01-30 21:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 97120 updates
2024-01-30 21:50:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint72.pt
2024-01-30 21:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint72.pt
2024-01-30 21:50:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint72.pt (epoch 72 @ 97120 updates, score 2.126) (writing took 2.2981136549497023 seconds)
2024-01-30 21:50:40 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-01-30 21:50:40 | INFO | train | epoch 072 | loss 1.81 | nll_loss 0.211 | ppl 1.16 | wps 59861.3 | ups 15.27 | wpb 3921.2 | bsz 214.5 | num_updates 97120 | lr 0.000101472 | gnorm 0.499 | loss_scale 8 | train_wall 81 | gb_free 29.7 | wall 6254
2024-01-30 21:50:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:50:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:50:41 | INFO | fairseq.trainer | begin training epoch 73
2024-01-30 21:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:50:45 | INFO | train_inner | epoch 073:     80 / 1349 loss=1.804, nll_loss=0.204, ppl=1.15, wps=38390.2, ups=9.85, wpb=3897.1, bsz=227.2, num_updates=97200, lr=0.00010143, gnorm=0.464, loss_scale=8, train_wall=6, gb_free=29.8, wall=6259
2024-01-30 21:50:51 | INFO | train_inner | epoch 073:    180 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65925.4, ups=16.57, wpb=3978.5, bsz=217.1, num_updates=97300, lr=0.000101378, gnorm=0.467, loss_scale=8, train_wall=6, gb_free=29.8, wall=6265
2024-01-30 21:50:57 | INFO | train_inner | epoch 073:    280 / 1349 loss=1.808, nll_loss=0.208, ppl=1.16, wps=65528.7, ups=16.67, wpb=3930.8, bsz=211.3, num_updates=97400, lr=0.000101326, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=6271
2024-01-30 21:51:03 | INFO | train_inner | epoch 073:    380 / 1349 loss=1.811, nll_loss=0.211, ppl=1.16, wps=65718.5, ups=16.68, wpb=3940.5, bsz=207, num_updates=97500, lr=0.000101274, gnorm=0.46, loss_scale=8, train_wall=6, gb_free=29.8, wall=6277
2024-01-30 21:51:09 | INFO | train_inner | epoch 073:    480 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=64823.1, ups=16.73, wpb=3875.6, bsz=201.4, num_updates=97600, lr=0.000101222, gnorm=0.597, loss_scale=8, train_wall=6, gb_free=29.8, wall=6283
2024-01-30 21:51:15 | INFO | train_inner | epoch 073:    580 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65728.3, ups=16.7, wpb=3936.5, bsz=214.4, num_updates=97700, lr=0.00010117, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=6289
2024-01-30 21:51:21 | INFO | train_inner | epoch 073:    680 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65310.5, ups=16.56, wpb=3942.8, bsz=226.2, num_updates=97800, lr=0.000101118, gnorm=0.459, loss_scale=8, train_wall=6, gb_free=29.8, wall=6295
2024-01-30 21:51:27 | INFO | train_inner | epoch 073:    780 / 1349 loss=1.814, nll_loss=0.215, ppl=1.16, wps=65602.5, ups=16.72, wpb=3922.9, bsz=208.7, num_updates=97900, lr=0.000101067, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=6301
2024-01-30 21:51:33 | INFO | train_inner | epoch 073:    880 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=64691.4, ups=16.55, wpb=3907.9, bsz=209.4, num_updates=98000, lr=0.000101015, gnorm=0.449, loss_scale=8, train_wall=6, gb_free=29.8, wall=6307
2024-01-30 21:51:39 | INFO | train_inner | epoch 073:    980 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=64503.8, ups=16.68, wpb=3868.2, bsz=238.1, num_updates=98100, lr=0.000100964, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=30, wall=6313
2024-01-30 21:51:45 | INFO | train_inner | epoch 073:   1080 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=64687.1, ups=16.68, wpb=3878.9, bsz=218.4, num_updates=98200, lr=0.000100912, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=6319
2024-01-30 21:51:51 | INFO | train_inner | epoch 073:   1180 / 1349 loss=1.815, nll_loss=0.216, ppl=1.16, wps=65449.6, ups=16.71, wpb=3917.1, bsz=205.7, num_updates=98300, lr=0.000100861, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=6325
2024-01-30 21:51:57 | INFO | train_inner | epoch 073:   1280 / 1349 loss=1.81, nll_loss=0.21, ppl=1.16, wps=65907.5, ups=16.63, wpb=3962.8, bsz=209.2, num_updates=98400, lr=0.00010081, gnorm=0.454, loss_scale=8, train_wall=6, gb_free=29.7, wall=6331
2024-01-30 21:52:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:52:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:52:03 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 2.124 | nll_loss 0.377 | ppl 1.3 | wps 172546 | wpb 3275.2 | bsz 176.5 | num_updates 98469 | best_loss 2.063
2024-01-30 21:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 98469 updates
2024-01-30 21:52:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint73.pt
2024-01-30 21:52:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint73.pt
2024-01-30 21:52:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint73.pt (epoch 73 @ 98469 updates, score 2.124) (writing took 2.240168620017357 seconds)
2024-01-30 21:52:06 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-01-30 21:52:06 | INFO | train | epoch 073 | loss 1.81 | nll_loss 0.21 | ppl 1.16 | wps 62168.3 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 98469 | lr 0.000100774 | gnorm 0.489 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6339
2024-01-30 21:52:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:52:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:52:06 | INFO | fairseq.trainer | begin training epoch 74
2024-01-30 21:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:52:07 | INFO | train_inner | epoch 074:     31 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=39006.4, ups=9.92, wpb=3933.4, bsz=205.9, num_updates=98500, lr=0.000100759, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=6341
2024-01-30 21:52:14 | INFO | train_inner | epoch 074:    131 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=64646.8, ups=16.58, wpb=3899.5, bsz=212.4, num_updates=98600, lr=0.000100707, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=6347
2024-01-30 21:52:20 | INFO | train_inner | epoch 074:    231 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=64770.5, ups=16.48, wpb=3931.2, bsz=207.8, num_updates=98700, lr=0.000100656, gnorm=0.541, loss_scale=8, train_wall=6, gb_free=29.7, wall=6353
2024-01-30 21:52:26 | INFO | train_inner | epoch 074:    331 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=65688.2, ups=16.73, wpb=3925.6, bsz=211.8, num_updates=98800, lr=0.000100605, gnorm=0.538, loss_scale=8, train_wall=6, gb_free=29.8, wall=6359
2024-01-30 21:52:32 | INFO | train_inner | epoch 074:    431 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65528, ups=16.74, wpb=3913.8, bsz=206.9, num_updates=98900, lr=0.000100555, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=6365
2024-01-30 21:52:38 | INFO | train_inner | epoch 074:    531 / 1349 loss=1.808, nll_loss=0.208, ppl=1.16, wps=66169.6, ups=16.69, wpb=3963.6, bsz=219.2, num_updates=99000, lr=0.000100504, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.8, wall=6371
2024-01-30 21:52:44 | INFO | train_inner | epoch 074:    631 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=65608.4, ups=16.66, wpb=3938.7, bsz=208.8, num_updates=99100, lr=0.000100453, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=6377
2024-01-30 21:52:50 | INFO | train_inner | epoch 074:    731 / 1349 loss=1.807, nll_loss=0.208, ppl=1.15, wps=64953.3, ups=16.62, wpb=3909.2, bsz=220.7, num_updates=99200, lr=0.000100402, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.8, wall=6383
2024-01-30 21:52:56 | INFO | train_inner | epoch 074:    831 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65993.8, ups=16.61, wpb=3974, bsz=221.3, num_updates=99300, lr=0.000100352, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.8, wall=6389
2024-01-30 21:53:02 | INFO | train_inner | epoch 074:    931 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65124.1, ups=16.58, wpb=3928.1, bsz=213.9, num_updates=99400, lr=0.000100301, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.7, wall=6395
2024-01-30 21:53:08 | INFO | train_inner | epoch 074:   1031 / 1349 loss=1.814, nll_loss=0.214, ppl=1.16, wps=65075.3, ups=16.75, wpb=3884.6, bsz=210.6, num_updates=99500, lr=0.000100251, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=6401
2024-01-30 21:53:14 | INFO | train_inner | epoch 074:   1131 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=64984.7, ups=16.62, wpb=3910.8, bsz=217.3, num_updates=99600, lr=0.000100201, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=6407
2024-01-30 21:53:20 | INFO | train_inner | epoch 074:   1231 / 1349 loss=1.812, nll_loss=0.213, ppl=1.16, wps=64419.7, ups=16.7, wpb=3857.6, bsz=211.4, num_updates=99700, lr=0.00010015, gnorm=0.539, loss_scale=8, train_wall=6, gb_free=29.8, wall=6413
2024-01-30 21:53:26 | INFO | train_inner | epoch 074:   1331 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65426.6, ups=16.64, wpb=3931.4, bsz=227.7, num_updates=99800, lr=0.0001001, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=6419
2024-01-30 21:53:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:53:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:53:28 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 2.134 | nll_loss 0.388 | ppl 1.31 | wps 172941 | wpb 3275.2 | bsz 176.5 | num_updates 99818 | best_loss 2.063
2024-01-30 21:53:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 99818 updates
2024-01-30 21:53:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint74.pt
2024-01-30 21:53:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint74.pt
2024-01-30 21:53:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint74.pt (epoch 74 @ 99818 updates, score 2.134) (writing took 2.3785284330369905 seconds)
2024-01-30 21:53:31 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-01-30 21:53:31 | INFO | train | epoch 074 | loss 1.808 | nll_loss 0.209 | ppl 1.16 | wps 62037.2 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 99818 | lr 0.000100091 | gnorm 0.504 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6424
2024-01-30 21:53:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:53:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:53:31 | INFO | fairseq.trainer | begin training epoch 75
2024-01-30 21:53:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:53:36 | INFO | train_inner | epoch 075:     82 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=38421, ups=9.73, wpb=3950.6, bsz=217.6, num_updates=99900, lr=0.00010005, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.8, wall=6429
2024-01-30 21:53:42 | INFO | train_inner | epoch 075:    182 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=64850.8, ups=16.58, wpb=3910.9, bsz=211.5, num_updates=100000, lr=0.0001, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.7, wall=6435
2024-01-30 21:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:53:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:53:44 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 2.134 | nll_loss 0.388 | ppl 1.31 | wps 171465 | wpb 3275.2 | bsz 176.5 | num_updates 100000 | best_loss 2.063
2024-01-30 21:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 100000 updates
2024-01-30 21:53:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_75_100000.pt
2024-01-30 21:53:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_75_100000.pt
2024-01-30 21:53:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint_75_100000.pt (epoch 75 @ 100000 updates, score 2.134) (writing took 2.295988988946192 seconds)
2024-01-30 21:53:52 | INFO | train_inner | epoch 075:    282 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=38671.1, ups=9.99, wpb=3872.2, bsz=211, num_updates=100100, lr=9.995e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.8, wall=6445
2024-01-30 21:53:58 | INFO | train_inner | epoch 075:    382 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=64877.2, ups=16.72, wpb=3879.5, bsz=213.4, num_updates=100200, lr=9.99001e-05, gnorm=0.568, loss_scale=8, train_wall=6, gb_free=29.7, wall=6451
2024-01-30 21:54:04 | INFO | train_inner | epoch 075:    482 / 1349 loss=1.812, nll_loss=0.213, ppl=1.16, wps=65557.8, ups=16.74, wpb=3916.3, bsz=206.6, num_updates=100300, lr=9.98503e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=6457
2024-01-30 21:54:10 | INFO | train_inner | epoch 075:    582 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=63990.5, ups=16.47, wpb=3884.4, bsz=203, num_updates=100400, lr=9.98006e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.7, wall=6463
2024-01-30 21:54:16 | INFO | train_inner | epoch 075:    682 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=66131.7, ups=16.64, wpb=3974.8, bsz=216.5, num_updates=100500, lr=9.97509e-05, gnorm=0.461, loss_scale=8, train_wall=6, gb_free=29.8, wall=6469
2024-01-30 21:54:22 | INFO | train_inner | epoch 075:    782 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65379.1, ups=16.69, wpb=3916.4, bsz=213.8, num_updates=100600, lr=9.97013e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=6475
2024-01-30 21:54:28 | INFO | train_inner | epoch 075:    882 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65388.1, ups=16.57, wpb=3946.4, bsz=225, num_updates=100700, lr=9.96518e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=6481
2024-01-30 21:54:34 | INFO | train_inner | epoch 075:    982 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=64655.7, ups=16.63, wpb=3887.4, bsz=216, num_updates=100800, lr=9.96024e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=6487
2024-01-30 21:54:40 | INFO | train_inner | epoch 075:   1082 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65489.4, ups=16.58, wpb=3950.7, bsz=228.6, num_updates=100900, lr=9.9553e-05, gnorm=0.452, loss_scale=8, train_wall=6, gb_free=29.8, wall=6493
2024-01-30 21:54:46 | INFO | train_inner | epoch 075:   1182 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=65532.6, ups=16.62, wpb=3943.9, bsz=211.5, num_updates=101000, lr=9.95037e-05, gnorm=0.459, loss_scale=8, train_wall=6, gb_free=29.8, wall=6499
2024-01-30 21:54:52 | INFO | train_inner | epoch 075:   1282 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65708.3, ups=16.64, wpb=3949.8, bsz=216.7, num_updates=101100, lr=9.94545e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=6505
2024-01-30 21:54:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:54:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:54:58 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 2.132 | nll_loss 0.389 | ppl 1.31 | wps 172673 | wpb 3275.2 | bsz 176.5 | num_updates 101167 | best_loss 2.063
2024-01-30 21:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 101167 updates
2024-01-30 21:54:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint75.pt
2024-01-30 21:54:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint75.pt
2024-01-30 21:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint75.pt (epoch 75 @ 101167 updates, score 2.132) (writing took 2.3314413849730045 seconds)
2024-01-30 21:55:00 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-01-30 21:55:00 | INFO | train | epoch 075 | loss 1.808 | nll_loss 0.209 | ppl 1.16 | wps 59219.9 | ups 15.1 | wpb 3921.2 | bsz 214.5 | num_updates 101167 | lr 9.94216e-05 | gnorm 0.504 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6513
2024-01-30 21:55:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:55:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:55:00 | INFO | fairseq.trainer | begin training epoch 76
2024-01-30 21:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:55:02 | INFO | train_inner | epoch 076:     33 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=38696.1, ups=9.84, wpb=3933.3, bsz=225.4, num_updates=101200, lr=9.94053e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=6515
2024-01-30 21:55:08 | INFO | train_inner | epoch 076:    133 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=64748.9, ups=16.7, wpb=3878.3, bsz=210.4, num_updates=101300, lr=9.93563e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.7, wall=6521
2024-01-30 21:55:14 | INFO | train_inner | epoch 076:    233 / 1349 loss=1.802, nll_loss=0.202, ppl=1.15, wps=65178.4, ups=16.5, wpb=3950.5, bsz=225.8, num_updates=101400, lr=9.93073e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.8, wall=6528
2024-01-30 21:55:20 | INFO | train_inner | epoch 076:    333 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65947, ups=16.78, wpb=3929, bsz=210.2, num_updates=101500, lr=9.92583e-05, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=30, wall=6533
2024-01-30 21:55:26 | INFO | train_inner | epoch 076:    433 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65373.6, ups=16.64, wpb=3928.5, bsz=213.8, num_updates=101600, lr=9.92095e-05, gnorm=0.476, loss_scale=8, train_wall=6, gb_free=29.8, wall=6539
2024-01-30 21:55:32 | INFO | train_inner | epoch 076:    533 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=65193.5, ups=16.61, wpb=3925, bsz=214.9, num_updates=101700, lr=9.91607e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=6545
2024-01-30 21:55:38 | INFO | train_inner | epoch 076:    633 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=65572.4, ups=16.5, wpb=3973.3, bsz=218.3, num_updates=101800, lr=9.9112e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=6552
2024-01-30 21:55:44 | INFO | train_inner | epoch 076:    733 / 1349 loss=1.807, nll_loss=0.208, ppl=1.15, wps=65298, ups=16.54, wpb=3948.6, bsz=216.4, num_updates=101900, lr=9.90633e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.9, wall=6558
2024-01-30 21:55:50 | INFO | train_inner | epoch 076:    833 / 1349 loss=1.817, nll_loss=0.218, ppl=1.16, wps=65303.5, ups=16.89, wpb=3867.4, bsz=206.9, num_updates=102000, lr=9.90148e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=6564
2024-01-30 21:55:56 | INFO | train_inner | epoch 076:    933 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=65135, ups=16.62, wpb=3918.9, bsz=209.3, num_updates=102100, lr=9.89663e-05, gnorm=0.478, loss_scale=8, train_wall=6, gb_free=29.7, wall=6570
2024-01-30 21:56:02 | INFO | train_inner | epoch 076:   1033 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=65158.8, ups=16.71, wpb=3899.9, bsz=204.3, num_updates=102200, lr=9.89178e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.9, wall=6576
2024-01-30 21:56:08 | INFO | train_inner | epoch 076:   1133 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65526.3, ups=16.58, wpb=3952.5, bsz=220, num_updates=102300, lr=9.88695e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=6582
2024-01-30 21:56:14 | INFO | train_inner | epoch 076:   1233 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65602.4, ups=16.68, wpb=3934, bsz=218, num_updates=102400, lr=9.88212e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.7, wall=6588
2024-01-30 21:56:20 | INFO | train_inner | epoch 076:   1333 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=64711.3, ups=16.81, wpb=3849.5, bsz=209, num_updates=102500, lr=9.8773e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=6594
2024-01-30 21:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:56:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:56:23 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 2.128 | nll_loss 0.385 | ppl 1.31 | wps 170942 | wpb 3275.2 | bsz 176.5 | num_updates 102516 | best_loss 2.063
2024-01-30 21:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 102516 updates
2024-01-30 21:56:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint76.pt
2024-01-30 21:56:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint76.pt
2024-01-30 21:56:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint76.pt (epoch 76 @ 102516 updates, score 2.128) (writing took 2.3994146189652383 seconds)
2024-01-30 21:56:25 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-01-30 21:56:25 | INFO | train | epoch 076 | loss 1.808 | nll_loss 0.209 | ppl 1.16 | wps 62055.7 | ups 15.83 | wpb 3921.2 | bsz 214.5 | num_updates 102516 | lr 9.87653e-05 | gnorm 0.498 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6599
2024-01-30 21:56:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:56:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:56:25 | INFO | fairseq.trainer | begin training epoch 77
2024-01-30 21:56:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:56:30 | INFO | train_inner | epoch 077:     84 / 1349 loss=1.807, nll_loss=0.208, ppl=1.15, wps=38761.9, ups=9.8, wpb=3956.6, bsz=212.6, num_updates=102600, lr=9.87248e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=6604
2024-01-30 21:56:37 | INFO | train_inner | epoch 077:    184 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65523.4, ups=16.54, wpb=3962, bsz=232.7, num_updates=102700, lr=9.86767e-05, gnorm=0.459, loss_scale=8, train_wall=6, gb_free=29.8, wall=6610
2024-01-30 21:56:43 | INFO | train_inner | epoch 077:    284 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65591.1, ups=16.56, wpb=3961.1, bsz=216.7, num_updates=102800, lr=9.86287e-05, gnorm=0.492, loss_scale=8, train_wall=6, gb_free=29.8, wall=6616
2024-01-30 21:56:49 | INFO | train_inner | epoch 077:    384 / 1349 loss=1.804, nll_loss=0.204, ppl=1.15, wps=65130.5, ups=16.61, wpb=3922.1, bsz=217.2, num_updates=102900, lr=9.85808e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.8, wall=6622
2024-01-30 21:56:55 | INFO | train_inner | epoch 077:    484 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65575.8, ups=16.62, wpb=3945.6, bsz=208.8, num_updates=103000, lr=9.85329e-05, gnorm=0.574, loss_scale=8, train_wall=6, gb_free=29.8, wall=6628
2024-01-30 21:57:01 | INFO | train_inner | epoch 077:    584 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=63902.9, ups=16.58, wpb=3853.7, bsz=217.4, num_updates=103100, lr=9.84851e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=6634
2024-01-30 21:57:07 | INFO | train_inner | epoch 077:    684 / 1349 loss=1.813, nll_loss=0.215, ppl=1.16, wps=65415.1, ups=16.71, wpb=3915.6, bsz=203.7, num_updates=103200, lr=9.84374e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=6640
2024-01-30 21:57:13 | INFO | train_inner | epoch 077:    784 / 1349 loss=1.807, nll_loss=0.208, ppl=1.15, wps=65837.1, ups=16.68, wpb=3946.4, bsz=206, num_updates=103300, lr=9.83897e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.7, wall=6646
2024-01-30 21:57:19 | INFO | train_inner | epoch 077:    884 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=64527.2, ups=16.72, wpb=3860.3, bsz=202.9, num_updates=103400, lr=9.83422e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.8, wall=6652
2024-01-30 21:57:25 | INFO | train_inner | epoch 077:    984 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65624.4, ups=16.75, wpb=3918.7, bsz=214.9, num_updates=103500, lr=9.82946e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.9, wall=6658
2024-01-30 21:57:31 | INFO | train_inner | epoch 077:   1084 / 1349 loss=1.811, nll_loss=0.212, ppl=1.16, wps=65343.6, ups=16.71, wpb=3910.2, bsz=210.5, num_updates=103600, lr=9.82472e-05, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.9, wall=6664
2024-01-30 21:57:37 | INFO | train_inner | epoch 077:   1184 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=64598.8, ups=16.72, wpb=3863.1, bsz=222.9, num_updates=103700, lr=9.81998e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=6670
2024-01-30 21:57:43 | INFO | train_inner | epoch 077:   1284 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65577.5, ups=16.55, wpb=3961.8, bsz=217.2, num_updates=103800, lr=9.81525e-05, gnorm=0.608, loss_scale=8, train_wall=6, gb_free=29.8, wall=6676
2024-01-30 21:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:57:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:57:48 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 2.133 | nll_loss 0.387 | ppl 1.31 | wps 172217 | wpb 3275.2 | bsz 176.5 | num_updates 103865 | best_loss 2.063
2024-01-30 21:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 103865 updates
2024-01-30 21:57:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint77.pt
2024-01-30 21:57:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint77.pt
2024-01-30 21:57:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint77.pt (epoch 77 @ 103865 updates, score 2.133) (writing took 2.334908775985241 seconds)
2024-01-30 21:57:51 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-01-30 21:57:51 | INFO | train | epoch 077 | loss 1.807 | nll_loss 0.208 | ppl 1.15 | wps 62097.6 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 103865 | lr 9.81218e-05 | gnorm 0.507 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6684
2024-01-30 21:57:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:57:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:57:51 | INFO | fairseq.trainer | begin training epoch 78
2024-01-30 21:57:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:57:53 | INFO | train_inner | epoch 078:     35 / 1349 loss=1.806, nll_loss=0.206, ppl=1.15, wps=38950.8, ups=9.85, wpb=3953.2, bsz=219.4, num_updates=103900, lr=9.81052e-05, gnorm=0.501, loss_scale=8, train_wall=6, gb_free=29.8, wall=6686
2024-01-30 21:57:59 | INFO | train_inner | epoch 078:    135 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65778.5, ups=16.62, wpb=3958.1, bsz=212.9, num_updates=104000, lr=9.80581e-05, gnorm=0.483, loss_scale=8, train_wall=6, gb_free=29.7, wall=6692
2024-01-30 21:58:05 | INFO | train_inner | epoch 078:    235 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=66107.4, ups=16.71, wpb=3955.7, bsz=211.4, num_updates=104100, lr=9.8011e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.8, wall=6698
2024-01-30 21:58:11 | INFO | train_inner | epoch 078:    335 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=64783.7, ups=16.58, wpb=3907.9, bsz=211.6, num_updates=104200, lr=9.79639e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.7, wall=6704
2024-01-30 21:58:17 | INFO | train_inner | epoch 078:    435 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=65920.5, ups=16.67, wpb=3953.3, bsz=212.7, num_updates=104300, lr=9.79169e-05, gnorm=0.45, loss_scale=8, train_wall=6, gb_free=29.8, wall=6710
2024-01-30 21:58:23 | INFO | train_inner | epoch 078:    535 / 1349 loss=1.807, nll_loss=0.207, ppl=1.15, wps=65206.6, ups=16.62, wpb=3922.4, bsz=211, num_updates=104400, lr=9.787e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=6716
2024-01-30 21:58:29 | INFO | train_inner | epoch 078:    635 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=64631.9, ups=16.74, wpb=3860, bsz=211, num_updates=104500, lr=9.78232e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=6722
2024-01-30 21:58:35 | INFO | train_inner | epoch 078:    735 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=64444.1, ups=16.69, wpb=3860.4, bsz=209, num_updates=104600, lr=9.77764e-05, gnorm=0.53, loss_scale=8, train_wall=6, gb_free=29.8, wall=6728
2024-01-30 21:58:41 | INFO | train_inner | epoch 078:    835 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65210, ups=16.7, wpb=3905.7, bsz=220.8, num_updates=104700, lr=9.77297e-05, gnorm=0.468, loss_scale=8, train_wall=6, gb_free=29.8, wall=6734
2024-01-30 21:58:47 | INFO | train_inner | epoch 078:    935 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=65348.4, ups=16.76, wpb=3899.1, bsz=213.4, num_updates=104800, lr=9.76831e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=6740
2024-01-30 21:58:53 | INFO | train_inner | epoch 078:   1035 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=65547.9, ups=16.58, wpb=3954.5, bsz=234.3, num_updates=104900, lr=9.76365e-05, gnorm=0.463, loss_scale=8, train_wall=6, gb_free=29.8, wall=6746
2024-01-30 21:58:59 | INFO | train_inner | epoch 078:   1135 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=65502.8, ups=16.67, wpb=3928.4, bsz=213.1, num_updates=105000, lr=9.759e-05, gnorm=0.455, loss_scale=8, train_wall=6, gb_free=29.7, wall=6752
2024-01-30 21:59:05 | INFO | train_inner | epoch 078:   1235 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65102.2, ups=16.65, wpb=3909.4, bsz=214.3, num_updates=105100, lr=9.75436e-05, gnorm=0.48, loss_scale=8, train_wall=6, gb_free=29.7, wall=6758
2024-01-30 21:59:11 | INFO | train_inner | epoch 078:   1335 / 1349 loss=1.805, nll_loss=0.207, ppl=1.15, wps=65273.6, ups=16.56, wpb=3942, bsz=218.9, num_updates=105200, lr=9.74972e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.8, wall=6764
2024-01-30 21:59:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 21:59:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:59:13 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 2.138 | nll_loss 0.392 | ppl 1.31 | wps 171817 | wpb 3275.2 | bsz 176.5 | num_updates 105214 | best_loss 2.063
2024-01-30 21:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 105214 updates
2024-01-30 21:59:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint78.pt
2024-01-30 21:59:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint78.pt
2024-01-30 21:59:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint78.pt (epoch 78 @ 105214 updates, score 2.138) (writing took 2.294199067982845 seconds)
2024-01-30 21:59:16 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-01-30 21:59:16 | INFO | train | epoch 078 | loss 1.806 | nll_loss 0.207 | ppl 1.15 | wps 62174.4 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 105214 | lr 9.74907e-05 | gnorm 0.492 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6769
2024-01-30 21:59:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 21:59:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 21:59:16 | INFO | fairseq.trainer | begin training epoch 79
2024-01-30 21:59:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 21:59:21 | INFO | train_inner | epoch 079:     86 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=38514, ups=9.87, wpb=3901, bsz=209.4, num_updates=105300, lr=9.74509e-05, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.8, wall=6774
2024-01-30 21:59:27 | INFO | train_inner | epoch 079:    186 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65868.3, ups=16.78, wpb=3925.6, bsz=209, num_updates=105400, lr=9.74047e-05, gnorm=0.538, loss_scale=8, train_wall=6, gb_free=29.8, wall=6780
2024-01-30 21:59:33 | INFO | train_inner | epoch 079:    286 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65240.6, ups=16.69, wpb=3908.1, bsz=213.3, num_updates=105500, lr=9.73585e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=6786
2024-01-30 21:59:39 | INFO | train_inner | epoch 079:    386 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65738.1, ups=16.64, wpb=3951.6, bsz=215.5, num_updates=105600, lr=9.73124e-05, gnorm=0.467, loss_scale=8, train_wall=6, gb_free=29.7, wall=6792
2024-01-30 21:59:45 | INFO | train_inner | epoch 079:    486 / 1349 loss=1.804, nll_loss=0.204, ppl=1.15, wps=65443.7, ups=16.49, wpb=3969.8, bsz=223.2, num_updates=105700, lr=9.72663e-05, gnorm=0.447, loss_scale=8, train_wall=6, gb_free=29.8, wall=6798
2024-01-30 21:59:51 | INFO | train_inner | epoch 079:    586 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65218.1, ups=16.59, wpb=3930.3, bsz=223, num_updates=105800, lr=9.72203e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.8, wall=6804
2024-01-30 21:59:57 | INFO | train_inner | epoch 079:    686 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65611.5, ups=16.64, wpb=3943.9, bsz=207.7, num_updates=105900, lr=9.71744e-05, gnorm=0.499, loss_scale=8, train_wall=6, gb_free=29.7, wall=6810
2024-01-30 22:00:03 | INFO | train_inner | epoch 079:    786 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=64605, ups=16.63, wpb=3884.1, bsz=215.8, num_updates=106000, lr=9.71286e-05, gnorm=0.511, loss_scale=8, train_wall=6, gb_free=29.8, wall=6816
2024-01-30 22:00:09 | INFO | train_inner | epoch 079:    886 / 1349 loss=1.81, nll_loss=0.211, ppl=1.16, wps=65383.9, ups=16.72, wpb=3910.8, bsz=204.8, num_updates=106100, lr=9.70828e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.9, wall=6822
2024-01-30 22:00:15 | INFO | train_inner | epoch 079:    986 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=64606.2, ups=16.51, wpb=3912.6, bsz=216.2, num_updates=106200, lr=9.70371e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=6828
2024-01-30 22:00:21 | INFO | train_inner | epoch 079:   1086 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=64560.2, ups=16.61, wpb=3886, bsz=211.8, num_updates=106300, lr=9.69914e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=6834
2024-01-30 22:00:27 | INFO | train_inner | epoch 079:   1186 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=65125.6, ups=16.81, wpb=3875.2, bsz=216.2, num_updates=106400, lr=9.69458e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.9, wall=6840
2024-01-30 22:00:33 | INFO | train_inner | epoch 079:   1286 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=66026.6, ups=16.72, wpb=3949.2, bsz=210.8, num_updates=106500, lr=9.69003e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=6846
2024-01-30 22:00:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:00:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:00:38 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 2.133 | nll_loss 0.388 | ppl 1.31 | wps 172422 | wpb 3275.2 | bsz 176.5 | num_updates 106563 | best_loss 2.063
2024-01-30 22:00:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 106563 updates
2024-01-30 22:00:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint79.pt
2024-01-30 22:00:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint79.pt
2024-01-30 22:00:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint79.pt (epoch 79 @ 106563 updates, score 2.133) (writing took 2.459689329029061 seconds)
2024-01-30 22:00:41 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-01-30 22:00:41 | INFO | train | epoch 079 | loss 1.805 | nll_loss 0.206 | ppl 1.15 | wps 62015 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 106563 | lr 9.68717e-05 | gnorm 0.496 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6854
2024-01-30 22:00:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:00:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:00:41 | INFO | fairseq.trainer | begin training epoch 80
2024-01-30 22:00:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:00:43 | INFO | train_inner | epoch 080:     37 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=38436.7, ups=9.72, wpb=3953.6, bsz=229.5, num_updates=106600, lr=9.68549e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.7, wall=6856
2024-01-30 22:00:49 | INFO | train_inner | epoch 080:    137 / 1349 loss=1.805, nll_loss=0.205, ppl=1.15, wps=65271.2, ups=16.78, wpb=3890.9, bsz=212.4, num_updates=106700, lr=9.68095e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.9, wall=6862
2024-01-30 22:00:55 | INFO | train_inner | epoch 080:    237 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65576.6, ups=16.6, wpb=3950, bsz=218.6, num_updates=106800, lr=9.67641e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.7, wall=6868
2024-01-30 22:01:01 | INFO | train_inner | epoch 080:    337 / 1349 loss=1.8, nll_loss=0.2, ppl=1.15, wps=65281.4, ups=16.57, wpb=3938.6, bsz=218.6, num_updates=106900, lr=9.67189e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=6875
2024-01-30 22:01:07 | INFO | train_inner | epoch 080:    437 / 1349 loss=1.799, nll_loss=0.199, ppl=1.15, wps=65844.7, ups=16.56, wpb=3975.1, bsz=217.4, num_updates=107000, lr=9.66736e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.8, wall=6881
2024-01-30 22:01:13 | INFO | train_inner | epoch 080:    537 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=64573.1, ups=16.68, wpb=3870.4, bsz=217.3, num_updates=107100, lr=9.66285e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=6887
2024-01-30 22:01:19 | INFO | train_inner | epoch 080:    637 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65403.9, ups=16.55, wpb=3951.8, bsz=222.7, num_updates=107200, lr=9.65834e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=6893
2024-01-30 22:01:25 | INFO | train_inner | epoch 080:    737 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65153.2, ups=16.79, wpb=3879.7, bsz=205, num_updates=107300, lr=9.65384e-05, gnorm=0.465, loss_scale=8, train_wall=6, gb_free=29.7, wall=6899
2024-01-30 22:01:31 | INFO | train_inner | epoch 080:    837 / 1349 loss=1.808, nll_loss=0.21, ppl=1.16, wps=66033.7, ups=16.68, wpb=3959.6, bsz=211.6, num_updates=107400, lr=9.64935e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.7, wall=6905
2024-01-30 22:01:37 | INFO | train_inner | epoch 080:    937 / 1349 loss=1.813, nll_loss=0.214, ppl=1.16, wps=64873, ups=16.67, wpb=3891.5, bsz=215.4, num_updates=107500, lr=9.64486e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.7, wall=6911
2024-01-30 22:01:43 | INFO | train_inner | epoch 080:   1037 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65692, ups=16.58, wpb=3961.5, bsz=207.7, num_updates=107600, lr=9.64037e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=6917
2024-01-30 22:01:49 | INFO | train_inner | epoch 080:   1137 / 1349 loss=1.808, nll_loss=0.21, ppl=1.16, wps=64553, ups=16.73, wpb=3857.4, bsz=205.7, num_updates=107700, lr=9.6359e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=6923
2024-01-30 22:01:55 | INFO | train_inner | epoch 080:   1237 / 1349 loss=1.806, nll_loss=0.208, ppl=1.15, wps=65066.6, ups=16.52, wpb=3938.1, bsz=222.3, num_updates=107800, lr=9.63143e-05, gnorm=0.541, loss_scale=8, train_wall=6, gb_free=29.8, wall=6929
2024-01-30 22:02:01 | INFO | train_inner | epoch 080:   1337 / 1349 loss=1.804, nll_loss=0.206, ppl=1.15, wps=65372.1, ups=16.71, wpb=3912.9, bsz=211, num_updates=107900, lr=9.62696e-05, gnorm=0.439, loss_scale=8, train_wall=6, gb_free=29.8, wall=6935
2024-01-30 22:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:02:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:02:04 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 2.138 | nll_loss 0.394 | ppl 1.31 | wps 172010 | wpb 3275.2 | bsz 176.5 | num_updates 107912 | best_loss 2.063
2024-01-30 22:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 107912 updates
2024-01-30 22:02:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint80.pt
2024-01-30 22:02:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint80.pt
2024-01-30 22:02:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint80.pt (epoch 80 @ 107912 updates, score 2.138) (writing took 2.2846624340163544 seconds)
2024-01-30 22:02:06 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-01-30 22:02:06 | INFO | train | epoch 080 | loss 1.805 | nll_loss 0.206 | ppl 1.15 | wps 62119.8 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 107912 | lr 9.62643e-05 | gnorm 0.505 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 6939
2024-01-30 22:02:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:02:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:02:06 | INFO | fairseq.trainer | begin training epoch 81
2024-01-30 22:02:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:02:12 | INFO | train_inner | epoch 081:     88 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=38295.9, ups=9.85, wpb=3889.3, bsz=219.5, num_updates=108000, lr=9.6225e-05, gnorm=0.448, loss_scale=8, train_wall=6, gb_free=29.8, wall=6945
2024-01-30 22:02:18 | INFO | train_inner | epoch 081:    188 / 1349 loss=1.799, nll_loss=0.199, ppl=1.15, wps=64858.5, ups=16.52, wpb=3927.2, bsz=220.7, num_updates=108100, lr=9.61805e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=6951
2024-01-30 22:02:24 | INFO | train_inner | epoch 081:    288 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=65392.5, ups=16.74, wpb=3905.9, bsz=202, num_updates=108200, lr=9.61361e-05, gnorm=0.534, loss_scale=8, train_wall=6, gb_free=29.8, wall=6957
2024-01-30 22:02:30 | INFO | train_inner | epoch 081:    388 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65588.4, ups=16.72, wpb=3923.7, bsz=204.9, num_updates=108300, lr=9.60917e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.7, wall=6963
2024-01-30 22:02:36 | INFO | train_inner | epoch 081:    488 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65072.5, ups=16.58, wpb=3924.2, bsz=223.8, num_updates=108400, lr=9.60473e-05, gnorm=0.504, loss_scale=8, train_wall=6, gb_free=29.8, wall=6969
2024-01-30 22:02:42 | INFO | train_inner | epoch 081:    588 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65782.9, ups=16.53, wpb=3978.5, bsz=223.3, num_updates=108500, lr=9.60031e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=6975
2024-01-30 22:02:48 | INFO | train_inner | epoch 081:    688 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=65354, ups=16.65, wpb=3924.8, bsz=206.9, num_updates=108600, lr=9.59589e-05, gnorm=0.604, loss_scale=8, train_wall=6, gb_free=29.8, wall=6981
2024-01-30 22:02:54 | INFO | train_inner | epoch 081:    788 / 1349 loss=1.809, nll_loss=0.21, ppl=1.16, wps=65410, ups=16.82, wpb=3889.4, bsz=204.5, num_updates=108700, lr=9.59147e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.8, wall=6987
2024-01-30 22:03:00 | INFO | train_inner | epoch 081:    888 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65441.8, ups=16.66, wpb=3928.1, bsz=221.8, num_updates=108800, lr=9.58706e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=6993
2024-01-30 22:03:06 | INFO | train_inner | epoch 081:    988 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65973.1, ups=16.65, wpb=3962.4, bsz=213.6, num_updates=108900, lr=9.58266e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=6999
2024-01-30 22:03:12 | INFO | train_inner | epoch 081:   1088 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=64102.9, ups=16.67, wpb=3845.7, bsz=221.3, num_updates=109000, lr=9.57826e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.7, wall=7005
2024-01-30 22:03:18 | INFO | train_inner | epoch 081:   1188 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65016.5, ups=16.59, wpb=3919.3, bsz=210.4, num_updates=109100, lr=9.57387e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=7011
2024-01-30 22:03:24 | INFO | train_inner | epoch 081:   1288 / 1349 loss=1.805, nll_loss=0.207, ppl=1.15, wps=65969.8, ups=16.7, wpb=3950.3, bsz=211.8, num_updates=109200, lr=9.56949e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=7017
2024-01-30 22:03:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:03:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:03:29 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 2.143 | nll_loss 0.399 | ppl 1.32 | wps 170768 | wpb 3275.2 | bsz 176.5 | num_updates 109261 | best_loss 2.063
2024-01-30 22:03:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 109261 updates
2024-01-30 22:03:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint81.pt
2024-01-30 22:03:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint81.pt
2024-01-30 22:03:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint81.pt (epoch 81 @ 109261 updates, score 2.143) (writing took 2.3880440859356895 seconds)
2024-01-30 22:03:31 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-01-30 22:03:31 | INFO | train | epoch 081 | loss 1.804 | nll_loss 0.205 | ppl 1.15 | wps 62031.3 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 109261 | lr 9.56682e-05 | gnorm 0.514 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 7025
2024-01-30 22:03:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:03:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:03:31 | INFO | fairseq.trainer | begin training epoch 82
2024-01-30 22:03:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:03:34 | INFO | train_inner | epoch 082:     39 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=38421.3, ups=9.82, wpb=3912.3, bsz=201.8, num_updates=109300, lr=9.56511e-05, gnorm=0.508, loss_scale=8, train_wall=6, gb_free=29.8, wall=7027
2024-01-30 22:03:40 | INFO | train_inner | epoch 082:    139 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=64990.2, ups=16.58, wpb=3920.2, bsz=216.1, num_updates=109400, lr=9.56074e-05, gnorm=0.516, loss_scale=8, train_wall=6, gb_free=29.8, wall=7033
2024-01-30 22:03:46 | INFO | train_inner | epoch 082:    239 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65058.9, ups=16.6, wpb=3920.1, bsz=230.3, num_updates=109500, lr=9.55637e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=7039
2024-01-30 22:03:52 | INFO | train_inner | epoch 082:    339 / 1349 loss=1.803, nll_loss=0.203, ppl=1.15, wps=65510.2, ups=16.7, wpb=3923.1, bsz=208.8, num_updates=109600, lr=9.55201e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.9, wall=7045
2024-01-30 22:03:58 | INFO | train_inner | epoch 082:    439 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=64345.7, ups=16.85, wpb=3819.4, bsz=203.6, num_updates=109700, lr=9.54765e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=30, wall=7051
2024-01-30 22:04:04 | INFO | train_inner | epoch 082:    539 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=66081.8, ups=16.78, wpb=3937.1, bsz=211.1, num_updates=109800, lr=9.54331e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=7057
2024-01-30 22:04:10 | INFO | train_inner | epoch 082:    639 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65380.1, ups=16.58, wpb=3942.3, bsz=208.6, num_updates=109900, lr=9.53896e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.8, wall=7063
2024-01-30 22:04:16 | INFO | train_inner | epoch 082:    739 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65689, ups=16.6, wpb=3957.9, bsz=218.4, num_updates=110000, lr=9.53463e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=7069
2024-01-30 22:04:22 | INFO | train_inner | epoch 082:    839 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65137.6, ups=16.57, wpb=3930, bsz=216.9, num_updates=110100, lr=9.53029e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=7075
2024-01-30 22:04:28 | INFO | train_inner | epoch 082:    939 / 1349 loss=1.805, nll_loss=0.207, ppl=1.15, wps=64676, ups=16.74, wpb=3862.5, bsz=213.5, num_updates=110200, lr=9.52597e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.7, wall=7081
2024-01-30 22:04:34 | INFO | train_inner | epoch 082:   1039 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65483, ups=16.64, wpb=3934.6, bsz=213, num_updates=110300, lr=9.52165e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=7087
2024-01-30 22:04:40 | INFO | train_inner | epoch 082:   1139 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65688.7, ups=16.52, wpb=3976, bsz=232.1, num_updates=110400, lr=9.51734e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.9, wall=7093
2024-01-30 22:04:46 | INFO | train_inner | epoch 082:   1239 / 1349 loss=1.808, nll_loss=0.209, ppl=1.16, wps=65552.3, ups=16.74, wpb=3916.9, bsz=208.9, num_updates=110500, lr=9.51303e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=7099
2024-01-30 22:04:52 | INFO | train_inner | epoch 082:   1339 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65959.7, ups=16.58, wpb=3979.2, bsz=223.4, num_updates=110600, lr=9.50873e-05, gnorm=0.447, loss_scale=8, train_wall=6, gb_free=29.7, wall=7105
2024-01-30 22:04:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:04:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:04:54 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 2.137 | nll_loss 0.393 | ppl 1.31 | wps 172389 | wpb 3275.2 | bsz 176.5 | num_updates 110610 | best_loss 2.063
2024-01-30 22:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 110610 updates
2024-01-30 22:04:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint82.pt
2024-01-30 22:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint82.pt
2024-01-30 22:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint82.pt (epoch 82 @ 110610 updates, score 2.137) (writing took 2.2906633419916034 seconds)
2024-01-30 22:04:56 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-01-30 22:04:56 | INFO | train | epoch 082 | loss 1.803 | nll_loss 0.205 | ppl 1.15 | wps 62169.6 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 110610 | lr 9.5083e-05 | gnorm 0.495 | loss_scale 8 | train_wall 78 | gb_free 29.9 | wall 7110
2024-01-30 22:04:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:04:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:04:56 | INFO | fairseq.trainer | begin training epoch 83
2024-01-30 22:04:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:05:02 | INFO | train_inner | epoch 083:     90 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=37963.3, ups=9.9, wpb=3834.8, bsz=206.1, num_updates=110700, lr=9.50443e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=7115
2024-01-30 22:05:08 | INFO | train_inner | epoch 083:    190 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=65022.3, ups=16.57, wpb=3924.1, bsz=223.2, num_updates=110800, lr=9.50014e-05, gnorm=0.458, loss_scale=8, train_wall=6, gb_free=29.7, wall=7121
2024-01-30 22:05:14 | INFO | train_inner | epoch 083:    290 / 1349 loss=1.807, nll_loss=0.208, ppl=1.16, wps=65302.4, ups=16.73, wpb=3903.9, bsz=209, num_updates=110900, lr=9.49586e-05, gnorm=0.565, loss_scale=8, train_wall=6, gb_free=29.9, wall=7127
2024-01-30 22:05:20 | INFO | train_inner | epoch 083:    390 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65077.7, ups=16.61, wpb=3916.9, bsz=219.2, num_updates=111000, lr=9.49158e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.7, wall=7133
2024-01-30 22:05:26 | INFO | train_inner | epoch 083:    490 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=66019.7, ups=16.8, wpb=3929.5, bsz=203, num_updates=111100, lr=9.48731e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=30, wall=7139
2024-01-30 22:05:32 | INFO | train_inner | epoch 083:    590 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65167.6, ups=16.71, wpb=3899.9, bsz=210, num_updates=111200, lr=9.48304e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.7, wall=7145
2024-01-30 22:05:38 | INFO | train_inner | epoch 083:    690 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65430.4, ups=16.63, wpb=3934.8, bsz=216.1, num_updates=111300, lr=9.47878e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=7151
2024-01-30 22:05:44 | INFO | train_inner | epoch 083:    790 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=65444.6, ups=16.64, wpb=3932.8, bsz=211.7, num_updates=111400, lr=9.47452e-05, gnorm=0.567, loss_scale=8, train_wall=6, gb_free=29.7, wall=7157
2024-01-30 22:05:50 | INFO | train_inner | epoch 083:    890 / 1349 loss=1.804, nll_loss=0.206, ppl=1.15, wps=64690.5, ups=16.73, wpb=3867, bsz=213, num_updates=111500, lr=9.47027e-05, gnorm=0.568, loss_scale=8, train_wall=6, gb_free=29.7, wall=7163
2024-01-30 22:05:56 | INFO | train_inner | epoch 083:    990 / 1349 loss=1.805, nll_loss=0.207, ppl=1.15, wps=65773.8, ups=16.69, wpb=3942.1, bsz=216.4, num_updates=111600, lr=9.46603e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.8, wall=7169
2024-01-30 22:06:02 | INFO | train_inner | epoch 083:   1090 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65729.5, ups=16.57, wpb=3967.9, bsz=221.8, num_updates=111700, lr=9.46179e-05, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.8, wall=7175
2024-01-30 22:06:08 | INFO | train_inner | epoch 083:   1190 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65161.2, ups=16.54, wpb=3938.7, bsz=221.9, num_updates=111800, lr=9.45756e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.7, wall=7181
2024-01-30 22:06:14 | INFO | train_inner | epoch 083:   1290 / 1349 loss=1.803, nll_loss=0.205, ppl=1.15, wps=64840.5, ups=16.63, wpb=3898.3, bsz=211.5, num_updates=111900, lr=9.45333e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.8, wall=7187
2024-01-30 22:06:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:06:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:06:19 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 2.139 | nll_loss 0.392 | ppl 1.31 | wps 171822 | wpb 3275.2 | bsz 176.5 | num_updates 111959 | best_loss 2.063
2024-01-30 22:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 111959 updates
2024-01-30 22:06:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint83.pt
2024-01-30 22:06:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint83.pt
2024-01-30 22:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint83.pt (epoch 83 @ 111959 updates, score 2.139) (writing took 2.2903607599437237 seconds)
2024-01-30 22:06:22 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-01-30 22:06:22 | INFO | train | epoch 083 | loss 1.803 | nll_loss 0.205 | ppl 1.15 | wps 62131 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 111959 | lr 9.45084e-05 | gnorm 0.516 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 7195
2024-01-30 22:06:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:06:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:06:22 | INFO | fairseq.trainer | begin training epoch 84
2024-01-30 22:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:06:24 | INFO | train_inner | epoch 084:     41 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=39060.8, ups=9.85, wpb=3965.3, bsz=224.3, num_updates=112000, lr=9.44911e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.8, wall=7197
2024-01-30 22:06:30 | INFO | train_inner | epoch 084:    141 / 1349 loss=1.801, nll_loss=0.201, ppl=1.15, wps=64653.6, ups=16.79, wpb=3851.1, bsz=199.4, num_updates=112100, lr=9.4449e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.7, wall=7203
2024-01-30 22:06:36 | INFO | train_inner | epoch 084:    241 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=66315.5, ups=16.72, wpb=3966.6, bsz=211.4, num_updates=112200, lr=9.44069e-05, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=7209
2024-01-30 22:06:42 | INFO | train_inner | epoch 084:    341 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=64203.3, ups=16.67, wpb=3852.2, bsz=217.4, num_updates=112300, lr=9.43648e-05, gnorm=0.53, loss_scale=16, train_wall=6, gb_free=29.8, wall=7215
2024-01-30 22:06:48 | INFO | train_inner | epoch 084:    441 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=64655.6, ups=16.63, wpb=3887.5, bsz=220.1, num_updates=112400, lr=9.43228e-05, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=7221
2024-01-30 22:06:54 | INFO | train_inner | epoch 084:    541 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=65856.3, ups=16.7, wpb=3943.1, bsz=215.9, num_updates=112500, lr=9.42809e-05, gnorm=0.534, loss_scale=16, train_wall=6, gb_free=30, wall=7227
2024-01-30 22:07:00 | INFO | train_inner | epoch 084:    641 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65478.9, ups=16.64, wpb=3935.1, bsz=209.8, num_updates=112600, lr=9.4239e-05, gnorm=0.533, loss_scale=16, train_wall=6, gb_free=29.8, wall=7233
2024-01-30 22:07:06 | INFO | train_inner | epoch 084:    741 / 1349 loss=1.804, nll_loss=0.206, ppl=1.15, wps=65344.6, ups=16.64, wpb=3927.3, bsz=218.3, num_updates=112700, lr=9.41972e-05, gnorm=0.55, loss_scale=16, train_wall=6, gb_free=29.7, wall=7239
2024-01-30 22:07:12 | INFO | train_inner | epoch 084:    841 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=64764.1, ups=16.46, wpb=3934.4, bsz=224.5, num_updates=112800, lr=9.41554e-05, gnorm=0.474, loss_scale=16, train_wall=6, gb_free=29.9, wall=7245
2024-01-30 22:07:18 | INFO | train_inner | epoch 084:    941 / 1349 loss=1.803, nll_loss=0.205, ppl=1.15, wps=65784.8, ups=16.59, wpb=3965.2, bsz=222.2, num_updates=112900, lr=9.41137e-05, gnorm=0.53, loss_scale=16, train_wall=6, gb_free=29.8, wall=7251
2024-01-30 22:07:24 | INFO | train_inner | epoch 084:   1041 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=64567.5, ups=16.71, wpb=3863.3, bsz=204.2, num_updates=113000, lr=9.40721e-05, gnorm=0.581, loss_scale=16, train_wall=6, gb_free=29.8, wall=7257
2024-01-30 22:07:30 | INFO | train_inner | epoch 084:   1141 / 1349 loss=1.804, nll_loss=0.206, ppl=1.15, wps=66236.3, ups=16.63, wpb=3982.4, bsz=207, num_updates=113100, lr=9.40305e-05, gnorm=0.485, loss_scale=16, train_wall=6, gb_free=29.7, wall=7263
2024-01-30 22:07:36 | INFO | train_inner | epoch 084:   1241 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65360.9, ups=16.68, wpb=3917.8, bsz=221.9, num_updates=113200, lr=9.39889e-05, gnorm=0.442, loss_scale=16, train_wall=6, gb_free=29.8, wall=7269
2024-01-30 22:07:42 | INFO | train_inner | epoch 084:   1341 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65768.9, ups=16.68, wpb=3942.1, bsz=209.8, num_updates=113300, lr=9.39475e-05, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=7275
2024-01-30 22:07:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:07:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:07:44 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 2.142 | nll_loss 0.397 | ppl 1.32 | wps 171843 | wpb 3275.2 | bsz 176.5 | num_updates 113308 | best_loss 2.063
2024-01-30 22:07:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 113308 updates
2024-01-30 22:07:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint84.pt
2024-01-30 22:07:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint84.pt
2024-01-30 22:07:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint84.pt (epoch 84 @ 113308 updates, score 2.142) (writing took 2.3074724769685417 seconds)
2024-01-30 22:07:47 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-01-30 22:07:47 | INFO | train | epoch 084 | loss 1.802 | nll_loss 0.203 | ppl 1.15 | wps 62111.6 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 113308 | lr 9.39441e-05 | gnorm 0.513 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 7280
2024-01-30 22:07:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:07:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:07:47 | INFO | fairseq.trainer | begin training epoch 85
2024-01-30 22:07:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:07:52 | INFO | train_inner | epoch 085:     92 / 1349 loss=1.8, nll_loss=0.2, ppl=1.15, wps=38374.5, ups=9.83, wpb=3905.4, bsz=202.2, num_updates=113400, lr=9.3906e-05, gnorm=0.458, loss_scale=16, train_wall=6, gb_free=29.8, wall=7286
2024-01-30 22:07:58 | INFO | train_inner | epoch 085:    192 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=65227.5, ups=16.56, wpb=3939, bsz=234.8, num_updates=113500, lr=9.38647e-05, gnorm=0.469, loss_scale=16, train_wall=6, gb_free=29.8, wall=7292
2024-01-30 22:08:04 | INFO | train_inner | epoch 085:    292 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65981, ups=16.7, wpb=3950.5, bsz=207.1, num_updates=113600, lr=9.38233e-05, gnorm=0.468, loss_scale=16, train_wall=6, gb_free=29.8, wall=7298
2024-01-30 22:08:10 | INFO | train_inner | epoch 085:    392 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=64921.9, ups=16.57, wpb=3918.8, bsz=231, num_updates=113700, lr=9.37821e-05, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=7304
2024-01-30 22:08:16 | INFO | train_inner | epoch 085:    492 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=64691.5, ups=16.66, wpb=3882.6, bsz=211.8, num_updates=113800, lr=9.37408e-05, gnorm=0.464, loss_scale=16, train_wall=6, gb_free=29.8, wall=7310
2024-01-30 22:08:22 | INFO | train_inner | epoch 085:    592 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=66014.1, ups=16.64, wpb=3967.3, bsz=203.3, num_updates=113900, lr=9.36997e-05, gnorm=0.465, loss_scale=16, train_wall=6, gb_free=29.8, wall=7316
2024-01-30 22:08:28 | INFO | train_inner | epoch 085:    692 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=66350.7, ups=16.67, wpb=3980, bsz=218.1, num_updates=114000, lr=9.36586e-05, gnorm=0.473, loss_scale=16, train_wall=6, gb_free=29.7, wall=7322
2024-01-30 22:08:34 | INFO | train_inner | epoch 085:    792 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=64740, ups=16.64, wpb=3889.5, bsz=216.8, num_updates=114100, lr=9.36175e-05, gnorm=0.518, loss_scale=16, train_wall=6, gb_free=29.8, wall=7328
2024-01-30 22:08:41 | INFO | train_inner | epoch 085:    892 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=64122.8, ups=16.53, wpb=3878.7, bsz=223.9, num_updates=114200, lr=9.35765e-05, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=7334
2024-01-30 22:08:46 | INFO | train_inner | epoch 085:    992 / 1349 loss=1.806, nll_loss=0.207, ppl=1.15, wps=64865.2, ups=16.77, wpb=3868.9, bsz=211.6, num_updates=114300, lr=9.35356e-05, gnorm=0.506, loss_scale=16, train_wall=6, gb_free=29.8, wall=7340
2024-01-30 22:08:52 | INFO | train_inner | epoch 085:   1092 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65808.6, ups=16.73, wpb=3933.2, bsz=207.8, num_updates=114400, lr=9.34947e-05, gnorm=0.491, loss_scale=16, train_wall=6, gb_free=29.8, wall=7346
2024-01-30 22:08:58 | INFO | train_inner | epoch 085:   1192 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65120.8, ups=16.68, wpb=3905.1, bsz=210.5, num_updates=114500, lr=9.34539e-05, gnorm=0.465, loss_scale=16, train_wall=6, gb_free=29.7, wall=7352
2024-01-30 22:09:04 | INFO | train_inner | epoch 085:   1292 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65499.4, ups=16.59, wpb=3947.2, bsz=213.7, num_updates=114600, lr=9.34131e-05, gnorm=0.515, loss_scale=16, train_wall=6, gb_free=29.8, wall=7358
2024-01-30 22:09:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:09:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:09:10 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 2.141 | nll_loss 0.397 | ppl 1.32 | wps 169827 | wpb 3275.2 | bsz 176.5 | num_updates 114657 | best_loss 2.063
2024-01-30 22:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 114657 updates
2024-01-30 22:09:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint85.pt
2024-01-30 22:09:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint85.pt
2024-01-30 22:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint85.pt (epoch 85 @ 114657 updates, score 2.141) (writing took 2.2544992360053584 seconds)
2024-01-30 22:09:12 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2024-01-30 22:09:12 | INFO | train | epoch 085 | loss 1.801 | nll_loss 0.203 | ppl 1.15 | wps 62127.4 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 114657 | lr 9.33899e-05 | gnorm 0.492 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 7365
2024-01-30 22:09:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:09:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:09:12 | INFO | fairseq.trainer | begin training epoch 86
2024-01-30 22:09:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:09:15 | INFO | train_inner | epoch 086:     43 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=38829.9, ups=9.9, wpb=3921.5, bsz=209, num_updates=114700, lr=9.33724e-05, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.8, wall=7368
2024-01-30 22:09:21 | INFO | train_inner | epoch 086:    143 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=65213.2, ups=16.67, wpb=3911.9, bsz=211.8, num_updates=114800, lr=9.33317e-05, gnorm=0.465, loss_scale=16, train_wall=6, gb_free=29.8, wall=7374
2024-01-30 22:09:27 | INFO | train_inner | epoch 086:    243 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=66138, ups=16.6, wpb=3983.8, bsz=214.2, num_updates=114900, lr=9.32911e-05, gnorm=0.422, loss_scale=16, train_wall=6, gb_free=29.8, wall=7380
2024-01-30 22:09:33 | INFO | train_inner | epoch 086:    343 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65651.2, ups=16.7, wpb=3930.8, bsz=212.6, num_updates=115000, lr=9.32505e-05, gnorm=0.543, loss_scale=16, train_wall=6, gb_free=29.8, wall=7386
2024-01-30 22:09:39 | INFO | train_inner | epoch 086:    443 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65490.2, ups=16.69, wpb=3924.3, bsz=215.2, num_updates=115100, lr=9.321e-05, gnorm=0.523, loss_scale=16, train_wall=6, gb_free=29.8, wall=7392
2024-01-30 22:09:45 | INFO | train_inner | epoch 086:    543 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65017.4, ups=16.56, wpb=3925, bsz=215.3, num_updates=115200, lr=9.31695e-05, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=7398
2024-01-30 22:09:51 | INFO | train_inner | epoch 086:    643 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=64796.5, ups=16.62, wpb=3897.6, bsz=205.1, num_updates=115300, lr=9.31291e-05, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.7, wall=7404
2024-01-30 22:09:57 | INFO | train_inner | epoch 086:    743 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=65213.4, ups=16.67, wpb=3913.2, bsz=217.2, num_updates=115400, lr=9.30887e-05, gnorm=0.481, loss_scale=16, train_wall=6, gb_free=29.9, wall=7410
2024-01-30 22:10:03 | INFO | train_inner | epoch 086:    843 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=66118.8, ups=16.62, wpb=3979.1, bsz=222.1, num_updates=115500, lr=9.30484e-05, gnorm=0.44, loss_scale=16, train_wall=6, gb_free=29.9, wall=7416
2024-01-30 22:10:09 | INFO | train_inner | epoch 086:    943 / 1349 loss=1.804, nll_loss=0.206, ppl=1.15, wps=65254.3, ups=16.82, wpb=3878.4, bsz=201.8, num_updates=115600, lr=9.30082e-05, gnorm=0.528, loss_scale=16, train_wall=6, gb_free=29.8, wall=7422
2024-01-30 22:10:15 | INFO | train_inner | epoch 086:   1043 / 1349 loss=1.805, nll_loss=0.206, ppl=1.15, wps=65046.2, ups=16.67, wpb=3901.8, bsz=209.8, num_updates=115700, lr=9.2968e-05, gnorm=0.529, loss_scale=16, train_wall=6, gb_free=29.8, wall=7428
2024-01-30 22:10:21 | INFO | train_inner | epoch 086:   1143 / 1349 loss=1.804, nll_loss=0.206, ppl=1.15, wps=64941.5, ups=16.52, wpb=3930.8, bsz=225.4, num_updates=115800, lr=9.29278e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.8, wall=7434
2024-01-30 22:10:27 | INFO | train_inner | epoch 086:   1243 / 1349 loss=1.803, nll_loss=0.205, ppl=1.15, wps=64336.8, ups=16.6, wpb=3876.3, bsz=222.6, num_updates=115900, lr=9.28877e-05, gnorm=0.566, loss_scale=16, train_wall=6, gb_free=29.8, wall=7440
2024-01-30 22:10:33 | INFO | train_inner | epoch 086:   1343 / 1349 loss=1.803, nll_loss=0.205, ppl=1.15, wps=65747.3, ups=16.74, wpb=3928.7, bsz=215.7, num_updates=116000, lr=9.28477e-05, gnorm=0.509, loss_scale=16, train_wall=6, gb_free=29.8, wall=7446
2024-01-30 22:10:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:10:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:10:35 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 2.142 | nll_loss 0.4 | ppl 1.32 | wps 171930 | wpb 3275.2 | bsz 176.5 | num_updates 116006 | best_loss 2.063
2024-01-30 22:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 116006 updates
2024-01-30 22:10:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint86.pt
2024-01-30 22:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint86.pt
2024-01-30 22:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint86.pt (epoch 86 @ 116006 updates, score 2.142) (writing took 2.309018295025453 seconds)
2024-01-30 22:10:37 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2024-01-30 22:10:37 | INFO | train | epoch 086 | loss 1.801 | nll_loss 0.203 | ppl 1.15 | wps 62096.1 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 116006 | lr 9.28453e-05 | gnorm 0.499 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 7450
2024-01-30 22:10:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:10:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:10:37 | INFO | fairseq.trainer | begin training epoch 87
2024-01-30 22:10:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:10:43 | INFO | train_inner | epoch 087:     94 / 1349 loss=1.798, nll_loss=0.198, ppl=1.15, wps=38366.8, ups=9.79, wpb=3919.7, bsz=225.8, num_updates=116100, lr=9.28077e-05, gnorm=0.445, loss_scale=16, train_wall=6, gb_free=29.8, wall=7456
2024-01-30 22:10:49 | INFO | train_inner | epoch 087:    194 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=64551.8, ups=16.54, wpb=3901.9, bsz=221.4, num_updates=116200, lr=9.27677e-05, gnorm=0.48, loss_scale=16, train_wall=6, gb_free=29.8, wall=7462
2024-01-30 22:10:55 | INFO | train_inner | epoch 087:    294 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65018.8, ups=16.66, wpb=3901.7, bsz=209, num_updates=116300, lr=9.27278e-05, gnorm=0.451, loss_scale=16, train_wall=6, gb_free=29.8, wall=7468
2024-01-30 22:11:01 | INFO | train_inner | epoch 087:    394 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65351.2, ups=16.81, wpb=3888.3, bsz=194.6, num_updates=116400, lr=9.2688e-05, gnorm=0.577, loss_scale=16, train_wall=6, gb_free=29.8, wall=7474
2024-01-30 22:11:07 | INFO | train_inner | epoch 087:    494 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=65468.2, ups=16.67, wpb=3926.3, bsz=214.2, num_updates=116500, lr=9.26482e-05, gnorm=0.442, loss_scale=16, train_wall=6, gb_free=29.7, wall=7480
2024-01-30 22:11:13 | INFO | train_inner | epoch 087:    594 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65667.1, ups=16.75, wpb=3919.7, bsz=208.5, num_updates=116600, lr=9.26085e-05, gnorm=0.575, loss_scale=16, train_wall=6, gb_free=29.9, wall=7486
2024-01-30 22:11:19 | INFO | train_inner | epoch 087:    694 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65513.3, ups=16.76, wpb=3908, bsz=209.6, num_updates=116700, lr=9.25688e-05, gnorm=0.451, loss_scale=16, train_wall=6, gb_free=29.8, wall=7492
2024-01-30 22:11:25 | INFO | train_inner | epoch 087:    794 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65071, ups=16.68, wpb=3901.5, bsz=211.4, num_updates=116800, lr=9.25292e-05, gnorm=0.461, loss_scale=16, train_wall=6, gb_free=29.7, wall=7498
2024-01-30 22:11:31 | INFO | train_inner | epoch 087:    894 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=65344.8, ups=16.61, wpb=3933.7, bsz=218, num_updates=116900, lr=9.24896e-05, gnorm=0.51, loss_scale=16, train_wall=6, gb_free=29.9, wall=7504
2024-01-30 22:11:37 | INFO | train_inner | epoch 087:    994 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65075, ups=16.63, wpb=3913.5, bsz=219.4, num_updates=117000, lr=9.245e-05, gnorm=0.536, loss_scale=16, train_wall=6, gb_free=29.8, wall=7510
2024-01-30 22:11:43 | INFO | train_inner | epoch 087:   1094 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=66035.7, ups=16.61, wpb=3974.6, bsz=222.4, num_updates=117100, lr=9.24105e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.7, wall=7516
2024-01-30 22:11:49 | INFO | train_inner | epoch 087:   1194 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65724.4, ups=16.57, wpb=3966.7, bsz=214.6, num_updates=117200, lr=9.23711e-05, gnorm=0.482, loss_scale=16, train_wall=6, gb_free=29.8, wall=7522
2024-01-30 22:11:55 | INFO | train_inner | epoch 087:   1294 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65261.8, ups=16.57, wpb=3937.9, bsz=222, num_updates=117300, lr=9.23317e-05, gnorm=0.473, loss_scale=16, train_wall=6, gb_free=29.7, wall=7528
2024-01-30 22:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:11:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:12:00 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 2.146 | nll_loss 0.402 | ppl 1.32 | wps 170814 | wpb 3275.2 | bsz 176.5 | num_updates 117355 | best_loss 2.063
2024-01-30 22:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 117355 updates
2024-01-30 22:12:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint87.pt
2024-01-30 22:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint87.pt
2024-01-30 22:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint87.pt (epoch 87 @ 117355 updates, score 2.146) (writing took 2.2686084730084985 seconds)
2024-01-30 22:12:02 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2024-01-30 22:12:02 | INFO | train | epoch 087 | loss 1.8 | nll_loss 0.202 | ppl 1.15 | wps 62128.8 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 117355 | lr 9.23101e-05 | gnorm 0.491 | loss_scale 16 | train_wall 78 | gb_free 29.8 | wall 7535
2024-01-30 22:12:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:12:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:12:02 | INFO | fairseq.trainer | begin training epoch 88
2024-01-30 22:12:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:12:05 | INFO | train_inner | epoch 088:     45 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=38789.8, ups=9.89, wpb=3922.4, bsz=207, num_updates=117400, lr=9.22924e-05, gnorm=0.517, loss_scale=16, train_wall=6, gb_free=29.8, wall=7538
2024-01-30 22:12:11 | INFO | train_inner | epoch 088:    145 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=64348.1, ups=16.63, wpb=3869.6, bsz=224.6, num_updates=117500, lr=9.22531e-05, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=7544
2024-01-30 22:12:17 | INFO | train_inner | epoch 088:    245 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=65532.8, ups=16.68, wpb=3929.3, bsz=205.5, num_updates=117600, lr=9.22139e-05, gnorm=0.504, loss_scale=16, train_wall=6, gb_free=29.8, wall=7550
2024-01-30 22:12:23 | INFO | train_inner | epoch 088:    345 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=64918.4, ups=16.57, wpb=3918.8, bsz=216.7, num_updates=117700, lr=9.21747e-05, gnorm=0.44, loss_scale=16, train_wall=6, gb_free=29.9, wall=7556
2024-01-30 22:12:29 | INFO | train_inner | epoch 088:    445 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65456, ups=16.57, wpb=3949.1, bsz=236.5, num_updates=117800, lr=9.21356e-05, gnorm=0.494, loss_scale=16, train_wall=6, gb_free=29.8, wall=7562
2024-01-30 22:12:35 | INFO | train_inner | epoch 088:    545 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=65601.2, ups=16.52, wpb=3971, bsz=218.6, num_updates=117900, lr=9.20965e-05, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.8, wall=7568
2024-01-30 22:12:41 | INFO | train_inner | epoch 088:    645 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65181.4, ups=16.73, wpb=3895.7, bsz=209.6, num_updates=118000, lr=9.20575e-05, gnorm=0.496, loss_scale=16, train_wall=6, gb_free=29.7, wall=7574
2024-01-30 22:12:47 | INFO | train_inner | epoch 088:    745 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=64896.5, ups=16.78, wpb=3868.2, bsz=207.1, num_updates=118100, lr=9.20185e-05, gnorm=0.516, loss_scale=16, train_wall=6, gb_free=29.8, wall=7580
2024-01-30 22:12:53 | INFO | train_inner | epoch 088:    845 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65971.5, ups=16.68, wpb=3954, bsz=205, num_updates=118200, lr=9.19795e-05, gnorm=0.507, loss_scale=16, train_wall=6, gb_free=29.8, wall=7586
2024-01-30 22:12:59 | INFO | train_inner | epoch 088:    945 / 1349 loss=1.806, nll_loss=0.208, ppl=1.16, wps=64969.4, ups=16.73, wpb=3884, bsz=206.5, num_updates=118300, lr=9.19407e-05, gnorm=0.52, loss_scale=16, train_wall=6, gb_free=29.8, wall=7592
2024-01-30 22:13:05 | INFO | train_inner | epoch 088:   1045 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=65006.4, ups=16.7, wpb=3893, bsz=217.1, num_updates=118400, lr=9.19018e-05, gnorm=0.566, loss_scale=16, train_wall=6, gb_free=30, wall=7598
2024-01-30 22:13:11 | INFO | train_inner | epoch 088:   1145 / 1349 loss=1.802, nll_loss=0.203, ppl=1.15, wps=65441.9, ups=16.63, wpb=3934.4, bsz=211.8, num_updates=118500, lr=9.1863e-05, gnorm=0.562, loss_scale=16, train_wall=6, gb_free=29.7, wall=7604
2024-01-30 22:13:17 | INFO | train_inner | epoch 088:   1245 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65570.7, ups=16.65, wpb=3937.5, bsz=209.4, num_updates=118600, lr=9.18243e-05, gnorm=0.493, loss_scale=16, train_wall=6, gb_free=29.8, wall=7610
2024-01-30 22:13:23 | INFO | train_inner | epoch 088:   1345 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=66398.4, ups=16.77, wpb=3959.5, bsz=224.6, num_updates=118700, lr=9.17856e-05, gnorm=0.483, loss_scale=16, train_wall=6, gb_free=29.8, wall=7616
2024-01-30 22:13:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:13:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:13:25 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 2.144 | nll_loss 0.4 | ppl 1.32 | wps 171697 | wpb 3275.2 | bsz 176.5 | num_updates 118704 | best_loss 2.063
2024-01-30 22:13:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 118704 updates
2024-01-30 22:13:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint88.pt
2024-01-30 22:13:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint88.pt
2024-01-30 22:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint88.pt (epoch 88 @ 118704 updates, score 2.144) (writing took 2.3671078260522336 seconds)
2024-01-30 22:13:27 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2024-01-30 22:13:27 | INFO | train | epoch 088 | loss 1.8 | nll_loss 0.202 | ppl 1.15 | wps 62081.6 | ups 15.83 | wpb 3921.2 | bsz 214.5 | num_updates 118704 | lr 9.17841e-05 | gnorm 0.505 | loss_scale 16 | train_wall 78 | gb_free 30 | wall 7621
2024-01-30 22:13:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:13:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:13:27 | INFO | fairseq.trainer | begin training epoch 89
2024-01-30 22:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:13:33 | INFO | train_inner | epoch 089:     96 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=38081.1, ups=9.76, wpb=3900.7, bsz=215.8, num_updates=118800, lr=9.1747e-05, gnorm=0.522, loss_scale=16, train_wall=6, gb_free=29.8, wall=7626
2024-01-30 22:13:39 | INFO | train_inner | epoch 089:    196 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65628.2, ups=16.67, wpb=3935.8, bsz=213.1, num_updates=118900, lr=9.17084e-05, gnorm=0.484, loss_scale=16, train_wall=6, gb_free=29.7, wall=7632
2024-01-30 22:13:45 | INFO | train_inner | epoch 089:    296 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65554.5, ups=16.53, wpb=3964.8, bsz=216.6, num_updates=119000, lr=9.16698e-05, gnorm=0.443, loss_scale=16, train_wall=6, gb_free=29.8, wall=7639
2024-01-30 22:13:51 | INFO | train_inner | epoch 089:    396 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=64638.4, ups=16.6, wpb=3892.8, bsz=216.6, num_updates=119100, lr=9.16314e-05, gnorm=0.475, loss_scale=16, train_wall=6, gb_free=29.8, wall=7645
2024-01-30 22:13:57 | INFO | train_inner | epoch 089:    496 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=65803.6, ups=16.74, wpb=3930.9, bsz=198.3, num_updates=119200, lr=9.15929e-05, gnorm=0.532, loss_scale=16, train_wall=6, gb_free=29.7, wall=7651
2024-01-30 22:14:03 | INFO | train_inner | epoch 089:    596 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=63638.3, ups=16.67, wpb=3816.4, bsz=222.8, num_updates=119300, lr=9.15545e-05, gnorm=0.583, loss_scale=16, train_wall=6, gb_free=29.7, wall=7657
2024-01-30 22:14:09 | INFO | train_inner | epoch 089:    696 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65482.4, ups=16.65, wpb=3933.2, bsz=209.8, num_updates=119400, lr=9.15162e-05, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.8, wall=7663
2024-01-30 22:14:15 | INFO | train_inner | epoch 089:    796 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=65944, ups=16.64, wpb=3962.6, bsz=204.4, num_updates=119500, lr=9.14779e-05, gnorm=0.541, loss_scale=16, train_wall=6, gb_free=29.7, wall=7669
2024-01-30 22:14:21 | INFO | train_inner | epoch 089:    896 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65828.9, ups=16.61, wpb=3963.4, bsz=218.4, num_updates=119600, lr=9.14396e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.8, wall=7675
2024-01-30 22:14:27 | INFO | train_inner | epoch 089:    996 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65319.3, ups=16.58, wpb=3939.3, bsz=229, num_updates=119700, lr=9.14014e-05, gnorm=0.467, loss_scale=16, train_wall=6, gb_free=29.8, wall=7681
2024-01-30 22:14:33 | INFO | train_inner | epoch 089:   1096 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=65734.9, ups=16.79, wpb=3915.8, bsz=199.2, num_updates=119800, lr=9.13633e-05, gnorm=0.561, loss_scale=16, train_wall=6, gb_free=29.7, wall=7687
2024-01-30 22:14:39 | INFO | train_inner | epoch 089:   1196 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65105, ups=16.56, wpb=3931.1, bsz=215.4, num_updates=119900, lr=9.13252e-05, gnorm=0.466, loss_scale=16, train_wall=6, gb_free=29.9, wall=7693
2024-01-30 22:14:53 | INFO | train_inner | epoch 089:   1296 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=28828, ups=7.37, wpb=3912.4, bsz=222.5, num_updates=120000, lr=9.12871e-05, gnorm=0.555, loss_scale=16, train_wall=6, gb_free=29.8, wall=7706
2024-01-30 22:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:15:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:15:02 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 2.145 | nll_loss 0.402 | ppl 1.32 | wps 172089 | wpb 3275.2 | bsz 176.5 | num_updates 120053 | best_loss 2.063
2024-01-30 22:15:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 120053 updates
2024-01-30 22:15:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint89.pt
2024-01-30 22:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint89.pt
2024-01-30 22:15:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint89.pt (epoch 89 @ 120053 updates, score 2.145) (writing took 2.554752934956923 seconds)
2024-01-30 22:15:05 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2024-01-30 22:15:05 | INFO | train | epoch 089 | loss 1.8 | nll_loss 0.201 | ppl 1.15 | wps 54199.8 | ups 13.82 | wpb 3921.2 | bsz 214.5 | num_updates 120053 | lr 9.12669e-05 | gnorm 0.509 | loss_scale 16 | train_wall 82 | gb_free 29.8 | wall 7718
2024-01-30 22:15:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:15:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:15:05 | INFO | fairseq.trainer | begin training epoch 90
2024-01-30 22:15:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:15:08 | INFO | train_inner | epoch 090:     47 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=26049.6, ups=6.68, wpb=3901.4, bsz=218.5, num_updates=120100, lr=9.12491e-05, gnorm=0.488, loss_scale=16, train_wall=10, gb_free=29.8, wall=7721
2024-01-30 22:15:14 | INFO | train_inner | epoch 090:    147 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=64277.9, ups=16.55, wpb=3884.3, bsz=217.9, num_updates=120200, lr=9.12111e-05, gnorm=0.5, loss_scale=16, train_wall=6, gb_free=29.9, wall=7727
2024-01-30 22:15:20 | INFO | train_inner | epoch 090:    247 / 1349 loss=1.803, nll_loss=0.204, ppl=1.15, wps=65091.6, ups=16.79, wpb=3877.9, bsz=199.3, num_updates=120300, lr=9.11732e-05, gnorm=0.502, loss_scale=16, train_wall=6, gb_free=29.8, wall=7733
2024-01-30 22:15:26 | INFO | train_inner | epoch 090:    347 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65681.5, ups=16.65, wpb=3944.3, bsz=207.6, num_updates=120400, lr=9.11353e-05, gnorm=0.449, loss_scale=16, train_wall=6, gb_free=29.7, wall=7739
2024-01-30 22:15:32 | INFO | train_inner | epoch 090:    447 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65594, ups=16.64, wpb=3941.1, bsz=208.2, num_updates=120500, lr=9.10975e-05, gnorm=0.461, loss_scale=16, train_wall=6, gb_free=29.7, wall=7745
2024-01-30 22:15:38 | INFO | train_inner | epoch 090:    547 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=65319.9, ups=16.56, wpb=3943.7, bsz=222.7, num_updates=120600, lr=9.10597e-05, gnorm=0.499, loss_scale=16, train_wall=6, gb_free=29.8, wall=7751
2024-01-30 22:15:44 | INFO | train_inner | epoch 090:    647 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=64714.3, ups=16.64, wpb=3888.4, bsz=216.6, num_updates=120700, lr=9.1022e-05, gnorm=0.497, loss_scale=16, train_wall=6, gb_free=29.8, wall=7757
2024-01-30 22:15:50 | INFO | train_inner | epoch 090:    747 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65277.3, ups=16.62, wpb=3928.6, bsz=212.6, num_updates=120800, lr=9.09843e-05, gnorm=0.583, loss_scale=16, train_wall=6, gb_free=29.7, wall=7763
2024-01-30 22:15:56 | INFO | train_inner | epoch 090:    847 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65170.3, ups=16.56, wpb=3935.2, bsz=226.6, num_updates=120900, lr=9.09467e-05, gnorm=0.47, loss_scale=16, train_wall=6, gb_free=29.7, wall=7769
2024-01-30 22:16:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-30 22:16:02 | INFO | train_inner | epoch 090:    948 / 1349 loss=1.801, nll_loss=0.203, ppl=1.15, wps=64962.4, ups=16.43, wpb=3952.7, bsz=220.5, num_updates=121000, lr=9.09091e-05, gnorm=0.534, loss_scale=8, train_wall=6, gb_free=29.7, wall=7775
2024-01-30 22:16:08 | INFO | train_inner | epoch 090:   1048 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65504.9, ups=16.59, wpb=3948.4, bsz=218.2, num_updates=121100, lr=9.08715e-05, gnorm=0.446, loss_scale=8, train_wall=6, gb_free=29.8, wall=7781
2024-01-30 22:16:14 | INFO | train_inner | epoch 090:   1148 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65230.6, ups=16.64, wpb=3919.2, bsz=208.2, num_updates=121200, lr=9.08341e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=7787
2024-01-30 22:16:20 | INFO | train_inner | epoch 090:   1248 / 1349 loss=1.803, nll_loss=0.205, ppl=1.15, wps=65811.2, ups=16.69, wpb=3944.1, bsz=219.3, num_updates=121300, lr=9.07966e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=7793
2024-01-30 22:16:27 | INFO | train_inner | epoch 090:   1348 / 1349 loss=1.804, nll_loss=0.205, ppl=1.15, wps=53053.6, ups=13.75, wpb=3858.1, bsz=210.8, num_updates=121400, lr=9.07592e-05, gnorm=0.623, loss_scale=8, train_wall=6, gb_free=29.8, wall=7801
2024-01-30 22:16:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:16:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:16:29 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 2.147 | nll_loss 0.402 | ppl 1.32 | wps 171040 | wpb 3275.2 | bsz 176.5 | num_updates 121401 | best_loss 2.063
2024-01-30 22:16:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 121401 updates
2024-01-30 22:16:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint90.pt
2024-01-30 22:16:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint90.pt
2024-01-30 22:16:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint90.pt (epoch 90 @ 121401 updates, score 2.147) (writing took 2.3012416700366884 seconds)
2024-01-30 22:16:32 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2024-01-30 22:16:32 | INFO | train | epoch 090 | loss 1.799 | nll_loss 0.201 | ppl 1.15 | wps 61030 | ups 15.56 | wpb 3921.1 | bsz 214.4 | num_updates 121401 | lr 9.07588e-05 | gnorm 0.509 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 7805
2024-01-30 22:16:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:16:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:16:32 | INFO | fairseq.trainer | begin training epoch 91
2024-01-30 22:16:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:16:38 | INFO | train_inner | epoch 091:     99 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=38449, ups=9.67, wpb=3977.7, bsz=233.8, num_updates=121500, lr=9.07218e-05, gnorm=0.561, loss_scale=8, train_wall=6, gb_free=29.7, wall=7811
2024-01-30 22:16:44 | INFO | train_inner | epoch 091:    199 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=65618.1, ups=16.54, wpb=3966, bsz=217.1, num_updates=121600, lr=9.06845e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=7817
2024-01-30 22:16:50 | INFO | train_inner | epoch 091:    299 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65387.9, ups=16.75, wpb=3903.5, bsz=211.9, num_updates=121700, lr=9.06473e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.8, wall=7823
2024-01-30 22:17:04 | INFO | train_inner | epoch 091:    399 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=28102.9, ups=7.28, wpb=3860, bsz=209.3, num_updates=121800, lr=9.061e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.8, wall=7837
2024-01-30 22:17:10 | INFO | train_inner | epoch 091:    499 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=65687.8, ups=16.71, wpb=3931.1, bsz=223.3, num_updates=121900, lr=9.05729e-05, gnorm=0.515, loss_scale=8, train_wall=6, gb_free=29.9, wall=7843
2024-01-30 22:17:16 | INFO | train_inner | epoch 091:    599 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65334.7, ups=16.65, wpb=3924.5, bsz=204.1, num_updates=122000, lr=9.05357e-05, gnorm=0.527, loss_scale=8, train_wall=6, gb_free=29.8, wall=7849
2024-01-30 22:17:22 | INFO | train_inner | epoch 091:    699 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65544.6, ups=16.63, wpb=3940.2, bsz=224.1, num_updates=122100, lr=9.04987e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=7855
2024-01-30 22:17:28 | INFO | train_inner | epoch 091:    799 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65260.1, ups=16.75, wpb=3897, bsz=211.7, num_updates=122200, lr=9.04616e-05, gnorm=0.548, loss_scale=8, train_wall=6, gb_free=29.8, wall=7861
2024-01-30 22:17:33 | INFO | train_inner | epoch 091:    899 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=64754.3, ups=16.72, wpb=3873, bsz=211.1, num_updates=122300, lr=9.04246e-05, gnorm=0.564, loss_scale=8, train_wall=6, gb_free=29.7, wall=7867
2024-01-30 22:17:39 | INFO | train_inner | epoch 091:    999 / 1349 loss=1.801, nll_loss=0.202, ppl=1.15, wps=65343.9, ups=16.72, wpb=3907.1, bsz=211, num_updates=122400, lr=9.03877e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=7873
2024-01-30 22:17:46 | INFO | train_inner | epoch 091:   1099 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65837.5, ups=16.57, wpb=3973.3, bsz=207.4, num_updates=122500, lr=9.03508e-05, gnorm=0.466, loss_scale=8, train_wall=6, gb_free=29.8, wall=7879
2024-01-30 22:17:51 | INFO | train_inner | epoch 091:   1199 / 1349 loss=1.801, nll_loss=0.203, ppl=1.15, wps=65250.8, ups=16.71, wpb=3905.8, bsz=204.9, num_updates=122600, lr=9.03139e-05, gnorm=0.583, loss_scale=8, train_wall=6, gb_free=29.7, wall=7885
2024-01-30 22:17:58 | INFO | train_inner | epoch 091:   1299 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65027.8, ups=16.63, wpb=3909.7, bsz=217.3, num_updates=122700, lr=9.02771e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.7, wall=7891
2024-01-30 22:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:18:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:18:03 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 2.149 | nll_loss 0.408 | ppl 1.33 | wps 173040 | wpb 3275.2 | bsz 176.5 | num_updates 122750 | best_loss 2.063
2024-01-30 22:18:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 122750 updates
2024-01-30 22:18:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint91.pt
2024-01-30 22:18:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint91.pt
2024-01-30 22:18:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint91.pt (epoch 91 @ 122750 updates, score 2.149) (writing took 2.3347925540292636 seconds)
2024-01-30 22:18:05 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2024-01-30 22:18:05 | INFO | train | epoch 091 | loss 1.799 | nll_loss 0.201 | ppl 1.15 | wps 56617.5 | ups 14.44 | wpb 3921.2 | bsz 214.5 | num_updates 122750 | lr 9.02587e-05 | gnorm 0.53 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 7898
2024-01-30 22:18:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:18:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:18:05 | INFO | fairseq.trainer | begin training epoch 92
2024-01-30 22:18:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:18:08 | INFO | train_inner | epoch 092:     50 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=36983.3, ups=9.41, wpb=3932, bsz=214.9, num_updates=122800, lr=9.02404e-05, gnorm=0.525, loss_scale=8, train_wall=6, gb_free=30.4, wall=7901
2024-01-30 22:18:14 | INFO | train_inner | epoch 092:    150 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65297.7, ups=16.74, wpb=3900.7, bsz=203.7, num_updates=122900, lr=9.02036e-05, gnorm=0.486, loss_scale=8, train_wall=6, gb_free=29.8, wall=7907
2024-01-30 22:18:20 | INFO | train_inner | epoch 092:    250 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=65705.9, ups=16.62, wpb=3954.1, bsz=219.8, num_updates=123000, lr=9.0167e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.9, wall=7913
2024-01-30 22:18:26 | INFO | train_inner | epoch 092:    350 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=65343.2, ups=16.61, wpb=3932.9, bsz=209.2, num_updates=123100, lr=9.01303e-05, gnorm=0.543, loss_scale=8, train_wall=6, gb_free=29.7, wall=7919
2024-01-30 22:18:32 | INFO | train_inner | epoch 092:    450 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=66004.5, ups=16.74, wpb=3942.5, bsz=215, num_updates=123200, lr=9.00937e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.7, wall=7925
2024-01-30 22:18:38 | INFO | train_inner | epoch 092:    550 / 1349 loss=1.801, nll_loss=0.203, ppl=1.15, wps=65260.1, ups=16.84, wpb=3876.2, bsz=203.6, num_updates=123300, lr=9.00572e-05, gnorm=0.542, loss_scale=8, train_wall=6, gb_free=29.9, wall=7931
2024-01-30 22:18:44 | INFO | train_inner | epoch 092:    650 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=65444.9, ups=16.52, wpb=3962.2, bsz=226.2, num_updates=123400, lr=9.00207e-05, gnorm=0.449, loss_scale=8, train_wall=6, gb_free=29.7, wall=7937
2024-01-30 22:18:50 | INFO | train_inner | epoch 092:    750 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65376.1, ups=16.62, wpb=3932.8, bsz=228.8, num_updates=123500, lr=8.99843e-05, gnorm=0.453, loss_scale=8, train_wall=6, gb_free=29.8, wall=7943
2024-01-30 22:18:56 | INFO | train_inner | epoch 092:    850 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65073.1, ups=16.62, wpb=3915.2, bsz=217.2, num_updates=123600, lr=8.99478e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.8, wall=7949
2024-01-30 22:19:02 | INFO | train_inner | epoch 092:    950 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=65067.8, ups=16.57, wpb=3927.9, bsz=217.9, num_updates=123700, lr=8.99115e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.7, wall=7955
2024-01-30 22:19:08 | INFO | train_inner | epoch 092:   1050 / 1349 loss=1.801, nll_loss=0.203, ppl=1.15, wps=66254.7, ups=16.7, wpb=3966.4, bsz=204.6, num_updates=123800, lr=8.98752e-05, gnorm=0.485, loss_scale=8, train_wall=6, gb_free=29.8, wall=7961
2024-01-30 22:19:14 | INFO | train_inner | epoch 092:   1150 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=64401.3, ups=16.7, wpb=3856.3, bsz=210.6, num_updates=123900, lr=8.98389e-05, gnorm=0.545, loss_scale=8, train_wall=6, gb_free=29.7, wall=7967
2024-01-30 22:19:20 | INFO | train_inner | epoch 092:   1250 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=64688.9, ups=16.67, wpb=3880.3, bsz=217.1, num_updates=124000, lr=8.98027e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.7, wall=7973
2024-01-30 22:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:19:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:19:28 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 2.145 | nll_loss 0.402 | ppl 1.32 | wps 173405 | wpb 3275.2 | bsz 176.5 | num_updates 124099 | best_loss 2.063
2024-01-30 22:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 124099 updates
2024-01-30 22:19:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint92.pt
2024-01-30 22:19:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint92.pt
2024-01-30 22:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint92.pt (epoch 92 @ 124099 updates, score 2.145) (writing took 2.3286077349912375 seconds)
2024-01-30 22:19:30 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2024-01-30 22:19:30 | INFO | train | epoch 092 | loss 1.798 | nll_loss 0.2 | ppl 1.15 | wps 62162.9 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 124099 | lr 8.97668e-05 | gnorm 0.5 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 7983
2024-01-30 22:19:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:19:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:19:30 | INFO | fairseq.trainer | begin training epoch 93
2024-01-30 22:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:19:30 | INFO | train_inner | epoch 093:      1 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=38812.6, ups=9.89, wpb=3925.9, bsz=217.4, num_updates=124100, lr=8.97665e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.7, wall=7983
2024-01-30 22:19:36 | INFO | train_inner | epoch 093:    101 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65539, ups=16.66, wpb=3934.8, bsz=208.3, num_updates=124200, lr=8.97303e-05, gnorm=0.489, loss_scale=8, train_wall=6, gb_free=29.7, wall=7989
2024-01-30 22:19:42 | INFO | train_inner | epoch 093:    201 / 1349 loss=1.794, nll_loss=0.195, ppl=1.14, wps=65604.8, ups=16.58, wpb=3957.2, bsz=225.4, num_updates=124300, lr=8.96942e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.7, wall=7996
2024-01-30 22:19:48 | INFO | train_inner | epoch 093:    301 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65457, ups=16.71, wpb=3917.7, bsz=207.8, num_updates=124400, lr=8.96582e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.7, wall=8002
2024-01-30 22:19:54 | INFO | train_inner | epoch 093:    401 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65893.8, ups=16.77, wpb=3928.3, bsz=205.9, num_updates=124500, lr=8.96221e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=8007
2024-01-30 22:20:00 | INFO | train_inner | epoch 093:    501 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=64738.9, ups=16.68, wpb=3881.3, bsz=215, num_updates=124600, lr=8.95862e-05, gnorm=0.551, loss_scale=8, train_wall=6, gb_free=29.7, wall=8013
2024-01-30 22:20:06 | INFO | train_inner | epoch 093:    601 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65030.6, ups=16.77, wpb=3877.5, bsz=212.6, num_updates=124700, lr=8.95502e-05, gnorm=0.494, loss_scale=8, train_wall=6, gb_free=29.7, wall=8019
2024-01-30 22:20:12 | INFO | train_inner | epoch 093:    701 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=64401.6, ups=16.55, wpb=3891.9, bsz=220.9, num_updates=124800, lr=8.95144e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.7, wall=8025
2024-01-30 22:20:18 | INFO | train_inner | epoch 093:    801 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65330.8, ups=16.67, wpb=3920.2, bsz=204.9, num_updates=124900, lr=8.94785e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.7, wall=8031
2024-01-30 22:20:24 | INFO | train_inner | epoch 093:    901 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=66097.9, ups=16.73, wpb=3949.8, bsz=212.1, num_updates=125000, lr=8.94427e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=8037
2024-01-30 22:20:30 | INFO | train_inner | epoch 093:   1001 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65493.7, ups=16.57, wpb=3951.4, bsz=223, num_updates=125100, lr=8.9407e-05, gnorm=0.55, loss_scale=8, train_wall=6, gb_free=29.8, wall=8043
2024-01-30 22:20:36 | INFO | train_inner | epoch 093:   1101 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=65365.3, ups=16.69, wpb=3917.1, bsz=205.2, num_updates=125200, lr=8.93713e-05, gnorm=0.57, loss_scale=8, train_wall=6, gb_free=29.8, wall=8049
2024-01-30 22:20:42 | INFO | train_inner | epoch 093:   1201 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65239.7, ups=16.62, wpb=3926.2, bsz=224, num_updates=125300, lr=8.93356e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.8, wall=8056
2024-01-30 22:20:48 | INFO | train_inner | epoch 093:   1301 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=64691.3, ups=16.62, wpb=3892.6, bsz=220.6, num_updates=125400, lr=8.93e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.7, wall=8062
2024-01-30 22:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:20:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:20:53 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 2.149 | nll_loss 0.406 | ppl 1.33 | wps 172842 | wpb 3275.2 | bsz 176.5 | num_updates 125448 | best_loss 2.063
2024-01-30 22:20:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 125448 updates
2024-01-30 22:20:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint93.pt
2024-01-30 22:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint93.pt
2024-01-30 22:20:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint93.pt (epoch 93 @ 125448 updates, score 2.149) (writing took 2.3969670339720324 seconds)
2024-01-30 22:20:55 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2024-01-30 22:20:55 | INFO | train | epoch 093 | loss 1.798 | nll_loss 0.199 | ppl 1.15 | wps 62117.1 | ups 15.84 | wpb 3921.2 | bsz 214.5 | num_updates 125448 | lr 8.92829e-05 | gnorm 0.507 | loss_scale 8 | train_wall 78 | gb_free 29.9 | wall 8069
2024-01-30 22:20:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:20:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:20:55 | INFO | fairseq.trainer | begin training epoch 94
2024-01-30 22:20:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:20:58 | INFO | train_inner | epoch 094:     52 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=38347.9, ups=9.84, wpb=3898.9, bsz=202.2, num_updates=125500, lr=8.92644e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.9, wall=8072
2024-01-30 22:21:05 | INFO | train_inner | epoch 094:    152 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=66054.7, ups=16.56, wpb=3989.2, bsz=210.4, num_updates=125600, lr=8.92288e-05, gnorm=0.476, loss_scale=8, train_wall=6, gb_free=29.7, wall=8078
2024-01-30 22:21:10 | INFO | train_inner | epoch 094:    252 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65074.1, ups=16.81, wpb=3871.1, bsz=198.9, num_updates=125700, lr=8.91933e-05, gnorm=0.519, loss_scale=8, train_wall=6, gb_free=29.7, wall=8084
2024-01-30 22:21:16 | INFO | train_inner | epoch 094:    352 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65375.9, ups=16.69, wpb=3916.1, bsz=215.2, num_updates=125800, lr=8.91579e-05, gnorm=0.495, loss_scale=8, train_wall=6, gb_free=29.8, wall=8090
2024-01-30 22:21:22 | INFO | train_inner | epoch 094:    452 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=65736.7, ups=16.59, wpb=3961.3, bsz=220.3, num_updates=125900, lr=8.91225e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.9, wall=8096
2024-01-30 22:21:28 | INFO | train_inner | epoch 094:    552 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65005.2, ups=16.73, wpb=3885.2, bsz=214.5, num_updates=126000, lr=8.90871e-05, gnorm=0.544, loss_scale=8, train_wall=6, gb_free=29.8, wall=8102
2024-01-30 22:21:34 | INFO | train_inner | epoch 094:    652 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65386, ups=16.71, wpb=3912.5, bsz=206.8, num_updates=126100, lr=8.90517e-05, gnorm=0.536, loss_scale=8, train_wall=6, gb_free=29.8, wall=8108
2024-01-30 22:21:40 | INFO | train_inner | epoch 094:    752 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65081.9, ups=16.5, wpb=3944.9, bsz=228.8, num_updates=126200, lr=8.90165e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.9, wall=8114
2024-01-30 22:21:47 | INFO | train_inner | epoch 094:    852 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=65525.6, ups=16.61, wpb=3945.2, bsz=215.9, num_updates=126300, lr=8.89812e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.7, wall=8120
2024-01-30 22:21:52 | INFO | train_inner | epoch 094:    952 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=65257.9, ups=16.75, wpb=3896.6, bsz=219, num_updates=126400, lr=8.8946e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.8, wall=8126
2024-01-30 22:21:58 | INFO | train_inner | epoch 094:   1052 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65816.5, ups=16.65, wpb=3953.2, bsz=215.3, num_updates=126500, lr=8.89108e-05, gnorm=0.535, loss_scale=8, train_wall=6, gb_free=29.8, wall=8132
2024-01-30 22:22:05 | INFO | train_inner | epoch 094:   1152 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=64809.3, ups=16.63, wpb=3897.6, bsz=224.2, num_updates=126600, lr=8.88757e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.8, wall=8138
2024-01-30 22:22:11 | INFO | train_inner | epoch 094:   1252 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65416.1, ups=16.63, wpb=3934.4, bsz=215.4, num_updates=126700, lr=8.88406e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=8144
2024-01-30 22:22:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:22:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:22:18 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 2.148 | nll_loss 0.407 | ppl 1.33 | wps 172648 | wpb 3275.2 | bsz 176.5 | num_updates 126797 | best_loss 2.063
2024-01-30 22:22:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 126797 updates
2024-01-30 22:22:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint94.pt
2024-01-30 22:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint94.pt
2024-01-30 22:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint94.pt (epoch 94 @ 126797 updates, score 2.148) (writing took 2.3047409309074283 seconds)
2024-01-30 22:22:20 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2024-01-30 22:22:20 | INFO | train | epoch 094 | loss 1.798 | nll_loss 0.199 | ppl 1.15 | wps 62200.9 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 126797 | lr 8.88067e-05 | gnorm 0.507 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 8154
2024-01-30 22:22:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:22:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:22:20 | INFO | fairseq.trainer | begin training epoch 95
2024-01-30 22:22:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:22:21 | INFO | train_inner | epoch 095:      3 / 1349 loss=1.802, nll_loss=0.204, ppl=1.15, wps=38901.4, ups=9.93, wpb=3918.8, bsz=217.6, num_updates=126800, lr=8.88056e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=8154
2024-01-30 22:22:27 | INFO | train_inner | epoch 095:    103 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=64881.9, ups=16.71, wpb=3882.4, bsz=202.9, num_updates=126900, lr=8.87706e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=8160
2024-01-30 22:22:33 | INFO | train_inner | epoch 095:    203 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=64563.4, ups=16.61, wpb=3887.1, bsz=220.2, num_updates=127000, lr=8.87357e-05, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.8, wall=8166
2024-01-30 22:22:39 | INFO | train_inner | epoch 095:    303 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=65450, ups=16.57, wpb=3949.3, bsz=224.6, num_updates=127100, lr=8.87007e-05, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.7, wall=8172
2024-01-30 22:22:45 | INFO | train_inner | epoch 095:    403 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65523.6, ups=16.65, wpb=3936.1, bsz=222.6, num_updates=127200, lr=8.86659e-05, gnorm=0.453, loss_scale=8, train_wall=6, gb_free=29.7, wall=8178
2024-01-30 22:22:51 | INFO | train_inner | epoch 095:    503 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65106.2, ups=16.66, wpb=3908.3, bsz=210, num_updates=127300, lr=8.8631e-05, gnorm=0.488, loss_scale=8, train_wall=6, gb_free=29.8, wall=8184
2024-01-30 22:22:57 | INFO | train_inner | epoch 095:    603 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65875.7, ups=16.62, wpb=3963.9, bsz=219.2, num_updates=127400, lr=8.85962e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.8, wall=8190
2024-01-30 22:23:03 | INFO | train_inner | epoch 095:    703 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65661.8, ups=16.8, wpb=3909.6, bsz=205.9, num_updates=127500, lr=8.85615e-05, gnorm=0.552, loss_scale=8, train_wall=6, gb_free=29.8, wall=8196
2024-01-30 22:23:09 | INFO | train_inner | epoch 095:    803 / 1349 loss=1.8, nll_loss=0.201, ppl=1.15, wps=65608.5, ups=16.75, wpb=3916.4, bsz=209, num_updates=127600, lr=8.85268e-05, gnorm=0.54, loss_scale=8, train_wall=6, gb_free=29.8, wall=8202
2024-01-30 22:23:15 | INFO | train_inner | epoch 095:    903 / 1349 loss=1.799, nll_loss=0.2, ppl=1.15, wps=65589.8, ups=16.66, wpb=3937.4, bsz=214.9, num_updates=127700, lr=8.84921e-05, gnorm=0.537, loss_scale=8, train_wall=6, gb_free=29.7, wall=8208
2024-01-30 22:23:21 | INFO | train_inner | epoch 095:   1003 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65709, ups=16.63, wpb=3950.7, bsz=219, num_updates=127800, lr=8.84575e-05, gnorm=0.473, loss_scale=8, train_wall=6, gb_free=29.8, wall=8214
2024-01-30 22:23:27 | INFO | train_inner | epoch 095:   1103 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=65035.7, ups=16.64, wpb=3909.2, bsz=220.2, num_updates=127900, lr=8.84229e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.7, wall=8220
2024-01-30 22:23:33 | INFO | train_inner | epoch 095:   1203 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=64797.7, ups=16.69, wpb=3882.4, bsz=199.5, num_updates=128000, lr=8.83883e-05, gnorm=0.512, loss_scale=8, train_wall=6, gb_free=29.8, wall=8226
2024-01-30 22:23:39 | INFO | train_inner | epoch 095:   1303 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65560.1, ups=16.66, wpb=3934.7, bsz=216.4, num_updates=128100, lr=8.83538e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.8, wall=8232
2024-01-30 22:23:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:23:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:23:43 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 2.153 | nll_loss 0.41 | ppl 1.33 | wps 173649 | wpb 3275.2 | bsz 176.5 | num_updates 128146 | best_loss 2.063
2024-01-30 22:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 128146 updates
2024-01-30 22:23:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint95.pt
2024-01-30 22:23:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint95.pt
2024-01-30 22:23:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint95.pt (epoch 95 @ 128146 updates, score 2.153) (writing took 2.29335313802585 seconds)
2024-01-30 22:23:45 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2024-01-30 22:23:45 | INFO | train | epoch 095 | loss 1.797 | nll_loss 0.198 | ppl 1.15 | wps 62178.5 | ups 15.86 | wpb 3921.2 | bsz 214.5 | num_updates 128146 | lr 8.8338e-05 | gnorm 0.503 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 8239
2024-01-30 22:23:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:23:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:23:45 | INFO | fairseq.trainer | begin training epoch 96
2024-01-30 22:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:23:49 | INFO | train_inner | epoch 096:     54 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=38553.8, ups=9.86, wpb=3908.8, bsz=222.9, num_updates=128200, lr=8.83194e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.7, wall=8242
2024-01-30 22:23:55 | INFO | train_inner | epoch 096:    154 / 1349 loss=1.794, nll_loss=0.195, ppl=1.14, wps=64578.9, ups=16.73, wpb=3860.9, bsz=218.7, num_updates=128300, lr=8.82849e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.8, wall=8248
2024-01-30 22:24:01 | INFO | train_inner | epoch 096:    254 / 1349 loss=1.791, nll_loss=0.192, ppl=1.14, wps=65955.3, ups=16.5, wpb=3998.1, bsz=218.9, num_updates=128400, lr=8.82506e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=8254
2024-01-30 22:24:07 | INFO | train_inner | epoch 096:    354 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=65200.1, ups=16.67, wpb=3910.5, bsz=223.8, num_updates=128500, lr=8.82162e-05, gnorm=0.469, loss_scale=8, train_wall=6, gb_free=29.8, wall=8260
2024-01-30 22:24:13 | INFO | train_inner | epoch 096:    454 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65614.9, ups=16.62, wpb=3948.5, bsz=214.8, num_updates=128600, lr=8.81819e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=8266
2024-01-30 22:24:19 | INFO | train_inner | epoch 096:    554 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=65744.3, ups=16.6, wpb=3961.4, bsz=214.9, num_updates=128700, lr=8.81476e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=8272
2024-01-30 22:24:25 | INFO | train_inner | epoch 096:    654 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65002, ups=16.72, wpb=3888.1, bsz=217, num_updates=128800, lr=8.81134e-05, gnorm=0.487, loss_scale=8, train_wall=6, gb_free=29.9, wall=8278
2024-01-30 22:24:31 | INFO | train_inner | epoch 096:    754 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65000.6, ups=16.77, wpb=3875.7, bsz=208.5, num_updates=128900, lr=8.80792e-05, gnorm=0.547, loss_scale=8, train_wall=6, gb_free=29.8, wall=8284
2024-01-30 22:24:37 | INFO | train_inner | epoch 096:    854 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65634.9, ups=16.69, wpb=3931.7, bsz=207.6, num_updates=129000, lr=8.80451e-05, gnorm=0.503, loss_scale=8, train_wall=6, gb_free=29.8, wall=8290
2024-01-30 22:24:43 | INFO | train_inner | epoch 096:    954 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65841.6, ups=16.71, wpb=3940.9, bsz=215.7, num_updates=129100, lr=8.8011e-05, gnorm=0.531, loss_scale=8, train_wall=6, gb_free=29.7, wall=8296
2024-01-30 22:24:49 | INFO | train_inner | epoch 096:   1054 / 1349 loss=1.801, nll_loss=0.203, ppl=1.15, wps=65535.8, ups=16.64, wpb=3937.4, bsz=201.5, num_updates=129200, lr=8.79769e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.8, wall=8302
2024-01-30 22:24:55 | INFO | train_inner | epoch 096:   1154 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65225.2, ups=16.72, wpb=3901.3, bsz=205, num_updates=129300, lr=8.79429e-05, gnorm=0.532, loss_scale=8, train_wall=6, gb_free=29.7, wall=8308
2024-01-30 22:25:01 | INFO | train_inner | epoch 096:   1254 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65455.3, ups=16.56, wpb=3952.7, bsz=221.4, num_updates=129400, lr=8.79089e-05, gnorm=0.533, loss_scale=8, train_wall=6, gb_free=29.8, wall=8314
2024-01-30 22:25:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:25:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:25:08 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 2.151 | nll_loss 0.409 | ppl 1.33 | wps 172872 | wpb 3275.2 | bsz 176.5 | num_updates 129495 | best_loss 2.063
2024-01-30 22:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 129495 updates
2024-01-30 22:25:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint96.pt
2024-01-30 22:25:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint96.pt
2024-01-30 22:25:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint96.pt (epoch 96 @ 129495 updates, score 2.151) (writing took 2.3306502420455217 seconds)
2024-01-30 22:25:11 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2024-01-30 22:25:11 | INFO | train | epoch 096 | loss 1.797 | nll_loss 0.198 | ppl 1.15 | wps 62150.6 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 129495 | lr 8.78767e-05 | gnorm 0.505 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 8324
2024-01-30 22:25:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:25:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:25:11 | INFO | fairseq.trainer | begin training epoch 97
2024-01-30 22:25:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:25:11 | INFO | train_inner | epoch 097:      5 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=38406.7, ups=9.87, wpb=3891.8, bsz=219.6, num_updates=129500, lr=8.7875e-05, gnorm=0.482, loss_scale=8, train_wall=6, gb_free=29.8, wall=8324
2024-01-30 22:25:17 | INFO | train_inner | epoch 097:    105 / 1349 loss=1.793, nll_loss=0.194, ppl=1.14, wps=64869.5, ups=16.62, wpb=3902.9, bsz=215, num_updates=129600, lr=8.7841e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=8330
2024-01-30 22:25:23 | INFO | train_inner | epoch 097:    205 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65507.2, ups=16.77, wpb=3906.1, bsz=215, num_updates=129700, lr=8.78072e-05, gnorm=0.491, loss_scale=8, train_wall=6, gb_free=29.8, wall=8336
2024-01-30 22:25:29 | INFO | train_inner | epoch 097:    305 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65527.6, ups=16.69, wpb=3927.1, bsz=206.4, num_updates=129800, lr=8.77733e-05, gnorm=0.47, loss_scale=8, train_wall=6, gb_free=29.8, wall=8342
2024-01-30 22:25:35 | INFO | train_inner | epoch 097:    405 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65876.1, ups=16.66, wpb=3955, bsz=211.4, num_updates=129900, lr=8.77396e-05, gnorm=0.526, loss_scale=8, train_wall=6, gb_free=29.8, wall=8348
2024-01-30 22:25:41 | INFO | train_inner | epoch 097:    505 / 1349 loss=1.79, nll_loss=0.192, ppl=1.14, wps=65543.5, ups=16.44, wpb=3987.4, bsz=235.2, num_updates=130000, lr=8.77058e-05, gnorm=0.479, loss_scale=8, train_wall=6, gb_free=29.8, wall=8354
2024-01-30 22:25:47 | INFO | train_inner | epoch 097:    605 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=65197.1, ups=16.68, wpb=3909.3, bsz=216.7, num_updates=130100, lr=8.76721e-05, gnorm=0.49, loss_scale=8, train_wall=6, gb_free=29.8, wall=8360
2024-01-30 22:25:53 | INFO | train_inner | epoch 097:    705 / 1349 loss=1.795, nll_loss=0.196, ppl=1.15, wps=65855.7, ups=16.68, wpb=3947.3, bsz=209.8, num_updates=130200, lr=8.76384e-05, gnorm=0.462, loss_scale=8, train_wall=6, gb_free=29.9, wall=8366
2024-01-30 22:25:59 | INFO | train_inner | epoch 097:    805 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=66093.7, ups=16.72, wpb=3952.1, bsz=218.5, num_updates=130300, lr=8.76048e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=8372
2024-01-30 22:26:05 | INFO | train_inner | epoch 097:    905 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65288.7, ups=16.63, wpb=3926.4, bsz=207.8, num_updates=130400, lr=8.75712e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.8, wall=8378
2024-01-30 22:26:11 | INFO | train_inner | epoch 097:   1005 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=65587.5, ups=16.66, wpb=3935.7, bsz=211.4, num_updates=130500, lr=8.75376e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.8, wall=8384
2024-01-30 22:26:17 | INFO | train_inner | epoch 097:   1105 / 1349 loss=1.8, nll_loss=0.202, ppl=1.15, wps=64363, ups=16.84, wpb=3822.7, bsz=202.3, num_updates=130600, lr=8.75041e-05, gnorm=0.522, loss_scale=8, train_wall=6, gb_free=29.8, wall=8390
2024-01-30 22:26:23 | INFO | train_inner | epoch 097:   1205 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=64322.2, ups=16.74, wpb=3843.6, bsz=215.9, num_updates=130700, lr=8.74706e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=8396
2024-01-30 22:26:29 | INFO | train_inner | epoch 097:   1305 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65294.7, ups=16.52, wpb=3951.7, bsz=216.3, num_updates=130800, lr=8.74372e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.7, wall=8402
2024-01-30 22:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:26:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:26:33 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 2.154 | nll_loss 0.413 | ppl 1.33 | wps 173606 | wpb 3275.2 | bsz 176.5 | num_updates 130844 | best_loss 2.063
2024-01-30 22:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 130844 updates
2024-01-30 22:26:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint97.pt
2024-01-30 22:26:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint97.pt
2024-01-30 22:26:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint97.pt (epoch 97 @ 130844 updates, score 2.154) (writing took 2.322768877958879 seconds)
2024-01-30 22:26:36 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2024-01-30 22:26:36 | INFO | train | epoch 097 | loss 1.796 | nll_loss 0.198 | ppl 1.15 | wps 62164.7 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 130844 | lr 8.74225e-05 | gnorm 0.501 | loss_scale 8 | train_wall 78 | gb_free 29.7 | wall 8409
2024-01-30 22:26:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:26:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:26:36 | INFO | fairseq.trainer | begin training epoch 98
2024-01-30 22:26:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:26:39 | INFO | train_inner | epoch 098:     56 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=38331.2, ups=9.88, wpb=3879.2, bsz=219.6, num_updates=130900, lr=8.74038e-05, gnorm=0.528, loss_scale=8, train_wall=6, gb_free=29.8, wall=8412
2024-01-30 22:26:45 | INFO | train_inner | epoch 098:    156 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=64565.8, ups=16.75, wpb=3855.7, bsz=208.3, num_updates=131000, lr=8.73704e-05, gnorm=0.529, loss_scale=8, train_wall=6, gb_free=29.7, wall=8418
2024-01-30 22:26:51 | INFO | train_inner | epoch 098:    256 / 1349 loss=1.791, nll_loss=0.192, ppl=1.14, wps=65664.1, ups=16.6, wpb=3955.6, bsz=217, num_updates=131100, lr=8.73371e-05, gnorm=0.442, loss_scale=8, train_wall=6, gb_free=29.8, wall=8424
2024-01-30 22:26:57 | INFO | train_inner | epoch 098:    356 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=65264.2, ups=16.72, wpb=3904.2, bsz=220.6, num_updates=131200, lr=8.73038e-05, gnorm=0.46, loss_scale=8, train_wall=6, gb_free=29.8, wall=8430
2024-01-30 22:27:03 | INFO | train_inner | epoch 098:    456 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=65604.2, ups=16.6, wpb=3952, bsz=226.2, num_updates=131300, lr=8.72705e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.8, wall=8436
2024-01-30 22:27:09 | INFO | train_inner | epoch 098:    556 / 1349 loss=1.796, nll_loss=0.197, ppl=1.15, wps=65032.2, ups=16.6, wpb=3917.6, bsz=205, num_updates=131400, lr=8.72373e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.8, wall=8442
2024-01-30 22:27:15 | INFO | train_inner | epoch 098:    656 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65543.4, ups=16.68, wpb=3929.5, bsz=209.7, num_updates=131500, lr=8.72041e-05, gnorm=0.472, loss_scale=8, train_wall=6, gb_free=29.8, wall=8448
2024-01-30 22:27:21 | INFO | train_inner | epoch 098:    756 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65199.4, ups=16.68, wpb=3909.1, bsz=210.2, num_updates=131600, lr=8.7171e-05, gnorm=0.502, loss_scale=8, train_wall=6, gb_free=29.8, wall=8454
2024-01-30 22:27:27 | INFO | train_inner | epoch 098:    856 / 1349 loss=1.794, nll_loss=0.195, ppl=1.15, wps=65316.4, ups=16.68, wpb=3914.9, bsz=226.5, num_updates=131700, lr=8.71379e-05, gnorm=0.474, loss_scale=8, train_wall=6, gb_free=29.8, wall=8460
2024-01-30 22:27:33 | INFO | train_inner | epoch 098:    956 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=64946.1, ups=16.67, wpb=3897.1, bsz=214.2, num_updates=131800, lr=8.71048e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.7, wall=8466
2024-01-30 22:27:39 | INFO | train_inner | epoch 098:   1056 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65974.1, ups=16.62, wpb=3969.2, bsz=212.1, num_updates=131900, lr=8.70718e-05, gnorm=0.474, loss_scale=8, train_wall=6, gb_free=29.7, wall=8472
2024-01-30 22:27:45 | INFO | train_inner | epoch 098:   1156 / 1349 loss=1.798, nll_loss=0.199, ppl=1.15, wps=65666.6, ups=16.65, wpb=3943.3, bsz=208.7, num_updates=132000, lr=8.70388e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.7, wall=8478
2024-01-30 22:27:51 | INFO | train_inner | epoch 098:   1256 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=64972, ups=16.68, wpb=3894.5, bsz=209.2, num_updates=132100, lr=8.70059e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=8484
2024-01-30 22:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:27:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:27:58 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 2.154 | nll_loss 0.413 | ppl 1.33 | wps 163677 | wpb 3275.2 | bsz 176.5 | num_updates 132193 | best_loss 2.063
2024-01-30 22:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 132193 updates
2024-01-30 22:27:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint98.pt
2024-01-30 22:27:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint98.pt
2024-01-30 22:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint98.pt (epoch 98 @ 132193 updates, score 2.154) (writing took 2.3868616849649698 seconds)
2024-01-30 22:28:01 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2024-01-30 22:28:01 | INFO | train | epoch 098 | loss 1.796 | nll_loss 0.197 | ppl 1.15 | wps 62029.8 | ups 15.82 | wpb 3921.2 | bsz 214.5 | num_updates 132193 | lr 8.69753e-05 | gnorm 0.491 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 8494
2024-01-30 22:28:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:28:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:28:01 | INFO | fairseq.trainer | begin training epoch 99
2024-01-30 22:28:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:28:01 | INFO | train_inner | epoch 099:      7 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=38511.3, ups=9.68, wpb=3978.4, bsz=225.2, num_updates=132200, lr=8.6973e-05, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.8, wall=8495
2024-01-30 22:28:07 | INFO | train_inner | epoch 099:    107 / 1349 loss=1.792, nll_loss=0.193, ppl=1.14, wps=65085.4, ups=16.6, wpb=3921.3, bsz=209.4, num_updates=132300, lr=8.69401e-05, gnorm=0.497, loss_scale=8, train_wall=6, gb_free=29.7, wall=8501
2024-01-30 22:28:13 | INFO | train_inner | epoch 099:    207 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65716.5, ups=16.77, wpb=3919.6, bsz=208.4, num_updates=132400, lr=8.69072e-05, gnorm=0.599, loss_scale=8, train_wall=6, gb_free=29.8, wall=8507
2024-01-30 22:28:19 | INFO | train_inner | epoch 099:    307 / 1349 loss=1.792, nll_loss=0.193, ppl=1.14, wps=65029.3, ups=16.48, wpb=3945.4, bsz=227, num_updates=132500, lr=8.68744e-05, gnorm=0.52, loss_scale=8, train_wall=6, gb_free=29.7, wall=8513
2024-01-30 22:28:25 | INFO | train_inner | epoch 099:    407 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65594.7, ups=16.7, wpb=3929, bsz=216.1, num_updates=132600, lr=8.68417e-05, gnorm=0.493, loss_scale=8, train_wall=6, gb_free=29.8, wall=8519
2024-01-30 22:28:31 | INFO | train_inner | epoch 099:    507 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65259.8, ups=16.6, wpb=3931.9, bsz=216.2, num_updates=132700, lr=8.6809e-05, gnorm=0.715, loss_scale=8, train_wall=6, gb_free=29.8, wall=8525
2024-01-30 22:28:38 | INFO | train_inner | epoch 099:    607 / 1349 loss=1.795, nll_loss=0.197, ppl=1.15, wps=61250.8, ups=15.56, wpb=3936.6, bsz=213.4, num_updates=132800, lr=8.67763e-05, gnorm=0.546, loss_scale=8, train_wall=6, gb_free=29.7, wall=8531
2024-01-30 22:28:44 | INFO | train_inner | epoch 099:    707 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65143.4, ups=16.59, wpb=3927.2, bsz=209.6, num_updates=132900, lr=8.67436e-05, gnorm=0.518, loss_scale=8, train_wall=6, gb_free=29.8, wall=8537
2024-01-30 22:28:50 | INFO | train_inner | epoch 099:    807 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65899.5, ups=16.66, wpb=3955.5, bsz=212.4, num_updates=133000, lr=8.6711e-05, gnorm=0.521, loss_scale=8, train_wall=6, gb_free=29.7, wall=8543
2024-01-30 22:28:56 | INFO | train_inner | epoch 099:    907 / 1349 loss=1.797, nll_loss=0.198, ppl=1.15, wps=65293.5, ups=16.79, wpb=3889.1, bsz=206, num_updates=133100, lr=8.66784e-05, gnorm=0.507, loss_scale=8, train_wall=6, gb_free=29.8, wall=8549
2024-01-30 22:29:02 | INFO | train_inner | epoch 099:   1007 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=65520, ups=16.77, wpb=3907.1, bsz=215.1, num_updates=133200, lr=8.66459e-05, gnorm=0.513, loss_scale=8, train_wall=6, gb_free=29.7, wall=8555
2024-01-30 22:29:08 | INFO | train_inner | epoch 099:   1107 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65692.4, ups=16.65, wpb=3946.2, bsz=221.8, num_updates=133300, lr=8.66134e-05, gnorm=0.524, loss_scale=8, train_wall=6, gb_free=29.7, wall=8561
2024-01-30 22:29:14 | INFO | train_inner | epoch 099:   1207 / 1349 loss=1.797, nll_loss=0.199, ppl=1.15, wps=65565.4, ups=16.66, wpb=3934.6, bsz=213.7, num_updates=133400, lr=8.65809e-05, gnorm=0.5, loss_scale=8, train_wall=6, gb_free=29.7, wall=8567
2024-01-30 22:29:20 | INFO | train_inner | epoch 099:   1307 / 1349 loss=1.8, nll_loss=0.203, ppl=1.15, wps=64774.3, ups=16.77, wpb=3862.2, bsz=214.6, num_updates=133500, lr=8.65485e-05, gnorm=0.609, loss_scale=8, train_wall=6, gb_free=30, wall=8573
2024-01-30 22:29:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:29:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:29:24 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 2.156 | nll_loss 0.414 | ppl 1.33 | wps 172863 | wpb 3275.2 | bsz 176.5 | num_updates 133542 | best_loss 2.063
2024-01-30 22:29:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 133542 updates
2024-01-30 22:29:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint99.pt
2024-01-30 22:29:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint99.pt
2024-01-30 22:29:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint99.pt (epoch 99 @ 133542 updates, score 2.156) (writing took 2.3362384299980476 seconds)
2024-01-30 22:29:26 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2024-01-30 22:29:26 | INFO | train | epoch 099 | loss 1.796 | nll_loss 0.198 | ppl 1.15 | wps 61862.9 | ups 15.78 | wpb 3921.2 | bsz 214.5 | num_updates 133542 | lr 8.65349e-05 | gnorm 0.541 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 8580
2024-01-30 22:29:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:29:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1349
2024-01-30 22:29:26 | INFO | fairseq.trainer | begin training epoch 100
2024-01-30 22:29:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 22:29:30 | INFO | train_inner | epoch 100:     58 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=38067.9, ups=9.82, wpb=3877.1, bsz=227.4, num_updates=133600, lr=8.65161e-05, gnorm=0.456, loss_scale=8, train_wall=6, gb_free=29.9, wall=8583
2024-01-30 22:29:36 | INFO | train_inner | epoch 100:    158 / 1349 loss=1.791, nll_loss=0.192, ppl=1.14, wps=65279.8, ups=16.7, wpb=3908.1, bsz=215.4, num_updates=133700, lr=8.64837e-05, gnorm=0.51, loss_scale=8, train_wall=6, gb_free=29.8, wall=8589
2024-01-30 22:29:42 | INFO | train_inner | epoch 100:    258 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=65456.4, ups=16.57, wpb=3950.3, bsz=221.2, num_updates=133800, lr=8.64514e-05, gnorm=0.505, loss_scale=8, train_wall=6, gb_free=29.7, wall=8595
2024-01-30 22:29:48 | INFO | train_inner | epoch 100:    358 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65731.2, ups=16.65, wpb=3946.8, bsz=203, num_updates=133900, lr=8.64191e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.7, wall=8601
2024-01-30 22:29:54 | INFO | train_inner | epoch 100:    458 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=64557.4, ups=16.58, wpb=3894.8, bsz=227.7, num_updates=134000, lr=8.63868e-05, gnorm=0.477, loss_scale=8, train_wall=6, gb_free=29.8, wall=8607
2024-01-30 22:30:00 | INFO | train_inner | epoch 100:    558 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=65863.7, ups=16.62, wpb=3962.5, bsz=221.4, num_updates=134100, lr=8.63546e-05, gnorm=0.484, loss_scale=8, train_wall=6, gb_free=29.8, wall=8613
2024-01-30 22:30:06 | INFO | train_inner | epoch 100:    658 / 1349 loss=1.799, nll_loss=0.201, ppl=1.15, wps=66414.7, ups=16.8, wpb=3952.3, bsz=199.5, num_updates=134200, lr=8.63224e-05, gnorm=0.498, loss_scale=8, train_wall=6, gb_free=29.8, wall=8619
2024-01-30 22:30:12 | INFO | train_inner | epoch 100:    758 / 1349 loss=1.794, nll_loss=0.196, ppl=1.15, wps=64343, ups=16.68, wpb=3857.1, bsz=214.3, num_updates=134300, lr=8.62903e-05, gnorm=0.475, loss_scale=8, train_wall=6, gb_free=29.8, wall=8625
2024-01-30 22:30:18 | INFO | train_inner | epoch 100:    858 / 1349 loss=1.794, nll_loss=0.195, ppl=1.15, wps=64938.9, ups=16.61, wpb=3909.1, bsz=215.8, num_updates=134400, lr=8.62582e-05, gnorm=0.523, loss_scale=8, train_wall=6, gb_free=29.8, wall=8631
2024-01-30 22:30:24 | INFO | train_inner | epoch 100:    958 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=64947.9, ups=16.7, wpb=3888.3, bsz=199.8, num_updates=134500, lr=8.62261e-05, gnorm=0.549, loss_scale=8, train_wall=6, gb_free=29.7, wall=8637
2024-01-30 22:30:30 | INFO | train_inner | epoch 100:   1058 / 1349 loss=1.798, nll_loss=0.2, ppl=1.15, wps=66348.6, ups=16.74, wpb=3962.9, bsz=201.7, num_updates=134600, lr=8.61941e-05, gnorm=0.517, loss_scale=8, train_wall=6, gb_free=29.9, wall=8643
2024-01-30 22:30:36 | INFO | train_inner | epoch 100:   1158 / 1349 loss=1.796, nll_loss=0.198, ppl=1.15, wps=65705.3, ups=16.79, wpb=3914.3, bsz=204.4, num_updates=134700, lr=8.61621e-05, gnorm=0.506, loss_scale=8, train_wall=6, gb_free=29.8, wall=8649
2024-01-30 22:30:42 | INFO | train_inner | epoch 100:   1258 / 1349 loss=1.793, nll_loss=0.195, ppl=1.14, wps=64917.1, ups=16.63, wpb=3903.1, bsz=229, num_updates=134800, lr=8.61301e-05, gnorm=0.496, loss_scale=8, train_wall=6, gb_free=29.7, wall=8655
2024-01-30 22:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 22:30:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:30:49 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 2.157 | nll_loss 0.415 | ppl 1.33 | wps 170974 | wpb 3275.2 | bsz 176.5 | num_updates 134891 | best_loss 2.063
2024-01-30 22:30:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 134891 updates
2024-01-30 22:30:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint100.pt
2024-01-30 22:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint100.pt
2024-01-30 22:30:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/checkpoint100.pt (epoch 100 @ 134891 updates, score 2.157) (writing took 2.282351272064261 seconds)
2024-01-30 22:30:52 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2024-01-30 22:30:52 | INFO | train | epoch 100 | loss 1.795 | nll_loss 0.197 | ppl 1.15 | wps 62148 | ups 15.85 | wpb 3921.2 | bsz 214.5 | num_updates 134891 | lr 8.61011e-05 | gnorm 0.5 | loss_scale 8 | train_wall 78 | gb_free 29.8 | wall 8665
2024-01-30 22:30:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 22:30:52 | INFO | fairseq_cli.train | done training in 8659.8 seconds
Training complete.
Training complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 4504 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.en: 15529 sents, 225049 tokens, 0.00355% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.de: 15529 sents, 256398 tokens, 0.0215% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
Traceback (most recent call last):
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 176, in <module>
    main()
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 149, in main
    args.inputs = last_n_checkpoints(
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 94, in last_n_checkpoints
    raise Exception(
Exception: ('Found {} checkpoint files but need at least {}', 1, 5)
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/train_punctuation_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 4504 types
INFO:fairseq.tasks.translation:[de] dictionary: 6520 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Traceback (most recent call last):
  File "/home/kit/stud/uxude/miniconda3/envs/nmt/bin/fairseq-generate", line 8, in <module>
    sys.exit(cli_main())
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 413, in cli_main
    main(args)
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 50, in main
    return _main(cfg, sys.stdout)
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 96, in _main
    models, saved_cfg = checkpoint_utils.load_model_ensemble(
  File "/home/kit/stud/uxude/fairseq/fairseq/checkpoint_utils.py", line 392, in load_model_ensemble
    ensemble, args, _task = load_model_ensemble_and_task(
  File "/home/kit/stud/uxude/fairseq/fairseq/checkpoint_utils.py", line 448, in load_model_ensemble_and_task
    raise IOError("Model file not found: {}".format(filename))
OSError: Model file not found: /pfs/work7/workspace/scratch/uxude-MT/train/train_punctuation_covost/models/avg_last_5_checkpoint.pt
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/train_punctuation_covost/hyp.txt and /home/kit/stud/uxude/predictions/train_punctuation_covost/ref.txt
Sample predictions:
Sample:
Reference:
Sample:
Reference:
WER:
BLEU:
sacreBLEU: System and reference streams have different lengths.
Translation complete.

============================= JOB FEEDBACK =============================