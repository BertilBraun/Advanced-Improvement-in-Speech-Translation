(base) [uxude@uc2n995 train]$ cat finetune_mt_covost_cosine_23133165.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...

[notice] A new release of pip is available: 23.3.2 -> 24.0
[notice] To update, run: pip install --upgrade pip
fairseq                  0.12.2       /pfs/data5/home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[INFO] 17:02:56 [Dataset::Prepare Datasets]: Skipping dataset preparation, all config data already exists
Finetuning the MT model...
Binarizing the data...
Binarized data will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Training data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train
Validation data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train', validpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev', testpref=None, align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train.en: 289024 sents, 5282168 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev.en: 15520 sents, 288077 tokens, 0.00903% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 7560 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train.de: 289024 sents, 5563087 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 7560 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev.de: 15520 sents, 302966 tokens, 0.0152% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Binarization complete.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Training time: 6 hours
2024-02-09 17:04:47 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 30000, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 25, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='cosine', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=30000, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[4], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=25, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6000, warmup_init_lr=-1, min_lr=0.0, t_mult=1.0, lr_period_updates=-1, lr_shrink=0.1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 6000, 'warmup_init_lr': -1.0, 'lr': [0.001], 'min_lr': 0.0, 't_mult': 1.0, 'lr_period_updates': -1.0, 'lr_shrink': 0.1, 'max_update': 30000}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-02-09 17:04:47 | INFO | fairseq.tasks.translation | [en] dictionary: 6520 types
2024-02-09 17:04:47 | INFO | fairseq.tasks.translation | [de] dictionary: 7560 types
2024-02-09 17:04:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7560, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=7560, bias=False)
  )
)
2024-02-09 17:04:48 | INFO | fairseq_cli.train | task: TranslationTask
2024-02-09 17:04:48 | INFO | fairseq_cli.train | model: TransformerModel
2024-02-09 17:04:48 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-02-09 17:04:48 | INFO | fairseq_cli.train | num. shared model params: 51,347,456 (num. trained: 51,347,456)
2024-02-09 17:04:48 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-02-09 17:04:48 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.en
2024-02-09 17:04:48 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.de
2024-02-09 17:04:48 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset valid en-de 15520 examples
2024-02-09 17:04:48 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-02-09 17:04:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-09 17:04:48 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-02-09 17:04:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-09 17:04:48 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-02-09 17:04:48 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-02-09 17:04:48 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-02-09 17:04:48 | INFO | fairseq.trainer | No existing checkpoint found /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-02-09 17:04:48 | INFO | fairseq.trainer | loading train data for epoch 1
2024-02-09 17:04:48 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.en
2024-02-09 17:04:48 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.de
2024-02-09 17:04:48 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset train en-de 289024 examples
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-09 17:04:48 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-09 17:04:48 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-09 17:04:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:04:50 | INFO | fairseq.trainer | begin training epoch 1
2024-02-09 17:04:50 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-02-09 17:04:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2024-02-09 17:04:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-02-09 17:04:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-02-09 17:05:20 | INFO | train_inner | epoch 001:    103 / 369 loss=12.141, nll_loss=11.97, ppl=4012.2, wps=75381.2, ups=5, wpb=15090.4, bsz=774.9, num_updates=100, lr=1.66667e-05, gnorm=2.44, loss_scale=16, train_wall=29, gb_free=29.6, wall=32
2024-02-09 17:05:39 | INFO | train_inner | epoch 001:    203 / 369 loss=10.69, nll_loss=10.344, ppl=1299.55, wps=77115.7, ups=5.11, wpb=15085.9, bsz=781, num_updates=200, lr=3.33333e-05, gnorm=1.372, loss_scale=16, train_wall=19, gb_free=29.6, wall=51
2024-02-09 17:05:59 | INFO | train_inner | epoch 001:    303 / 369 loss=9.935, nll_loss=9.454, ppl=701.22, wps=77481.6, ups=5.13, wpb=15103.6, bsz=801.1, num_updates=300, lr=5e-05, gnorm=1.384, loss_scale=16, train_wall=19, gb_free=29.6, wall=71
2024-02-09 17:06:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:06:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:06:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.615 | nll_loss 9.035 | ppl 524.43 | wps 158860 | wpb 3189.1 | bsz 163.4 | num_updates 366
2024-02-09 17:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 366 updates
2024-02-09 17:06:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint1.pt
2024-02-09 17:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint1.pt
2024-02-09 17:06:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint1.pt (epoch 1 @ 366 updates, score 9.615) (writing took 3.5676933240611106 seconds)
2024-02-09 17:06:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-02-09 17:06:17 | INFO | train | epoch 001 | loss 10.714 | nll_loss 10.345 | ppl 1300.98 | wps 71360 | ups 4.73 | wpb 15076.3 | bsz 782.7 | num_updates 366 | lr 6.1e-05 | gnorm 1.666 | loss_scale 16 | train_wall 80 | gb_free 29.6 | wall 89
2024-02-09 17:06:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:06:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:06:17 | INFO | fairseq.trainer | begin training epoch 2
2024-02-09 17:06:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:06:24 | INFO | train_inner | epoch 002:     34 / 369 loss=9.725, nll_loss=9.188, ppl=583.16, wps=60016.2, ups=4, wpb=15013.1, bsz=778.1, num_updates=400, lr=6.66667e-05, gnorm=1.318, loss_scale=16, train_wall=19, gb_free=29.6, wall=96
2024-02-09 17:06:43 | INFO | train_inner | epoch 002:    134 / 369 loss=9.515, nll_loss=8.942, ppl=491.9, wps=77382.7, ups=5.1, wpb=15166.2, bsz=807.3, num_updates=500, lr=8.33333e-05, gnorm=1.243, loss_scale=16, train_wall=19, gb_free=29.5, wall=116
2024-02-09 17:07:03 | INFO | train_inner | epoch 002:    234 / 369 loss=9.286, nll_loss=8.678, ppl=409.72, wps=76827.6, ups=5.08, wpb=15119.5, bsz=764.1, num_updates=600, lr=0.0001, gnorm=1.202, loss_scale=16, train_wall=19, gb_free=29.6, wall=135
2024-02-09 17:07:23 | INFO | train_inner | epoch 002:    334 / 369 loss=8.934, nll_loss=8.27, ppl=308.61, wps=73834.5, ups=4.91, wpb=15047.8, bsz=776.9, num_updates=700, lr=0.000116667, gnorm=1.311, loss_scale=16, train_wall=20, gb_free=29.6, wall=156
2024-02-09 17:07:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:07:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:07:32 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.481 | nll_loss 7.723 | ppl 211.23 | wps 153687 | wpb 3189.1 | bsz 163.4 | num_updates 735 | best_loss 8.481
2024-02-09 17:07:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 735 updates
2024-02-09 17:07:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint2.pt
2024-02-09 17:07:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint2.pt
2024-02-09 17:07:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint2.pt (epoch 2 @ 735 updates, score 8.481) (writing took 4.494656600989401 seconds)
2024-02-09 17:07:37 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-02-09 17:07:37 | INFO | train | epoch 002 | loss 9.236 | nll_loss 8.619 | ppl 393.29 | wps 69442.1 | ups 4.61 | wpb 15076.1 | bsz 783.3 | num_updates 735 | lr 0.0001225 | gnorm 1.259 | loss_scale 16 | train_wall 73 | gb_free 29.6 | wall 169
2024-02-09 17:07:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:07:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:07:37 | INFO | fairseq.trainer | begin training epoch 3
2024-02-09 17:07:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:07:51 | INFO | train_inner | epoch 003:     65 / 369 loss=8.639, nll_loss=7.926, ppl=243.14, wps=53726.4, ups=3.63, wpb=14811.8, bsz=765.6, num_updates=800, lr=0.000133333, gnorm=1.283, loss_scale=16, train_wall=21, gb_free=29.7, wall=183
2024-02-09 17:08:10 | INFO | train_inner | epoch 003:    165 / 369 loss=8.34, nll_loss=7.579, ppl=191.15, wps=78072.9, ups=5.14, wpb=15187.8, bsz=801.7, num_updates=900, lr=0.00015, gnorm=1.244, loss_scale=16, train_wall=19, gb_free=29.6, wall=203
2024-02-09 17:08:30 | INFO | train_inner | epoch 003:    265 / 369 loss=8.122, nll_loss=7.323, ppl=160.15, wps=78183.5, ups=5.15, wpb=15195.9, bsz=778.2, num_updates=1000, lr=0.000166667, gnorm=1.358, loss_scale=16, train_wall=19, gb_free=29.6, wall=222
2024-02-09 17:08:49 | INFO | train_inner | epoch 003:    365 / 369 loss=7.836, nll_loss=6.992, ppl=127.31, wps=77299, ups=5.14, wpb=15047.1, bsz=785.8, num_updates=1100, lr=0.000183333, gnorm=1.243, loss_scale=16, train_wall=19, gb_free=29.6, wall=242
2024-02-09 17:08:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:08:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:08:52 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.456 | nll_loss 6.519 | ppl 91.7 | wps 166444 | wpb 3189.1 | bsz 163.4 | num_updates 1104 | best_loss 7.456
2024-02-09 17:08:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1104 updates
2024-02-09 17:08:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint3.pt
2024-02-09 17:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint3.pt
2024-02-09 17:08:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint3.pt (epoch 3 @ 1104 updates, score 7.456) (writing took 3.6577512610238045 seconds)
2024-02-09 17:08:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-02-09 17:08:55 | INFO | train | epoch 003 | loss 8.18 | nll_loss 7.391 | ppl 167.9 | wps 70872.4 | ups 4.7 | wpb 15076.1 | bsz 783.3 | num_updates 1104 | lr 0.000184 | gnorm 1.275 | loss_scale 16 | train_wall 72 | gb_free 29.6 | wall 248
2024-02-09 17:08:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:08:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:08:55 | INFO | fairseq.trainer | begin training epoch 4
2024-02-09 17:08:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:09:14 | INFO | train_inner | epoch 004:     96 / 369 loss=7.588, nll_loss=6.704, ppl=104.23, wps=60382, ups=3.99, wpb=15132, bsz=776.4, num_updates=1200, lr=0.0002, gnorm=1.326, loss_scale=16, train_wall=19, gb_free=29.6, wall=267
2024-02-09 17:09:34 | INFO | train_inner | epoch 004:    196 / 369 loss=7.318, nll_loss=6.391, ppl=83.92, wps=77700.6, ups=5.15, wpb=15099.3, bsz=752.2, num_updates=1300, lr=0.000216667, gnorm=1.251, loss_scale=16, train_wall=19, gb_free=29.6, wall=286
2024-02-09 17:09:53 | INFO | train_inner | epoch 004:    296 / 369 loss=6.965, nll_loss=5.984, ppl=63.29, wps=76819.6, ups=5.14, wpb=14945.4, bsz=804.2, num_updates=1400, lr=0.000233333, gnorm=1.229, loss_scale=16, train_wall=19, gb_free=29.6, wall=305
2024-02-09 17:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:10:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:10:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.431 | nll_loss 5.265 | ppl 38.45 | wps 163825 | wpb 3189.1 | bsz 163.4 | num_updates 1473 | best_loss 6.431
2024-02-09 17:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1473 updates
2024-02-09 17:10:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint4.pt
2024-02-09 17:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint4.pt
2024-02-09 17:10:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint4.pt (epoch 4 @ 1473 updates, score 6.431) (writing took 3.738956025103107 seconds)
2024-02-09 17:10:13 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-02-09 17:10:13 | INFO | train | epoch 004 | loss 7.186 | nll_loss 6.239 | ppl 75.53 | wps 71813.6 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 1473 | lr 0.0002455 | gnorm 1.268 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 325
2024-02-09 17:10:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:10:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:10:13 | INFO | fairseq.trainer | begin training epoch 5
2024-02-09 17:10:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:10:18 | INFO | train_inner | epoch 005:     27 / 369 loss=6.741, nll_loss=5.721, ppl=52.76, wps=60147.7, ups=3.98, wpb=15119.1, bsz=798.2, num_updates=1500, lr=0.00025, gnorm=1.265, loss_scale=16, train_wall=19, gb_free=29.5, wall=331
2024-02-09 17:10:38 | INFO | train_inner | epoch 005:    127 / 369 loss=6.477, nll_loss=5.414, ppl=42.65, wps=76971.5, ups=5.14, wpb=14973.8, bsz=789.4, num_updates=1600, lr=0.000266667, gnorm=1.193, loss_scale=16, train_wall=19, gb_free=29.6, wall=350
2024-02-09 17:10:57 | INFO | train_inner | epoch 005:    227 / 369 loss=6.246, nll_loss=5.144, ppl=35.35, wps=77672.1, ups=5.15, wpb=15089, bsz=767.4, num_updates=1700, lr=0.000283333, gnorm=1.175, loss_scale=16, train_wall=19, gb_free=29.6, wall=369
2024-02-09 17:11:17 | INFO | train_inner | epoch 005:    327 / 369 loss=5.961, nll_loss=4.811, ppl=28.07, wps=77582.3, ups=5.13, wpb=15116.5, bsz=801.6, num_updates=1800, lr=0.0003, gnorm=1.11, loss_scale=16, train_wall=19, gb_free=29.6, wall=389
2024-02-09 17:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:11:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:11:27 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.633 | nll_loss 4.277 | ppl 19.39 | wps 163565 | wpb 3189.1 | bsz 163.4 | num_updates 1842 | best_loss 5.633
2024-02-09 17:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1842 updates
2024-02-09 17:11:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint5.pt
2024-02-09 17:11:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint5.pt
2024-02-09 17:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint5.pt (epoch 5 @ 1842 updates, score 5.633) (writing took 3.739315403159708 seconds)
2024-02-09 17:11:30 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-02-09 17:11:30 | INFO | train | epoch 005 | loss 6.21 | nll_loss 5.102 | ppl 34.33 | wps 71823.8 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 1842 | lr 0.000307 | gnorm 1.163 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 403
2024-02-09 17:11:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:11:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:11:30 | INFO | fairseq.trainer | begin training epoch 6
2024-02-09 17:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:11:42 | INFO | train_inner | epoch 006:     58 / 369 loss=5.74, nll_loss=4.552, ppl=23.45, wps=60493.8, ups=3.99, wpb=15160.5, bsz=784.7, num_updates=1900, lr=0.000316667, gnorm=1.17, loss_scale=16, train_wall=19, gb_free=29.6, wall=414
2024-02-09 17:12:01 | INFO | train_inner | epoch 006:    158 / 369 loss=5.511, nll_loss=4.282, ppl=19.45, wps=78375.9, ups=5.16, wpb=15184.3, bsz=774, num_updates=2000, lr=0.000333333, gnorm=1.026, loss_scale=16, train_wall=19, gb_free=29.6, wall=433
2024-02-09 17:12:20 | INFO | train_inner | epoch 006:    258 / 369 loss=5.327, nll_loss=4.067, ppl=16.76, wps=77238.5, ups=5.16, wpb=14980.1, bsz=780.3, num_updates=2100, lr=0.00035, gnorm=1.083, loss_scale=16, train_wall=19, gb_free=29.5, wall=453
2024-02-09 17:12:40 | INFO | train_inner | epoch 006:    358 / 369 loss=5.161, nll_loss=3.871, ppl=14.63, wps=76930.1, ups=5.14, wpb=14974, bsz=792.9, num_updates=2200, lr=0.000366667, gnorm=1.033, loss_scale=16, train_wall=19, gb_free=29.5, wall=472
2024-02-09 17:12:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:12:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:12:44 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.863 | nll_loss 3.348 | ppl 10.18 | wps 163618 | wpb 3189.1 | bsz 163.4 | num_updates 2211 | best_loss 4.863
2024-02-09 17:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2211 updates
2024-02-09 17:12:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint6.pt
2024-02-09 17:12:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint6.pt
2024-02-09 17:12:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint6.pt (epoch 6 @ 2211 updates, score 4.863) (writing took 3.7544547468423843 seconds)
2024-02-09 17:12:48 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-02-09 17:12:48 | INFO | train | epoch 006 | loss 5.382 | nll_loss 4.131 | ppl 17.52 | wps 71985.9 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 2211 | lr 0.0003685 | gnorm 1.072 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 480
2024-02-09 17:12:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:12:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:12:48 | INFO | fairseq.trainer | begin training epoch 7
2024-02-09 17:12:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:13:05 | INFO | train_inner | epoch 007:     89 / 369 loss=4.937, nll_loss=3.612, ppl=12.23, wps=60815.1, ups=3.98, wpb=15265.7, bsz=790.3, num_updates=2300, lr=0.000383333, gnorm=0.951, loss_scale=16, train_wall=19, gb_free=29.5, wall=497
2024-02-09 17:13:24 | INFO | train_inner | epoch 007:    189 / 369 loss=4.854, nll_loss=3.513, ppl=11.41, wps=77917.8, ups=5.17, wpb=15083.3, bsz=785, num_updates=2400, lr=0.0004, gnorm=0.963, loss_scale=16, train_wall=19, gb_free=29.7, wall=517
2024-02-09 17:13:44 | INFO | train_inner | epoch 007:    289 / 369 loss=4.74, nll_loss=3.382, ppl=10.43, wps=77432, ups=5.17, wpb=14964.9, bsz=771.4, num_updates=2500, lr=0.000416667, gnorm=0.948, loss_scale=16, train_wall=19, gb_free=29.6, wall=536
2024-02-09 17:13:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:13:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:14:01 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.515 | nll_loss 2.937 | ppl 7.66 | wps 164108 | wpb 3189.1 | bsz 163.4 | num_updates 2580 | best_loss 4.515
2024-02-09 17:14:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2580 updates
2024-02-09 17:14:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint7.pt
2024-02-09 17:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint7.pt
2024-02-09 17:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint7.pt (epoch 7 @ 2580 updates, score 4.515) (writing took 3.6851425871718675 seconds)
2024-02-09 17:14:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-02-09 17:14:05 | INFO | train | epoch 007 | loss 4.792 | nll_loss 3.442 | ppl 10.87 | wps 72150.4 | ups 4.79 | wpb 15076.1 | bsz 783.3 | num_updates 2580 | lr 0.00043 | gnorm 0.924 | loss_scale 16 | train_wall 71 | gb_free 29.5 | wall 557
2024-02-09 17:14:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:14:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:14:05 | INFO | fairseq.trainer | begin training epoch 8
2024-02-09 17:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:14:09 | INFO | train_inner | epoch 008:     20 / 369 loss=4.61, nll_loss=3.232, ppl=9.4, wps=60173.3, ups=4, wpb=15055.8, bsz=775.4, num_updates=2600, lr=0.000433333, gnorm=0.845, loss_scale=16, train_wall=19, gb_free=29.6, wall=561
2024-02-09 17:14:28 | INFO | train_inner | epoch 008:    120 / 369 loss=4.483, nll_loss=3.085, ppl=8.48, wps=78144.2, ups=5.18, wpb=15086.9, bsz=772.4, num_updates=2700, lr=0.00045, gnorm=0.829, loss_scale=16, train_wall=19, gb_free=29.6, wall=580
2024-02-09 17:14:48 | INFO | train_inner | epoch 008:    220 / 369 loss=4.423, nll_loss=3.016, ppl=8.09, wps=77884.5, ups=5.15, wpb=15130.2, bsz=797.8, num_updates=2800, lr=0.000466667, gnorm=0.832, loss_scale=16, train_wall=19, gb_free=29.5, wall=600
2024-02-09 17:15:07 | INFO | train_inner | epoch 008:    320 / 369 loss=4.325, nll_loss=2.905, ppl=7.49, wps=77517.1, ups=5.16, wpb=15035.2, bsz=794.1, num_updates=2900, lr=0.000483333, gnorm=0.774, loss_scale=16, train_wall=19, gb_free=29.6, wall=619
2024-02-09 17:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:15:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:15:18 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.258 | nll_loss 2.647 | ppl 6.26 | wps 165957 | wpb 3189.1 | bsz 163.4 | num_updates 2949 | best_loss 4.258
2024-02-09 17:15:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 2949 updates
2024-02-09 17:15:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint8.pt
2024-02-09 17:15:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint8.pt
2024-02-09 17:15:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint8.pt (epoch 8 @ 2949 updates, score 4.258) (writing took 3.7381802301388234 seconds)
2024-02-09 17:15:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-02-09 17:15:22 | INFO | train | epoch 008 | loss 4.406 | nll_loss 2.998 | ppl 7.99 | wps 72094.8 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 2949 | lr 0.0004915 | gnorm 0.815 | loss_scale 16 | train_wall 71 | gb_free 29.5 | wall 634
2024-02-09 17:15:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:15:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:15:22 | INFO | fairseq.trainer | begin training epoch 9
2024-02-09 17:15:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:15:32 | INFO | train_inner | epoch 009:     51 / 369 loss=4.254, nll_loss=2.825, ppl=7.08, wps=59777.3, ups=3.99, wpb=14988.4, bsz=785.5, num_updates=3000, lr=0.0005, gnorm=0.78, loss_scale=16, train_wall=19, gb_free=29.6, wall=644
2024-02-09 17:15:51 | INFO | train_inner | epoch 009:    151 / 369 loss=4.166, nll_loss=2.723, ppl=6.6, wps=77906.2, ups=5.15, wpb=15128.5, bsz=766.6, num_updates=3100, lr=0.000516667, gnorm=0.732, loss_scale=16, train_wall=19, gb_free=29.6, wall=664
2024-02-09 17:16:11 | INFO | train_inner | epoch 009:    251 / 369 loss=4.125, nll_loss=2.678, ppl=6.4, wps=76909.6, ups=5.13, wpb=14990, bsz=778.6, num_updates=3200, lr=0.000533333, gnorm=0.717, loss_scale=16, train_wall=19, gb_free=29.6, wall=683
2024-02-09 17:16:30 | INFO | train_inner | epoch 009:    351 / 369 loss=4.063, nll_loss=2.609, ppl=6.1, wps=78126, ups=5.14, wpb=15211.6, bsz=797.7, num_updates=3300, lr=0.00055, gnorm=0.7, loss_scale=16, train_wall=19, gb_free=29.6, wall=703
2024-02-09 17:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:16:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:16:36 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.094 | nll_loss 2.468 | ppl 5.53 | wps 168019 | wpb 3189.1 | bsz 163.4 | num_updates 3318 | best_loss 4.094
2024-02-09 17:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3318 updates
2024-02-09 17:16:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint9.pt
2024-02-09 17:16:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint9.pt
2024-02-09 17:16:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint9.pt (epoch 9 @ 3318 updates, score 4.094) (writing took 3.633296526968479 seconds)
2024-02-09 17:16:39 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-02-09 17:16:39 | INFO | train | epoch 009 | loss 4.122 | nll_loss 2.674 | ppl 6.38 | wps 71945.1 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 3318 | lr 0.000553 | gnorm 0.723 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 712
2024-02-09 17:16:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:16:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:16:39 | INFO | fairseq.trainer | begin training epoch 10
2024-02-09 17:16:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:16:55 | INFO | train_inner | epoch 010:     82 / 369 loss=3.951, nll_loss=2.481, ppl=5.58, wps=60164.2, ups=4.01, wpb=14994.8, bsz=786.1, num_updates=3400, lr=0.000566667, gnorm=0.712, loss_scale=16, train_wall=19, gb_free=29.6, wall=728
2024-02-09 17:17:15 | INFO | train_inner | epoch 010:    182 / 369 loss=3.942, nll_loss=2.471, ppl=5.54, wps=78122, ups=5.17, wpb=15111.8, bsz=760.3, num_updates=3500, lr=0.000583333, gnorm=0.689, loss_scale=16, train_wall=19, gb_free=29.6, wall=747
2024-02-09 17:17:34 | INFO | train_inner | epoch 010:    282 / 369 loss=3.913, nll_loss=2.439, ppl=5.42, wps=77447, ups=5.14, wpb=15056.9, bsz=798.8, num_updates=3600, lr=0.0006, gnorm=0.678, loss_scale=16, train_wall=19, gb_free=29.6, wall=766
2024-02-09 17:17:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:17:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:17:53 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.945 | nll_loss 2.298 | ppl 4.92 | wps 164212 | wpb 3189.1 | bsz 163.4 | num_updates 3687 | best_loss 3.945
2024-02-09 17:17:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3687 updates
2024-02-09 17:17:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint10.pt
2024-02-09 17:17:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint10.pt
2024-02-09 17:17:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint10.pt (epoch 10 @ 3687 updates, score 3.945) (writing took 3.664622263982892 seconds)
2024-02-09 17:17:56 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-02-09 17:17:56 | INFO | train | epoch 010 | loss 3.921 | nll_loss 2.448 | ppl 5.46 | wps 72106.5 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 3687 | lr 0.0006145 | gnorm 0.692 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 789
2024-02-09 17:17:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:17:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:17:56 | INFO | fairseq.trainer | begin training epoch 11
2024-02-09 17:17:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:17:59 | INFO | train_inner | epoch 011:     13 / 369 loss=3.878, nll_loss=2.401, ppl=5.28, wps=60241.4, ups=4, wpb=15055, bsz=786.1, num_updates=3700, lr=0.000616667, gnorm=0.692, loss_scale=16, train_wall=19, gb_free=29.6, wall=791
2024-02-09 17:18:18 | INFO | train_inner | epoch 011:    113 / 369 loss=3.784, nll_loss=2.293, ppl=4.9, wps=78096.5, ups=5.17, wpb=15096.7, bsz=770.6, num_updates=3800, lr=0.000633333, gnorm=0.641, loss_scale=16, train_wall=19, gb_free=29.5, wall=811
2024-02-09 17:18:38 | INFO | train_inner | epoch 011:    213 / 369 loss=3.763, nll_loss=2.27, ppl=4.82, wps=78091.6, ups=5.14, wpb=15192.1, bsz=789.1, num_updates=3900, lr=0.00065, gnorm=0.65, loss_scale=16, train_wall=19, gb_free=29.6, wall=830
2024-02-09 17:18:57 | INFO | train_inner | epoch 011:    313 / 369 loss=3.766, nll_loss=2.275, ppl=4.84, wps=77312.4, ups=5.14, wpb=15038.7, bsz=803.1, num_updates=4000, lr=0.000666667, gnorm=0.678, loss_scale=16, train_wall=19, gb_free=29.6, wall=850
2024-02-09 17:19:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:19:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:19:10 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.84 | nll_loss 2.191 | ppl 4.57 | wps 164557 | wpb 3189.1 | bsz 163.4 | num_updates 4056 | best_loss 3.84
2024-02-09 17:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4056 updates
2024-02-09 17:19:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint11.pt
2024-02-09 17:19:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint11.pt
2024-02-09 17:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint11.pt (epoch 11 @ 4056 updates, score 3.84) (writing took 3.6805559310596436 seconds)
2024-02-09 17:19:14 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-02-09 17:19:14 | INFO | train | epoch 011 | loss 3.765 | nll_loss 2.273 | ppl 4.83 | wps 72065 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 4056 | lr 0.000676 | gnorm 0.652 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 866
2024-02-09 17:19:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:19:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:19:14 | INFO | fairseq.trainer | begin training epoch 12
2024-02-09 17:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:19:22 | INFO | train_inner | epoch 012:     44 / 369 loss=3.703, nll_loss=2.204, ppl=4.61, wps=60465, ups=4.02, wpb=15052.5, bsz=750.6, num_updates=4100, lr=0.000683333, gnorm=0.633, loss_scale=32, train_wall=19, gb_free=29.6, wall=875
2024-02-09 17:19:42 | INFO | train_inner | epoch 012:    144 / 369 loss=3.627, nll_loss=2.118, ppl=4.34, wps=77015.7, ups=5.15, wpb=14954.6, bsz=819.5, num_updates=4200, lr=0.0007, gnorm=0.661, loss_scale=32, train_wall=19, gb_free=29.6, wall=894
2024-02-09 17:20:01 | INFO | train_inner | epoch 012:    244 / 369 loss=3.654, nll_loss=2.149, ppl=4.43, wps=77722.6, ups=5.16, wpb=15072.3, bsz=781.4, num_updates=4300, lr=0.000716667, gnorm=0.65, loss_scale=32, train_wall=19, gb_free=29.6, wall=913
2024-02-09 17:20:20 | INFO | train_inner | epoch 012:    344 / 369 loss=3.66, nll_loss=2.158, ppl=4.46, wps=78031.5, ups=5.14, wpb=15190.8, bsz=774.3, num_updates=4400, lr=0.000733333, gnorm=0.644, loss_scale=32, train_wall=19, gb_free=29.5, wall=933
2024-02-09 17:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:20:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:20:27 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.81 | nll_loss 2.15 | ppl 4.44 | wps 163910 | wpb 3189.1 | bsz 163.4 | num_updates 4425 | best_loss 3.81
2024-02-09 17:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4425 updates
2024-02-09 17:20:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint12.pt
2024-02-09 17:20:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint12.pt
2024-02-09 17:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint12.pt (epoch 12 @ 4425 updates, score 3.81) (writing took 3.723350680898875 seconds)
2024-02-09 17:20:31 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-02-09 17:20:31 | INFO | train | epoch 012 | loss 3.65 | nll_loss 2.145 | ppl 4.42 | wps 72058.7 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 4425 | lr 0.0007375 | gnorm 0.651 | loss_scale 32 | train_wall 71 | gb_free 29.6 | wall 943
2024-02-09 17:20:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:20:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:20:31 | INFO | fairseq.trainer | begin training epoch 13
2024-02-09 17:20:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:20:45 | INFO | train_inner | epoch 013:     75 / 369 loss=3.569, nll_loss=2.053, ppl=4.15, wps=60124.7, ups=4, wpb=15036.3, bsz=753.8, num_updates=4500, lr=0.00075, gnorm=0.633, loss_scale=32, train_wall=19, gb_free=29.6, wall=958
2024-02-09 17:21:05 | INFO | train_inner | epoch 013:    175 / 369 loss=3.572, nll_loss=2.058, ppl=4.16, wps=77657.1, ups=5.15, wpb=15074.8, bsz=781.7, num_updates=4600, lr=0.000766667, gnorm=0.644, loss_scale=32, train_wall=19, gb_free=29.5, wall=977
2024-02-09 17:21:24 | INFO | train_inner | epoch 013:    275 / 369 loss=3.572, nll_loss=2.058, ppl=4.16, wps=78272.2, ups=5.19, wpb=15087.5, bsz=784.3, num_updates=4700, lr=0.000783333, gnorm=0.649, loss_scale=32, train_wall=19, gb_free=29.6, wall=996
2024-02-09 17:21:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:21:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:21:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.744 | nll_loss 2.083 | ppl 4.24 | wps 161731 | wpb 3189.1 | bsz 163.4 | num_updates 4794 | best_loss 3.744
2024-02-09 17:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 4794 updates
2024-02-09 17:21:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint13.pt
2024-02-09 17:21:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint13.pt
2024-02-09 17:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint13.pt (epoch 13 @ 4794 updates, score 3.744) (writing took 3.6749757619109005 seconds)
2024-02-09 17:21:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-02-09 17:21:48 | INFO | train | epoch 013 | loss 3.562 | nll_loss 2.047 | ppl 4.13 | wps 72104.7 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 4794 | lr 0.000799 | gnorm 0.645 | loss_scale 32 | train_wall 71 | gb_free 29.5 | wall 1020
2024-02-09 17:21:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:21:48 | INFO | fairseq.trainer | begin training epoch 14
2024-02-09 17:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:21:49 | INFO | train_inner | epoch 014:      6 / 369 loss=3.559, nll_loss=2.046, ppl=4.13, wps=59933.8, ups=3.99, wpb=15014.5, bsz=796.3, num_updates=4800, lr=0.0008, gnorm=0.654, loss_scale=32, train_wall=19, gb_free=29.6, wall=1022
2024-02-09 17:22:09 | INFO | train_inner | epoch 014:    106 / 369 loss=3.467, nll_loss=1.939, ppl=3.83, wps=77251.2, ups=5.15, wpb=14989.6, bsz=777.9, num_updates=4900, lr=0.000816667, gnorm=0.65, loss_scale=32, train_wall=19, gb_free=29.6, wall=1041
2024-02-09 17:22:28 | INFO | train_inner | epoch 014:    206 / 369 loss=3.488, nll_loss=1.965, ppl=3.9, wps=77642.3, ups=5.15, wpb=15077.6, bsz=796.8, num_updates=5000, lr=0.000833333, gnorm=0.647, loss_scale=32, train_wall=19, gb_free=29.6, wall=1060
2024-02-09 17:22:47 | INFO | train_inner | epoch 014:    306 / 369 loss=3.522, nll_loss=2.005, ppl=4.01, wps=77794.5, ups=5.15, wpb=15108.1, bsz=777.3, num_updates=5100, lr=0.00085, gnorm=0.667, loss_scale=32, train_wall=19, gb_free=29.6, wall=1080
2024-02-09 17:23:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:23:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:23:01 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.701 | nll_loss 2.039 | ppl 4.11 | wps 164902 | wpb 3189.1 | bsz 163.4 | num_updates 5163 | best_loss 3.701
2024-02-09 17:23:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5163 updates
2024-02-09 17:23:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint14.pt
2024-02-09 17:23:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint14.pt
2024-02-09 17:23:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint14.pt (epoch 14 @ 5163 updates, score 3.701) (writing took 3.8262247699312866 seconds)
2024-02-09 17:23:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-02-09 17:23:05 | INFO | train | epoch 014 | loss 3.496 | nll_loss 1.974 | ppl 3.93 | wps 71909.3 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 5163 | lr 0.0008605 | gnorm 0.651 | loss_scale 32 | train_wall 71 | gb_free 29.5 | wall 1098
2024-02-09 17:23:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:23:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:23:05 | INFO | fairseq.trainer | begin training epoch 15
2024-02-09 17:23:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:23:13 | INFO | train_inner | epoch 015:     37 / 369 loss=3.459, nll_loss=1.934, ppl=3.82, wps=60467.3, ups=3.97, wpb=15248.4, bsz=793.4, num_updates=5200, lr=0.000866667, gnorm=0.634, loss_scale=32, train_wall=19, gb_free=29.6, wall=1105
2024-02-09 17:23:32 | INFO | train_inner | epoch 015:    137 / 369 loss=3.434, nll_loss=1.905, ppl=3.75, wps=76887.7, ups=5.16, wpb=14901.8, bsz=775.2, num_updates=5300, lr=0.000883333, gnorm=0.66, loss_scale=32, train_wall=19, gb_free=29.6, wall=1124
2024-02-09 17:23:51 | INFO | train_inner | epoch 015:    237 / 369 loss=3.442, nll_loss=1.914, ppl=3.77, wps=78186.9, ups=5.15, wpb=15185, bsz=792.5, num_updates=5400, lr=0.0009, gnorm=0.653, loss_scale=32, train_wall=19, gb_free=29.6, wall=1144
2024-02-09 17:24:11 | INFO | train_inner | epoch 015:    337 / 369 loss=3.47, nll_loss=1.948, ppl=3.86, wps=77890.5, ups=5.16, wpb=15095.7, bsz=775.9, num_updates=5500, lr=0.000916667, gnorm=0.661, loss_scale=32, train_wall=19, gb_free=29.6, wall=1163
2024-02-09 17:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:24:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:24:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.673 | nll_loss 2.021 | ppl 4.06 | wps 164183 | wpb 3189.1 | bsz 163.4 | num_updates 5532 | best_loss 3.673
2024-02-09 17:24:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5532 updates
2024-02-09 17:24:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint15.pt
2024-02-09 17:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint15.pt
2024-02-09 17:24:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint15.pt (epoch 15 @ 5532 updates, score 3.673) (writing took 3.7000691699795425 seconds)
2024-02-09 17:24:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-02-09 17:24:23 | INFO | train | epoch 015 | loss 3.444 | nll_loss 1.918 | ppl 3.78 | wps 72018.6 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 5532 | lr 0.000922 | gnorm 0.662 | loss_scale 32 | train_wall 71 | gb_free 29.6 | wall 1175
2024-02-09 17:24:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:24:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:24:23 | INFO | fairseq.trainer | begin training epoch 16
2024-02-09 17:24:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:24:36 | INFO | train_inner | epoch 016:     68 / 369 loss=3.401, nll_loss=1.869, ppl=3.65, wps=60089.3, ups=3.99, wpb=15045.7, bsz=790.3, num_updates=5600, lr=0.000933333, gnorm=0.682, loss_scale=32, train_wall=19, gb_free=29.6, wall=1188
2024-02-09 17:24:55 | INFO | train_inner | epoch 016:    168 / 369 loss=3.385, nll_loss=1.851, ppl=3.61, wps=77534.1, ups=5.15, wpb=15044, bsz=796.2, num_updates=5700, lr=0.00095, gnorm=0.658, loss_scale=32, train_wall=19, gb_free=29.6, wall=1208
2024-02-09 17:25:15 | INFO | train_inner | epoch 016:    268 / 369 loss=3.417, nll_loss=1.889, ppl=3.7, wps=78085, ups=5.14, wpb=15201.9, bsz=785.1, num_updates=5800, lr=0.000966667, gnorm=0.685, loss_scale=32, train_wall=19, gb_free=29.6, wall=1227
2024-02-09 17:25:34 | INFO | train_inner | epoch 016:    368 / 369 loss=3.441, nll_loss=1.917, ppl=3.78, wps=77614.8, ups=5.17, wpb=15016.4, bsz=764.4, num_updates=5900, lr=0.000983333, gnorm=0.684, loss_scale=32, train_wall=19, gb_free=29.5, wall=1246
2024-02-09 17:25:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:25:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:25:36 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.649 | nll_loss 1.985 | ppl 3.96 | wps 164388 | wpb 3189.1 | bsz 163.4 | num_updates 5901 | best_loss 3.649
2024-02-09 17:25:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 5901 updates
2024-02-09 17:25:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint16.pt
2024-02-09 17:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint16.pt
2024-02-09 17:25:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint16.pt (epoch 16 @ 5901 updates, score 3.649) (writing took 3.7145740140695125 seconds)
2024-02-09 17:25:40 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-02-09 17:25:40 | INFO | train | epoch 016 | loss 3.405 | nll_loss 1.874 | ppl 3.67 | wps 71962.2 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 5901 | lr 0.0009835 | gnorm 0.672 | loss_scale 32 | train_wall 71 | gb_free 29.6 | wall 1252
2024-02-09 17:25:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:25:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:25:40 | INFO | fairseq.trainer | begin training epoch 17
2024-02-09 17:25:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:25:59 | INFO | train_inner | epoch 017:     99 / 369 loss=3.329, nll_loss=1.787, ppl=3.45, wps=60478.6, ups=3.99, wpb=15144.4, bsz=773.8, num_updates=6000, lr=0.001, gnorm=0.667, loss_scale=32, train_wall=19, gb_free=29.6, wall=1271
2024-02-09 17:26:19 | INFO | train_inner | epoch 017:    199 / 369 loss=3.357, nll_loss=1.821, ppl=3.53, wps=77618, ups=5.13, wpb=15118.3, bsz=805.4, num_updates=6100, lr=0.000999957, gnorm=0.678, loss_scale=32, train_wall=19, gb_free=29.6, wall=1291
2024-02-09 17:26:38 | INFO | train_inner | epoch 017:    299 / 369 loss=3.395, nll_loss=1.866, ppl=3.64, wps=77086.4, ups=5.16, wpb=14946.9, bsz=784.8, num_updates=6200, lr=0.000999829, gnorm=0.696, loss_scale=32, train_wall=19, gb_free=29.6, wall=1310
2024-02-09 17:26:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:26:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:26:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.648 | nll_loss 1.977 | ppl 3.94 | wps 164474 | wpb 3189.1 | bsz 163.4 | num_updates 6270 | best_loss 3.648
2024-02-09 17:26:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6270 updates
2024-02-09 17:26:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint17.pt
2024-02-09 17:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint17.pt
2024-02-09 17:26:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint17.pt (epoch 17 @ 6270 updates, score 3.648) (writing took 3.68090332695283 seconds)
2024-02-09 17:26:57 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-02-09 17:26:57 | INFO | train | epoch 017 | loss 3.371 | nll_loss 1.838 | ppl 3.57 | wps 72103.4 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 6270 | lr 0.000999688 | gnorm 0.679 | loss_scale 32 | train_wall 71 | gb_free 29.6 | wall 1329
2024-02-09 17:26:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:26:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:26:57 | INFO | fairseq.trainer | begin training epoch 18
2024-02-09 17:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:27:03 | INFO | train_inner | epoch 018:     30 / 369 loss=3.387, nll_loss=1.857, ppl=3.62, wps=60741.8, ups=4.02, wpb=15109.8, bsz=746.5, num_updates=6300, lr=0.000999615, gnorm=0.669, loss_scale=32, train_wall=19, gb_free=29.6, wall=1335
2024-02-09 17:27:22 | INFO | train_inner | epoch 018:    130 / 369 loss=3.298, nll_loss=1.755, ppl=3.38, wps=77427.5, ups=5.17, wpb=14981.1, bsz=771, num_updates=6400, lr=0.000999315, gnorm=0.67, loss_scale=32, train_wall=19, gb_free=29.5, wall=1355
2024-02-09 17:27:42 | INFO | train_inner | epoch 018:    230 / 369 loss=3.316, nll_loss=1.777, ppl=3.43, wps=78312.9, ups=5.16, wpb=15183, bsz=802.2, num_updates=6500, lr=0.000998929, gnorm=0.662, loss_scale=32, train_wall=19, gb_free=29.6, wall=1374
2024-02-09 17:28:01 | INFO | train_inner | epoch 018:    330 / 369 loss=3.344, nll_loss=1.811, ppl=3.51, wps=77708.8, ups=5.17, wpb=15029, bsz=797.6, num_updates=6600, lr=0.000998459, gnorm=0.677, loss_scale=32, train_wall=19, gb_free=29.6, wall=1393
2024-02-09 17:28:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:28:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:28:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.584 | nll_loss 1.933 | ppl 3.82 | wps 163883 | wpb 3189.1 | bsz 163.4 | num_updates 6639 | best_loss 3.584
2024-02-09 17:28:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 6639 updates
2024-02-09 17:28:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint18.pt
2024-02-09 17:28:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint18.pt
2024-02-09 17:28:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint18.pt (epoch 18 @ 6639 updates, score 3.584) (writing took 3.7875028220005333 seconds)
2024-02-09 17:28:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-02-09 17:28:14 | INFO | train | epoch 018 | loss 3.32 | nll_loss 1.783 | ppl 3.44 | wps 72160.3 | ups 4.79 | wpb 15076.1 | bsz 783.3 | num_updates 6639 | lr 0.000998252 | gnorm 0.666 | loss_scale 32 | train_wall 71 | gb_free 29.6 | wall 1406
2024-02-09 17:28:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:28:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:28:14 | INFO | fairseq.trainer | begin training epoch 19
2024-02-09 17:28:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:28:26 | INFO | train_inner | epoch 019:     61 / 369 loss=3.256, nll_loss=1.71, ppl=3.27, wps=60567.8, ups=3.98, wpb=15205.6, bsz=786.2, num_updates=6700, lr=0.000997902, gnorm=0.636, loss_scale=32, train_wall=19, gb_free=29.6, wall=1418
2024-02-09 17:28:45 | INFO | train_inner | epoch 019:    161 / 369 loss=3.259, nll_loss=1.713, ppl=3.28, wps=77419, ups=5.17, wpb=14971, bsz=791.4, num_updates=6800, lr=0.000997261, gnorm=0.671, loss_scale=32, train_wall=19, gb_free=29.5, wall=1438
2024-02-09 17:29:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-02-09 17:29:05 | INFO | train_inner | epoch 019:    262 / 369 loss=3.293, nll_loss=1.753, ppl=3.37, wps=77279.8, ups=5.1, wpb=15156.9, bsz=786.7, num_updates=6900, lr=0.000996534, gnorm=0.666, loss_scale=16, train_wall=19, gb_free=29.6, wall=1457
2024-02-09 17:29:24 | INFO | train_inner | epoch 019:    362 / 369 loss=3.3, nll_loss=1.763, ppl=3.39, wps=77389.7, ups=5.15, wpb=15017.4, bsz=781.2, num_updates=7000, lr=0.000995722, gnorm=0.664, loss_scale=16, train_wall=19, gb_free=29.6, wall=1477
2024-02-09 17:29:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:29:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:29:28 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.573 | nll_loss 1.911 | ppl 3.76 | wps 167014 | wpb 3189.1 | bsz 163.4 | num_updates 7007 | best_loss 3.573
2024-02-09 17:29:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7007 updates
2024-02-09 17:29:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint19.pt
2024-02-09 17:29:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint19.pt
2024-02-09 17:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint19.pt (epoch 19 @ 7007 updates, score 3.573) (writing took 3.668433970073238 seconds)
2024-02-09 17:29:31 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-02-09 17:29:31 | INFO | train | epoch 019 | loss 3.272 | nll_loss 1.729 | ppl 3.32 | wps 71950.2 | ups 4.77 | wpb 15078 | bsz 784 | num_updates 7007 | lr 0.000995662 | gnorm 0.661 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 1484
2024-02-09 17:29:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:29:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:29:31 | INFO | fairseq.trainer | begin training epoch 20
2024-02-09 17:29:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:29:49 | INFO | train_inner | epoch 020:     93 / 369 loss=3.195, nll_loss=1.641, ppl=3.12, wps=60589.9, ups=4.01, wpb=15121.4, bsz=781.8, num_updates=7100, lr=0.000994826, gnorm=0.632, loss_scale=16, train_wall=19, gb_free=29.6, wall=1502
2024-02-09 17:30:09 | INFO | train_inner | epoch 020:    193 / 369 loss=3.236, nll_loss=1.689, ppl=3.22, wps=78006.9, ups=5.17, wpb=15086.1, bsz=769.2, num_updates=7200, lr=0.000993844, gnorm=0.649, loss_scale=16, train_wall=19, gb_free=29.5, wall=1521
2024-02-09 17:30:28 | INFO | train_inner | epoch 020:    293 / 369 loss=3.232, nll_loss=1.686, ppl=3.22, wps=77209.1, ups=5.14, wpb=15018.2, bsz=797.2, num_updates=7300, lr=0.000992778, gnorm=0.642, loss_scale=16, train_wall=19, gb_free=29.6, wall=1541
2024-02-09 17:30:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:30:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:30:45 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.531 | nll_loss 1.878 | ppl 3.68 | wps 162524 | wpb 3189.1 | bsz 163.4 | num_updates 7376 | best_loss 3.531
2024-02-09 17:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7376 updates
2024-02-09 17:30:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint20.pt
2024-02-09 17:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint20.pt
2024-02-09 17:30:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint20.pt (epoch 20 @ 7376 updates, score 3.531) (writing took 3.6917154758702964 seconds)
2024-02-09 17:30:49 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-02-09 17:30:49 | INFO | train | epoch 020 | loss 3.226 | nll_loss 1.679 | ppl 3.2 | wps 71935.6 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 7376 | lr 0.000991911 | gnorm 0.645 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 1561
2024-02-09 17:30:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:30:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:30:49 | INFO | fairseq.trainer | begin training epoch 21
2024-02-09 17:30:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:30:53 | INFO | train_inner | epoch 021:     24 / 369 loss=3.231, nll_loss=1.686, ppl=3.22, wps=59579.7, ups=3.98, wpb=14981.6, bsz=775.7, num_updates=7400, lr=0.000991627, gnorm=0.656, loss_scale=16, train_wall=19, gb_free=29.6, wall=1566
2024-02-09 17:31:13 | INFO | train_inner | epoch 021:    124 / 369 loss=3.148, nll_loss=1.589, ppl=3.01, wps=78479.3, ups=5.15, wpb=15229.6, bsz=781.3, num_updates=7500, lr=0.000990393, gnorm=0.614, loss_scale=16, train_wall=19, gb_free=29.6, wall=1585
2024-02-09 17:31:32 | INFO | train_inner | epoch 021:    224 / 369 loss=3.195, nll_loss=1.645, ppl=3.13, wps=76728.1, ups=5.12, wpb=14981.2, bsz=783.4, num_updates=7600, lr=0.000989074, gnorm=0.641, loss_scale=16, train_wall=19, gb_free=29.6, wall=1605
2024-02-09 17:31:52 | INFO | train_inner | epoch 021:    324 / 369 loss=3.204, nll_loss=1.657, ppl=3.15, wps=77404.4, ups=5.14, wpb=15053.8, bsz=790.2, num_updates=7700, lr=0.000987671, gnorm=0.646, loss_scale=16, train_wall=19, gb_free=29.6, wall=1624
2024-02-09 17:32:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:32:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:32:02 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.512 | nll_loss 1.857 | ppl 3.62 | wps 162701 | wpb 3189.1 | bsz 163.4 | num_updates 7745 | best_loss 3.512
2024-02-09 17:32:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 7745 updates
2024-02-09 17:32:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint21.pt
2024-02-09 17:32:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint21.pt
2024-02-09 17:32:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint21.pt (epoch 21 @ 7745 updates, score 3.512) (writing took 3.718707816908136 seconds)
2024-02-09 17:32:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-02-09 17:32:06 | INFO | train | epoch 021 | loss 3.185 | nll_loss 1.633 | ppl 3.1 | wps 71822.7 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 7745 | lr 0.000987013 | gnorm 0.634 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 1638
2024-02-09 17:32:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:32:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:32:06 | INFO | fairseq.trainer | begin training epoch 22
2024-02-09 17:32:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:32:17 | INFO | train_inner | epoch 022:     55 / 369 loss=3.164, nll_loss=1.609, ppl=3.05, wps=60266.9, ups=3.99, wpb=15102.2, bsz=771.7, num_updates=7800, lr=0.000986185, gnorm=0.618, loss_scale=16, train_wall=19, gb_free=29.6, wall=1649
2024-02-09 17:32:36 | INFO | train_inner | epoch 022:    155 / 369 loss=3.121, nll_loss=1.561, ppl=2.95, wps=77700.4, ups=5.14, wpb=15113.4, bsz=796.9, num_updates=7900, lr=0.000984615, gnorm=0.625, loss_scale=16, train_wall=19, gb_free=29.6, wall=1669
2024-02-09 17:32:56 | INFO | train_inner | epoch 022:    255 / 369 loss=3.169, nll_loss=1.618, ppl=3.07, wps=77128.9, ups=5.15, wpb=14989.3, bsz=769.8, num_updates=8000, lr=0.000982963, gnorm=0.642, loss_scale=16, train_wall=19, gb_free=29.6, wall=1688
2024-02-09 17:33:15 | INFO | train_inner | epoch 022:    355 / 369 loss=3.175, nll_loss=1.624, ppl=3.08, wps=77692.1, ups=5.15, wpb=15091.6, bsz=782, num_updates=8100, lr=0.000981228, gnorm=0.625, loss_scale=16, train_wall=19, gb_free=29.6, wall=1707
2024-02-09 17:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:33:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:33:20 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.509 | nll_loss 1.854 | ppl 3.61 | wps 165676 | wpb 3189.1 | bsz 163.4 | num_updates 8114 | best_loss 3.509
2024-02-09 17:33:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8114 updates
2024-02-09 17:33:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint22.pt
2024-02-09 17:33:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint22.pt
2024-02-09 17:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint22.pt (epoch 22 @ 8114 updates, score 3.509) (writing took 3.6884477681014687 seconds)
2024-02-09 17:33:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-02-09 17:33:23 | INFO | train | epoch 022 | loss 3.149 | nll_loss 1.594 | ppl 3.02 | wps 71975.5 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 8114 | lr 0.000980978 | gnorm 0.625 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 1716
2024-02-09 17:33:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:33:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:33:23 | INFO | fairseq.trainer | begin training epoch 23
2024-02-09 17:33:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:33:40 | INFO | train_inner | epoch 023:     86 / 369 loss=3.073, nll_loss=1.507, ppl=2.84, wps=61084.8, ups=3.99, wpb=15296.5, bsz=797.7, num_updates=8200, lr=0.00097941, gnorm=0.587, loss_scale=16, train_wall=19, gb_free=29.6, wall=1732
2024-02-09 17:34:00 | INFO | train_inner | epoch 023:    186 / 369 loss=3.116, nll_loss=1.557, ppl=2.94, wps=77400.3, ups=5.15, wpb=15024.2, bsz=778, num_updates=8300, lr=0.00097751, gnorm=0.622, loss_scale=16, train_wall=19, gb_free=29.6, wall=1752
2024-02-09 17:34:19 | INFO | train_inner | epoch 023:    286 / 369 loss=3.14, nll_loss=1.586, ppl=3, wps=77513.7, ups=5.17, wpb=14984.7, bsz=784.9, num_updates=8400, lr=0.000975528, gnorm=0.628, loss_scale=16, train_wall=19, gb_free=29.5, wall=1771
2024-02-09 17:34:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:34:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:34:37 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.481 | nll_loss 1.828 | ppl 3.55 | wps 162821 | wpb 3189.1 | bsz 163.4 | num_updates 8483 | best_loss 3.481
2024-02-09 17:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8483 updates
2024-02-09 17:34:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint23.pt
2024-02-09 17:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint23.pt
2024-02-09 17:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint23.pt (epoch 23 @ 8483 updates, score 3.481) (writing took 3.790506476070732 seconds)
2024-02-09 17:34:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-02-09 17:34:41 | INFO | train | epoch 023 | loss 3.112 | nll_loss 1.554 | ppl 2.94 | wps 71986.6 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 8483 | lr 0.000973822 | gnorm 0.611 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 1793
2024-02-09 17:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:34:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:34:41 | INFO | fairseq.trainer | begin training epoch 24
2024-02-09 17:34:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:34:44 | INFO | train_inner | epoch 024:     17 / 369 loss=3.109, nll_loss=1.551, ppl=2.93, wps=59614.2, ups=3.97, wpb=15028.2, bsz=787.3, num_updates=8500, lr=0.000973465, gnorm=0.598, loss_scale=16, train_wall=19, gb_free=29.6, wall=1796
2024-02-09 17:35:03 | INFO | train_inner | epoch 024:    117 / 369 loss=3.063, nll_loss=1.496, ppl=2.82, wps=77354.6, ups=5.15, wpb=15007.3, bsz=770.3, num_updates=8600, lr=0.000971321, gnorm=0.604, loss_scale=16, train_wall=19, gb_free=29.6, wall=1816
2024-02-09 17:35:23 | INFO | train_inner | epoch 024:    217 / 369 loss=3.085, nll_loss=1.523, ppl=2.87, wps=77510.6, ups=5.15, wpb=15042.7, bsz=779.4, num_updates=8700, lr=0.000969096, gnorm=0.612, loss_scale=16, train_wall=19, gb_free=29.6, wall=1835
2024-02-09 17:35:42 | INFO | train_inner | epoch 024:    317 / 369 loss=3.094, nll_loss=1.536, ppl=2.9, wps=77763.2, ups=5.14, wpb=15114.6, bsz=782.8, num_updates=8800, lr=0.00096679, gnorm=0.603, loss_scale=16, train_wall=19, gb_free=29.5, wall=1855
2024-02-09 17:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:35:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:35:54 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.47 | nll_loss 1.818 | ppl 3.53 | wps 160800 | wpb 3189.1 | bsz 163.4 | num_updates 8852 | best_loss 3.47
2024-02-09 17:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 8852 updates
2024-02-09 17:35:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint24.pt
2024-02-09 17:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint24.pt
2024-02-09 17:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint24.pt (epoch 24 @ 8852 updates, score 3.47) (writing took 3.720722339814529 seconds)
2024-02-09 17:35:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-02-09 17:35:58 | INFO | train | epoch 024 | loss 3.081 | nll_loss 1.519 | ppl 2.87 | wps 71856.6 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 8852 | lr 0.00096556 | gnorm 0.603 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 1870
2024-02-09 17:35:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:35:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:35:58 | INFO | fairseq.trainer | begin training epoch 25
2024-02-09 17:35:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:36:07 | INFO | train_inner | epoch 025:     48 / 369 loss=3.071, nll_loss=1.509, ppl=2.85, wps=60210.2, ups=3.99, wpb=15108.7, bsz=776.1, num_updates=8900, lr=0.000964405, gnorm=0.597, loss_scale=16, train_wall=19, gb_free=29.5, wall=1880
2024-02-09 17:36:27 | INFO | train_inner | epoch 025:    148 / 369 loss=3.024, nll_loss=1.454, ppl=2.74, wps=78584.6, ups=5.18, wpb=15185.1, bsz=783.5, num_updates=9000, lr=0.00096194, gnorm=0.595, loss_scale=16, train_wall=19, gb_free=29.6, wall=1899
2024-02-09 17:36:46 | INFO | train_inner | epoch 025:    248 / 369 loss=3.054, nll_loss=1.489, ppl=2.81, wps=77575.9, ups=5.15, wpb=15071.6, bsz=782.7, num_updates=9100, lr=0.000959396, gnorm=0.591, loss_scale=16, train_wall=19, gb_free=29.6, wall=1918
2024-02-09 17:37:06 | INFO | train_inner | epoch 025:    348 / 369 loss=3.077, nll_loss=1.516, ppl=2.86, wps=77435.9, ups=5.16, wpb=15012.9, bsz=803.8, num_updates=9200, lr=0.000956773, gnorm=0.608, loss_scale=16, train_wall=19, gb_free=29.6, wall=1938
2024-02-09 17:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:37:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:37:11 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.459 | nll_loss 1.808 | ppl 3.5 | wps 164789 | wpb 3189.1 | bsz 163.4 | num_updates 9221 | best_loss 3.459
2024-02-09 17:37:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9221 updates
2024-02-09 17:37:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint25.pt
2024-02-09 17:37:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint25.pt
2024-02-09 17:37:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint25.pt (epoch 25 @ 9221 updates, score 3.459) (writing took 3.75776816601865 seconds)
2024-02-09 17:37:15 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-02-09 17:37:15 | INFO | train | epoch 025 | loss 3.05 | nll_loss 1.485 | ppl 2.8 | wps 72112.6 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 9221 | lr 0.000956212 | gnorm 0.597 | loss_scale 16 | train_wall 71 | gb_free 29.5 | wall 1947
2024-02-09 17:37:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:37:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:37:15 | INFO | fairseq.trainer | begin training epoch 26
2024-02-09 17:37:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:37:31 | INFO | train_inner | epoch 026:     79 / 369 loss=2.997, nll_loss=1.423, ppl=2.68, wps=60289.9, ups=4, wpb=15072.7, bsz=783.4, num_updates=9300, lr=0.000954072, gnorm=0.576, loss_scale=16, train_wall=19, gb_free=29.6, wall=1963
2024-02-09 17:37:50 | INFO | train_inner | epoch 026:    179 / 369 loss=3.012, nll_loss=1.442, ppl=2.72, wps=78130.5, ups=5.16, wpb=15139.7, bsz=785, num_updates=9400, lr=0.000951293, gnorm=0.597, loss_scale=16, train_wall=19, gb_free=29.6, wall=1982
2024-02-09 17:38:09 | INFO | train_inner | epoch 026:    279 / 369 loss=3.034, nll_loss=1.469, ppl=2.77, wps=77843.4, ups=5.16, wpb=15085, bsz=784.2, num_updates=9500, lr=0.000948436, gnorm=0.575, loss_scale=16, train_wall=19, gb_free=29.6, wall=2002
2024-02-09 17:38:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:38:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:38:29 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.44 | nll_loss 1.792 | ppl 3.46 | wps 167032 | wpb 3189.1 | bsz 163.4 | num_updates 9590 | best_loss 3.44
2024-02-09 17:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 9590 updates
2024-02-09 17:38:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint26.pt
2024-02-09 17:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint26.pt
2024-02-09 17:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint26.pt (epoch 26 @ 9590 updates, score 3.44) (writing took 3.706367172067985 seconds)
2024-02-09 17:38:32 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-02-09 17:38:32 | INFO | train | epoch 026 | loss 3.022 | nll_loss 1.454 | ppl 2.74 | wps 72184.9 | ups 4.79 | wpb 15076.1 | bsz 783.3 | num_updates 9590 | lr 0.0009458 | gnorm 0.585 | loss_scale 16 | train_wall 71 | gb_free 29.5 | wall 2025
2024-02-09 17:38:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:38:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:38:32 | INFO | fairseq.trainer | begin training epoch 27
2024-02-09 17:38:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:38:34 | INFO | train_inner | epoch 027:     10 / 369 loss=3.052, nll_loss=1.489, ppl=2.81, wps=59939.3, ups=4.01, wpb=14962.7, bsz=770.7, num_updates=9600, lr=0.000945503, gnorm=0.591, loss_scale=16, train_wall=19, gb_free=29.6, wall=2027
2024-02-09 17:38:54 | INFO | train_inner | epoch 027:    110 / 369 loss=2.957, nll_loss=1.379, ppl=2.6, wps=77910.3, ups=5.16, wpb=15097.2, bsz=781.3, num_updates=9700, lr=0.000942494, gnorm=0.551, loss_scale=16, train_wall=19, gb_free=29.6, wall=2046
2024-02-09 17:39:13 | INFO | train_inner | epoch 027:    210 / 369 loss=3, nll_loss=1.429, ppl=2.69, wps=78091.5, ups=5.18, wpb=15066.2, bsz=766.3, num_updates=9800, lr=0.000939409, gnorm=0.589, loss_scale=16, train_wall=19, gb_free=29.5, wall=2065
2024-02-09 17:39:32 | INFO | train_inner | epoch 027:    310 / 369 loss=3.007, nll_loss=1.439, ppl=2.71, wps=78041.7, ups=5.15, wpb=15163.9, bsz=806.8, num_updates=9900, lr=0.000936248, gnorm=0.579, loss_scale=16, train_wall=19, gb_free=29.6, wall=2085
2024-02-09 17:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:39:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:39:46 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.434 | nll_loss 1.785 | ppl 3.45 | wps 163574 | wpb 3189.1 | bsz 163.4 | num_updates 9959 | best_loss 3.434
2024-02-09 17:39:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 9959 updates
2024-02-09 17:39:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint27.pt
2024-02-09 17:39:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint27.pt
2024-02-09 17:39:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint27.pt (epoch 27 @ 9959 updates, score 3.434) (writing took 3.706036620074883 seconds)
2024-02-09 17:39:49 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-02-09 17:39:49 | INFO | train | epoch 027 | loss 2.994 | nll_loss 1.422 | ppl 2.68 | wps 72206.3 | ups 4.79 | wpb 15076.1 | bsz 783.3 | num_updates 9959 | lr 0.000934348 | gnorm 0.573 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 2102
2024-02-09 17:39:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:39:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:39:49 | INFO | fairseq.trainer | begin training epoch 28
2024-02-09 17:39:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:39:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-09 17:39:58 | INFO | train_inner | epoch 028:     42 / 369 loss=2.989, nll_loss=1.417, ppl=2.67, wps=60049.3, ups=3.98, wpb=15104.8, bsz=783.1, num_updates=10000, lr=0.000933013, gnorm=0.583, loss_scale=8, train_wall=19, gb_free=29.6, wall=2110
2024-02-09 17:40:17 | INFO | train_inner | epoch 028:    142 / 369 loss=2.953, nll_loss=1.376, ppl=2.6, wps=77342.6, ups=5.18, wpb=14940.1, bsz=781.4, num_updates=10100, lr=0.000929703, gnorm=0.562, loss_scale=8, train_wall=19, gb_free=29.6, wall=2129
2024-02-09 17:40:36 | INFO | train_inner | epoch 028:    242 / 369 loss=2.983, nll_loss=1.412, ppl=2.66, wps=78187.8, ups=5.17, wpb=15116, bsz=768.9, num_updates=10200, lr=0.00092632, gnorm=0.555, loss_scale=8, train_wall=19, gb_free=29.6, wall=2149
2024-02-09 17:40:56 | INFO | train_inner | epoch 028:    342 / 369 loss=2.972, nll_loss=1.4, ppl=2.64, wps=77868.6, ups=5.15, wpb=15115.1, bsz=799.3, num_updates=10300, lr=0.000922864, gnorm=0.546, loss_scale=8, train_wall=19, gb_free=29.6, wall=2168
2024-02-09 17:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:41:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:41:03 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.426 | nll_loss 1.782 | ppl 3.44 | wps 166231 | wpb 3189.1 | bsz 163.4 | num_updates 10327 | best_loss 3.426
2024-02-09 17:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10327 updates
2024-02-09 17:41:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint28.pt
2024-02-09 17:41:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint28.pt
2024-02-09 17:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint28.pt (epoch 28 @ 10327 updates, score 3.426) (writing took 3.7436940581537783 seconds)
2024-02-09 17:41:06 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-02-09 17:41:06 | INFO | train | epoch 028 | loss 2.967 | nll_loss 1.393 | ppl 2.63 | wps 71916.4 | ups 4.77 | wpb 15074.5 | bsz 783.8 | num_updates 10327 | lr 0.000921918 | gnorm 0.563 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 2179
2024-02-09 17:41:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:41:06 | INFO | fairseq.trainer | begin training epoch 29
2024-02-09 17:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:41:21 | INFO | train_inner | epoch 029:     73 / 369 loss=2.936, nll_loss=1.357, ppl=2.56, wps=60165.6, ups=3.99, wpb=15077, bsz=786.9, num_updates=10400, lr=0.000919335, gnorm=0.572, loss_scale=8, train_wall=19, gb_free=29.6, wall=2193
2024-02-09 17:41:40 | INFO | train_inner | epoch 029:    173 / 369 loss=2.942, nll_loss=1.365, ppl=2.58, wps=78057.2, ups=5.15, wpb=15171.1, bsz=770.1, num_updates=10500, lr=0.000915735, gnorm=0.585, loss_scale=8, train_wall=19, gb_free=29.6, wall=2212
2024-02-09 17:42:00 | INFO | train_inner | epoch 029:    273 / 369 loss=2.939, nll_loss=1.363, ppl=2.57, wps=77008.2, ups=5.13, wpb=15014.3, bsz=801.7, num_updates=10600, lr=0.000912063, gnorm=0.548, loss_scale=8, train_wall=19, gb_free=29.6, wall=2232
2024-02-09 17:42:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:42:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:42:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.41 | nll_loss 1.761 | ppl 3.39 | wps 162227 | wpb 3189.1 | bsz 163.4 | num_updates 10696 | best_loss 3.41
2024-02-09 17:42:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 10696 updates
2024-02-09 17:42:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint29.pt
2024-02-09 17:42:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint29.pt
2024-02-09 17:42:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint29.pt (epoch 29 @ 10696 updates, score 3.41) (writing took 3.8124797851778567 seconds)
2024-02-09 17:42:24 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-02-09 17:42:24 | INFO | train | epoch 029 | loss 2.944 | nll_loss 1.368 | ppl 2.58 | wps 71702.2 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 10696 | lr 0.000908472 | gnorm 0.567 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 2256
2024-02-09 17:42:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:42:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:42:30 | INFO | fairseq.trainer | begin training epoch 30
2024-02-09 17:42:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:42:31 | INFO | train_inner | epoch 030:      4 / 369 loss=2.968, nll_loss=1.397, ppl=2.63, wps=47436.5, ups=3.16, wpb=15004.1, bsz=778.1, num_updates=10700, lr=0.000908321, gnorm=0.571, loss_scale=8, train_wall=19, gb_free=29.6, wall=2264
2024-02-09 17:42:51 | INFO | train_inner | epoch 030:    104 / 369 loss=2.885, nll_loss=1.3, ppl=2.46, wps=77698.1, ups=5.15, wpb=15099.7, bsz=787.4, num_updates=10800, lr=0.000904508, gnorm=0.549, loss_scale=8, train_wall=19, gb_free=29.6, wall=2283
2024-02-09 17:43:10 | INFO | train_inner | epoch 030:    204 / 369 loss=2.915, nll_loss=1.336, ppl=2.52, wps=77439.9, ups=5.15, wpb=15031.7, bsz=800.2, num_updates=10900, lr=0.000900627, gnorm=0.554, loss_scale=8, train_wall=19, gb_free=29.5, wall=2302
2024-02-09 17:43:29 | INFO | train_inner | epoch 030:    304 / 369 loss=2.946, nll_loss=1.371, ppl=2.59, wps=78277, ups=5.18, wpb=15111.2, bsz=771.1, num_updates=11000, lr=0.000896677, gnorm=0.535, loss_scale=8, train_wall=19, gb_free=29.6, wall=2322
2024-02-09 17:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:43:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:43:44 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.411 | nll_loss 1.762 | ppl 3.39 | wps 161811 | wpb 3189.1 | bsz 163.4 | num_updates 11065 | best_loss 3.41
2024-02-09 17:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11065 updates
2024-02-09 17:43:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint30.pt
2024-02-09 17:43:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint30.pt
2024-02-09 17:43:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint30.pt (epoch 30 @ 11065 updates, score 3.411) (writing took 2.3974620420485735 seconds)
2024-02-09 17:43:46 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-02-09 17:43:46 | INFO | train | epoch 030 | loss 2.919 | nll_loss 1.341 | ppl 2.53 | wps 67537.9 | ups 4.48 | wpb 15076.1 | bsz 783.3 | num_updates 11065 | lr 0.000894073 | gnorm 0.549 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 2339
2024-02-09 17:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:43:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:43:46 | INFO | fairseq.trainer | begin training epoch 31
2024-02-09 17:43:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:43:53 | INFO | train_inner | epoch 031:     35 / 369 loss=2.925, nll_loss=1.347, ppl=2.54, wps=63063, ups=4.2, wpb=15016.3, bsz=752.6, num_updates=11100, lr=0.000892658, gnorm=0.556, loss_scale=8, train_wall=19, gb_free=29.6, wall=2346
2024-02-09 17:44:13 | INFO | train_inner | epoch 031:    135 / 369 loss=2.859, nll_loss=1.272, ppl=2.41, wps=77494.2, ups=5.16, wpb=15024.9, bsz=804.8, num_updates=11200, lr=0.000888573, gnorm=0.554, loss_scale=8, train_wall=19, gb_free=29.6, wall=2365
2024-02-09 17:44:32 | INFO | train_inner | epoch 031:    235 / 369 loss=2.902, nll_loss=1.322, ppl=2.5, wps=77707.8, ups=5.16, wpb=15046.6, bsz=780.7, num_updates=11300, lr=0.000884421, gnorm=0.52, loss_scale=8, train_wall=19, gb_free=29.6, wall=2384
2024-02-09 17:44:51 | INFO | train_inner | epoch 031:    335 / 369 loss=2.924, nll_loss=1.347, ppl=2.54, wps=77848.9, ups=5.15, wpb=15118.8, bsz=776.1, num_updates=11400, lr=0.000880203, gnorm=0.55, loss_scale=8, train_wall=19, gb_free=29.6, wall=2404
2024-02-09 17:44:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:44:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:45:00 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.385 | nll_loss 1.742 | ppl 3.35 | wps 161745 | wpb 3189.1 | bsz 163.4 | num_updates 11434 | best_loss 3.385
2024-02-09 17:45:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 11434 updates
2024-02-09 17:45:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint31.pt
2024-02-09 17:45:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint31.pt
2024-02-09 17:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint31.pt (epoch 31 @ 11434 updates, score 3.385) (writing took 3.8093940438702703 seconds)
2024-02-09 17:45:04 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-02-09 17:45:04 | INFO | train | epoch 031 | loss 2.895 | nll_loss 1.313 | ppl 2.49 | wps 71767.4 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 11434 | lr 0.000878754 | gnorm 0.538 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 2416
2024-02-09 17:45:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:45:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:45:04 | INFO | fairseq.trainer | begin training epoch 32
2024-02-09 17:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:45:17 | INFO | train_inner | epoch 032:     66 / 369 loss=2.854, nll_loss=1.266, ppl=2.4, wps=59705.9, ups=3.93, wpb=15193.1, bsz=787.1, num_updates=11500, lr=0.00087592, gnorm=0.502, loss_scale=8, train_wall=19, gb_free=29.6, wall=2429
2024-02-09 17:45:36 | INFO | train_inner | epoch 032:    166 / 369 loss=2.868, nll_loss=1.283, ppl=2.43, wps=77300.3, ups=5.11, wpb=15121.3, bsz=762.7, num_updates=11600, lr=0.000871572, gnorm=0.544, loss_scale=8, train_wall=19, gb_free=29.6, wall=2449
2024-02-09 17:45:56 | INFO | train_inner | epoch 032:    266 / 369 loss=2.881, nll_loss=1.299, ppl=2.46, wps=77448.1, ups=5.15, wpb=15050.4, bsz=776.5, num_updates=11700, lr=0.000867161, gnorm=0.531, loss_scale=8, train_wall=19, gb_free=29.6, wall=2468
2024-02-09 17:46:15 | INFO | train_inner | epoch 032:    366 / 369 loss=2.892, nll_loss=1.312, ppl=2.48, wps=77327, ups=5.14, wpb=15040.4, bsz=815.8, num_updates=11800, lr=0.000862687, gnorm=0.513, loss_scale=8, train_wall=19, gb_free=29.5, wall=2488
2024-02-09 17:46:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:46:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:46:18 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.377 | nll_loss 1.739 | ppl 3.34 | wps 167176 | wpb 3189.1 | bsz 163.4 | num_updates 11803 | best_loss 3.377
2024-02-09 17:46:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 11803 updates
2024-02-09 17:46:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint32.pt
2024-02-09 17:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint32.pt
2024-02-09 17:46:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint32.pt (epoch 32 @ 11803 updates, score 3.377) (writing took 3.727825474925339 seconds)
2024-02-09 17:46:21 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-02-09 17:46:21 | INFO | train | epoch 032 | loss 2.871 | nll_loss 1.287 | ppl 2.44 | wps 71789.5 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 11803 | lr 0.000862552 | gnorm 0.524 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 2494
2024-02-09 17:46:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:46:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:46:21 | INFO | fairseq.trainer | begin training epoch 33
2024-02-09 17:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:46:40 | INFO | train_inner | epoch 033:     97 / 369 loss=2.804, nll_loss=1.209, ppl=2.31, wps=60410.8, ups=3.99, wpb=15152.6, bsz=805, num_updates=11900, lr=0.000858151, gnorm=0.513, loss_scale=8, train_wall=19, gb_free=29.5, wall=2513
2024-02-09 17:46:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-09 17:47:00 | INFO | train_inner | epoch 033:    198 / 369 loss=2.852, nll_loss=1.266, ppl=2.4, wps=77312.8, ups=5.11, wpb=15140.2, bsz=767, num_updates=12000, lr=0.000853553, gnorm=0.526, loss_scale=4, train_wall=19, gb_free=29.6, wall=2532
2024-02-09 17:47:19 | INFO | train_inner | epoch 033:    298 / 369 loss=2.866, nll_loss=1.283, ppl=2.43, wps=76951.9, ups=5.14, wpb=14959.9, bsz=777, num_updates=12100, lr=0.000848895, gnorm=0.54, loss_scale=4, train_wall=19, gb_free=29.6, wall=2552
2024-02-09 17:47:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:47:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:47:35 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.381 | nll_loss 1.738 | ppl 3.33 | wps 161870 | wpb 3189.1 | bsz 163.4 | num_updates 12171 | best_loss 3.377
2024-02-09 17:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12171 updates
2024-02-09 17:47:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint33.pt
2024-02-09 17:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint33.pt
2024-02-09 17:47:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint33.pt (epoch 33 @ 12171 updates, score 3.381) (writing took 2.4796721760649234 seconds)
2024-02-09 17:47:37 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-02-09 17:47:37 | INFO | train | epoch 033 | loss 2.849 | nll_loss 1.263 | ppl 2.4 | wps 72962.3 | ups 4.84 | wpb 15080.1 | bsz 783.6 | num_updates 12171 | lr 0.000845552 | gnorm 0.528 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 2570
2024-02-09 17:47:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:47:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:47:37 | INFO | fairseq.trainer | begin training epoch 34
2024-02-09 17:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:47:43 | INFO | train_inner | epoch 034:     29 / 369 loss=2.855, nll_loss=1.27, ppl=2.41, wps=62911.1, ups=4.2, wpb=14992.9, bsz=781.1, num_updates=12200, lr=0.000844177, gnorm=0.523, loss_scale=4, train_wall=19, gb_free=29.6, wall=2576
2024-02-09 17:48:03 | INFO | train_inner | epoch 034:    129 / 369 loss=2.788, nll_loss=1.192, ppl=2.28, wps=78524.5, ups=5.16, wpb=15227.6, bsz=797.3, num_updates=12300, lr=0.0008394, gnorm=0.501, loss_scale=4, train_wall=19, gb_free=29.6, wall=2595
2024-02-09 17:48:22 | INFO | train_inner | epoch 034:    229 / 369 loss=2.828, nll_loss=1.24, ppl=2.36, wps=77838.4, ups=5.17, wpb=15043, bsz=783.4, num_updates=12400, lr=0.000834565, gnorm=0.514, loss_scale=4, train_wall=19, gb_free=29.6, wall=2614
2024-02-09 17:48:41 | INFO | train_inner | epoch 034:    329 / 369 loss=2.849, nll_loss=1.264, ppl=2.4, wps=77926.8, ups=5.15, wpb=15124.9, bsz=794.9, num_updates=12500, lr=0.000829673, gnorm=0.55, loss_scale=4, train_wall=19, gb_free=29.6, wall=2634
2024-02-09 17:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:48:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:48:51 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.378 | nll_loss 1.732 | ppl 3.32 | wps 167465 | wpb 3189.1 | bsz 163.4 | num_updates 12540 | best_loss 3.377
2024-02-09 17:48:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 12540 updates
2024-02-09 17:48:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint34.pt
2024-02-09 17:48:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint34.pt
2024-02-09 17:48:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint34.pt (epoch 34 @ 12540 updates, score 3.378) (writing took 2.3935455260798335 seconds)
2024-02-09 17:48:53 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-02-09 17:48:53 | INFO | train | epoch 034 | loss 2.826 | nll_loss 1.237 | ppl 2.36 | wps 73378.3 | ups 4.87 | wpb 15076.1 | bsz 783.3 | num_updates 12540 | lr 0.0008277 | gnorm 0.517 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 2646
2024-02-09 17:48:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:48:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:48:53 | INFO | fairseq.trainer | begin training epoch 35
2024-02-09 17:48:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:49:05 | INFO | train_inner | epoch 035:     60 / 369 loss=2.81, nll_loss=1.219, ppl=2.33, wps=63051.9, ups=4.22, wpb=14956.5, bsz=766, num_updates=12600, lr=0.000824724, gnorm=0.525, loss_scale=4, train_wall=19, gb_free=29.6, wall=2657
2024-02-09 17:49:24 | INFO | train_inner | epoch 035:    160 / 369 loss=2.789, nll_loss=1.195, ppl=2.29, wps=77801.5, ups=5.14, wpb=15136, bsz=792.5, num_updates=12700, lr=0.00081972, gnorm=0.489, loss_scale=4, train_wall=19, gb_free=29.5, wall=2677
2024-02-09 17:49:44 | INFO | train_inner | epoch 035:    260 / 369 loss=2.819, nll_loss=1.229, ppl=2.34, wps=77855.1, ups=5.17, wpb=15065.4, bsz=773.3, num_updates=12800, lr=0.00081466, gnorm=0.516, loss_scale=4, train_wall=19, gb_free=29.6, wall=2696
2024-02-09 17:50:03 | INFO | train_inner | epoch 035:    360 / 369 loss=2.828, nll_loss=1.241, ppl=2.36, wps=77998.9, ups=5.16, wpb=15101.9, bsz=790.1, num_updates=12900, lr=0.000809547, gnorm=0.496, loss_scale=4, train_wall=19, gb_free=29.5, wall=2716
2024-02-09 17:50:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:50:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:50:07 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.383 | nll_loss 1.734 | ppl 3.33 | wps 163560 | wpb 3189.1 | bsz 163.4 | num_updates 12909 | best_loss 3.377
2024-02-09 17:50:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 12909 updates
2024-02-09 17:50:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint35.pt
2024-02-09 17:50:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint35.pt
2024-02-09 17:50:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint35.pt (epoch 35 @ 12909 updates, score 3.383) (writing took 2.3424012439791113 seconds)
2024-02-09 17:50:09 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-02-09 17:50:09 | INFO | train | epoch 035 | loss 2.805 | nll_loss 1.213 | ppl 2.32 | wps 73346.4 | ups 4.87 | wpb 15076.1 | bsz 783.3 | num_updates 12909 | lr 0.000809084 | gnorm 0.507 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 2721
2024-02-09 17:50:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:50:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:50:09 | INFO | fairseq.trainer | begin training epoch 36
2024-02-09 17:50:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:50:27 | INFO | train_inner | epoch 036:     91 / 369 loss=2.767, nll_loss=1.17, ppl=2.25, wps=63112.6, ups=4.21, wpb=14996.9, bsz=768.3, num_updates=13000, lr=0.000804381, gnorm=0.5, loss_scale=4, train_wall=19, gb_free=29.5, wall=2739
2024-02-09 17:50:46 | INFO | train_inner | epoch 036:    191 / 369 loss=2.782, nll_loss=1.188, ppl=2.28, wps=77520.1, ups=5.13, wpb=15105.1, bsz=786.2, num_updates=13100, lr=0.000799162, gnorm=0.501, loss_scale=4, train_wall=19, gb_free=29.6, wall=2759
2024-02-09 17:51:06 | INFO | train_inner | epoch 036:    291 / 369 loss=2.803, nll_loss=1.212, ppl=2.32, wps=77423.3, ups=5.15, wpb=15047.6, bsz=779.5, num_updates=13200, lr=0.000793893, gnorm=0.577, loss_scale=4, train_wall=19, gb_free=29.6, wall=2778
2024-02-09 17:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:51:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:51:23 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.376 | nll_loss 1.73 | ppl 3.32 | wps 165136 | wpb 3189.1 | bsz 163.4 | num_updates 13278 | best_loss 3.376
2024-02-09 17:51:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 13278 updates
2024-02-09 17:51:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint36.pt
2024-02-09 17:51:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint36.pt
2024-02-09 17:51:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint36.pt (epoch 36 @ 13278 updates, score 3.376) (writing took 3.7687964329961687 seconds)
2024-02-09 17:51:27 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-02-09 17:51:27 | INFO | train | epoch 036 | loss 2.783 | nll_loss 1.19 | ppl 2.28 | wps 71734.7 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 13278 | lr 0.000789747 | gnorm 0.516 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 2799
2024-02-09 17:51:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:51:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:51:27 | INFO | fairseq.trainer | begin training epoch 37
2024-02-09 17:51:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:51:31 | INFO | train_inner | epoch 037:     22 / 369 loss=2.777, nll_loss=1.183, ppl=2.27, wps=59879.8, ups=3.97, wpb=15067.1, bsz=787.5, num_updates=13300, lr=0.000788573, gnorm=0.484, loss_scale=4, train_wall=19, gb_free=29.6, wall=2803
2024-02-09 17:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-09 17:51:51 | INFO | train_inner | epoch 037:    123 / 369 loss=2.745, nll_loss=1.145, ppl=2.21, wps=77024.1, ups=5.1, wpb=15100.5, bsz=769.4, num_updates=13400, lr=0.000783203, gnorm=0.504, loss_scale=2, train_wall=19, gb_free=29.6, wall=2823
2024-02-09 17:52:10 | INFO | train_inner | epoch 037:    223 / 369 loss=2.758, nll_loss=1.162, ppl=2.24, wps=77562.5, ups=5.15, wpb=15075.2, bsz=789, num_updates=13500, lr=0.000777785, gnorm=0.507, loss_scale=2, train_wall=19, gb_free=29.6, wall=2842
2024-02-09 17:52:30 | INFO | train_inner | epoch 037:    323 / 369 loss=2.784, nll_loss=1.192, ppl=2.28, wps=77064.8, ups=5.12, wpb=15038.6, bsz=786.6, num_updates=13600, lr=0.00077232, gnorm=0.512, loss_scale=2, train_wall=19, gb_free=29.6, wall=2862
2024-02-09 17:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:52:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:52:40 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.363 | nll_loss 1.722 | ppl 3.3 | wps 163764 | wpb 3189.1 | bsz 163.4 | num_updates 13646 | best_loss 3.363
2024-02-09 17:52:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 13646 updates
2024-02-09 17:52:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint37.pt
2024-02-09 17:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint37.pt
2024-02-09 17:52:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint37.pt (epoch 37 @ 13646 updates, score 3.363) (writing took 3.710900431033224 seconds)
2024-02-09 17:52:44 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-02-09 17:52:44 | INFO | train | epoch 037 | loss 2.763 | nll_loss 1.167 | ppl 2.25 | wps 71633.5 | ups 4.75 | wpb 15077.1 | bsz 783.4 | num_updates 13646 | lr 0.00076979 | gnorm 0.508 | loss_scale 2 | train_wall 71 | gb_free 29.5 | wall 2876
2024-02-09 17:52:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:52:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:52:44 | INFO | fairseq.trainer | begin training epoch 38
2024-02-09 17:52:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:52:55 | INFO | train_inner | epoch 038:     54 / 369 loss=2.741, nll_loss=1.142, ppl=2.21, wps=60162.9, ups=3.98, wpb=15098, bsz=793.7, num_updates=13700, lr=0.000766807, gnorm=0.608, loss_scale=2, train_wall=19, gb_free=29.6, wall=2887
2024-02-09 17:53:14 | INFO | train_inner | epoch 038:    154 / 369 loss=2.719, nll_loss=1.117, ppl=2.17, wps=78039.6, ups=5.14, wpb=15175.1, bsz=792, num_updates=13800, lr=0.000761249, gnorm=0.473, loss_scale=2, train_wall=19, gb_free=29.5, wall=2906
2024-02-09 17:53:34 | INFO | train_inner | epoch 038:    254 / 369 loss=2.75, nll_loss=1.153, ppl=2.22, wps=78295.7, ups=5.15, wpb=15188.4, bsz=790.4, num_updates=13900, lr=0.000755647, gnorm=0.489, loss_scale=2, train_wall=19, gb_free=29.6, wall=2926
2024-02-09 17:53:53 | INFO | train_inner | epoch 038:    354 / 369 loss=2.776, nll_loss=1.184, ppl=2.27, wps=77441.7, ups=5.18, wpb=14946.8, bsz=763.4, num_updates=14000, lr=0.00075, gnorm=0.491, loss_scale=2, train_wall=19, gb_free=29.6, wall=2945
2024-02-09 17:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:53:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:53:58 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.369 | nll_loss 1.724 | ppl 3.3 | wps 163586 | wpb 3189.1 | bsz 163.4 | num_updates 14015 | best_loss 3.363
2024-02-09 17:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14015 updates
2024-02-09 17:53:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint38.pt
2024-02-09 17:53:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint38.pt
2024-02-09 17:54:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint38.pt (epoch 38 @ 14015 updates, score 3.369) (writing took 2.347060794942081 seconds)
2024-02-09 17:54:00 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-02-09 17:54:00 | INFO | train | epoch 038 | loss 2.743 | nll_loss 1.145 | ppl 2.21 | wps 73382.4 | ups 4.87 | wpb 15076.1 | bsz 783.3 | num_updates 14015 | lr 0.000749149 | gnorm 0.517 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 2952
2024-02-09 17:54:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:54:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:54:00 | INFO | fairseq.trainer | begin training epoch 39
2024-02-09 17:54:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:54:16 | INFO | train_inner | epoch 039:     85 / 369 loss=2.704, nll_loss=1.1, ppl=2.14, wps=63874.4, ups=4.23, wpb=15087.8, bsz=784.9, num_updates=14100, lr=0.000744311, gnorm=0.488, loss_scale=2, train_wall=19, gb_free=29.5, wall=2969
2024-02-09 17:54:36 | INFO | train_inner | epoch 039:    185 / 369 loss=2.712, nll_loss=1.11, ppl=2.16, wps=77624.4, ups=5.14, wpb=15094.6, bsz=798, num_updates=14200, lr=0.000738579, gnorm=0.478, loss_scale=2, train_wall=19, gb_free=29.6, wall=2988
2024-02-09 17:54:55 | INFO | train_inner | epoch 039:    285 / 369 loss=2.738, nll_loss=1.14, ppl=2.2, wps=77391.4, ups=5.16, wpb=14989.2, bsz=767.9, num_updates=14300, lr=0.000732807, gnorm=0.487, loss_scale=2, train_wall=19, gb_free=29.6, wall=3008
2024-02-09 17:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:55:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:55:13 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.354 | nll_loss 1.718 | ppl 3.29 | wps 161819 | wpb 3189.1 | bsz 163.4 | num_updates 14384 | best_loss 3.354
2024-02-09 17:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 14384 updates
2024-02-09 17:55:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint39.pt
2024-02-09 17:55:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint39.pt
2024-02-09 17:55:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint39.pt (epoch 39 @ 14384 updates, score 3.354) (writing took 3.688736757962033 seconds)
2024-02-09 17:55:17 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-02-09 17:55:17 | INFO | train | epoch 039 | loss 2.722 | nll_loss 1.121 | ppl 2.18 | wps 72069.1 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 14384 | lr 0.000727928 | gnorm 0.489 | loss_scale 2 | train_wall 71 | gb_free 29.7 | wall 3029
2024-02-09 17:55:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:55:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:55:17 | INFO | fairseq.trainer | begin training epoch 40
2024-02-09 17:55:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:55:20 | INFO | train_inner | epoch 040:     16 / 369 loss=2.732, nll_loss=1.134, ppl=2.19, wps=60261.2, ups=3.99, wpb=15084.4, bsz=778.2, num_updates=14400, lr=0.000726995, gnorm=0.5, loss_scale=2, train_wall=19, gb_free=29.5, wall=3033
2024-02-09 17:55:40 | INFO | train_inner | epoch 040:    116 / 369 loss=2.693, nll_loss=1.088, ppl=2.13, wps=78110.4, ups=5.17, wpb=15098.6, bsz=764.9, num_updates=14500, lr=0.000721144, gnorm=0.466, loss_scale=2, train_wall=19, gb_free=29.6, wall=3052
2024-02-09 17:55:59 | INFO | train_inner | epoch 040:    216 / 369 loss=2.703, nll_loss=1.101, ppl=2.14, wps=77855, ups=5.15, wpb=15129.9, bsz=791, num_updates=14600, lr=0.000715256, gnorm=0.476, loss_scale=2, train_wall=19, gb_free=29.6, wall=3071
2024-02-09 17:56:19 | INFO | train_inner | epoch 040:    316 / 369 loss=2.703, nll_loss=1.1, ppl=2.14, wps=77047.9, ups=5.15, wpb=14962.6, bsz=799.8, num_updates=14700, lr=0.00070933, gnorm=0.468, loss_scale=2, train_wall=19, gb_free=29.6, wall=3091
2024-02-09 17:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:56:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:56:31 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.351 | nll_loss 1.712 | ppl 3.28 | wps 165808 | wpb 3189.1 | bsz 163.4 | num_updates 14753 | best_loss 3.351
2024-02-09 17:56:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 14753 updates
2024-02-09 17:56:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint40.pt
2024-02-09 17:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint40.pt
2024-02-09 17:56:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint40.pt (epoch 40 @ 14753 updates, score 3.351) (writing took 3.8408503429964185 seconds)
2024-02-09 17:56:34 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-02-09 17:56:34 | INFO | train | epoch 040 | loss 2.702 | nll_loss 1.099 | ppl 2.14 | wps 71906.7 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 14753 | lr 0.000706175 | gnorm 0.47 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3107
2024-02-09 17:56:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:56:35 | INFO | fairseq.trainer | begin training epoch 41
2024-02-09 17:56:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:56:44 | INFO | train_inner | epoch 041:     47 / 369 loss=2.69, nll_loss=1.085, ppl=2.12, wps=59761.2, ups=3.97, wpb=15066, bsz=784.6, num_updates=14800, lr=0.000703368, gnorm=0.463, loss_scale=2, train_wall=19, gb_free=29.6, wall=3116
2024-02-09 17:57:03 | INFO | train_inner | epoch 041:    147 / 369 loss=2.676, nll_loss=1.07, ppl=2.1, wps=77551.8, ups=5.16, wpb=15043.5, bsz=765.5, num_updates=14900, lr=0.000697372, gnorm=0.483, loss_scale=2, train_wall=19, gb_free=29.5, wall=3135
2024-02-09 17:57:23 | INFO | train_inner | epoch 041:    247 / 369 loss=2.667, nll_loss=1.06, ppl=2.08, wps=77046.1, ups=5.13, wpb=15015, bsz=825.9, num_updates=15000, lr=0.000691342, gnorm=0.473, loss_scale=2, train_wall=19, gb_free=29.6, wall=3155
2024-02-09 17:57:42 | INFO | train_inner | epoch 041:    347 / 369 loss=2.714, nll_loss=1.115, ppl=2.17, wps=78360.8, ups=5.16, wpb=15191.1, bsz=761.4, num_updates=15100, lr=0.000685279, gnorm=0.461, loss_scale=2, train_wall=19, gb_free=29.6, wall=3174
2024-02-09 17:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:57:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:57:48 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.347 | nll_loss 1.714 | ppl 3.28 | wps 162758 | wpb 3189.1 | bsz 163.4 | num_updates 15122 | best_loss 3.347
2024-02-09 17:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15122 updates
2024-02-09 17:57:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint41.pt
2024-02-09 17:57:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint41.pt
2024-02-09 17:57:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint41.pt (epoch 41 @ 15122 updates, score 3.347) (writing took 3.726691935909912 seconds)
2024-02-09 17:57:52 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-02-09 17:57:52 | INFO | train | epoch 041 | loss 2.682 | nll_loss 1.077 | ppl 2.11 | wps 71938.4 | ups 4.77 | wpb 15076.1 | bsz 783.3 | num_updates 15122 | lr 0.000683941 | gnorm 0.469 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3184
2024-02-09 17:57:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:57:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:57:52 | INFO | fairseq.trainer | begin training epoch 42
2024-02-09 17:57:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:58:07 | INFO | train_inner | epoch 042:     78 / 369 loss=2.646, nll_loss=1.036, ppl=2.05, wps=60192.6, ups=3.99, wpb=15089.9, bsz=778.6, num_updates=15200, lr=0.000679184, gnorm=0.46, loss_scale=2, train_wall=19, gb_free=29.6, wall=3199
2024-02-09 17:58:26 | INFO | train_inner | epoch 042:    178 / 369 loss=2.665, nll_loss=1.058, ppl=2.08, wps=77540.4, ups=5.16, wpb=15031.7, bsz=780.8, num_updates=15300, lr=0.000673059, gnorm=0.461, loss_scale=2, train_wall=19, gb_free=29.6, wall=3219
2024-02-09 17:58:46 | INFO | train_inner | epoch 042:    278 / 369 loss=2.666, nll_loss=1.06, ppl=2.08, wps=77928.5, ups=5.14, wpb=15164.9, bsz=792.5, num_updates=15400, lr=0.000666903, gnorm=0.466, loss_scale=2, train_wall=19, gb_free=29.5, wall=3238
2024-02-09 17:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 17:59:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:59:05 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.337 | nll_loss 1.702 | ppl 3.25 | wps 166756 | wpb 3189.1 | bsz 163.4 | num_updates 15491 | best_loss 3.337
2024-02-09 17:59:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 15491 updates
2024-02-09 17:59:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint42.pt
2024-02-09 17:59:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint42.pt
2024-02-09 17:59:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint42.pt (epoch 42 @ 15491 updates, score 3.337) (writing took 3.7125445217825472 seconds)
2024-02-09 17:59:09 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-02-09 17:59:09 | INFO | train | epoch 042 | loss 2.663 | nll_loss 1.056 | ppl 2.08 | wps 72099.2 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 15491 | lr 0.000661277 | gnorm 0.462 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3261
2024-02-09 17:59:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 17:59:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 17:59:09 | INFO | fairseq.trainer | begin training epoch 43
2024-02-09 17:59:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 17:59:11 | INFO | train_inner | epoch 043:      9 / 369 loss=2.683, nll_loss=1.079, ppl=2.11, wps=60142, ups=4.01, wpb=15000.3, bsz=776.9, num_updates=15500, lr=0.00066072, gnorm=0.467, loss_scale=2, train_wall=19, gb_free=29.6, wall=3263
2024-02-09 17:59:30 | INFO | train_inner | epoch 043:    109 / 369 loss=2.625, nll_loss=1.012, ppl=2.02, wps=77569.2, ups=5.17, wpb=15013.6, bsz=772.6, num_updates=15600, lr=0.000654508, gnorm=0.456, loss_scale=2, train_wall=19, gb_free=29.6, wall=3283
2024-02-09 17:59:50 | INFO | train_inner | epoch 043:    209 / 369 loss=2.638, nll_loss=1.028, ppl=2.04, wps=77343.8, ups=5.12, wpb=15104.2, bsz=786.6, num_updates=15700, lr=0.000648271, gnorm=0.486, loss_scale=2, train_wall=19, gb_free=29.6, wall=3302
2024-02-09 18:00:09 | INFO | train_inner | epoch 043:    309 / 369 loss=2.664, nll_loss=1.058, ppl=2.08, wps=77946.4, ups=5.14, wpb=15153.1, bsz=778.7, num_updates=15800, lr=0.000642008, gnorm=0.467, loss_scale=2, train_wall=19, gb_free=29.6, wall=3321
2024-02-09 18:00:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:00:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:00:23 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.355 | nll_loss 1.718 | ppl 3.29 | wps 164361 | wpb 3189.1 | bsz 163.4 | num_updates 15860 | best_loss 3.337
2024-02-09 18:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 15860 updates
2024-02-09 18:00:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint43.pt
2024-02-09 18:00:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint43.pt
2024-02-09 18:00:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint43.pt (epoch 43 @ 15860 updates, score 3.355) (writing took 2.354897809913382 seconds)
2024-02-09 18:00:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-02-09 18:00:25 | INFO | train | epoch 043 | loss 2.644 | nll_loss 1.035 | ppl 2.05 | wps 73073.7 | ups 4.85 | wpb 15076.1 | bsz 783.3 | num_updates 15860 | lr 0.000638238 | gnorm 0.474 | loss_scale 2 | train_wall 71 | gb_free 29.5 | wall 3337
2024-02-09 18:00:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:00:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:00:25 | INFO | fairseq.trainer | begin training epoch 44
2024-02-09 18:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:00:33 | INFO | train_inner | epoch 044:     40 / 369 loss=2.631, nll_loss=1.02, ppl=2.03, wps=63306.8, ups=4.21, wpb=15046.4, bsz=790.6, num_updates=15900, lr=0.00063572, gnorm=0.48, loss_scale=2, train_wall=19, gb_free=29.6, wall=3345
2024-02-09 18:00:52 | INFO | train_inner | epoch 044:    140 / 369 loss=2.608, nll_loss=0.994, ppl=1.99, wps=77425.9, ups=5.15, wpb=15038.3, bsz=794.8, num_updates=16000, lr=0.00062941, gnorm=0.444, loss_scale=2, train_wall=19, gb_free=29.5, wall=3365
2024-02-09 18:01:12 | INFO | train_inner | epoch 044:    240 / 369 loss=2.625, nll_loss=1.014, ppl=2.02, wps=77887.8, ups=5.16, wpb=15107, bsz=794.5, num_updates=16100, lr=0.000623077, gnorm=0.453, loss_scale=2, train_wall=19, gb_free=29.6, wall=3384
2024-02-09 18:01:31 | INFO | train_inner | epoch 044:    340 / 369 loss=2.648, nll_loss=1.041, ppl=2.06, wps=78209.1, ups=5.17, wpb=15135.6, bsz=771.5, num_updates=16200, lr=0.000616723, gnorm=0.453, loss_scale=2, train_wall=19, gb_free=29.6, wall=3403
2024-02-09 18:01:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:01:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:01:38 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.335 | nll_loss 1.703 | ppl 3.26 | wps 166822 | wpb 3189.1 | bsz 163.4 | num_updates 16229 | best_loss 3.335
2024-02-09 18:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 16229 updates
2024-02-09 18:01:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint44.pt
2024-02-09 18:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint44.pt
2024-02-09 18:01:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint44.pt (epoch 44 @ 16229 updates, score 3.335) (writing took 3.7513710190542042 seconds)
2024-02-09 18:01:42 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-02-09 18:01:42 | INFO | train | epoch 044 | loss 2.626 | nll_loss 1.015 | ppl 2.02 | wps 72112 | ups 4.78 | wpb 15076.1 | bsz 783.3 | num_updates 16229 | lr 0.000614876 | gnorm 0.457 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3415
2024-02-09 18:01:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:01:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:01:42 | INFO | fairseq.trainer | begin training epoch 45
2024-02-09 18:01:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:01:56 | INFO | train_inner | epoch 045:     71 / 369 loss=2.603, nll_loss=0.988, ppl=1.98, wps=59785.4, ups=4, wpb=14933, bsz=777.9, num_updates=16300, lr=0.000610349, gnorm=0.47, loss_scale=2, train_wall=19, gb_free=29.6, wall=3428
2024-02-09 18:02:15 | INFO | train_inner | epoch 045:    171 / 369 loss=2.603, nll_loss=0.988, ppl=1.98, wps=78328.8, ups=5.18, wpb=15133.2, bsz=775.4, num_updates=16400, lr=0.000603956, gnorm=0.445, loss_scale=2, train_wall=19, gb_free=29.6, wall=3448
2024-02-09 18:02:35 | INFO | train_inner | epoch 045:    271 / 369 loss=2.606, nll_loss=0.992, ppl=1.99, wps=78179.7, ups=5.16, wpb=15139.5, bsz=792.2, num_updates=16500, lr=0.000597545, gnorm=0.457, loss_scale=2, train_wall=19, gb_free=29.6, wall=3467
2024-02-09 18:02:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:02:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:02:56 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.342 | nll_loss 1.71 | ppl 3.27 | wps 162940 | wpb 3189.1 | bsz 163.4 | num_updates 16598 | best_loss 3.335
2024-02-09 18:02:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 16598 updates
2024-02-09 18:02:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint45.pt
2024-02-09 18:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint45.pt
2024-02-09 18:02:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint45.pt (epoch 45 @ 16598 updates, score 3.342) (writing took 2.3248616370838135 seconds)
2024-02-09 18:02:58 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-02-09 18:02:58 | INFO | train | epoch 045 | loss 2.607 | nll_loss 0.993 | ppl 1.99 | wps 73537.2 | ups 4.88 | wpb 15076.1 | bsz 783.3 | num_updates 16598 | lr 0.000591246 | gnorm 0.452 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3490
2024-02-09 18:02:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:02:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:02:58 | INFO | fairseq.trainer | begin training epoch 46
2024-02-09 18:02:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:02:58 | INFO | train_inner | epoch 046:      2 / 369 loss=2.628, nll_loss=1.019, ppl=2.03, wps=63771.2, ups=4.24, wpb=15057.8, bsz=781.4, num_updates=16600, lr=0.000591118, gnorm=0.46, loss_scale=2, train_wall=19, gb_free=29.6, wall=3491
2024-02-09 18:03:18 | INFO | train_inner | epoch 046:    102 / 369 loss=2.564, nll_loss=0.944, ppl=1.92, wps=77353.2, ups=5.16, wpb=14981.7, bsz=786.2, num_updates=16700, lr=0.000584675, gnorm=0.445, loss_scale=2, train_wall=19, gb_free=29.5, wall=3510
2024-02-09 18:03:37 | INFO | train_inner | epoch 046:    202 / 369 loss=2.59, nll_loss=0.974, ppl=1.96, wps=78034.6, ups=5.14, wpb=15167.4, bsz=774.5, num_updates=16800, lr=0.000578217, gnorm=0.452, loss_scale=2, train_wall=19, gb_free=29.6, wall=3530
2024-02-09 18:03:57 | INFO | train_inner | epoch 046:    302 / 369 loss=2.596, nll_loss=0.983, ppl=1.98, wps=77552.1, ups=5.15, wpb=15072.5, bsz=789.3, num_updates=16900, lr=0.000571746, gnorm=0.428, loss_scale=2, train_wall=19, gb_free=29.5, wall=3549
2024-02-09 18:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:04:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:04:11 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.344 | nll_loss 1.71 | ppl 3.27 | wps 163937 | wpb 3189.1 | bsz 163.4 | num_updates 16967 | best_loss 3.335
2024-02-09 18:04:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 16967 updates
2024-02-09 18:04:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint46.pt
2024-02-09 18:04:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint46.pt
2024-02-09 18:04:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint46.pt (epoch 46 @ 16967 updates, score 3.344) (writing took 2.4819255189504474 seconds)
2024-02-09 18:04:14 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-02-09 18:04:14 | INFO | train | epoch 046 | loss 2.588 | nll_loss 0.972 | ppl 1.96 | wps 73197.4 | ups 4.86 | wpb 15076.1 | bsz 783.3 | num_updates 16967 | lr 0.000567404 | gnorm 0.442 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3566
2024-02-09 18:04:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:04:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:04:14 | INFO | fairseq.trainer | begin training epoch 47
2024-02-09 18:04:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:04:20 | INFO | train_inner | epoch 047:     33 / 369 loss=2.586, nll_loss=0.971, ppl=1.96, wps=63249.8, ups=4.2, wpb=15050, bsz=779, num_updates=17000, lr=0.000565263, gnorm=0.447, loss_scale=2, train_wall=19, gb_free=29.6, wall=3573
2024-02-09 18:04:40 | INFO | train_inner | epoch 047:    133 / 369 loss=2.56, nll_loss=0.94, ppl=1.92, wps=78337, ups=5.16, wpb=15177.2, bsz=775.9, num_updates=17100, lr=0.000558769, gnorm=0.431, loss_scale=2, train_wall=19, gb_free=29.6, wall=3592
2024-02-09 18:04:59 | INFO | train_inner | epoch 047:    233 / 369 loss=2.566, nll_loss=0.948, ppl=1.93, wps=77983.4, ups=5.15, wpb=15130.1, bsz=791.8, num_updates=17200, lr=0.000552264, gnorm=0.492, loss_scale=2, train_wall=19, gb_free=29.6, wall=3612
2024-02-09 18:05:19 | INFO | train_inner | epoch 047:    333 / 369 loss=2.583, nll_loss=0.968, ppl=1.96, wps=77548.6, ups=5.15, wpb=15053.4, bsz=792.6, num_updates=17300, lr=0.000545751, gnorm=0.431, loss_scale=2, train_wall=19, gb_free=29.6, wall=3631
2024-02-09 18:05:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:05:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:05:27 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.348 | nll_loss 1.713 | ppl 3.28 | wps 164401 | wpb 3189.1 | bsz 163.4 | num_updates 17336 | best_loss 3.335
2024-02-09 18:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 17336 updates
2024-02-09 18:05:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint47.pt
2024-02-09 18:05:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint47.pt
2024-02-09 18:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint47.pt (epoch 47 @ 17336 updates, score 3.348) (writing took 2.33368464699015 seconds)
2024-02-09 18:05:30 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-02-09 18:05:30 | INFO | train | epoch 047 | loss 2.57 | nll_loss 0.953 | ppl 1.94 | wps 73365.3 | ups 4.87 | wpb 15076.1 | bsz 783.3 | num_updates 17336 | lr 0.000543404 | gnorm 0.45 | loss_scale 2 | train_wall 71 | gb_free 29.6 | wall 3642
2024-02-09 18:05:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:05:30 | INFO | fairseq.trainer | begin training epoch 48
2024-02-09 18:05:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:05:42 | INFO | train_inner | epoch 048:     64 / 369 loss=2.555, nll_loss=0.935, ppl=1.91, wps=63053.7, ups=4.23, wpb=14908.5, bsz=777.4, num_updates=17400, lr=0.00053923, gnorm=0.429, loss_scale=2, train_wall=19, gb_free=29.6, wall=3655
2024-02-09 18:06:02 | INFO | train_inner | epoch 048:    164 / 369 loss=2.537, nll_loss=0.915, ppl=1.88, wps=77762.5, ups=5.12, wpb=15176.2, bsz=792.2, num_updates=17500, lr=0.000532702, gnorm=0.415, loss_scale=4, train_wall=19, gb_free=29.6, wall=3674
2024-02-09 18:06:21 | INFO | train_inner | epoch 048:    264 / 369 loss=2.571, nll_loss=0.953, ppl=1.94, wps=77972.4, ups=5.17, wpb=15095.2, bsz=770.8, num_updates=17600, lr=0.000526168, gnorm=0.429, loss_scale=4, train_wall=19, gb_free=29.5, wall=3693
2024-02-09 18:06:41 | INFO | train_inner | epoch 048:    364 / 369 loss=2.564, nll_loss=0.947, ppl=1.93, wps=77000.2, ups=5.11, wpb=15073.2, bsz=788.5, num_updates=17700, lr=0.00051963, gnorm=0.429, loss_scale=4, train_wall=19, gb_free=29.5, wall=3713
2024-02-09 18:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:06:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:06:44 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.347 | nll_loss 1.712 | ppl 3.28 | wps 158806 | wpb 3189.1 | bsz 163.4 | num_updates 17705 | best_loss 3.335
2024-02-09 18:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 17705 updates
2024-02-09 18:06:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint48.pt
2024-02-09 18:06:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint48.pt
2024-02-09 18:06:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint48.pt (epoch 48 @ 17705 updates, score 3.347) (writing took 2.375932198949158 seconds)
2024-02-09 18:06:46 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-02-09 18:06:46 | INFO | train | epoch 048 | loss 2.552 | nll_loss 0.933 | ppl 1.91 | wps 73003 | ups 4.84 | wpb 15076.1 | bsz 783.3 | num_updates 17705 | lr 0.000519303 | gnorm 0.424 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 3718
2024-02-09 18:06:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:06:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:06:46 | INFO | fairseq.trainer | begin training epoch 49
2024-02-09 18:06:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:07:05 | INFO | train_inner | epoch 049:     95 / 369 loss=2.512, nll_loss=0.886, ppl=1.85, wps=62883.5, ups=4.18, wpb=15042.5, bsz=794.6, num_updates=17800, lr=0.000513088, gnorm=0.458, loss_scale=4, train_wall=19, gb_free=29.5, wall=3737
2024-02-09 18:07:24 | INFO | train_inner | epoch 049:    195 / 369 loss=2.533, nll_loss=0.91, ppl=1.88, wps=78271.7, ups=5.15, wpb=15206.4, bsz=786.2, num_updates=17900, lr=0.000506545, gnorm=0.415, loss_scale=4, train_wall=19, gb_free=29.6, wall=3756
2024-02-09 18:07:43 | INFO | train_inner | epoch 049:    295 / 369 loss=2.55, nll_loss=0.93, ppl=1.91, wps=77634.5, ups=5.19, wpb=14964.8, bsz=766.3, num_updates=18000, lr=0.0005, gnorm=0.437, loss_scale=4, train_wall=19, gb_free=29.6, wall=3776
2024-02-09 18:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:08:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:08:08 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.336 | nll_loss 1.704 | ppl 3.26 | wps 158790 | wpb 3189.1 | bsz 163.4 | num_updates 18074 | best_loss 3.335
2024-02-09 18:08:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 18074 updates
2024-02-09 18:08:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint49.pt
2024-02-09 18:08:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint49.pt
2024-02-09 18:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint49.pt (epoch 49 @ 18074 updates, score 3.336) (writing took 2.370602199109271 seconds)
2024-02-09 18:08:10 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-02-09 18:08:10 | INFO | train | epoch 049 | loss 2.535 | nll_loss 0.913 | ppl 1.88 | wps 66209.7 | ups 4.39 | wpb 15076.1 | bsz 783.3 | num_updates 18074 | lr 0.000495157 | gnorm 0.435 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 3802
2024-02-09 18:08:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:08:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:08:11 | INFO | fairseq.trainer | begin training epoch 50
2024-02-09 18:08:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:08:18 | INFO | train_inner | epoch 050:     26 / 369 loss=2.536, nll_loss=0.914, ppl=1.88, wps=43591, ups=2.9, wpb=15031.6, bsz=786.8, num_updates=18100, lr=0.000493455, gnorm=0.424, loss_scale=4, train_wall=19, gb_free=29.5, wall=3810
2024-02-09 18:08:37 | INFO | train_inner | epoch 050:    126 / 369 loss=2.506, nll_loss=0.879, ppl=1.84, wps=77000.4, ups=5.12, wpb=15027, bsz=775.4, num_updates=18200, lr=0.000486912, gnorm=0.412, loss_scale=4, train_wall=19, gb_free=29.6, wall=3830
2024-02-09 18:08:57 | INFO | train_inner | epoch 050:    226 / 369 loss=2.525, nll_loss=0.902, ppl=1.87, wps=77419.6, ups=5.15, wpb=15034.8, bsz=771.5, num_updates=18300, lr=0.00048037, gnorm=0.423, loss_scale=4, train_wall=19, gb_free=29.6, wall=3849
2024-02-09 18:09:16 | INFO | train_inner | epoch 050:    326 / 369 loss=2.515, nll_loss=0.892, ppl=1.86, wps=77857.7, ups=5.12, wpb=15197.3, bsz=809.5, num_updates=18400, lr=0.000473832, gnorm=0.466, loss_scale=4, train_wall=19, gb_free=29.6, wall=3869
2024-02-09 18:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:09:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:09:26 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.335 | nll_loss 1.706 | ppl 3.26 | wps 165049 | wpb 3189.1 | bsz 163.4 | num_updates 18443 | best_loss 3.335
2024-02-09 18:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 18443 updates
2024-02-09 18:09:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint50.pt
2024-02-09 18:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint50.pt
2024-02-09 18:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint50.pt (epoch 50 @ 18443 updates, score 3.335) (writing took 3.777624798938632 seconds)
2024-02-09 18:09:30 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-02-09 18:09:30 | INFO | train | epoch 050 | loss 2.518 | nll_loss 0.894 | ppl 1.86 | wps 69253.7 | ups 4.59 | wpb 15076.1 | bsz 783.3 | num_updates 18443 | lr 0.000471022 | gnorm 0.432 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 3883
2024-02-09 18:09:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:09:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:09:30 | INFO | fairseq.trainer | begin training epoch 51
2024-02-09 18:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:09:41 | INFO | train_inner | epoch 051:     57 / 369 loss=2.506, nll_loss=0.881, ppl=1.84, wps=59402.6, ups=3.97, wpb=14976.8, bsz=784, num_updates=18500, lr=0.000467298, gnorm=0.412, loss_scale=4, train_wall=19, gb_free=29.5, wall=3894
2024-02-09 18:10:01 | INFO | train_inner | epoch 051:    157 / 369 loss=2.487, nll_loss=0.858, ppl=1.81, wps=77856.7, ups=5.15, wpb=15107.8, bsz=793.2, num_updates=18600, lr=0.00046077, gnorm=0.417, loss_scale=4, train_wall=19, gb_free=29.6, wall=3913
2024-02-09 18:10:20 | INFO | train_inner | epoch 051:    257 / 369 loss=2.521, nll_loss=0.897, ppl=1.86, wps=78308.6, ups=5.18, wpb=15127.3, bsz=750.7, num_updates=18700, lr=0.000454249, gnorm=0.421, loss_scale=4, train_wall=19, gb_free=29.6, wall=3933
2024-02-09 18:10:40 | INFO | train_inner | epoch 051:    357 / 369 loss=2.513, nll_loss=0.889, ppl=1.85, wps=77047.3, ups=5.11, wpb=15068.7, bsz=795.5, num_updates=18800, lr=0.000447736, gnorm=0.41, loss_scale=4, train_wall=19, gb_free=29.6, wall=3952
2024-02-09 18:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:10:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:10:44 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.333 | nll_loss 1.705 | ppl 3.26 | wps 162255 | wpb 3189.1 | bsz 163.4 | num_updates 18812 | best_loss 3.333
2024-02-09 18:10:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 18812 updates
2024-02-09 18:10:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint51.pt
2024-02-09 18:10:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint51.pt
2024-02-09 18:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint51.pt (epoch 51 @ 18812 updates, score 3.333) (writing took 3.8690294548869133 seconds)
2024-02-09 18:10:48 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-02-09 18:10:48 | INFO | train | epoch 051 | loss 2.502 | nll_loss 0.876 | ppl 1.83 | wps 71744.9 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 18812 | lr 0.000446955 | gnorm 0.412 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 3960
2024-02-09 18:10:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:10:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:10:48 | INFO | fairseq.trainer | begin training epoch 52
2024-02-09 18:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:11:05 | INFO | train_inner | epoch 052:     88 / 369 loss=2.467, nll_loss=0.836, ppl=1.79, wps=59422.1, ups=3.95, wpb=15046.3, bsz=785.7, num_updates=18900, lr=0.000441231, gnorm=0.409, loss_scale=4, train_wall=19, gb_free=29.6, wall=3977
2024-02-09 18:11:24 | INFO | train_inner | epoch 052:    188 / 369 loss=2.497, nll_loss=0.871, ppl=1.83, wps=77929.1, ups=5.16, wpb=15088.3, bsz=749.6, num_updates=19000, lr=0.000434737, gnorm=0.426, loss_scale=4, train_wall=19, gb_free=29.6, wall=3997
2024-02-09 18:11:44 | INFO | train_inner | epoch 052:    288 / 369 loss=2.491, nll_loss=0.865, ppl=1.82, wps=77554.6, ups=5.15, wpb=15064, bsz=790.2, num_updates=19100, lr=0.000428254, gnorm=0.414, loss_scale=4, train_wall=19, gb_free=29.6, wall=4016
2024-02-09 18:12:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:12:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:12:01 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.332 | nll_loss 1.704 | ppl 3.26 | wps 163084 | wpb 3189.1 | bsz 163.4 | num_updates 19181 | best_loss 3.332
2024-02-09 18:12:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 19181 updates
2024-02-09 18:12:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint52.pt
2024-02-09 18:12:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint52.pt
2024-02-09 18:12:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint52.pt (epoch 52 @ 19181 updates, score 3.332) (writing took 3.8228918469976634 seconds)
2024-02-09 18:12:05 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-02-09 18:12:05 | INFO | train | epoch 052 | loss 2.484 | nll_loss 0.856 | ppl 1.81 | wps 71806.4 | ups 4.76 | wpb 15076.1 | bsz 783.3 | num_updates 19181 | lr 0.000423011 | gnorm 0.417 | loss_scale 4 | train_wall 71 | gb_free 29.7 | wall 4038
2024-02-09 18:12:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:12:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:12:05 | INFO | fairseq.trainer | begin training epoch 53
2024-02-09 18:12:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:12:09 | INFO | train_inner | epoch 053:     19 / 369 loss=2.477, nll_loss=0.849, ppl=1.8, wps=60014.5, ups=3.96, wpb=15138.6, bsz=801.3, num_updates=19200, lr=0.000421783, gnorm=0.41, loss_scale=4, train_wall=19, gb_free=29.6, wall=4041
2024-02-09 18:12:28 | INFO | train_inner | epoch 053:    119 / 369 loss=2.464, nll_loss=0.833, ppl=1.78, wps=77680.5, ups=5.16, wpb=15048.3, bsz=755, num_updates=19300, lr=0.000415325, gnorm=0.4, loss_scale=4, train_wall=19, gb_free=29.6, wall=4061
2024-02-09 18:12:48 | INFO | train_inner | epoch 053:    219 / 369 loss=2.472, nll_loss=0.842, ppl=1.79, wps=77189.8, ups=5.13, wpb=15035.2, bsz=782.7, num_updates=19400, lr=0.000408882, gnorm=0.406, loss_scale=4, train_wall=19, gb_free=29.6, wall=4080
2024-02-09 18:13:07 | INFO | train_inner | epoch 053:    319 / 369 loss=2.472, nll_loss=0.844, ppl=1.79, wps=77396.7, ups=5.12, wpb=15114.7, bsz=807.2, num_updates=19500, lr=0.000402455, gnorm=0.416, loss_scale=4, train_wall=19, gb_free=29.6, wall=4100
2024-02-09 18:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:13:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:13:19 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.332 | nll_loss 1.707 | ppl 3.27 | wps 159268 | wpb 3189.1 | bsz 163.4 | num_updates 19550 | best_loss 3.332
2024-02-09 18:13:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 19550 updates
2024-02-09 18:13:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint53.pt
2024-02-09 18:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint53.pt
2024-02-09 18:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint53.pt (epoch 53 @ 19550 updates, score 3.332) (writing took 3.884139973204583 seconds)
2024-02-09 18:13:23 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-02-09 18:13:23 | INFO | train | epoch 053 | loss 2.469 | nll_loss 0.839 | ppl 1.79 | wps 71596 | ups 4.75 | wpb 15076.1 | bsz 783.3 | num_updates 19550 | lr 0.000399247 | gnorm 0.405 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 4115
2024-02-09 18:13:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:13:23 | INFO | fairseq.trainer | begin training epoch 54
2024-02-09 18:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:13:33 | INFO | train_inner | epoch 054:     50 / 369 loss=2.452, nll_loss=0.82, ppl=1.77, wps=59490.4, ups=3.95, wpb=15054, bsz=794.4, num_updates=19600, lr=0.000396044, gnorm=0.41, loss_scale=4, train_wall=19, gb_free=29.6, wall=4125
2024-02-09 18:13:52 | INFO | train_inner | epoch 054:    150 / 369 loss=2.454, nll_loss=0.821, ppl=1.77, wps=77999.5, ups=5.15, wpb=15157.3, bsz=763.6, num_updates=19700, lr=0.000389651, gnorm=0.404, loss_scale=4, train_wall=19, gb_free=29.6, wall=4145
2024-02-09 18:14:12 | INFO | train_inner | epoch 054:    250 / 369 loss=2.451, nll_loss=0.82, ppl=1.77, wps=77666.9, ups=5.14, wpb=15101.6, bsz=804.7, num_updates=19800, lr=0.000383277, gnorm=0.396, loss_scale=4, train_wall=19, gb_free=29.6, wall=4164
2024-02-09 18:14:31 | INFO | train_inner | epoch 054:    350 / 369 loss=2.468, nll_loss=0.84, ppl=1.79, wps=77072.1, ups=5.15, wpb=14976.8, bsz=776.1, num_updates=19900, lr=0.000376923, gnorm=0.412, loss_scale=4, train_wall=19, gb_free=29.6, wall=4183
2024-02-09 18:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:14:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:14:37 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.335 | nll_loss 1.708 | ppl 3.27 | wps 166287 | wpb 3189.1 | bsz 163.4 | num_updates 19919 | best_loss 3.332
2024-02-09 18:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 19919 updates
2024-02-09 18:14:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint54.pt
2024-02-09 18:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint54.pt
2024-02-09 18:14:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint54.pt (epoch 54 @ 19919 updates, score 3.335) (writing took 2.5024319658987224 seconds)
2024-02-09 18:14:39 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-02-09 18:14:39 | INFO | train | epoch 054 | loss 2.454 | nll_loss 0.823 | ppl 1.77 | wps 73085 | ups 4.85 | wpb 15076.1 | bsz 783.3 | num_updates 19919 | lr 0.000375718 | gnorm 0.406 | loss_scale 4 | train_wall 71 | gb_free 29.5 | wall 4191
2024-02-09 18:14:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:14:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:14:39 | INFO | fairseq.trainer | begin training epoch 55
2024-02-09 18:14:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:14:55 | INFO | train_inner | epoch 055:     81 / 369 loss=2.416, nll_loss=0.779, ppl=1.72, wps=63263.7, ups=4.19, wpb=15106.9, bsz=818.1, num_updates=20000, lr=0.00037059, gnorm=0.381, loss_scale=4, train_wall=19, gb_free=29.6, wall=4207
2024-02-09 18:15:14 | INFO | train_inner | epoch 055:    181 / 369 loss=2.438, nll_loss=0.804, ppl=1.75, wps=76799.2, ups=5.13, wpb=14956.8, bsz=774.8, num_updates=20100, lr=0.00036428, gnorm=0.393, loss_scale=4, train_wall=19, gb_free=29.6, wall=4227
2024-02-09 18:15:34 | INFO | train_inner | epoch 055:    281 / 369 loss=2.456, nll_loss=0.826, ppl=1.77, wps=77715.5, ups=5.14, wpb=15125.1, bsz=761.9, num_updates=20200, lr=0.000357992, gnorm=0.398, loss_scale=4, train_wall=19, gb_free=29.6, wall=4246
2024-02-09 18:15:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:15:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:15:53 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.334 | nll_loss 1.708 | ppl 3.27 | wps 167050 | wpb 3189.1 | bsz 163.4 | num_updates 20288 | best_loss 3.332
2024-02-09 18:15:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 20288 updates
2024-02-09 18:15:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint55.pt
2024-02-09 18:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint55.pt
2024-02-09 18:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint55.pt (epoch 55 @ 20288 updates, score 3.334) (writing took 2.4112923149950802 seconds)
2024-02-09 18:15:55 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-02-09 18:15:55 | INFO | train | epoch 055 | loss 2.439 | nll_loss 0.805 | ppl 1.75 | wps 73146.5 | ups 4.85 | wpb 15076.1 | bsz 783.3 | num_updates 20288 | lr 0.000352479 | gnorm 0.391 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 4267
2024-02-09 18:15:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:15:57 | INFO | fairseq.trainer | begin training epoch 56
2024-02-09 18:15:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:16:00 | INFO | train_inner | epoch 056:     12 / 369 loss=2.442, nll_loss=0.81, ppl=1.75, wps=58506.1, ups=3.87, wpb=15131.6, bsz=793.1, num_updates=20300, lr=0.000351729, gnorm=0.39, loss_scale=4, train_wall=19, gb_free=29.6, wall=4272
2024-02-09 18:16:19 | INFO | train_inner | epoch 056:    112 / 369 loss=2.404, nll_loss=0.765, ppl=1.7, wps=78182.6, ups=5.15, wpb=15190.5, bsz=799.3, num_updates=20400, lr=0.000345492, gnorm=0.407, loss_scale=4, train_wall=19, gb_free=29.6, wall=4292
2024-02-09 18:16:39 | INFO | train_inner | epoch 056:    212 / 369 loss=2.431, nll_loss=0.797, ppl=1.74, wps=77255.7, ups=5.16, wpb=14976.7, bsz=757.7, num_updates=20500, lr=0.00033928, gnorm=0.396, loss_scale=4, train_wall=19, gb_free=29.6, wall=4311
2024-02-09 18:16:58 | INFO | train_inner | epoch 056:    312 / 369 loss=2.431, nll_loss=0.798, ppl=1.74, wps=77641.9, ups=5.13, wpb=15149.2, bsz=796.3, num_updates=20600, lr=0.000333097, gnorm=0.382, loss_scale=4, train_wall=19, gb_free=29.6, wall=4330
2024-02-09 18:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:17:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:17:11 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.338 | nll_loss 1.718 | ppl 3.29 | wps 159925 | wpb 3189.1 | bsz 163.4 | num_updates 20657 | best_loss 3.332
2024-02-09 18:17:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 20657 updates
2024-02-09 18:17:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint56.pt
2024-02-09 18:17:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint56.pt
2024-02-09 18:17:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint56.pt (epoch 56 @ 20657 updates, score 3.338) (writing took 2.349331544013694 seconds)
2024-02-09 18:17:14 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-02-09 18:17:14 | INFO | train | epoch 056 | loss 2.423 | nll_loss 0.788 | ppl 1.73 | wps 70832.2 | ups 4.7 | wpb 15076.1 | bsz 783.3 | num_updates 20657 | lr 0.000329585 | gnorm 0.393 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 4346
2024-02-09 18:17:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:17:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:17:14 | INFO | fairseq.trainer | begin training epoch 57
2024-02-09 18:17:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:17:22 | INFO | train_inner | epoch 057:     43 / 369 loss=2.412, nll_loss=0.776, ppl=1.71, wps=62235.5, ups=4.14, wpb=15026.3, bsz=775.4, num_updates=20700, lr=0.000326941, gnorm=0.375, loss_scale=4, train_wall=19, gb_free=29.6, wall=4355
2024-02-09 18:17:42 | INFO | train_inner | epoch 057:    143 / 369 loss=2.404, nll_loss=0.766, ppl=1.7, wps=77938.6, ups=5.14, wpb=15149.5, bsz=776.5, num_updates=20800, lr=0.000320816, gnorm=0.38, loss_scale=4, train_wall=19, gb_free=29.5, wall=4374
2024-02-09 18:18:01 | INFO | train_inner | epoch 057:    243 / 369 loss=2.406, nll_loss=0.77, ppl=1.7, wps=76338, ups=5.1, wpb=14965, bsz=789.8, num_updates=20900, lr=0.000314721, gnorm=0.378, loss_scale=4, train_wall=19, gb_free=29.6, wall=4394
2024-02-09 18:18:21 | INFO | train_inner | epoch 057:    343 / 369 loss=2.422, nll_loss=0.787, ppl=1.73, wps=77399.1, ups=5.14, wpb=15066.8, bsz=781.3, num_updates=21000, lr=0.000308658, gnorm=0.383, loss_scale=4, train_wall=19, gb_free=29.6, wall=4413
2024-02-09 18:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:18:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:18:28 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.339 | nll_loss 1.717 | ppl 3.29 | wps 163495 | wpb 3189.1 | bsz 163.4 | num_updates 21026 | best_loss 3.332
2024-02-09 18:18:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 21026 updates
2024-02-09 18:18:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint57.pt
2024-02-09 18:18:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint57.pt
2024-02-09 18:18:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint57.pt (epoch 57 @ 21026 updates, score 3.339) (writing took 2.5158818799536675 seconds)
2024-02-09 18:18:30 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-02-09 18:18:30 | INFO | train | epoch 057 | loss 2.409 | nll_loss 0.772 | ppl 1.71 | wps 72799.3 | ups 4.83 | wpb 15076.1 | bsz 783.3 | num_updates 21026 | lr 0.000307087 | gnorm 0.379 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 4422
2024-02-09 18:18:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:18:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:18:30 | INFO | fairseq.trainer | begin training epoch 58
2024-02-09 18:18:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:18:45 | INFO | train_inner | epoch 058:     74 / 369 loss=2.393, nll_loss=0.753, ppl=1.69, wps=63251.4, ups=4.19, wpb=15099.1, bsz=791.1, num_updates=21100, lr=0.000302628, gnorm=0.373, loss_scale=4, train_wall=19, gb_free=29.5, wall=4437
2024-02-09 18:19:04 | INFO | train_inner | epoch 058:    174 / 369 loss=2.393, nll_loss=0.754, ppl=1.69, wps=77636.3, ups=5.17, wpb=15014.6, bsz=769.2, num_updates=21200, lr=0.000296632, gnorm=0.379, loss_scale=4, train_wall=19, gb_free=29.6, wall=4456
2024-02-09 18:19:23 | INFO | train_inner | epoch 058:    274 / 369 loss=2.404, nll_loss=0.767, ppl=1.7, wps=77841.5, ups=5.16, wpb=15092.3, bsz=779, num_updates=21300, lr=0.00029067, gnorm=0.382, loss_scale=4, train_wall=19, gb_free=29.6, wall=4476
2024-02-09 18:19:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:19:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:19:44 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.343 | nll_loss 1.722 | ppl 3.3 | wps 164364 | wpb 3189.1 | bsz 163.4 | num_updates 21395 | best_loss 3.332
2024-02-09 18:19:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 21395 updates
2024-02-09 18:19:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint58.pt
2024-02-09 18:19:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint58.pt
2024-02-09 18:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint58.pt (epoch 58 @ 21395 updates, score 3.343) (writing took 2.4045327838975936 seconds)
2024-02-09 18:19:46 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-02-09 18:19:46 | INFO | train | epoch 058 | loss 2.396 | nll_loss 0.757 | ppl 1.69 | wps 73250.7 | ups 4.86 | wpb 15076.1 | bsz 783.3 | num_updates 21395 | lr 0.00028504 | gnorm 0.378 | loss_scale 4 | train_wall 71 | gb_free 29.6 | wall 4498
2024-02-09 18:19:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:19:46 | INFO | fairseq.trainer | begin training epoch 59
2024-02-09 18:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:19:47 | INFO | train_inner | epoch 059:      5 / 369 loss=2.403, nll_loss=0.767, ppl=1.7, wps=63404.6, ups=4.2, wpb=15082.1, bsz=790.6, num_updates=21400, lr=0.000284744, gnorm=0.385, loss_scale=4, train_wall=19, gb_free=29.6, wall=4499
2024-02-09 18:20:07 | INFO | train_inner | epoch 059:    105 / 369 loss=2.37, nll_loss=0.727, ppl=1.66, wps=78333.9, ups=5.15, wpb=15197, bsz=786.1, num_updates=21500, lr=0.000278856, gnorm=0.366, loss_scale=4, train_wall=19, gb_free=29.6, wall=4519
2024-02-09 18:20:26 | INFO | train_inner | epoch 059:    205 / 369 loss=2.39, nll_loss=0.751, ppl=1.68, wps=77736.2, ups=5.18, wpb=15010.6, bsz=770.8, num_updates=21600, lr=0.000273005, gnorm=0.379, loss_scale=8, train_wall=19, gb_free=29.6, wall=4538
2024-02-09 18:20:45 | INFO | train_inner | epoch 059:    305 / 369 loss=2.385, nll_loss=0.746, ppl=1.68, wps=77559.8, ups=5.14, wpb=15083.7, bsz=791.3, num_updates=21700, lr=0.000267193, gnorm=0.377, loss_scale=8, train_wall=19, gb_free=29.6, wall=4558
2024-02-09 18:20:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:20:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:21:00 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.345 | nll_loss 1.726 | ppl 3.31 | wps 164971 | wpb 3189.1 | bsz 163.4 | num_updates 21764 | best_loss 3.332
2024-02-09 18:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 21764 updates
2024-02-09 18:21:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint59.pt
2024-02-09 18:21:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint59.pt
2024-02-09 18:21:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint59.pt (epoch 59 @ 21764 updates, score 3.345) (writing took 2.3838654428254813 seconds)
2024-02-09 18:21:02 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-02-09 18:21:02 | INFO | train | epoch 059 | loss 2.382 | nll_loss 0.742 | ppl 1.67 | wps 73325.9 | ups 4.86 | wpb 15076.1 | bsz 783.3 | num_updates 21764 | lr 0.000263494 | gnorm 0.374 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 4574
2024-02-09 18:21:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:21:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:21:02 | INFO | fairseq.trainer | begin training epoch 60
2024-02-09 18:21:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:21:09 | INFO | train_inner | epoch 060:     36 / 369 loss=2.372, nll_loss=0.731, ppl=1.66, wps=63374.5, ups=4.2, wpb=15083, bsz=794.3, num_updates=21800, lr=0.000261421, gnorm=0.368, loss_scale=8, train_wall=19, gb_free=29.5, wall=4581
2024-02-09 18:21:29 | INFO | train_inner | epoch 060:    136 / 369 loss=2.36, nll_loss=0.717, ppl=1.64, wps=77116.8, ups=5.15, wpb=14982.2, bsz=792.5, num_updates=21900, lr=0.000255689, gnorm=0.372, loss_scale=8, train_wall=19, gb_free=29.6, wall=4601
2024-02-09 18:21:48 | INFO | train_inner | epoch 060:    236 / 369 loss=2.379, nll_loss=0.739, ppl=1.67, wps=76922.9, ups=5.15, wpb=14947, bsz=758.2, num_updates=22000, lr=0.00025, gnorm=0.382, loss_scale=8, train_wall=19, gb_free=29.6, wall=4620
2024-02-09 18:22:07 | INFO | train_inner | epoch 060:    336 / 369 loss=2.377, nll_loss=0.736, ppl=1.67, wps=78273.2, ups=5.13, wpb=15248.1, bsz=793.8, num_updates=22100, lr=0.000244353, gnorm=0.371, loss_scale=8, train_wall=19, gb_free=29.6, wall=4640
2024-02-09 18:22:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:22:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:22:16 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.348 | nll_loss 1.728 | ppl 3.31 | wps 163250 | wpb 3189.1 | bsz 163.4 | num_updates 22133 | best_loss 3.332
2024-02-09 18:22:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 22133 updates
2024-02-09 18:22:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint60.pt
2024-02-09 18:22:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint60.pt
2024-02-09 18:22:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint60.pt (epoch 60 @ 22133 updates, score 3.348) (writing took 2.391517783049494 seconds)
2024-02-09 18:22:18 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-02-09 18:22:18 | INFO | train | epoch 060 | loss 2.37 | nll_loss 0.728 | ppl 1.66 | wps 72982.9 | ups 4.84 | wpb 15076.1 | bsz 783.3 | num_updates 22133 | lr 0.0002425 | gnorm 0.374 | loss_scale 8 | train_wall 71 | gb_free 29.5 | wall 4650
2024-02-09 18:22:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:22:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:22:18 | INFO | fairseq.trainer | begin training epoch 61
2024-02-09 18:22:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:22:31 | INFO | train_inner | epoch 061:     67 / 369 loss=2.347, nll_loss=0.703, ppl=1.63, wps=63187.2, ups=4.19, wpb=15095.1, bsz=802.1, num_updates=22200, lr=0.000238751, gnorm=0.361, loss_scale=8, train_wall=19, gb_free=29.5, wall=4664
2024-02-09 18:22:51 | INFO | train_inner | epoch 061:    167 / 369 loss=2.354, nll_loss=0.71, ppl=1.64, wps=78482, ups=5.17, wpb=15184.5, bsz=784.5, num_updates=22300, lr=0.000233193, gnorm=0.358, loss_scale=8, train_wall=19, gb_free=29.6, wall=4683
2024-02-09 18:23:10 | INFO | train_inner | epoch 061:    267 / 369 loss=2.367, nll_loss=0.726, ppl=1.65, wps=77788.2, ups=5.15, wpb=15101.2, bsz=763, num_updates=22400, lr=0.00022768, gnorm=0.377, loss_scale=8, train_wall=19, gb_free=29.6, wall=4702
2024-02-09 18:23:29 | INFO | train_inner | epoch 061:    367 / 369 loss=2.37, nll_loss=0.729, ppl=1.66, wps=77568.7, ups=5.17, wpb=15013, bsz=782.1, num_updates=22500, lr=0.000222215, gnorm=0.371, loss_scale=8, train_wall=19, gb_free=29.6, wall=4722
2024-02-09 18:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:23:32 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.346 | nll_loss 1.728 | ppl 3.31 | wps 161873 | wpb 3189.1 | bsz 163.4 | num_updates 22502 | best_loss 3.332
2024-02-09 18:23:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 22502 updates
2024-02-09 18:23:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint61.pt
2024-02-09 18:23:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint61.pt
2024-02-09 18:23:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint61.pt (epoch 61 @ 22502 updates, score 3.346) (writing took 2.398887722985819 seconds)
2024-02-09 18:23:34 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-02-09 18:23:34 | INFO | train | epoch 061 | loss 2.358 | nll_loss 0.715 | ppl 1.64 | wps 73274 | ups 4.86 | wpb 15076.1 | bsz 783.3 | num_updates 22502 | lr 0.000222106 | gnorm 0.367 | loss_scale 8 | train_wall 71 | gb_free 29.7 | wall 4726
2024-02-09 18:23:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:23:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:23:34 | INFO | fairseq.trainer | begin training epoch 62
2024-02-09 18:23:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:23:53 | INFO | train_inner | epoch 062:     98 / 369 loss=2.332, nll_loss=0.685, ppl=1.61, wps=62942.3, ups=4.19, wpb=15008.9, bsz=799.3, num_updates=22600, lr=0.000216797, gnorm=0.357, loss_scale=8, train_wall=19, gb_free=29.6, wall=4746
2024-02-09 18:24:13 | INFO | train_inner | epoch 062:    198 / 369 loss=2.341, nll_loss=0.697, ppl=1.62, wps=78148.9, ups=5.15, wpb=15160.9, bsz=780.5, num_updates=22700, lr=0.000211427, gnorm=0.358, loss_scale=8, train_wall=19, gb_free=29.6, wall=4765
2024-02-09 18:24:32 | INFO | train_inner | epoch 062:    298 / 369 loss=2.354, nll_loss=0.711, ppl=1.64, wps=77870.3, ups=5.17, wpb=15050.5, bsz=773.6, num_updates=22800, lr=0.000206107, gnorm=0.363, loss_scale=8, train_wall=19, gb_free=29.6, wall=4784
2024-02-09 18:24:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:24:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:24:48 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.35 | nll_loss 1.731 | ppl 3.32 | wps 161462 | wpb 3189.1 | bsz 163.4 | num_updates 22871 | best_loss 3.332
2024-02-09 18:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 22871 updates
2024-02-09 18:24:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint62.pt
2024-02-09 18:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint62.pt
2024-02-09 18:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint62.pt (epoch 62 @ 22871 updates, score 3.35) (writing took 2.5293472551275045 seconds)
2024-02-09 18:24:50 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-02-09 18:24:50 | INFO | train | epoch 062 | loss 2.346 | nll_loss 0.701 | ppl 1.63 | wps 73118 | ups 4.85 | wpb 15076.1 | bsz 783.3 | num_updates 22871 | lr 0.000202361 | gnorm 0.361 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 4802
2024-02-09 18:24:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:24:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:24:50 | INFO | fairseq.trainer | begin training epoch 63
2024-02-09 18:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:24:56 | INFO | train_inner | epoch 063:     29 / 369 loss=2.353, nll_loss=0.71, ppl=1.64, wps=62976.5, ups=4.19, wpb=15025, bsz=765.7, num_updates=22900, lr=0.000200838, gnorm=0.366, loss_scale=8, train_wall=19, gb_free=29.5, wall=4808
2024-02-09 18:25:15 | INFO | train_inner | epoch 063:    129 / 369 loss=2.331, nll_loss=0.685, ppl=1.61, wps=77256.4, ups=5.17, wpb=14953.5, bsz=760.6, num_updates=23000, lr=0.000195619, gnorm=0.357, loss_scale=8, train_wall=19, gb_free=29.6, wall=4828
2024-02-09 18:25:35 | INFO | train_inner | epoch 063:    229 / 369 loss=2.332, nll_loss=0.687, ppl=1.61, wps=77955.6, ups=5.1, wpb=15279.7, bsz=790.3, num_updates=23100, lr=0.000190453, gnorm=0.362, loss_scale=8, train_wall=19, gb_free=29.6, wall=4847
2024-02-09 18:25:54 | INFO | train_inner | epoch 063:    329 / 369 loss=2.333, nll_loss=0.688, ppl=1.61, wps=76989.3, ups=5.13, wpb=15019.2, bsz=814.7, num_updates=23200, lr=0.00018534, gnorm=0.363, loss_scale=8, train_wall=19, gb_free=29.6, wall=4867
2024-02-09 18:26:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:26:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:26:04 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.358 | nll_loss 1.741 | ppl 3.34 | wps 162412 | wpb 3189.1 | bsz 163.4 | num_updates 23240 | best_loss 3.332
2024-02-09 18:26:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 23240 updates
2024-02-09 18:26:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint63.pt
2024-02-09 18:26:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint63.pt
2024-02-09 18:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint63.pt (epoch 63 @ 23240 updates, score 3.358) (writing took 2.4532447189558297 seconds)
2024-02-09 18:26:06 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-02-09 18:26:06 | INFO | train | epoch 063 | loss 2.334 | nll_loss 0.689 | ppl 1.61 | wps 73041.3 | ups 4.84 | wpb 15076.1 | bsz 783.3 | num_updates 23240 | lr 0.00018331 | gnorm 0.36 | loss_scale 8 | train_wall 71 | gb_free 29.5 | wall 4879
2024-02-09 18:26:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:26:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:26:08 | INFO | fairseq.trainer | begin training epoch 64
2024-02-09 18:26:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:26:20 | INFO | train_inner | epoch 064:     60 / 369 loss=2.329, nll_loss=0.683, ppl=1.61, wps=59006.6, ups=3.91, wpb=15075.8, bsz=775, num_updates=23300, lr=0.00018028, gnorm=0.352, loss_scale=8, train_wall=19, gb_free=29.6, wall=4892
2024-02-09 18:26:39 | INFO | train_inner | epoch 064:    160 / 369 loss=2.321, nll_loss=0.673, ppl=1.59, wps=77172.8, ups=5.15, wpb=14975, bsz=789.5, num_updates=23400, lr=0.000175276, gnorm=0.359, loss_scale=8, train_wall=19, gb_free=29.6, wall=4912
2024-02-09 18:26:59 | INFO | train_inner | epoch 064:    260 / 369 loss=2.328, nll_loss=0.683, ppl=1.61, wps=78343.8, ups=5.15, wpb=15211.4, bsz=780.2, num_updates=23500, lr=0.000170327, gnorm=0.352, loss_scale=8, train_wall=19, gb_free=29.6, wall=4931
2024-02-09 18:27:18 | INFO | train_inner | epoch 064:    360 / 369 loss=2.331, nll_loss=0.686, ppl=1.61, wps=77331.4, ups=5.14, wpb=15036, bsz=781, num_updates=23600, lr=0.000165435, gnorm=0.356, loss_scale=8, train_wall=19, gb_free=29.6, wall=4950
2024-02-09 18:27:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:27:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:27:22 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.358 | nll_loss 1.743 | ppl 3.35 | wps 165540 | wpb 3189.1 | bsz 163.4 | num_updates 23609 | best_loss 3.332
2024-02-09 18:27:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 23609 updates
2024-02-09 18:27:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint64.pt
2024-02-09 18:27:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint64.pt
2024-02-09 18:27:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint64.pt (epoch 64 @ 23609 updates, score 3.358) (writing took 2.401137595064938 seconds)
2024-02-09 18:27:24 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-02-09 18:27:24 | INFO | train | epoch 064 | loss 2.325 | nll_loss 0.678 | ppl 1.6 | wps 71499.9 | ups 4.74 | wpb 15076.1 | bsz 783.3 | num_updates 23609 | lr 0.000164997 | gnorm 0.355 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 4956
2024-02-09 18:27:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:27:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:27:24 | INFO | fairseq.trainer | begin training epoch 65
2024-02-09 18:27:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:27:42 | INFO | train_inner | epoch 065:     91 / 369 loss=2.303, nll_loss=0.654, ppl=1.57, wps=63908.8, ups=4.21, wpb=15175.3, bsz=793.3, num_updates=23700, lr=0.0001606, gnorm=0.345, loss_scale=8, train_wall=19, gb_free=29.6, wall=4974
2024-02-09 18:28:01 | INFO | train_inner | epoch 065:    191 / 369 loss=2.309, nll_loss=0.66, ppl=1.58, wps=77011.7, ups=5.12, wpb=15035, bsz=792.6, num_updates=23800, lr=0.000155823, gnorm=0.352, loss_scale=8, train_wall=19, gb_free=29.6, wall=4994
2024-02-09 18:28:21 | INFO | train_inner | epoch 065:    291 / 369 loss=2.319, nll_loss=0.672, ppl=1.59, wps=77716.2, ups=5.17, wpb=15033.8, bsz=784.4, num_updates=23900, lr=0.000151105, gnorm=0.362, loss_scale=8, train_wall=19, gb_free=29.6, wall=5013
2024-02-09 18:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:28:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:28:38 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.359 | nll_loss 1.743 | ppl 3.35 | wps 161842 | wpb 3189.1 | bsz 163.4 | num_updates 23978 | best_loss 3.332
2024-02-09 18:28:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 23978 updates
2024-02-09 18:28:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint65.pt
2024-02-09 18:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint65.pt
2024-02-09 18:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint65.pt (epoch 65 @ 23978 updates, score 3.359) (writing took 2.430264755152166 seconds)
2024-02-09 18:28:40 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-02-09 18:28:40 | INFO | train | epoch 065 | loss 2.315 | nll_loss 0.667 | ppl 1.59 | wps 73209.7 | ups 4.86 | wpb 15076.1 | bsz 783.3 | num_updates 23978 | lr 0.000147466 | gnorm 0.353 | loss_scale 8 | train_wall 71 | gb_free 29.5 | wall 5032
2024-02-09 18:28:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:28:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:28:40 | INFO | fairseq.trainer | begin training epoch 66
2024-02-09 18:28:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:28:45 | INFO | train_inner | epoch 066:     22 / 369 loss=2.321, nll_loss=0.674, ppl=1.6, wps=63245.5, ups=4.21, wpb=15028.1, bsz=762.3, num_updates=24000, lr=0.000146447, gnorm=0.352, loss_scale=8, train_wall=19, gb_free=29.6, wall=5037
2024-02-09 18:29:04 | INFO | train_inner | epoch 066:    122 / 369 loss=2.295, nll_loss=0.644, ppl=1.56, wps=77181.1, ups=5.13, wpb=15032.3, bsz=800.1, num_updates=24100, lr=0.000141849, gnorm=0.34, loss_scale=8, train_wall=19, gb_free=29.6, wall=5056
2024-02-09 18:29:23 | INFO | train_inner | epoch 066:    222 / 369 loss=2.31, nll_loss=0.661, ppl=1.58, wps=77862.3, ups=5.18, wpb=15036.3, bsz=756.9, num_updates=24200, lr=0.000137313, gnorm=0.351, loss_scale=8, train_wall=19, gb_free=29.6, wall=5076
2024-02-09 18:29:43 | INFO | train_inner | epoch 066:    322 / 369 loss=2.312, nll_loss=0.664, ppl=1.58, wps=77867.4, ups=5.16, wpb=15100.8, bsz=786.4, num_updates=24300, lr=0.000132839, gnorm=0.346, loss_scale=8, train_wall=19, gb_free=29.6, wall=5095
2024-02-09 18:29:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:29:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:29:54 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.367 | nll_loss 1.75 | ppl 3.36 | wps 166447 | wpb 3189.1 | bsz 163.4 | num_updates 24347 | best_loss 3.332
2024-02-09 18:29:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 24347 updates
2024-02-09 18:29:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint66.pt
2024-02-09 18:29:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint66.pt
2024-02-09 18:29:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint66.pt (epoch 66 @ 24347 updates, score 3.367) (writing took 2.433242800878361 seconds)
2024-02-09 18:29:56 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-02-09 18:29:56 | INFO | train | epoch 066 | loss 2.305 | nll_loss 0.656 | ppl 1.58 | wps 73181.3 | ups 4.85 | wpb 15076.1 | bsz 783.3 | num_updates 24347 | lr 0.000130758 | gnorm 0.345 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 5108
2024-02-09 18:29:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:29:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:29:56 | INFO | fairseq.trainer | begin training epoch 67
2024-02-09 18:29:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:30:07 | INFO | train_inner | epoch 067:     53 / 369 loss=2.295, nll_loss=0.645, ppl=1.56, wps=63440, ups=4.19, wpb=15131.4, bsz=799.3, num_updates=24400, lr=0.000128428, gnorm=0.339, loss_scale=8, train_wall=19, gb_free=29.5, wall=5119
2024-02-09 18:30:26 | INFO | train_inner | epoch 067:    153 / 369 loss=2.295, nll_loss=0.645, ppl=1.56, wps=77787, ups=5.14, wpb=15144.6, bsz=783.1, num_updates=24500, lr=0.00012408, gnorm=0.345, loss_scale=8, train_wall=19, gb_free=29.6, wall=5138
2024-02-09 18:30:45 | INFO | train_inner | epoch 067:    253 / 369 loss=2.3, nll_loss=0.651, ppl=1.57, wps=77192.7, ups=5.16, wpb=14964.2, bsz=779.2, num_updates=24600, lr=0.000119797, gnorm=0.346, loss_scale=8, train_wall=19, gb_free=29.6, wall=5158
2024-02-09 18:31:05 | INFO | train_inner | epoch 067:    353 / 369 loss=2.303, nll_loss=0.654, ppl=1.57, wps=77503.8, ups=5.13, wpb=15108.9, bsz=784.9, num_updates=24700, lr=0.000115579, gnorm=0.343, loss_scale=8, train_wall=19, gb_free=29.6, wall=5177
2024-02-09 18:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:31:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:31:10 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.361 | nll_loss 1.747 | ppl 3.36 | wps 159845 | wpb 3189.1 | bsz 163.4 | num_updates 24716 | best_loss 3.332
2024-02-09 18:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 24716 updates
2024-02-09 18:31:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint67.pt
2024-02-09 18:31:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint67.pt
2024-02-09 18:31:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint67.pt (epoch 67 @ 24716 updates, score 3.361) (writing took 2.511287222150713 seconds)
2024-02-09 18:31:12 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-02-09 18:31:12 | INFO | train | epoch 067 | loss 2.297 | nll_loss 0.648 | ppl 1.57 | wps 72919.3 | ups 4.84 | wpb 15076.1 | bsz 783.3 | num_updates 24716 | lr 0.00011491 | gnorm 0.343 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 5185
2024-02-09 18:31:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:31:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:31:12 | INFO | fairseq.trainer | begin training epoch 68
2024-02-09 18:31:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:31:29 | INFO | train_inner | epoch 068:     84 / 369 loss=2.282, nll_loss=0.631, ppl=1.55, wps=62998.7, ups=4.17, wpb=15122.9, bsz=792.1, num_updates=24800, lr=0.000111427, gnorm=0.336, loss_scale=8, train_wall=19, gb_free=29.6, wall=5201
2024-02-09 18:31:48 | INFO | train_inner | epoch 068:    184 / 369 loss=2.287, nll_loss=0.636, ppl=1.55, wps=77908.3, ups=5.14, wpb=15161.2, bsz=792.7, num_updates=24900, lr=0.000107342, gnorm=0.338, loss_scale=8, train_wall=19, gb_free=29.6, wall=5221
2024-02-09 18:32:08 | INFO | train_inner | epoch 068:    284 / 369 loss=2.291, nll_loss=0.641, ppl=1.56, wps=76849.1, ups=5.16, wpb=14895.1, bsz=772.7, num_updates=25000, lr=0.000103323, gnorm=0.342, loss_scale=8, train_wall=19, gb_free=29.6, wall=5240
2024-02-09 18:32:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:32:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:32:26 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.361 | nll_loss 1.746 | ppl 3.36 | wps 162406 | wpb 3189.1 | bsz 163.4 | num_updates 25085 | best_loss 3.332
2024-02-09 18:32:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 25085 updates
2024-02-09 18:32:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint68.pt
2024-02-09 18:32:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint68.pt
2024-02-09 18:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint68.pt (epoch 68 @ 25085 updates, score 3.361) (writing took 2.3535894211381674 seconds)
2024-02-09 18:32:28 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-02-09 18:32:28 | INFO | train | epoch 068 | loss 2.289 | nll_loss 0.639 | ppl 1.56 | wps 73161.9 | ups 4.85 | wpb 15076.1 | bsz 783.3 | num_updates 25085 | lr 9.99613e-05 | gnorm 0.34 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 5261
2024-02-09 18:32:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:32:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:32:28 | INFO | fairseq.trainer | begin training epoch 69
2024-02-09 18:32:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:32:31 | INFO | train_inner | epoch 069:     15 / 369 loss=2.3, nll_loss=0.651, ppl=1.57, wps=63850, ups=4.22, wpb=15114.5, bsz=755.4, num_updates=25100, lr=9.93731e-05, gnorm=0.346, loss_scale=8, train_wall=19, gb_free=29.6, wall=5264
2024-02-09 18:32:51 | INFO | train_inner | epoch 069:    115 / 369 loss=2.276, nll_loss=0.624, ppl=1.54, wps=77728.9, ups=5.13, wpb=15140.4, bsz=800.7, num_updates=25200, lr=9.54915e-05, gnorm=0.332, loss_scale=8, train_wall=19, gb_free=29.5, wall=5283
2024-02-09 18:33:10 | INFO | train_inner | epoch 069:    215 / 369 loss=2.279, nll_loss=0.628, ppl=1.55, wps=77561.9, ups=5.13, wpb=15122.2, bsz=796.8, num_updates=25300, lr=9.16792e-05, gnorm=0.334, loss_scale=8, train_wall=19, gb_free=29.6, wall=5303
2024-02-09 18:33:30 | INFO | train_inner | epoch 069:    315 / 369 loss=2.289, nll_loss=0.639, ppl=1.56, wps=77965.7, ups=5.18, wpb=15044.2, bsz=763.5, num_updates=25400, lr=8.79369e-05, gnorm=0.341, loss_scale=8, train_wall=19, gb_free=29.6, wall=5322
2024-02-09 18:33:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:33:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:33:42 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.367 | nll_loss 1.754 | ppl 3.37 | wps 164228 | wpb 3189.1 | bsz 163.4 | num_updates 25454 | best_loss 3.332
2024-02-09 18:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 25454 updates
2024-02-09 18:33:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint69.pt
2024-02-09 18:33:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint69.pt
2024-02-09 18:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint69.pt (epoch 69 @ 25454 updates, score 3.367) (writing took 2.3989443429745734 seconds)
2024-02-09 18:33:44 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-02-09 18:33:44 | INFO | train | epoch 069 | loss 2.282 | nll_loss 0.631 | ppl 1.55 | wps 73254.7 | ups 4.86 | wpb 15076.1 | bsz 783.3 | num_updates 25454 | lr 8.59454e-05 | gnorm 0.337 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 5337
2024-02-09 18:33:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:33:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:33:47 | INFO | fairseq.trainer | begin training epoch 70
2024-02-09 18:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:33:56 | INFO | train_inner | epoch 070:     46 / 369 loss=2.271, nll_loss=0.619, ppl=1.54, wps=57834.8, ups=3.82, wpb=15121.5, bsz=803, num_updates=25500, lr=8.42652e-05, gnorm=0.332, loss_scale=8, train_wall=19, gb_free=29.6, wall=5348
2024-02-09 18:34:15 | INFO | train_inner | epoch 070:    146 / 369 loss=2.28, nll_loss=0.628, ppl=1.55, wps=76607.8, ups=5.17, wpb=14831.1, bsz=760.2, num_updates=25600, lr=8.06647e-05, gnorm=0.341, loss_scale=8, train_wall=19, gb_free=29.5, wall=5368
2024-02-09 18:34:35 | INFO | train_inner | epoch 070:    246 / 369 loss=2.276, nll_loss=0.624, ppl=1.54, wps=78137.5, ups=5.14, wpb=15194.2, bsz=778.8, num_updates=25700, lr=7.71361e-05, gnorm=0.332, loss_scale=16, train_wall=19, gb_free=29.6, wall=5387
2024-02-09 18:34:54 | INFO | train_inner | epoch 070:    346 / 369 loss=2.278, nll_loss=0.626, ppl=1.54, wps=77579, ups=5.13, wpb=15128.6, bsz=794.3, num_updates=25800, lr=7.36799e-05, gnorm=0.333, loss_scale=16, train_wall=19, gb_free=29.6, wall=5406
2024-02-09 18:34:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:34:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:35:00 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.366 | nll_loss 1.753 | ppl 3.37 | wps 160808 | wpb 3189.1 | bsz 163.4 | num_updates 25823 | best_loss 3.332
2024-02-09 18:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 25823 updates
2024-02-09 18:35:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint70.pt
2024-02-09 18:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint70.pt
2024-02-09 18:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint70.pt (epoch 70 @ 25823 updates, score 3.366) (writing took 2.4241293251980096 seconds)
2024-02-09 18:35:03 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-02-09 18:35:03 | INFO | train | epoch 070 | loss 2.276 | nll_loss 0.623 | ppl 1.54 | wps 70881.9 | ups 4.7 | wpb 15076.1 | bsz 783.3 | num_updates 25823 | lr 7.28953e-05 | gnorm 0.334 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 5415
2024-02-09 18:35:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:35:03 | INFO | fairseq.trainer | begin training epoch 71
2024-02-09 18:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:35:18 | INFO | train_inner | epoch 071:     77 / 369 loss=2.267, nll_loss=0.614, ppl=1.53, wps=63327.5, ups=4.19, wpb=15119.7, bsz=783.6, num_updates=25900, lr=7.02968e-05, gnorm=0.328, loss_scale=16, train_wall=19, gb_free=29.6, wall=5430
2024-02-09 18:35:37 | INFO | train_inner | epoch 071:    177 / 369 loss=2.267, nll_loss=0.614, ppl=1.53, wps=78474.8, ups=5.16, wpb=15200.9, bsz=789.4, num_updates=26000, lr=6.69873e-05, gnorm=0.328, loss_scale=16, train_wall=19, gb_free=29.6, wall=5450
2024-02-09 18:35:57 | INFO | train_inner | epoch 071:    277 / 369 loss=2.276, nll_loss=0.624, ppl=1.54, wps=77803, ups=5.16, wpb=15080, bsz=772.5, num_updates=26100, lr=6.3752e-05, gnorm=0.336, loss_scale=16, train_wall=19, gb_free=29.6, wall=5469
2024-02-09 18:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:36:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:36:17 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.369 | nll_loss 1.755 | ppl 3.37 | wps 166606 | wpb 3189.1 | bsz 163.4 | num_updates 26192 | best_loss 3.332
2024-02-09 18:36:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 26192 updates
2024-02-09 18:36:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint71.pt
2024-02-09 18:36:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint71.pt
2024-02-09 18:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint71.pt (epoch 71 @ 26192 updates, score 3.369) (writing took 2.50203661294654 seconds)
2024-02-09 18:36:19 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-02-09 18:36:19 | INFO | train | epoch 071 | loss 2.27 | nll_loss 0.617 | ppl 1.53 | wps 73027.1 | ups 4.84 | wpb 15076.1 | bsz 783.3 | num_updates 26192 | lr 6.08415e-05 | gnorm 0.332 | loss_scale 16 | train_wall 71 | gb_free 29.5 | wall 5491
2024-02-09 18:36:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:36:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:36:19 | INFO | fairseq.trainer | begin training epoch 72
2024-02-09 18:36:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:36:21 | INFO | train_inner | epoch 072:      8 / 369 loss=2.27, nll_loss=0.617, ppl=1.53, wps=62041.8, ups=4.18, wpb=14848.9, bsz=788.1, num_updates=26200, lr=6.05914e-05, gnorm=0.337, loss_scale=16, train_wall=19, gb_free=29.6, wall=5493
2024-02-09 18:36:40 | INFO | train_inner | epoch 072:    108 / 369 loss=2.261, nll_loss=0.606, ppl=1.52, wps=77974.5, ups=5.15, wpb=15130.8, bsz=784.2, num_updates=26300, lr=5.75062e-05, gnorm=0.326, loss_scale=16, train_wall=19, gb_free=29.6, wall=5512
2024-02-09 18:36:59 | INFO | train_inner | epoch 072:    208 / 369 loss=2.271, nll_loss=0.618, ppl=1.53, wps=77985.8, ups=5.17, wpb=15088.3, bsz=767.9, num_updates=26400, lr=5.44967e-05, gnorm=0.337, loss_scale=16, train_wall=19, gb_free=29.6, wall=5532
2024-02-09 18:37:19 | INFO | train_inner | epoch 072:    308 / 369 loss=2.258, nll_loss=0.604, ppl=1.52, wps=76834.5, ups=5.11, wpb=15045.1, bsz=813.7, num_updates=26500, lr=5.15636e-05, gnorm=0.327, loss_scale=16, train_wall=19, gb_free=29.5, wall=5551
2024-02-09 18:37:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:37:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:37:33 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.37 | nll_loss 1.758 | ppl 3.38 | wps 160699 | wpb 3189.1 | bsz 163.4 | num_updates 26561 | best_loss 3.332
2024-02-09 18:37:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 26561 updates
2024-02-09 18:37:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint72.pt
2024-02-09 18:37:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint72.pt
2024-02-09 18:37:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint72.pt (epoch 72 @ 26561 updates, score 3.37) (writing took 2.511850486975163 seconds)
2024-02-09 18:37:35 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-02-09 18:37:35 | INFO | train | epoch 072 | loss 2.264 | nll_loss 0.611 | ppl 1.53 | wps 73034.5 | ups 4.84 | wpb 15076.1 | bsz 783.3 | num_updates 26561 | lr 4.98121e-05 | gnorm 0.331 | loss_scale 16 | train_wall 71 | gb_free 29.6 | wall 5568
2024-02-09 18:37:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:37:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:37:35 | INFO | fairseq.trainer | begin training epoch 73
2024-02-09 18:37:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:37:43 | INFO | train_inner | epoch 073:     39 / 369 loss=2.264, nll_loss=0.611, ppl=1.53, wps=63380.2, ups=4.19, wpb=15121, bsz=768.9, num_updates=26600, lr=4.87074e-05, gnorm=0.332, loss_scale=16, train_wall=19, gb_free=29.6, wall=5575
2024-02-09 18:38:02 | INFO | train_inner | epoch 073:    139 / 369 loss=2.261, nll_loss=0.607, ppl=1.52, wps=77944.7, ups=5.18, wpb=15043.6, bsz=778.6, num_updates=26700, lr=4.59284e-05, gnorm=0.328, loss_scale=16, train_wall=19, gb_free=29.6, wall=5595
2024-02-09 18:38:22 | INFO | train_inner | epoch 073:    239 / 369 loss=2.257, nll_loss=0.603, ppl=1.52, wps=77580.4, ups=5.15, wpb=15071.9, bsz=795.9, num_updates=26800, lr=4.32273e-05, gnorm=0.337, loss_scale=16, train_wall=19, gb_free=29.6, wall=5614
2024-02-09 18:38:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-09 18:38:41 | INFO | train_inner | epoch 073:    340 / 369 loss=2.262, nll_loss=0.608, ppl=1.52, wps=76893.3, ups=5.12, wpb=15031.1, bsz=779.9, num_updates=26900, lr=4.06044e-05, gnorm=0.331, loss_scale=8, train_wall=19, gb_free=29.6, wall=5634
2024-02-09 18:38:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:38:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:38:49 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.369 | nll_loss 1.757 | ppl 3.38 | wps 166343 | wpb 3189.1 | bsz 163.4 | num_updates 26929 | best_loss 3.332
2024-02-09 18:38:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 26929 updates
2024-02-09 18:38:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt
2024-02-09 18:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt
2024-02-09 18:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt (epoch 73 @ 26929 updates, score 3.369) (writing took 2.562596593052149 seconds)
2024-02-09 18:38:51 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-02-09 18:38:51 | INFO | train | epoch 073 | loss 2.26 | nll_loss 0.606 | ppl 1.52 | wps 72879.9 | ups 4.83 | wpb 15078.3 | bsz 782.8 | num_updates 26929 | lr 3.98585e-05 | gnorm 0.332 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 5644
2024-02-09 18:38:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:38:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:38:56 | INFO | fairseq.trainer | begin training epoch 74
2024-02-09 18:38:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:39:15 | INFO | train_inner | epoch 074:     71 / 369 loss=2.259, nll_loss=0.605, ppl=1.52, wps=45057.3, ups=2.99, wpb=15090, bsz=767.6, num_updates=27000, lr=3.80602e-05, gnorm=0.327, loss_scale=8, train_wall=19, gb_free=29.6, wall=5667
2024-02-09 18:39:34 | INFO | train_inner | epoch 074:    171 / 369 loss=2.258, nll_loss=0.603, ppl=1.52, wps=77271.6, ups=5.15, wpb=14995.9, bsz=773.1, num_updates=27100, lr=3.55952e-05, gnorm=0.326, loss_scale=8, train_wall=19, gb_free=29.6, wall=5686
2024-02-09 18:39:53 | INFO | train_inner | epoch 074:    271 / 369 loss=2.256, nll_loss=0.602, ppl=1.52, wps=77881.7, ups=5.17, wpb=15070.4, bsz=790.6, num_updates=27200, lr=3.32098e-05, gnorm=0.326, loss_scale=8, train_wall=19, gb_free=29.6, wall=5706
2024-02-09 18:40:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:40:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:40:14 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.369 | nll_loss 1.757 | ppl 3.38 | wps 163204 | wpb 3189.1 | bsz 163.4 | num_updates 27298 | best_loss 3.332
2024-02-09 18:40:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 27298 updates
2024-02-09 18:40:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt
2024-02-09 18:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt
2024-02-09 18:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt (epoch 74 @ 27298 updates, score 3.369) (writing took 2.483249621000141 seconds)
2024-02-09 18:40:17 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-02-09 18:40:17 | INFO | train | epoch 074 | loss 2.256 | nll_loss 0.602 | ppl 1.52 | wps 65111.6 | ups 4.32 | wpb 15076.1 | bsz 783.3 | num_updates 27298 | lr 3.09497e-05 | gnorm 0.325 | loss_scale 8 | train_wall 71 | gb_free 29.6 | wall 5729
2024-02-09 18:40:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:40:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:40:17 | INFO | fairseq.trainer | begin training epoch 75
2024-02-09 18:40:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:40:17 | INFO | train_inner | epoch 075:      2 / 369 loss=2.255, nll_loss=0.601, ppl=1.52, wps=63481.4, ups=4.19, wpb=15151, bsz=792.7, num_updates=27300, lr=3.09043e-05, gnorm=0.324, loss_scale=8, train_wall=19, gb_free=29.6, wall=5730
2024-02-09 18:40:37 | INFO | train_inner | epoch 075:    102 / 369 loss=2.255, nll_loss=0.601, ppl=1.52, wps=78413.5, ups=5.19, wpb=15118.9, bsz=786.6, num_updates=27400, lr=2.86793e-05, gnorm=0.327, loss_scale=8, train_wall=19, gb_free=29.5, wall=5749
2024-02-09 18:40:56 | INFO | train_inner | epoch 075:    202 / 369 loss=2.257, nll_loss=0.602, ppl=1.52, wps=78433.3, ups=5.21, wpb=15065, bsz=774.6, num_updates=27500, lr=2.65349e-05, gnorm=0.328, loss_scale=8, train_wall=19, gb_free=29.6, wall=5768
2024-02-09 18:41:15 | INFO | train_inner | epoch 075:    302 / 369 loss=2.248, nll_loss=0.593, ppl=1.51, wps=77580.9, ups=5.17, wpb=14992.1, bsz=787.5, num_updates=27600, lr=2.44717e-05, gnorm=0.324, loss_scale=8, train_wall=19, gb_free=29.6, wall=5787
2024-02-09 18:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:41:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:41:30 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.37 | nll_loss 1.759 | ppl 3.38 | wps 168839 | wpb 3189.1 | bsz 163.4 | num_updates 27667 | best_loss 3.332
2024-02-09 18:41:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 27667 updates
2024-02-09 18:41:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt
2024-02-09 18:41:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt
2024-02-09 18:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt (epoch 75 @ 27667 updates, score 3.37) (writing took 2.4210813699755818 seconds)
2024-02-09 18:41:32 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-02-09 18:41:32 | INFO | train | epoch 075 | loss 2.253 | nll_loss 0.598 | ppl 1.51 | wps 73649.1 | ups 4.89 | wpb 15076.1 | bsz 783.3 | num_updates 27667 | lr 2.3135e-05 | gnorm 0.325 | loss_scale 8 | train_wall 70 | gb_free 29.6 | wall 5805
2024-02-09 18:41:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:41:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:41:32 | INFO | fairseq.trainer | begin training epoch 76
2024-02-09 18:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:41:39 | INFO | train_inner | epoch 076:     33 / 369 loss=2.245, nll_loss=0.59, ppl=1.51, wps=63807.2, ups=4.21, wpb=15148.6, bsz=806.7, num_updates=27700, lr=2.249e-05, gnorm=0.316, loss_scale=8, train_wall=19, gb_free=29.6, wall=5811
2024-02-09 18:41:58 | INFO | train_inner | epoch 076:    133 / 369 loss=2.248, nll_loss=0.593, ppl=1.51, wps=77765.9, ups=5.18, wpb=15019.9, bsz=787.9, num_updates=27800, lr=2.05901e-05, gnorm=0.326, loss_scale=8, train_wall=19, gb_free=29.5, wall=5830
2024-02-09 18:42:17 | INFO | train_inner | epoch 076:    233 / 369 loss=2.252, nll_loss=0.598, ppl=1.51, wps=78766.7, ups=5.19, wpb=15169.8, bsz=776.4, num_updates=27900, lr=1.87724e-05, gnorm=0.328, loss_scale=8, train_wall=19, gb_free=29.6, wall=5850
2024-02-09 18:42:37 | INFO | train_inner | epoch 076:    333 / 369 loss=2.255, nll_loss=0.601, ppl=1.52, wps=78396.3, ups=5.18, wpb=15135.6, bsz=771.1, num_updates=28000, lr=1.70371e-05, gnorm=0.327, loss_scale=8, train_wall=19, gb_free=29.6, wall=5869
2024-02-09 18:42:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:42:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:42:45 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.371 | nll_loss 1.759 | ppl 3.39 | wps 169178 | wpb 3189.1 | bsz 163.4 | num_updates 28036 | best_loss 3.332
2024-02-09 18:42:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 28036 updates
2024-02-09 18:42:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt
2024-02-09 18:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt
2024-02-09 18:42:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt (epoch 76 @ 28036 updates, score 3.371) (writing took 2.383050694828853 seconds)
2024-02-09 18:42:48 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-02-09 18:42:48 | INFO | train | epoch 076 | loss 2.25 | nll_loss 0.596 | ppl 1.51 | wps 73744 | ups 4.89 | wpb 15076.1 | bsz 783.3 | num_updates 28036 | lr 1.64326e-05 | gnorm 0.325 | loss_scale 8 | train_wall 70 | gb_free 29.6 | wall 5880
2024-02-09 18:42:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:42:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 369
2024-02-09 18:42:48 | INFO | fairseq.trainer | begin training epoch 77
2024-02-09 18:42:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 18:43:00 | INFO | train_inner | epoch 077:     64 / 369 loss=2.248, nll_loss=0.593, ppl=1.51, wps=63608.4, ups=4.25, wpb=14960.2, bsz=775.7, num_updates=28100, lr=1.53845e-05, gnorm=0.325, loss_scale=8, train_wall=19, gb_free=29.6, wall=5893
2024-02-09 18:43:20 | INFO | train_inner | epoch 077:    164 / 369 loss=2.25, nll_loss=0.595, ppl=1.51, wps=78379.2, ups=5.16, wpb=15187, bsz=785.8, num_updates=28200, lr=1.3815e-05, gnorm=0.321, loss_scale=8, train_wall=19, gb_free=29.6, wall=5912
2024-02-09 18:43:39 | INFO | train_inner | epoch 077:    264 / 369 loss=2.255, nll_loss=0.6, ppl=1.52, wps=78140.9, ups=5.16, wpb=15132.2, bsz=763.6, num_updates=28300, lr=1.23288e-05, gnorm=0.326, loss_scale=8, train_wall=19, gb_free=29.6, wall=5931
2024-02-09 18:43:58 | INFO | train_inner | epoch 077:    364 / 369 loss=2.243, nll_loss=0.587, ppl=1.5, wps=77880.2, ups=5.18, wpb=15027.4, bsz=799.1, num_updates=28400, lr=1.09262e-05, gnorm=0.324, loss_scale=8, train_wall=19, gb_free=29.6, wall=5951
2024-02-09 18:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 18:43:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 18:44:01 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.372 | nll_loss 1.761 | ppl 3.39 | wps 168886 | wpb 3189.1 | bsz 163.4 | num_updates 28405 | best_loss 3.332
2024-02-09 18:44:01 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 25 runs
2024-02-09 18:44:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 28405 updates
2024-02-09 18:44:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt
2024-02-09 18:44:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt
2024-02-09 18:44:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt (epoch 77 @ 28405 updates, score 3.372) (writing took 2.3795386299025267 seconds)
2024-02-09 18:44:03 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-02-09 18:44:03 | INFO | train | epoch 077 | loss 2.248 | nll_loss 0.593 | ppl 1.51 | wps 73565.8 | ups 4.88 | wpb 15076.1 | bsz 783.3 | num_updates 28405 | lr 1.08583e-05 | gnorm 0.324 | loss_scale 8 | train_wall 70 | gb_free 29.6 | wall 5956
2024-02-09 18:44:03 | INFO | fairseq_cli.train | done training in 5953.1 seconds
Training complete.
Finetuning complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt' are the same file
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt' are the same file
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.en: 15516 sents, 256102 tokens, 0.0215% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 7560 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.de: 15516 sents, 269168 tokens, 0.0241% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
averaging checkpoints:  ['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt']
Finished writing averaged checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/finetune_mt_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 16, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 6520 types
INFO:fairseq.tasks.translation:[de] dictionary: 7560 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/test.en-de.en
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/test.en-de.de
INFO:fairseq.tasks.translation:/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset test en-de 15516 examples
INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True
INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True
INFO:fairseq.tasks.fairseq_task:rebuild_batches = False
INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1
INFO:fairseq.logging.progress_bar::    101 / 243 wps=1809
INFO:fairseq.logging.progress_bar::    201 / 243 wps=2041
INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2
INFO:fairseq_cli.generate:Translated 15,512 sentences (263,460 tokens) in 76.9s (201.66 sentences/s, 3425.03 tokens/s)
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/finetune_mt_covost/hyp_mt.txt and /home/kit/stud/uxude/predictions/finetune_mt_covost/ref_mt.txt
Sampled predictions written to /home/kit/stud/uxude/predictions/finetune_mt_covost/hyp_mt.txt.sampled
Sample predictions:
Sample: Ich weiß es nicht.
Reference: Ich weiß nicht.
Sample: Lass mich das sehen!
Reference: Zeig mal!
Sample: Aber nicht lange.
Reference: Aber nicht lange.
Sample: Wie sieht er aus?
Reference: Wie war er?
Sample: Und das letzte Mal?
Reference: Und zuletzt?
Sample: Mit all dem?
Reference: Mit allem?
Sample: Kaiserliche Diät
Reference: Reichstag
Sample: Welt des Arbeitstags
Reference: Arbeitswelt
Sample: Ich tue es noch immer.
Reference: Das mache ich weiterhin.
Sample: Zeitfliegen.
Reference: Die Zeit rast.
Sample: Du machst es.
Reference: Du machst es.
Sample: Sie ist nur.
Reference: Sie ist gerecht.
Sample: Eine Sinfonie
Reference: Eine Sinfonie
Sample: Das Leben ist schön.
Reference: Das Leben ist schön.
Sample: Das goldene Land
Reference: Das goldene Land
Sample: Gebackene Erde
Reference: Gebackene Erde
Sample: Union ist Festigkeit.
Reference: Die Union ist stark.
Sample: Sehen Sie Dienstag!
Reference: Sehen uns Dienstag!
Sample: Ein Akt des Glaubens
Reference: Ein Akt des Glaubens
Sample: Das kann nicht funktionieren.
Reference: Das ist nicht möglich.
WER:
Generate test with beam=16: BLEU4 = 29.42, 56.9/36.0/24.0/16.5 (BP=0.982, ratio=0.982, syslen=137798, reflen=140351)
BLEU:
{
 "name": "BLEU",
 "score": 29.4,
 "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:none|version:2.4.0",
 "verbose_score": "56.9/36.0/24.0/16.5 (BP = 0.982 ratio = 0.982 hyp_len = 137798 ref_len = 140350)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "none",
 "smooth": "none",
 "version": "2.4.0"
}