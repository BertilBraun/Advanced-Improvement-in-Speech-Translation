(base) [uxude@uc2n994 train]$ cat finetune_mt_paraphrased_covost_23137012.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...

[notice] A new release of pip is available: 23.3.2 -> 24.0
[notice] To update, run: pip install --upgrade pip
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[INFO] 21:21:41 [Dataset::Prepare Datasets]: Skipping dataset preparation, all config data already exists
Finetuning the paraphrased MT model...
Binarized data already exists. Skipping binarization.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models
Training time: 6 hours
2024-02-09 21:22:00 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 60000, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 25, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='cosine', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=60000, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[4], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=25, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6000, warmup_init_lr=-1, min_lr=0.0, t_mult=1.0, lr_period_updates=-1, lr_shrink=0.1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 6000, 'warmup_init_lr': -1.0, 'lr': [0.001], 'min_lr': 0.0, 't_mult': 1.0, 'lr_period_updates': -1.0, 'lr_shrink': 0.1, 'max_update': 60000}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-02-09 21:22:00 | INFO | fairseq.tasks.translation | [en] dictionary: 6616 types
2024-02-09 21:22:00 | INFO | fairseq.tasks.translation | [de] dictionary: 8648 types
2024-02-09 21:22:03 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6616, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8648, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=8648, bias=False)
  )
)
2024-02-09 21:22:03 | INFO | fairseq_cli.train | task: TranslationTask
2024-02-09 21:22:03 | INFO | fairseq_cli.train | model: TransformerModel
2024-02-09 21:22:03 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-02-09 21:22:03 | INFO | fairseq_cli.train | num. shared model params: 51,953,664 (num. trained: 51,953,664)
2024-02-09 21:22:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-02-09 21:22:03 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/valid.en-de.en
2024-02-09 21:22:03 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/valid.en-de.de
2024-02-09 21:22:03 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset valid en-de 15520 examples
2024-02-09 21:22:03 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-02-09 21:22:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-09 21:22:03 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-02-09 21:22:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-09 21:22:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-02-09 21:22:03 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-02-09 21:22:03 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint_last.pt
2024-02-09 21:22:06 | INFO | fairseq.trainer | Loaded checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint_last.pt (epoch 10 @ 6161 updates)
2024-02-09 21:22:06 | INFO | fairseq.trainer | loading train data for epoch 10
2024-02-09 21:22:06 | INFO | fairseq.data.data_utils | loaded 512,755 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/train.en-de.en
2024-02-09 21:22:07 | INFO | fairseq.data.data_utils | loaded 512,755 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/train.en-de.de
2024-02-09 21:22:07 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset train en-de 512755 examples
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 10
2024-02-09 21:22:07 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-09 21:22:07 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-09 21:22:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:22:09 | INFO | fairseq.trainer | begin training epoch 10
2024-02-09 21:22:09 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-02-09 21:22:24 | INFO | train_inner | epoch 010:     39 / 685 loss=3.729, nll_loss=2.198, ppl=4.59, wps=70489.8, ups=4.85, wpb=14567.8, bsz=757.1, num_updates=6200, lr=0.000999966, gnorm=0.749, loss_scale=16, train_wall=14, gb_free=29.6, wall=21
2024-02-09 21:22:44 | INFO | train_inner | epoch 010:    139 / 685 loss=3.741, nll_loss=2.212, ppl=4.63, wps=73062.4, ups=4.99, wpb=14628.6, bsz=744.2, num_updates=6300, lr=0.000999924, gnorm=0.74, loss_scale=16, train_wall=20, gb_free=29.6, wall=41
2024-02-09 21:23:04 | INFO | train_inner | epoch 010:    239 / 685 loss=3.737, nll_loss=2.208, ppl=4.62, wps=72314.3, ups=5, wpb=14461.4, bsz=752.5, num_updates=6400, lr=0.000999865, gnorm=0.736, loss_scale=16, train_wall=20, gb_free=29.5, wall=61
2024-02-09 21:23:24 | INFO | train_inner | epoch 010:    339 / 685 loss=3.733, nll_loss=2.206, ppl=4.61, wps=72859, ups=4.99, wpb=14604.1, bsz=745.8, num_updates=6500, lr=0.000999788, gnorm=0.719, loss_scale=16, train_wall=20, gb_free=29.5, wall=81
2024-02-09 21:23:44 | INFO | train_inner | epoch 010:    439 / 685 loss=3.736, nll_loss=2.209, ppl=4.62, wps=72439.4, ups=5.01, wpb=14471.3, bsz=748.3, num_updates=6600, lr=0.000999695, gnorm=0.755, loss_scale=16, train_wall=20, gb_free=29.5, wall=101
2024-02-09 21:24:04 | INFO | train_inner | epoch 010:    539 / 685 loss=3.724, nll_loss=2.197, ppl=4.58, wps=73499, ups=5.02, wpb=14638.7, bsz=750, num_updates=6700, lr=0.000999585, gnorm=0.719, loss_scale=16, train_wall=20, gb_free=29.5, wall=121
2024-02-09 21:24:24 | INFO | train_inner | epoch 010:    639 / 685 loss=3.73, nll_loss=2.204, ppl=4.61, wps=72705.7, ups=5.03, wpb=14448.2, bsz=747.5, num_updates=6800, lr=0.000999459, gnorm=0.751, loss_scale=16, train_wall=20, gb_free=29.5, wall=141
2024-02-09 21:24:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:24:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:24:35 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.788 | nll_loss 2.165 | ppl 4.48 | wps 158937 | wpb 3189.1 | bsz 163.4 | num_updates 6846 | best_loss 3.788
2024-02-09 21:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6846 updates
2024-02-09 21:24:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt
2024-02-09 21:24:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt
2024-02-09 21:24:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt (epoch 10 @ 6846 updates, score 3.788) (writing took 3.924240530002862 seconds)
2024-02-09 21:24:39 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-02-09 21:24:39 | INFO | train | epoch 010 | loss 3.733 | nll_loss 2.205 | ppl 4.61 | wps 69689.7 | ups 4.8 | wpb 14524.4 | bsz 748.5 | num_updates 6846 | lr 0.000999395 | gnorm 0.738 | loss_scale 16 | train_wall 142 | gb_free 29.6 | wall 155
2024-02-09 21:24:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:24:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:24:39 | INFO | fairseq.trainer | begin training epoch 11
2024-02-09 21:24:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:24:50 | INFO | train_inner | epoch 011:     54 / 685 loss=3.651, nll_loss=2.111, ppl=4.32, wps=55575.5, ups=3.88, wpb=14311.9, bsz=738.4, num_updates=6900, lr=0.000999315, gnorm=0.749, loss_scale=16, train_wall=20, gb_free=29.5, wall=166
2024-02-09 21:25:09 | INFO | train_inner | epoch 011:    154 / 685 loss=3.625, nll_loss=2.081, ppl=4.23, wps=73014.3, ups=5.05, wpb=14445.8, bsz=738.6, num_updates=7000, lr=0.000999154, gnorm=0.739, loss_scale=16, train_wall=20, gb_free=29.6, wall=186
2024-02-09 21:25:29 | INFO | train_inner | epoch 011:    254 / 685 loss=3.63, nll_loss=2.089, ppl=4.25, wps=73119.6, ups=5.01, wpb=14585.5, bsz=747.8, num_updates=7100, lr=0.000998976, gnorm=0.719, loss_scale=16, train_wall=20, gb_free=29.5, wall=206
2024-02-09 21:25:49 | INFO | train_inner | epoch 011:    354 / 685 loss=3.637, nll_loss=2.097, ppl=4.28, wps=73066.3, ups=5, wpb=14627.3, bsz=775.2, num_updates=7200, lr=0.000998782, gnorm=0.747, loss_scale=16, train_wall=20, gb_free=29.5, wall=226
2024-02-09 21:26:09 | INFO | train_inner | epoch 011:    454 / 685 loss=3.625, nll_loss=2.085, ppl=4.24, wps=72054.8, ups=4.99, wpb=14434.9, bsz=760.1, num_updates=7300, lr=0.000998571, gnorm=0.732, loss_scale=16, train_wall=20, gb_free=29.5, wall=246
2024-02-09 21:26:29 | INFO | train_inner | epoch 011:    554 / 685 loss=3.633, nll_loss=2.094, ppl=4.27, wps=73242.5, ups=5, wpb=14659.1, bsz=731, num_updates=7400, lr=0.000998342, gnorm=0.729, loss_scale=16, train_wall=20, gb_free=29.5, wall=266
2024-02-09 21:26:49 | INFO | train_inner | epoch 011:    654 / 685 loss=3.632, nll_loss=2.094, ppl=4.27, wps=72820.5, ups=5.01, wpb=14548.3, bsz=751.2, num_updates=7500, lr=0.000998097, gnorm=0.735, loss_scale=16, train_wall=20, gb_free=29.5, wall=286
2024-02-09 21:26:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:26:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:26:57 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.753 | nll_loss 2.117 | ppl 4.34 | wps 154028 | wpb 3189.1 | bsz 163.4 | num_updates 7531 | best_loss 3.753
2024-02-09 21:26:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 7531 updates
2024-02-09 21:26:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt
2024-02-09 21:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt
2024-02-09 21:27:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt (epoch 11 @ 7531 updates, score 3.753) (writing took 27.54804197489284 seconds)
2024-02-09 21:27:25 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-02-09 21:27:25 | INFO | train | epoch 011 | loss 3.627 | nll_loss 2.086 | ppl 4.24 | wps 59844.4 | ups 4.12 | wpb 14524.4 | bsz 748.5 | num_updates 7531 | lr 0.000998018 | gnorm 0.735 | loss_scale 16 | train_wall 135 | gb_free 29.7 | wall 322
2024-02-09 21:27:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:27:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:27:25 | INFO | fairseq.trainer | begin training epoch 12
2024-02-09 21:27:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:27:39 | INFO | train_inner | epoch 012:     69 / 685 loss=3.549, nll_loss=1.995, ppl=3.99, wps=29124, ups=2.02, wpb=14414.9, bsz=734, num_updates=7600, lr=0.000997835, gnorm=0.722, loss_scale=16, train_wall=20, gb_free=29.5, wall=336
2024-02-09 21:27:59 | INFO | train_inner | epoch 012:    169 / 685 loss=3.515, nll_loss=1.959, ppl=3.89, wps=74226.6, ups=5.03, wpb=14758.4, bsz=769.9, num_updates=7700, lr=0.000997557, gnorm=0.702, loss_scale=16, train_wall=20, gb_free=29.6, wall=356
2024-02-09 21:28:19 | INFO | train_inner | epoch 012:    269 / 685 loss=3.538, nll_loss=1.985, ppl=3.96, wps=73812.6, ups=4.99, wpb=14799.4, bsz=752.5, num_updates=7800, lr=0.000997261, gnorm=0.729, loss_scale=16, train_wall=20, gb_free=29.5, wall=376
2024-02-09 21:28:39 | INFO | train_inner | epoch 012:    369 / 685 loss=3.547, nll_loss=1.996, ppl=3.99, wps=72403.2, ups=5.01, wpb=14461.5, bsz=732.8, num_updates=7900, lr=0.000996948, gnorm=0.757, loss_scale=16, train_wall=20, gb_free=29.6, wall=396
2024-02-09 21:28:59 | INFO | train_inner | epoch 012:    469 / 685 loss=3.534, nll_loss=1.983, ppl=3.95, wps=72948.2, ups=5.01, wpb=14563.8, bsz=758.2, num_updates=8000, lr=0.000996619, gnorm=0.724, loss_scale=16, train_wall=20, gb_free=29.5, wall=415
2024-02-09 21:29:19 | INFO | train_inner | epoch 012:    569 / 685 loss=3.544, nll_loss=1.993, ppl=3.98, wps=71302.6, ups=4.98, wpb=14310.2, bsz=748.6, num_updates=8100, lr=0.000996273, gnorm=0.724, loss_scale=16, train_wall=20, gb_free=29.5, wall=436
2024-02-09 21:29:39 | INFO | train_inner | epoch 012:    669 / 685 loss=3.544, nll_loss=1.994, ppl=3.98, wps=71619.2, ups=4.97, wpb=14412, bsz=742.6, num_updates=8200, lr=0.00099591, gnorm=0.718, loss_scale=16, train_wall=20, gb_free=29.5, wall=456
2024-02-09 21:29:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:29:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:29:44 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.788 | nll_loss 2.17 | ppl 4.5 | wps 158360 | wpb 3189.1 | bsz 163.4 | num_updates 8216 | best_loss 3.753
2024-02-09 21:29:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 8216 updates
2024-02-09 21:29:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt
2024-02-09 21:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt
2024-02-09 21:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt (epoch 12 @ 8216 updates, score 3.788) (writing took 2.4751451599877328 seconds)
2024-02-09 21:29:46 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-02-09 21:29:46 | INFO | train | epoch 012 | loss 3.536 | nll_loss 1.983 | ppl 3.95 | wps 70359 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 8216 | lr 0.000995851 | gnorm 0.727 | loss_scale 16 | train_wall 135 | gb_free 29.8 | wall 463
2024-02-09 21:29:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:29:46 | INFO | fairseq.trainer | begin training epoch 13
2024-02-09 21:29:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:30:03 | INFO | train_inner | epoch 013:     84 / 685 loss=3.437, nll_loss=1.869, ppl=3.65, wps=59101.7, ups=4.09, wpb=14436.1, bsz=755.5, num_updates=8300, lr=0.00099553, gnorm=0.726, loss_scale=16, train_wall=20, gb_free=29.6, wall=480
2024-02-09 21:30:23 | INFO | train_inner | epoch 013:    184 / 685 loss=3.45, nll_loss=1.884, ppl=3.69, wps=72261.7, ups=4.99, wpb=14478, bsz=771.1, num_updates=8400, lr=0.000995134, gnorm=0.722, loss_scale=16, train_wall=20, gb_free=29.5, wall=500
2024-02-09 21:30:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-09 21:30:43 | INFO | train_inner | epoch 013:    285 / 685 loss=3.468, nll_loss=1.905, ppl=3.75, wps=72129.7, ups=5, wpb=14435.3, bsz=733.4, num_updates=8500, lr=0.000994721, gnorm=0.749, loss_scale=8, train_wall=20, gb_free=29.6, wall=520
2024-02-09 21:31:03 | INFO | train_inner | epoch 013:    385 / 685 loss=3.482, nll_loss=1.922, ppl=3.79, wps=72033.5, ups=5.01, wpb=14381.1, bsz=723.8, num_updates=8600, lr=0.000994291, gnorm=0.711, loss_scale=8, train_wall=20, gb_free=29.5, wall=540
2024-02-09 21:31:23 | INFO | train_inner | epoch 013:    485 / 685 loss=3.474, nll_loss=1.914, ppl=3.77, wps=73557.4, ups=4.99, wpb=14749.4, bsz=754.9, num_updates=8700, lr=0.000993844, gnorm=0.7, loss_scale=8, train_wall=20, gb_free=29.5, wall=560
2024-02-09 21:31:44 | INFO | train_inner | epoch 013:    585 / 685 loss=3.465, nll_loss=1.904, ppl=3.74, wps=72655.3, ups=4.99, wpb=14574.8, bsz=768.4, num_updates=8800, lr=0.000993381, gnorm=0.701, loss_scale=8, train_wall=20, gb_free=29.5, wall=580
2024-02-09 21:32:03 | INFO | train_inner | epoch 013:    685 / 685 loss=3.47, nll_loss=1.911, ppl=3.76, wps=73133.4, ups=5.05, wpb=14484.9, bsz=731.6, num_updates=8900, lr=0.000992901, gnorm=0.74, loss_scale=8, train_wall=20, gb_free=29.6, wall=600
2024-02-09 21:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:32:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:32:05 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.657 | nll_loss 2.035 | ppl 4.1 | wps 155149 | wpb 3189.1 | bsz 163.4 | num_updates 8900 | best_loss 3.657
2024-02-09 21:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 8900 updates
2024-02-09 21:32:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt
2024-02-09 21:32:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt
2024-02-09 21:32:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt (epoch 13 @ 8900 updates, score 3.657) (writing took 3.8346351680811495 seconds)
2024-02-09 21:32:09 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-02-09 21:32:09 | INFO | train | epoch 013 | loss 3.461 | nll_loss 1.899 | ppl 3.73 | wps 69576.2 | ups 4.79 | wpb 14522 | bsz 748.6 | num_updates 8900 | lr 0.000992901 | gnorm 0.719 | loss_scale 8 | train_wall 135 | gb_free 29.6 | wall 606
2024-02-09 21:32:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:32:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:32:09 | INFO | fairseq.trainer | begin training epoch 14
2024-02-09 21:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-09 21:32:30 | INFO | train_inner | epoch 014:    101 / 685 loss=3.365, nll_loss=1.788, ppl=3.45, wps=56047.8, ups=3.82, wpb=14670.6, bsz=754.3, num_updates=9000, lr=0.000992404, gnorm=0.73, loss_scale=4, train_wall=20, gb_free=29.4, wall=626
2024-02-09 21:32:50 | INFO | train_inner | epoch 014:    201 / 685 loss=3.384, nll_loss=1.812, ppl=3.51, wps=73321.9, ups=4.99, wpb=14689.9, bsz=754.4, num_updates=9100, lr=0.00099189, gnorm=0.714, loss_scale=4, train_wall=20, gb_free=29.5, wall=646
2024-02-09 21:33:10 | INFO | train_inner | epoch 014:    301 / 685 loss=3.398, nll_loss=1.827, ppl=3.55, wps=71469.1, ups=5, wpb=14287.8, bsz=753.5, num_updates=9200, lr=0.00099136, gnorm=0.724, loss_scale=4, train_wall=20, gb_free=29.5, wall=666
2024-02-09 21:33:30 | INFO | train_inner | epoch 014:    401 / 685 loss=3.392, nll_loss=1.821, ppl=3.53, wps=72396.5, ups=5.01, wpb=14463.4, bsz=745.8, num_updates=9300, lr=0.000990814, gnorm=0.7, loss_scale=4, train_wall=20, gb_free=29.5, wall=686
2024-02-09 21:33:50 | INFO | train_inner | epoch 014:    501 / 685 loss=3.413, nll_loss=1.846, ppl=3.6, wps=71864.4, ups=4.98, wpb=14439, bsz=749.2, num_updates=9400, lr=0.00099025, gnorm=0.702, loss_scale=4, train_wall=20, gb_free=29.5, wall=706
2024-02-09 21:34:10 | INFO | train_inner | epoch 014:    601 / 685 loss=3.397, nll_loss=1.828, ppl=3.55, wps=72704.6, ups=4.99, wpb=14582, bsz=749, num_updates=9500, lr=0.00098967, gnorm=0.675, loss_scale=4, train_wall=20, gb_free=29.5, wall=726
2024-02-09 21:34:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:34:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:34:28 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.658 | nll_loss 2.024 | ppl 4.07 | wps 152936 | wpb 3189.1 | bsz 163.4 | num_updates 9584 | best_loss 3.657
2024-02-09 21:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 9584 updates
2024-02-09 21:34:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt
2024-02-09 21:34:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt
2024-02-09 21:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt (epoch 14 @ 9584 updates, score 3.658) (writing took 2.4298390599433333 seconds)
2024-02-09 21:34:31 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-02-09 21:34:31 | INFO | train | epoch 014 | loss 3.394 | nll_loss 1.824 | ppl 3.54 | wps 70127.3 | ups 4.83 | wpb 14525.6 | bsz 748.8 | num_updates 9584 | lr 0.00098917 | gnorm 0.713 | loss_scale 4 | train_wall 135 | gb_free 30 | wall 748
2024-02-09 21:34:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:34:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:34:31 | INFO | fairseq.trainer | begin training epoch 15
2024-02-09 21:34:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:34:34 | INFO | train_inner | epoch 015:     16 / 685 loss=3.4, nll_loss=1.832, ppl=3.56, wps=59557.4, ups=4.08, wpb=14590.9, bsz=730.2, num_updates=9600, lr=0.000989074, gnorm=0.745, loss_scale=4, train_wall=20, gb_free=29.5, wall=751
2024-02-09 21:34:54 | INFO | train_inner | epoch 015:    116 / 685 loss=3.313, nll_loss=1.729, ppl=3.32, wps=72686.8, ups=4.99, wpb=14558.2, bsz=726.7, num_updates=9700, lr=0.000988461, gnorm=0.673, loss_scale=4, train_wall=20, gb_free=29.6, wall=771
2024-02-09 21:35:14 | INFO | train_inner | epoch 015:    216 / 685 loss=3.33, nll_loss=1.751, ppl=3.37, wps=72220.9, ups=4.95, wpb=14576.9, bsz=760.7, num_updates=9800, lr=0.000987831, gnorm=0.693, loss_scale=4, train_wall=20, gb_free=29.5, wall=791
2024-02-09 21:35:35 | INFO | train_inner | epoch 015:    316 / 685 loss=3.332, nll_loss=1.754, ppl=3.37, wps=71301.9, ups=4.95, wpb=14396.1, bsz=755, num_updates=9900, lr=0.000987185, gnorm=0.719, loss_scale=4, train_wall=20, gb_free=29.5, wall=811
2024-02-09 21:35:55 | INFO | train_inner | epoch 015:    416 / 685 loss=3.33, nll_loss=1.752, ppl=3.37, wps=72103, ups=4.97, wpb=14515.5, bsz=764.2, num_updates=10000, lr=0.000986522, gnorm=0.698, loss_scale=4, train_wall=20, gb_free=29.5, wall=831
2024-02-09 21:36:15 | INFO | train_inner | epoch 015:    516 / 685 loss=3.349, nll_loss=1.775, ppl=3.42, wps=72775.2, ups=5, wpb=14556, bsz=733.8, num_updates=10100, lr=0.000985843, gnorm=0.69, loss_scale=4, train_wall=20, gb_free=29.6, wall=851
2024-02-09 21:36:35 | INFO | train_inner | epoch 015:    616 / 685 loss=3.345, nll_loss=1.769, ppl=3.41, wps=71922.5, ups=4.96, wpb=14513.1, bsz=757.3, num_updates=10200, lr=0.000985148, gnorm=0.687, loss_scale=4, train_wall=20, gb_free=29.6, wall=872
2024-02-09 21:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:36:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:36:51 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.624 | nll_loss 2.009 | ppl 4.02 | wps 151746 | wpb 3189.1 | bsz 163.4 | num_updates 10269 | best_loss 3.624
2024-02-09 21:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 10269 updates
2024-02-09 21:36:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt
2024-02-09 21:36:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt
2024-02-09 21:36:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt (epoch 15 @ 10269 updates, score 3.624) (writing took 3.8531442780513316 seconds)
2024-02-09 21:36:55 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-02-09 21:36:55 | INFO | train | epoch 015 | loss 3.334 | nll_loss 1.757 | ppl 3.38 | wps 69256.8 | ups 4.77 | wpb 14524.4 | bsz 748.5 | num_updates 10269 | lr 0.000984658 | gnorm 0.696 | loss_scale 4 | train_wall 136 | gb_free 29.7 | wall 891
2024-02-09 21:36:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:36:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:36:55 | INFO | fairseq.trainer | begin training epoch 16
2024-02-09 21:36:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:37:01 | INFO | train_inner | epoch 016:     31 / 685 loss=3.321, nll_loss=1.742, ppl=3.35, wps=55975.1, ups=3.85, wpb=14549.2, bsz=746.5, num_updates=10300, lr=0.000984436, gnorm=0.726, loss_scale=4, train_wall=20, gb_free=29.5, wall=898
2024-02-09 21:37:21 | INFO | train_inner | epoch 016:    131 / 685 loss=3.268, nll_loss=1.679, ppl=3.2, wps=72276.7, ups=4.96, wpb=14567.8, bsz=740.4, num_updates=10400, lr=0.000983708, gnorm=0.698, loss_scale=4, train_wall=20, gb_free=29.5, wall=918
2024-02-09 21:37:41 | INFO | train_inner | epoch 016:    231 / 685 loss=3.289, nll_loss=1.704, ppl=3.26, wps=70900, ups=4.95, wpb=14315.8, bsz=738.6, num_updates=10500, lr=0.000982963, gnorm=0.698, loss_scale=4, train_wall=20, gb_free=29.5, wall=938
2024-02-09 21:38:01 | INFO | train_inner | epoch 016:    331 / 685 loss=3.274, nll_loss=1.689, ppl=3.22, wps=72063.2, ups=4.95, wpb=14545.8, bsz=753.9, num_updates=10600, lr=0.000982202, gnorm=0.648, loss_scale=4, train_wall=20, gb_free=29.6, wall=958
2024-02-09 21:38:22 | INFO | train_inner | epoch 016:    431 / 685 loss=3.284, nll_loss=1.7, ppl=3.25, wps=72609.6, ups=4.94, wpb=14702.3, bsz=747.2, num_updates=10700, lr=0.000981425, gnorm=0.679, loss_scale=4, train_wall=20, gb_free=29.5, wall=978
2024-02-09 21:38:42 | INFO | train_inner | epoch 016:    531 / 685 loss=3.304, nll_loss=1.725, ppl=3.3, wps=71987.2, ups=4.92, wpb=14620.3, bsz=756.8, num_updates=10800, lr=0.000980631, gnorm=0.76, loss_scale=4, train_wall=20, gb_free=29.5, wall=999
2024-02-09 21:39:02 | INFO | train_inner | epoch 016:    631 / 685 loss=3.276, nll_loss=1.692, ppl=3.23, wps=73011.6, ups=5, wpb=14595.6, bsz=752.5, num_updates=10900, lr=0.000979821, gnorm=0.658, loss_scale=4, train_wall=20, gb_free=29.5, wall=1019
2024-02-09 21:39:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:39:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:39:15 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.609 | nll_loss 1.991 | ppl 3.98 | wps 156101 | wpb 3189.1 | bsz 163.4 | num_updates 10954 | best_loss 3.609
2024-02-09 21:39:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 10954 updates
2024-02-09 21:39:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt
2024-02-09 21:39:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt
2024-02-09 21:39:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt (epoch 16 @ 10954 updates, score 3.609) (writing took 4.139961776090786 seconds)
2024-02-09 21:39:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-02-09 21:39:19 | INFO | train | epoch 016 | loss 3.283 | nll_loss 1.699 | ppl 3.25 | wps 68889.7 | ups 4.74 | wpb 14524.4 | bsz 748.5 | num_updates 10954 | lr 0.000979377 | gnorm 0.693 | loss_scale 4 | train_wall 136 | gb_free 29.9 | wall 1036
2024-02-09 21:39:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:39:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:39:19 | INFO | fairseq.trainer | begin training epoch 17
2024-02-09 21:39:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:39:28 | INFO | train_inner | epoch 017:     46 / 685 loss=3.257, nll_loss=1.669, ppl=3.18, wps=54015.7, ups=3.79, wpb=14248.6, bsz=752.5, num_updates=11000, lr=0.000978995, gnorm=0.718, loss_scale=4, train_wall=20, gb_free=29.6, wall=1045
2024-02-09 21:39:48 | INFO | train_inner | epoch 017:    146 / 685 loss=3.207, nll_loss=1.61, ppl=3.05, wps=71575.9, ups=4.98, wpb=14375, bsz=742.5, num_updates=11100, lr=0.000978152, gnorm=0.704, loss_scale=4, train_wall=20, gb_free=29.6, wall=1065
2024-02-09 21:40:09 | INFO | train_inner | epoch 017:    246 / 685 loss=3.233, nll_loss=1.641, ppl=3.12, wps=72294.9, ups=4.98, wpb=14503.8, bsz=740.4, num_updates=11200, lr=0.000977294, gnorm=0.669, loss_scale=4, train_wall=20, gb_free=29.5, wall=1085
2024-02-09 21:40:29 | INFO | train_inner | epoch 017:    346 / 685 loss=3.24, nll_loss=1.652, ppl=3.14, wps=72231.7, ups=4.95, wpb=14594.3, bsz=768.2, num_updates=11300, lr=0.000976419, gnorm=0.678, loss_scale=4, train_wall=20, gb_free=29.5, wall=1105
2024-02-09 21:40:49 | INFO | train_inner | epoch 017:    446 / 685 loss=3.241, nll_loss=1.653, ppl=3.14, wps=71647.6, ups=4.96, wpb=14451.4, bsz=767.3, num_updates=11400, lr=0.000975528, gnorm=0.712, loss_scale=4, train_wall=20, gb_free=29.6, wall=1126
2024-02-09 21:41:09 | INFO | train_inner | epoch 017:    546 / 685 loss=3.247, nll_loss=1.661, ppl=3.16, wps=72456.2, ups=4.97, wpb=14589.5, bsz=743.8, num_updates=11500, lr=0.000974621, gnorm=0.671, loss_scale=4, train_wall=20, gb_free=29.5, wall=1146
2024-02-09 21:41:29 | INFO | train_inner | epoch 017:    646 / 685 loss=3.252, nll_loss=1.666, ppl=3.17, wps=73221, ups=4.98, wpb=14697, bsz=742.9, num_updates=11600, lr=0.000973698, gnorm=0.636, loss_scale=4, train_wall=20, gb_free=29.5, wall=1166
2024-02-09 21:41:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:41:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:41:39 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.608 | nll_loss 1.982 | ppl 3.95 | wps 161187 | wpb 3189.1 | bsz 163.4 | num_updates 11639 | best_loss 3.608
2024-02-09 21:41:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 11639 updates
2024-02-09 21:41:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt
2024-02-09 21:41:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt
2024-02-09 21:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt (epoch 17 @ 11639 updates, score 3.608) (writing took 3.888128135120496 seconds)
2024-02-09 21:41:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-02-09 21:41:42 | INFO | train | epoch 017 | loss 3.237 | nll_loss 1.647 | ppl 3.13 | wps 69356.5 | ups 4.78 | wpb 14524.4 | bsz 748.5 | num_updates 11639 | lr 0.000973334 | gnorm 0.684 | loss_scale 4 | train_wall 136 | gb_free 29.6 | wall 1179
2024-02-09 21:41:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:41:42 | INFO | fairseq.trainer | begin training epoch 18
2024-02-09 21:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:41:55 | INFO | train_inner | epoch 018:     61 / 685 loss=3.2, nll_loss=1.605, ppl=3.04, wps=56552.2, ups=3.89, wpb=14528.7, bsz=727.4, num_updates=11700, lr=0.000972759, gnorm=0.751, loss_scale=4, train_wall=20, gb_free=29.5, wall=1191
2024-02-09 21:42:15 | INFO | train_inner | epoch 018:    161 / 685 loss=3.182, nll_loss=1.584, ppl=3, wps=72675.7, ups=4.98, wpb=14605.7, bsz=740.6, num_updates=11800, lr=0.000971804, gnorm=0.675, loss_scale=4, train_wall=20, gb_free=29.5, wall=1212
2024-02-09 21:42:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-09 21:42:35 | INFO | train_inner | epoch 018:    262 / 685 loss=3.19, nll_loss=1.594, ppl=3.02, wps=71238.1, ups=4.92, wpb=14480.9, bsz=759.2, num_updates=11900, lr=0.000970833, gnorm=0.685, loss_scale=2, train_wall=20, gb_free=29.5, wall=1232
2024-02-09 21:42:55 | INFO | train_inner | epoch 018:    362 / 685 loss=3.189, nll_loss=1.594, ppl=3.02, wps=72801.8, ups=4.98, wpb=14630.1, bsz=756.4, num_updates=12000, lr=0.000969846, gnorm=0.63, loss_scale=2, train_wall=20, gb_free=29.6, wall=1252
2024-02-09 21:43:15 | INFO | train_inner | epoch 018:    462 / 685 loss=3.207, nll_loss=1.615, ppl=3.06, wps=72034.2, ups=5, wpb=14402.5, bsz=722.8, num_updates=12100, lr=0.000968843, gnorm=0.709, loss_scale=2, train_wall=20, gb_free=29.5, wall=1272
2024-02-09 21:43:35 | INFO | train_inner | epoch 018:    562 / 685 loss=3.2, nll_loss=1.608, ppl=3.05, wps=72393.2, ups=4.96, wpb=14591.6, bsz=757.8, num_updates=12200, lr=0.000967825, gnorm=0.653, loss_scale=2, train_wall=20, gb_free=29.5, wall=1292
2024-02-09 21:43:56 | INFO | train_inner | epoch 018:    662 / 685 loss=3.206, nll_loss=1.616, ppl=3.06, wps=71717.2, ups=4.96, wpb=14466, bsz=762.6, num_updates=12300, lr=0.00096679, gnorm=0.671, loss_scale=2, train_wall=20, gb_free=29.5, wall=1312
2024-02-09 21:44:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:44:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:44:02 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.563 | nll_loss 1.952 | ppl 3.87 | wps 156792 | wpb 3189.1 | bsz 163.4 | num_updates 12323 | best_loss 3.563
2024-02-09 21:44:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 12323 updates
2024-02-09 21:44:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt
2024-02-09 21:44:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt
2024-02-09 21:44:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt (epoch 18 @ 12323 updates, score 3.563) (writing took 3.8339251251891255 seconds)
2024-02-09 21:44:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-02-09 21:44:06 | INFO | train | epoch 018 | loss 3.193 | nll_loss 1.599 | ppl 3.03 | wps 69249.2 | ups 4.77 | wpb 14524.8 | bsz 748.2 | num_updates 12323 | lr 0.00096655 | gnorm 0.681 | loss_scale 2 | train_wall 136 | gb_free 29.8 | wall 1323
2024-02-09 21:44:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:44:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:44:06 | INFO | fairseq.trainer | begin training epoch 19
2024-02-09 21:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:44:21 | INFO | train_inner | epoch 019:     77 / 685 loss=3.143, nll_loss=1.54, ppl=2.91, wps=55939.4, ups=3.9, wpb=14352, bsz=747.2, num_updates=12400, lr=0.00096574, gnorm=0.691, loss_scale=2, train_wall=19, gb_free=29.5, wall=1338
2024-02-09 21:44:41 | INFO | train_inner | epoch 019:    177 / 685 loss=3.133, nll_loss=1.53, ppl=2.89, wps=72206.4, ups=4.96, wpb=14555, bsz=755, num_updates=12500, lr=0.000964674, gnorm=0.707, loss_scale=2, train_wall=20, gb_free=29.5, wall=1358
2024-02-09 21:45:01 | INFO | train_inner | epoch 019:    277 / 685 loss=3.164, nll_loss=1.566, ppl=2.96, wps=72602.8, ups=4.99, wpb=14543.7, bsz=736, num_updates=12600, lr=0.000963592, gnorm=0.653, loss_scale=2, train_wall=20, gb_free=29.5, wall=1378
2024-02-09 21:45:22 | INFO | train_inner | epoch 019:    377 / 685 loss=3.156, nll_loss=1.558, ppl=2.94, wps=72346.4, ups=4.96, wpb=14584.8, bsz=748.5, num_updates=12700, lr=0.000962494, gnorm=0.638, loss_scale=2, train_wall=20, gb_free=29.5, wall=1398
2024-02-09 21:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2024-02-09 21:45:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2024-02-09 21:45:42 | INFO | train_inner | epoch 019:    479 / 685 loss=3.166, nll_loss=1.57, ppl=2.97, wps=70772.2, ups=4.87, wpb=14518.1, bsz=756.2, num_updates=12800, lr=0.000961381, gnorm=0.662, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1419
2024-02-09 21:46:02 | INFO | train_inner | epoch 019:    579 / 685 loss=3.16, nll_loss=1.564, ppl=2.96, wps=72465.6, ups=4.95, wpb=14626, bsz=758.2, num_updates=12900, lr=0.000960252, gnorm=0.751, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1439
2024-02-09 21:46:22 | INFO | train_inner | epoch 019:    679 / 685 loss=3.171, nll_loss=1.577, ppl=2.98, wps=72080.7, ups=4.96, wpb=14518.8, bsz=738.9, num_updates=13000, lr=0.000959108, gnorm=0.748, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1459
2024-02-09 21:46:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:46:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:46:25 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.561 | nll_loss 1.964 | ppl 3.9 | wps 156584 | wpb 3189.1 | bsz 163.4 | num_updates 13006 | best_loss 3.561
2024-02-09 21:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 13006 updates
2024-02-09 21:46:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt
2024-02-09 21:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt
2024-02-09 21:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt (epoch 19 @ 13006 updates, score 3.561) (writing took 3.8977299630641937 seconds)
2024-02-09 21:46:29 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-02-09 21:46:29 | INFO | train | epoch 019 | loss 3.154 | nll_loss 1.555 | ppl 2.94 | wps 69123.5 | ups 4.76 | wpb 14523.1 | bsz 748.3 | num_updates 13006 | lr 0.000959039 | gnorm 0.695 | loss_scale 0.5 | train_wall 136 | gb_free 29.8 | wall 1466
2024-02-09 21:46:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:46:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:46:29 | INFO | fairseq.trainer | begin training epoch 20
2024-02-09 21:46:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:46:48 | INFO | train_inner | epoch 020:     94 / 685 loss=3.1, nll_loss=1.492, ppl=2.81, wps=55309.2, ups=3.85, wpb=14360.7, bsz=755.2, num_updates=13100, lr=0.000957948, gnorm=0.692, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1485
2024-02-09 21:47:09 | INFO | train_inner | epoch 020:    194 / 685 loss=3.12, nll_loss=1.516, ppl=2.86, wps=71848.8, ups=4.98, wpb=14435, bsz=729.8, num_updates=13200, lr=0.000956773, gnorm=0.672, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=1505
2024-02-09 21:47:29 | INFO | train_inner | epoch 020:    294 / 685 loss=3.123, nll_loss=1.52, ppl=2.87, wps=72722.5, ups=5, wpb=14541.4, bsz=734.2, num_updates=13300, lr=0.000955582, gnorm=0.694, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1525
2024-02-09 21:47:49 | INFO | train_inner | epoch 020:    394 / 685 loss=3.126, nll_loss=1.525, ppl=2.88, wps=73113.2, ups=4.99, wpb=14645.4, bsz=769.8, num_updates=13400, lr=0.000954376, gnorm=0.63, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=1545
2024-02-09 21:48:08 | INFO | train_inner | epoch 020:    494 / 685 loss=3.124, nll_loss=1.522, ppl=2.87, wps=72506.1, ups=5.02, wpb=14456.4, bsz=743.9, num_updates=13500, lr=0.000953154, gnorm=0.646, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1565
2024-02-09 21:48:28 | INFO | train_inner | epoch 020:    594 / 685 loss=3.128, nll_loss=1.528, ppl=2.88, wps=74180.2, ups=5.03, wpb=14747.2, bsz=742.7, num_updates=13600, lr=0.000951917, gnorm=0.657, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=1585
2024-02-09 21:48:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:48:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:48:48 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.524 | nll_loss 1.901 | ppl 3.74 | wps 159263 | wpb 3189.1 | bsz 163.4 | num_updates 13691 | best_loss 3.524
2024-02-09 21:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 13691 updates
2024-02-09 21:48:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt
2024-02-09 21:48:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt
2024-02-09 21:48:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt (epoch 20 @ 13691 updates, score 3.524) (writing took 3.814893828937784 seconds)
2024-02-09 21:48:52 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-02-09 21:48:52 | INFO | train | epoch 020 | loss 3.121 | nll_loss 1.518 | ppl 2.86 | wps 69643 | ups 4.79 | wpb 14524.4 | bsz 748.5 | num_updates 13691 | lr 0.000950778 | gnorm 0.671 | loss_scale 0.5 | train_wall 135 | gb_free 29.6 | wall 1609
2024-02-09 21:48:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:48:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:48:52 | INFO | fairseq.trainer | begin training epoch 21
2024-02-09 21:48:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:48:54 | INFO | train_inner | epoch 021:      9 / 685 loss=3.119, nll_loss=1.518, ppl=2.86, wps=55867.4, ups=3.88, wpb=14411.6, bsz=757.5, num_updates=13700, lr=0.000950665, gnorm=0.715, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1611
2024-02-09 21:49:14 | INFO | train_inner | epoch 021:    109 / 685 loss=3.055, nll_loss=1.443, ppl=2.72, wps=72887.9, ups=5.03, wpb=14487.4, bsz=740.9, num_updates=13800, lr=0.000949397, gnorm=0.613, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=1631
2024-02-09 21:49:34 | INFO | train_inner | epoch 021:    209 / 685 loss=3.066, nll_loss=1.457, ppl=2.75, wps=72506.8, ups=4.99, wpb=14543.8, bsz=763.8, num_updates=13900, lr=0.000948114, gnorm=0.633, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1651
2024-02-09 21:49:54 | INFO | train_inner | epoch 021:    309 / 685 loss=3.09, nll_loss=1.484, ppl=2.8, wps=73688.9, ups=5.04, wpb=14619.1, bsz=755.4, num_updates=14000, lr=0.000946816, gnorm=0.624, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=1671
2024-02-09 21:50:14 | INFO | train_inner | epoch 021:    409 / 685 loss=3.087, nll_loss=1.482, ppl=2.79, wps=71936.8, ups=4.98, wpb=14440.5, bsz=765.4, num_updates=14100, lr=0.000945503, gnorm=0.627, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1691
2024-02-09 21:50:34 | INFO | train_inner | epoch 021:    509 / 685 loss=3.097, nll_loss=1.493, ppl=2.81, wps=72100.7, ups=5.03, wpb=14345, bsz=727.5, num_updates=14200, lr=0.000944175, gnorm=0.634, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1711
2024-02-09 21:50:54 | INFO | train_inner | epoch 021:    609 / 685 loss=3.103, nll_loss=1.501, ppl=2.83, wps=73586.8, ups=5.01, wpb=14683.8, bsz=745.2, num_updates=14300, lr=0.000942832, gnorm=0.763, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1731
2024-02-09 21:51:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:51:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:51:11 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.52 | nll_loss 1.897 | ppl 3.73 | wps 157878 | wpb 3189.1 | bsz 163.4 | num_updates 14376 | best_loss 3.52
2024-02-09 21:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 14376 updates
2024-02-09 21:51:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt
2024-02-09 21:51:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt
2024-02-09 21:51:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt (epoch 21 @ 14376 updates, score 3.52) (writing took 3.829822472995147 seconds)
2024-02-09 21:51:15 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-02-09 21:51:15 | INFO | train | epoch 021 | loss 3.085 | nll_loss 1.478 | ppl 2.79 | wps 69865.6 | ups 4.81 | wpb 14524.4 | bsz 748.5 | num_updates 14376 | lr 0.000941801 | gnorm 0.649 | loss_scale 0.5 | train_wall 135 | gb_free 29.6 | wall 1751
2024-02-09 21:51:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:51:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:51:15 | INFO | fairseq.trainer | begin training epoch 22
2024-02-09 21:51:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:51:20 | INFO | train_inner | epoch 022:     24 / 685 loss=3.08, nll_loss=1.474, ppl=2.78, wps=56396, ups=3.89, wpb=14505.6, bsz=737.6, num_updates=14400, lr=0.000941474, gnorm=0.661, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1756
2024-02-09 21:51:40 | INFO | train_inner | epoch 022:    124 / 685 loss=3.022, nll_loss=1.405, ppl=2.65, wps=73200.1, ups=5.01, wpb=14617.2, bsz=752.7, num_updates=14500, lr=0.000940101, gnorm=0.618, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=1776
2024-02-09 21:52:00 | INFO | train_inner | epoch 022:    224 / 685 loss=3.025, nll_loss=1.411, ppl=2.66, wps=72848.6, ups=4.99, wpb=14602.5, bsz=741, num_updates=14600, lr=0.000938713, gnorm=0.63, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1796
2024-02-09 21:52:20 | INFO | train_inner | epoch 022:    324 / 685 loss=3.065, nll_loss=1.458, ppl=2.75, wps=73386.7, ups=4.95, wpb=14830.7, bsz=766.1, num_updates=14700, lr=0.00093731, gnorm=0.66, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1817
2024-02-09 21:52:40 | INFO | train_inner | epoch 022:    424 / 685 loss=3.062, nll_loss=1.454, ppl=2.74, wps=71693.7, ups=5.02, wpb=14283.3, bsz=764.1, num_updates=14800, lr=0.000935892, gnorm=0.645, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1836
2024-02-09 21:52:59 | INFO | train_inner | epoch 022:    524 / 685 loss=3.071, nll_loss=1.466, ppl=2.76, wps=74952.5, ups=5.12, wpb=14631, bsz=745.1, num_updates=14900, lr=0.00093446, gnorm=0.623, loss_scale=0.5, train_wall=19, gb_free=29.5, wall=1856
2024-02-09 21:53:19 | INFO | train_inner | epoch 022:    624 / 685 loss=3.075, nll_loss=1.47, ppl=2.77, wps=73183.3, ups=5.08, wpb=14396.1, bsz=738.1, num_updates=15000, lr=0.000933013, gnorm=0.683, loss_scale=0.5, train_wall=19, gb_free=29.6, wall=1876
2024-02-09 21:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:53:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:53:33 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.509 | nll_loss 1.895 | ppl 3.72 | wps 160766 | wpb 3189.1 | bsz 163.4 | num_updates 15061 | best_loss 3.509
2024-02-09 21:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 15061 updates
2024-02-09 21:53:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt
2024-02-09 21:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt
2024-02-09 21:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt (epoch 22 @ 15061 updates, score 3.509) (writing took 3.8572396049275994 seconds)
2024-02-09 21:53:36 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-02-09 21:53:36 | INFO | train | epoch 022 | loss 3.054 | nll_loss 1.444 | ppl 2.72 | wps 70138.8 | ups 4.83 | wpb 14524.4 | bsz 748.5 | num_updates 15061 | lr 0.000932123 | gnorm 0.646 | loss_scale 0.5 | train_wall 134 | gb_free 29.6 | wall 1893
2024-02-09 21:53:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:53:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:53:37 | INFO | fairseq.trainer | begin training epoch 23
2024-02-09 21:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:53:44 | INFO | train_inner | epoch 023:     39 / 685 loss=3.037, nll_loss=1.425, ppl=2.68, wps=56553.6, ups=3.94, wpb=14346.3, bsz=741.4, num_updates=15100, lr=0.000931551, gnorm=0.637, loss_scale=0.5, train_wall=19, gb_free=29.5, wall=1901
2024-02-09 21:54:04 | INFO | train_inner | epoch 023:    139 / 685 loss=2.992, nll_loss=1.374, ppl=2.59, wps=73284.4, ups=5.06, wpb=14479.3, bsz=755, num_updates=15200, lr=0.000930075, gnorm=0.582, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1921
2024-02-09 21:54:24 | INFO | train_inner | epoch 023:    239 / 685 loss=3.015, nll_loss=1.401, ppl=2.64, wps=73539.5, ups=5.09, wpb=14460.5, bsz=747.6, num_updates=15300, lr=0.000928584, gnorm=0.613, loss_scale=0.5, train_wall=19, gb_free=29.5, wall=1940
2024-02-09 21:54:44 | INFO | train_inner | epoch 023:    339 / 685 loss=3.037, nll_loss=1.427, ppl=2.69, wps=73349.7, ups=5.05, wpb=14510.9, bsz=734.6, num_updates=15400, lr=0.000927078, gnorm=0.682, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=1960
2024-02-09 21:55:03 | INFO | train_inner | epoch 023:    439 / 685 loss=3.02, nll_loss=1.408, ppl=2.65, wps=74827.5, ups=5.1, wpb=14672.9, bsz=744.5, num_updates=15500, lr=0.000925558, gnorm=0.607, loss_scale=0.5, train_wall=19, gb_free=29.5, wall=1980
2024-02-09 21:55:23 | INFO | train_inner | epoch 023:    539 / 685 loss=3.039, nll_loss=1.43, ppl=2.69, wps=73749.9, ups=5.1, wpb=14454.6, bsz=752.4, num_updates=15600, lr=0.000924024, gnorm=0.631, loss_scale=0.5, train_wall=19, gb_free=29.5, wall=1999
2024-02-09 21:55:43 | INFO | train_inner | epoch 023:    639 / 685 loss=3.037, nll_loss=1.428, ppl=2.69, wps=72255.6, ups=4.97, wpb=14552, bsz=746.2, num_updates=15700, lr=0.000922475, gnorm=0.633, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=2020
2024-02-09 21:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:55:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:55:54 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.521 | nll_loss 1.908 | ppl 3.75 | wps 155849 | wpb 3189.1 | bsz 163.4 | num_updates 15746 | best_loss 3.509
2024-02-09 21:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 15746 updates
2024-02-09 21:55:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt
2024-02-09 21:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt
2024-02-09 21:55:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt (epoch 23 @ 15746 updates, score 3.521) (writing took 2.509915897157043 seconds)
2024-02-09 21:55:56 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-02-09 21:55:56 | INFO | train | epoch 023 | loss 3.023 | nll_loss 1.41 | ppl 2.66 | wps 71063.2 | ups 4.89 | wpb 14524.4 | bsz 748.5 | num_updates 15746 | lr 0.000921758 | gnorm 0.624 | loss_scale 0.5 | train_wall 134 | gb_free 29.7 | wall 2033
2024-02-09 21:55:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:55:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:55:57 | INFO | fairseq.trainer | begin training epoch 24
2024-02-09 21:55:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:56:08 | INFO | train_inner | epoch 024:     54 / 685 loss=3.005, nll_loss=1.391, ppl=2.62, wps=59154.9, ups=4.05, wpb=14589.8, bsz=746, num_updates=15800, lr=0.000920912, gnorm=0.624, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=2044
2024-02-09 21:56:28 | INFO | train_inner | epoch 024:    154 / 685 loss=2.972, nll_loss=1.353, ppl=2.55, wps=71200.1, ups=4.92, wpb=14482.9, bsz=768.2, num_updates=15900, lr=0.000919335, gnorm=0.734, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=2065
2024-02-09 21:56:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2024-02-09 21:56:48 | INFO | train_inner | epoch 024:    255 / 685 loss=2.997, nll_loss=1.382, ppl=2.61, wps=71172.2, ups=4.89, wpb=14555.4, bsz=742.6, num_updates=16000, lr=0.000917744, gnorm=0.643, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2085
2024-02-09 21:57:09 | INFO | train_inner | epoch 024:    355 / 685 loss=3, nll_loss=1.385, ppl=2.61, wps=72316.3, ups=4.93, wpb=14669.8, bsz=741.8, num_updates=16100, lr=0.000916138, gnorm=0.629, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2105
2024-02-09 21:57:29 | INFO | train_inner | epoch 024:    455 / 685 loss=3.014, nll_loss=1.402, ppl=2.64, wps=71637.2, ups=4.95, wpb=14483.6, bsz=728.6, num_updates=16200, lr=0.000914519, gnorm=0.717, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2126
2024-02-09 21:57:49 | INFO | train_inner | epoch 024:    555 / 685 loss=3.014, nll_loss=1.403, ppl=2.65, wps=72116, ups=4.95, wpb=14571, bsz=749.3, num_updates=16300, lr=0.000912885, gnorm=0.623, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2146
2024-02-09 21:58:09 | INFO | train_inner | epoch 024:    655 / 685 loss=3.011, nll_loss=1.4, ppl=2.64, wps=71224.8, ups=4.93, wpb=14443, bsz=768.3, num_updates=16400, lr=0.000911238, gnorm=0.7, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2166
2024-02-09 21:58:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 21:58:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:58:17 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.543 | nll_loss 1.936 | ppl 3.83 | wps 153294 | wpb 3189.1 | bsz 163.4 | num_updates 16430 | best_loss 3.509
2024-02-09 21:58:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 16430 updates
2024-02-09 21:58:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt
2024-02-09 21:58:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt
2024-02-09 21:58:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt (epoch 24 @ 16430 updates, score 3.543) (writing took 2.4309028279967606 seconds)
2024-02-09 21:58:20 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-02-09 21:58:20 | INFO | train | epoch 024 | loss 2.999 | nll_loss 1.385 | ppl 2.61 | wps 69457.8 | ups 4.78 | wpb 14523.2 | bsz 748.6 | num_updates 16430 | lr 0.000910741 | gnorm 0.667 | loss_scale 0.25 | train_wall 137 | gb_free 29.9 | wall 2176
2024-02-09 21:58:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 21:58:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 21:58:20 | INFO | fairseq.trainer | begin training epoch 25
2024-02-09 21:58:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 21:58:34 | INFO | train_inner | epoch 025:     70 / 685 loss=2.955, nll_loss=1.334, ppl=2.52, wps=58623.8, ups=4.08, wpb=14356, bsz=765.6, num_updates=16500, lr=0.000909576, gnorm=0.608, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2190
2024-02-09 21:58:54 | INFO | train_inner | epoch 025:    170 / 685 loss=2.947, nll_loss=1.325, ppl=2.51, wps=71949, ups=4.95, wpb=14543.3, bsz=767, num_updates=16600, lr=0.000907901, gnorm=0.597, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2211
2024-02-09 21:59:14 | INFO | train_inner | epoch 025:    270 / 685 loss=2.972, nll_loss=1.354, ppl=2.56, wps=71823.5, ups=4.96, wpb=14490.5, bsz=725.3, num_updates=16700, lr=0.000906211, gnorm=0.638, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2231
2024-02-09 21:59:34 | INFO | train_inner | epoch 025:    370 / 685 loss=2.982, nll_loss=1.367, ppl=2.58, wps=71941.5, ups=4.93, wpb=14580.8, bsz=740.6, num_updates=16800, lr=0.000904508, gnorm=0.634, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2251
2024-02-09 21:59:55 | INFO | train_inner | epoch 025:    470 / 685 loss=2.987, nll_loss=1.373, ppl=2.59, wps=72197.3, ups=4.93, wpb=14639.5, bsz=734.8, num_updates=16900, lr=0.000902792, gnorm=0.613, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2271
2024-02-09 22:00:15 | INFO | train_inner | epoch 025:    570 / 685 loss=2.989, nll_loss=1.376, ppl=2.6, wps=72072.6, ups=4.96, wpb=14535.9, bsz=757.2, num_updates=17000, lr=0.000901062, gnorm=0.764, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2292
2024-02-09 22:00:35 | INFO | train_inner | epoch 025:    670 / 685 loss=2.987, nll_loss=1.373, ppl=2.59, wps=72177.2, ups=4.96, wpb=14537.4, bsz=765.8, num_updates=17100, lr=0.000899318, gnorm=0.651, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2312
2024-02-09 22:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:00:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:00:40 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.499 | nll_loss 1.894 | ppl 3.72 | wps 151298 | wpb 3189.1 | bsz 163.4 | num_updates 17115 | best_loss 3.499
2024-02-09 22:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 17115 updates
2024-02-09 22:00:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt
2024-02-09 22:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt
2024-02-09 22:00:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt (epoch 25 @ 17115 updates, score 3.499) (writing took 3.8333374040666968 seconds)
2024-02-09 22:00:44 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-02-09 22:00:44 | INFO | train | epoch 025 | loss 2.973 | nll_loss 1.356 | ppl 2.56 | wps 69000.8 | ups 4.75 | wpb 14524.4 | bsz 748.5 | num_updates 17115 | lr 0.000899055 | gnorm 0.645 | loss_scale 0.25 | train_wall 136 | gb_free 29.7 | wall 2320
2024-02-09 22:00:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:00:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:00:44 | INFO | fairseq.trainer | begin training epoch 26
2024-02-09 22:00:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:01:01 | INFO | train_inner | epoch 026:     85 / 685 loss=2.932, nll_loss=1.308, ppl=2.48, wps=55792.7, ups=3.88, wpb=14395, bsz=720.7, num_updates=17200, lr=0.00089756, gnorm=0.634, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2338
2024-02-09 22:01:21 | INFO | train_inner | epoch 026:    185 / 685 loss=2.927, nll_loss=1.303, ppl=2.47, wps=70902.8, ups=4.95, wpb=14335.7, bsz=780.9, num_updates=17300, lr=0.00089579, gnorm=0.618, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2358
2024-02-09 22:01:41 | INFO | train_inner | epoch 026:    285 / 685 loss=2.935, nll_loss=1.313, ppl=2.49, wps=73027.8, ups=4.98, wpb=14669.9, bsz=753.4, num_updates=17400, lr=0.000894005, gnorm=0.593, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2378
2024-02-09 22:02:01 | INFO | train_inner | epoch 026:    385 / 685 loss=2.965, nll_loss=1.347, ppl=2.54, wps=72343.9, ups=5.02, wpb=14404.7, bsz=722.5, num_updates=17500, lr=0.000892208, gnorm=0.617, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2398
2024-02-09 22:02:21 | INFO | train_inner | epoch 026:    485 / 685 loss=2.947, nll_loss=1.329, ppl=2.51, wps=72544, ups=4.98, wpb=14559.3, bsz=759.2, num_updates=17600, lr=0.000890397, gnorm=0.612, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2418
2024-02-09 22:02:41 | INFO | train_inner | epoch 026:    585 / 685 loss=2.965, nll_loss=1.35, ppl=2.55, wps=73311.9, ups=5, wpb=14666.8, bsz=743.7, num_updates=17700, lr=0.000888573, gnorm=0.652, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2438
2024-02-09 22:03:01 | INFO | train_inner | epoch 026:    685 / 685 loss=2.972, nll_loss=1.359, ppl=2.56, wps=73176.5, ups=5.05, wpb=14492.9, bsz=744.6, num_updates=17800, lr=0.000886736, gnorm=0.657, loss_scale=0.25, train_wall=20, gb_free=29.7, wall=2458
2024-02-09 22:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:03:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:03:03 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.477 | nll_loss 1.861 | ppl 3.63 | wps 153127 | wpb 3189.1 | bsz 163.4 | num_updates 17800 | best_loss 3.477
2024-02-09 22:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 17800 updates
2024-02-09 22:03:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt
2024-02-09 22:03:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt
2024-02-09 22:03:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt (epoch 26 @ 17800 updates, score 3.477) (writing took 3.842441380955279 seconds)
2024-02-09 22:03:07 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-02-09 22:03:07 | INFO | train | epoch 026 | loss 2.948 | nll_loss 1.329 | ppl 2.51 | wps 69525.4 | ups 4.79 | wpb 14524.4 | bsz 748.5 | num_updates 17800 | lr 0.000886736 | gnorm 0.626 | loss_scale 0.25 | train_wall 135 | gb_free 29.7 | wall 2463
2024-02-09 22:03:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:03:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:03:07 | INFO | fairseq.trainer | begin training epoch 27
2024-02-09 22:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:03:27 | INFO | train_inner | epoch 027:    100 / 685 loss=2.894, nll_loss=1.266, ppl=2.41, wps=55892.3, ups=3.85, wpb=14515.3, bsz=758.2, num_updates=17900, lr=0.000884886, gnorm=0.603, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2484
2024-02-09 22:03:47 | INFO | train_inner | epoch 027:    200 / 685 loss=2.92, nll_loss=1.297, ppl=2.46, wps=72586.9, ups=5, wpb=14511.9, bsz=742.4, num_updates=18000, lr=0.000883022, gnorm=1.149, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2504
2024-02-09 22:04:07 | INFO | train_inner | epoch 027:    300 / 685 loss=2.929, nll_loss=1.309, ppl=2.48, wps=73095.3, ups=5, wpb=14605.1, bsz=760.4, num_updates=18100, lr=0.000881146, gnorm=0.7, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2524
2024-02-09 22:04:27 | INFO | train_inner | epoch 027:    400 / 685 loss=2.932, nll_loss=1.312, ppl=2.48, wps=72160.8, ups=4.98, wpb=14502.1, bsz=758.2, num_updates=18200, lr=0.000879257, gnorm=0.914, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2544
2024-02-09 22:04:47 | INFO | train_inner | epoch 027:    500 / 685 loss=2.932, nll_loss=1.312, ppl=2.48, wps=72202, ups=4.98, wpb=14486.1, bsz=767, num_updates=18300, lr=0.000877355, gnorm=0.638, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2564
2024-02-09 22:05:07 | INFO | train_inner | epoch 027:    600 / 685 loss=2.947, nll_loss=1.33, ppl=2.51, wps=73314.1, ups=5, wpb=14674.9, bsz=719.2, num_updates=18400, lr=0.00087544, gnorm=0.644, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2584
2024-02-09 22:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:05:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:05:26 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.497 | nll_loss 1.894 | ppl 3.72 | wps 148731 | wpb 3189.1 | bsz 163.4 | num_updates 18485 | best_loss 3.477
2024-02-09 22:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 18485 updates
2024-02-09 22:05:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt
2024-02-09 22:05:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt
2024-02-09 22:05:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt (epoch 27 @ 18485 updates, score 3.497) (writing took 2.436770285014063 seconds)
2024-02-09 22:05:28 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-02-09 22:05:28 | INFO | train | epoch 027 | loss 2.928 | nll_loss 1.308 | ppl 2.48 | wps 70293.7 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 18485 | lr 0.000873802 | gnorm 0.758 | loss_scale 0.25 | train_wall 135 | gb_free 29.7 | wall 2605
2024-02-09 22:05:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:05:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:05:28 | INFO | fairseq.trainer | begin training epoch 28
2024-02-09 22:05:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:05:31 | INFO | train_inner | epoch 028:     15 / 685 loss=2.938, nll_loss=1.32, ppl=2.5, wps=58904.9, ups=4.1, wpb=14384.4, bsz=741.6, num_updates=18500, lr=0.000873513, gnorm=0.633, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2608
2024-02-09 22:05:52 | INFO | train_inner | epoch 028:    115 / 685 loss=2.87, nll_loss=1.24, ppl=2.36, wps=72094.9, ups=4.99, wpb=14449.5, bsz=755, num_updates=18600, lr=0.000871572, gnorm=0.612, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2628
2024-02-09 22:06:12 | INFO | train_inner | epoch 028:    215 / 685 loss=2.89, nll_loss=1.264, ppl=2.4, wps=73121.1, ups=4.98, wpb=14674.9, bsz=764.6, num_updates=18700, lr=0.00086962, gnorm=0.601, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2648
2024-02-09 22:06:32 | INFO | train_inner | epoch 028:    315 / 685 loss=2.905, nll_loss=1.282, ppl=2.43, wps=72650.5, ups=4.98, wpb=14588.4, bsz=742.6, num_updates=18800, lr=0.000867655, gnorm=0.649, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2668
2024-02-09 22:06:52 | INFO | train_inner | epoch 028:    415 / 685 loss=2.916, nll_loss=1.296, ppl=2.46, wps=73046.2, ups=5.04, wpb=14497.8, bsz=745.6, num_updates=18900, lr=0.000865677, gnorm=0.64, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2688
2024-02-09 22:07:12 | INFO | train_inner | epoch 028:    515 / 685 loss=2.915, nll_loss=1.294, ppl=2.45, wps=72707.2, ups=4.98, wpb=14603.7, bsz=760.3, num_updates=19000, lr=0.000863687, gnorm=0.605, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2708
2024-02-09 22:07:32 | INFO | train_inner | epoch 028:    615 / 685 loss=2.923, nll_loss=1.304, ppl=2.47, wps=73314.6, ups=5.03, wpb=14589.2, bsz=727.2, num_updates=19100, lr=0.000861684, gnorm=0.623, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2728
2024-02-09 22:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:07:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:07:47 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.482 | nll_loss 1.874 | ppl 3.66 | wps 155523 | wpb 3189.1 | bsz 163.4 | num_updates 19170 | best_loss 3.477
2024-02-09 22:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 19170 updates
2024-02-09 22:07:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt
2024-02-09 22:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt
2024-02-09 22:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt (epoch 28 @ 19170 updates, score 3.482) (writing took 2.4495403468608856 seconds)
2024-02-09 22:07:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-02-09 22:07:50 | INFO | train | epoch 028 | loss 2.904 | nll_loss 1.281 | ppl 2.43 | wps 70338.8 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 19170 | lr 0.000860276 | gnorm 0.625 | loss_scale 0.25 | train_wall 135 | gb_free 29.6 | wall 2746
2024-02-09 22:07:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:07:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:07:50 | INFO | fairseq.trainer | begin training epoch 29
2024-02-09 22:07:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:07:56 | INFO | train_inner | epoch 029:     30 / 685 loss=2.898, nll_loss=1.275, ppl=2.42, wps=58461.6, ups=4.1, wpb=14265.3, bsz=728.9, num_updates=19200, lr=0.00085967, gnorm=0.642, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2753
2024-02-09 22:08:16 | INFO | train_inner | epoch 029:    130 / 685 loss=2.855, nll_loss=1.225, ppl=2.34, wps=72781.5, ups=5, wpb=14558.4, bsz=729.2, num_updates=19300, lr=0.000857643, gnorm=0.589, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2773
2024-02-09 22:08:36 | INFO | train_inner | epoch 029:    230 / 685 loss=2.871, nll_loss=1.243, ppl=2.37, wps=71892.6, ups=5.01, wpb=14362.5, bsz=754.6, num_updates=19400, lr=0.000855604, gnorm=0.598, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2793
2024-02-09 22:08:56 | INFO | train_inner | epoch 029:    330 / 685 loss=2.883, nll_loss=1.258, ppl=2.39, wps=73169.2, ups=4.99, wpb=14663.2, bsz=752.7, num_updates=19500, lr=0.000853553, gnorm=0.622, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2813
2024-02-09 22:09:16 | INFO | train_inner | epoch 029:    430 / 685 loss=2.887, nll_loss=1.264, ppl=2.4, wps=72619.3, ups=4.97, wpb=14622.5, bsz=764.5, num_updates=19600, lr=0.000851491, gnorm=0.618, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2833
2024-02-09 22:09:36 | INFO | train_inner | epoch 029:    530 / 685 loss=2.9, nll_loss=1.279, ppl=2.43, wps=72634.6, ups=5.02, wpb=14476.9, bsz=729.8, num_updates=19700, lr=0.000849416, gnorm=0.593, loss_scale=0.25, train_wall=20, gb_free=29.6, wall=2853
2024-02-09 22:09:56 | INFO | train_inner | epoch 029:    630 / 685 loss=2.891, nll_loss=1.268, ppl=2.41, wps=72106.6, ups=4.97, wpb=14519.6, bsz=763, num_updates=19800, lr=0.000847329, gnorm=0.618, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2873
2024-02-09 22:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:10:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:10:09 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.467 | nll_loss 1.855 | ppl 3.62 | wps 151570 | wpb 3189.1 | bsz 163.4 | num_updates 19855 | best_loss 3.467
2024-02-09 22:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 19855 updates
2024-02-09 22:10:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt
2024-02-09 22:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt
2024-02-09 22:10:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt (epoch 29 @ 19855 updates, score 3.467) (writing took 3.854527937946841 seconds)
2024-02-09 22:10:13 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-02-09 22:10:13 | INFO | train | epoch 029 | loss 2.881 | nll_loss 1.256 | ppl 2.39 | wps 69485.8 | ups 4.78 | wpb 14524.4 | bsz 748.5 | num_updates 19855 | lr 0.000846177 | gnorm 0.607 | loss_scale 0.25 | train_wall 135 | gb_free 29.6 | wall 2890
2024-02-09 22:10:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:10:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:10:13 | INFO | fairseq.trainer | begin training epoch 30
2024-02-09 22:10:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:10:22 | INFO | train_inner | epoch 030:     45 / 685 loss=2.864, nll_loss=1.237, ppl=2.36, wps=55469, ups=3.85, wpb=14425.2, bsz=751, num_updates=19900, lr=0.000845231, gnorm=0.6, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2899
2024-02-09 22:10:42 | INFO | train_inner | epoch 030:    145 / 685 loss=2.846, nll_loss=1.216, ppl=2.32, wps=72309.3, ups=4.97, wpb=14543.7, bsz=751.7, num_updates=20000, lr=0.000843121, gnorm=0.589, loss_scale=0.25, train_wall=20, gb_free=29.5, wall=2919
2024-02-09 22:11:02 | INFO | train_inner | epoch 030:    245 / 685 loss=2.852, nll_loss=1.223, ppl=2.33, wps=72640.2, ups=4.97, wpb=14613.9, bsz=742.2, num_updates=20100, lr=0.000840999, gnorm=0.601, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=2939
2024-02-09 22:11:23 | INFO | train_inner | epoch 030:    345 / 685 loss=2.855, nll_loss=1.228, ppl=2.34, wps=71439, ups=4.96, wpb=14406.4, bsz=754.7, num_updates=20200, lr=0.000838866, gnorm=0.595, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=2959
2024-02-09 22:11:43 | INFO | train_inner | epoch 030:    445 / 685 loss=2.867, nll_loss=1.241, ppl=2.36, wps=72714.9, ups=4.98, wpb=14595.6, bsz=741.8, num_updates=20300, lr=0.000836721, gnorm=0.603, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=2979
2024-02-09 22:12:03 | INFO | train_inner | epoch 030:    545 / 685 loss=2.875, nll_loss=1.252, ppl=2.38, wps=71608.5, ups=4.94, wpb=14483, bsz=749.3, num_updates=20400, lr=0.000834565, gnorm=0.6, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3000
2024-02-09 22:12:23 | INFO | train_inner | epoch 030:    645 / 685 loss=2.876, nll_loss=1.252, ppl=2.38, wps=72476.9, ups=4.98, wpb=14550.5, bsz=754.5, num_updates=20500, lr=0.000832398, gnorm=0.82, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3020
2024-02-09 22:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:12:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:12:33 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.484 | nll_loss 1.875 | ppl 3.67 | wps 157677 | wpb 3189.1 | bsz 163.4 | num_updates 20540 | best_loss 3.467
2024-02-09 22:12:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 20540 updates
2024-02-09 22:12:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt
2024-02-09 22:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt
2024-02-09 22:12:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt (epoch 30 @ 20540 updates, score 3.484) (writing took 2.421575830085203 seconds)
2024-02-09 22:12:35 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-02-09 22:12:35 | INFO | train | epoch 030 | loss 2.86 | nll_loss 1.233 | ppl 2.35 | wps 69918.8 | ups 4.81 | wpb 14524.4 | bsz 748.5 | num_updates 20540 | lr 0.000831528 | gnorm 0.631 | loss_scale 0.5 | train_wall 136 | gb_free 29.7 | wall 3032
2024-02-09 22:12:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:12:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:12:35 | INFO | fairseq.trainer | begin training epoch 31
2024-02-09 22:12:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:12:47 | INFO | train_inner | epoch 031:     60 / 685 loss=2.829, nll_loss=1.197, ppl=2.29, wps=59256.2, ups=4.07, wpb=14560.9, bsz=751, num_updates=20600, lr=0.000830219, gnorm=0.655, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3044
2024-02-09 22:13:08 | INFO | train_inner | epoch 031:    160 / 685 loss=2.822, nll_loss=1.191, ppl=2.28, wps=73588.5, ups=4.99, wpb=14740.1, bsz=741.7, num_updates=20700, lr=0.00082803, gnorm=0.588, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3064
2024-02-09 22:13:28 | INFO | train_inner | epoch 031:    260 / 685 loss=2.843, nll_loss=1.214, ppl=2.32, wps=72731.7, ups=4.96, wpb=14677, bsz=742.9, num_updates=20800, lr=0.000825829, gnorm=0.639, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3084
2024-02-09 22:13:48 | INFO | train_inner | epoch 031:    360 / 685 loss=2.851, nll_loss=1.224, ppl=2.34, wps=71582.1, ups=4.96, wpb=14431.8, bsz=742.6, num_updates=20900, lr=0.000823617, gnorm=0.588, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3105
2024-02-09 22:14:08 | INFO | train_inner | epoch 031:    460 / 685 loss=2.838, nll_loss=1.209, ppl=2.31, wps=71860.4, ups=4.98, wpb=14415.8, bsz=756.6, num_updates=21000, lr=0.000821394, gnorm=0.678, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=3125
2024-02-09 22:14:28 | INFO | train_inner | epoch 031:    560 / 685 loss=2.862, nll_loss=1.238, ppl=2.36, wps=71745, ups=4.96, wpb=14460.3, bsz=747.6, num_updates=21100, lr=0.00081916, gnorm=0.634, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3145
2024-02-09 22:14:48 | INFO | train_inner | epoch 031:    660 / 685 loss=2.855, nll_loss=1.23, ppl=2.35, wps=72166, ups=4.97, wpb=14514.9, bsz=755.5, num_updates=21200, lr=0.000816915, gnorm=0.673, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3165
2024-02-09 22:14:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:14:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:14:55 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.463 | nll_loss 1.854 | ppl 3.61 | wps 156950 | wpb 3189.1 | bsz 163.4 | num_updates 21225 | best_loss 3.463
2024-02-09 22:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 21225 updates
2024-02-09 22:14:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt
2024-02-09 22:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt
2024-02-09 22:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt (epoch 31 @ 21225 updates, score 3.463) (writing took 3.858326558023691 seconds)
2024-02-09 22:14:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-02-09 22:14:59 | INFO | train | epoch 031 | loss 2.842 | nll_loss 1.214 | ppl 2.32 | wps 69327 | ups 4.77 | wpb 14524.4 | bsz 748.5 | num_updates 21225 | lr 0.000816353 | gnorm 0.637 | loss_scale 0.5 | train_wall 136 | gb_free 29.7 | wall 3175
2024-02-09 22:14:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:14:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:14:59 | INFO | fairseq.trainer | begin training epoch 32
2024-02-09 22:14:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:15:14 | INFO | train_inner | epoch 032:     75 / 685 loss=2.813, nll_loss=1.18, ppl=2.27, wps=55911, ups=3.86, wpb=14493.8, bsz=763.2, num_updates=21300, lr=0.00081466, gnorm=0.665, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3191
2024-02-09 22:15:34 | INFO | train_inner | epoch 032:    175 / 685 loss=2.809, nll_loss=1.175, ppl=2.26, wps=72029.6, ups=4.95, wpb=14553.5, bsz=729.2, num_updates=21400, lr=0.000812394, gnorm=0.59, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3211
2024-02-09 22:15:55 | INFO | train_inner | epoch 032:    275 / 685 loss=2.812, nll_loss=1.18, ppl=2.27, wps=72210.1, ups=4.95, wpb=14577.9, bsz=759.1, num_updates=21500, lr=0.000810118, gnorm=0.595, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3231
2024-02-09 22:16:15 | INFO | train_inner | epoch 032:    375 / 685 loss=2.824, nll_loss=1.194, ppl=2.29, wps=72347.6, ups=4.98, wpb=14531.5, bsz=757.6, num_updates=21600, lr=0.000807831, gnorm=0.6, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3251
2024-02-09 22:16:35 | INFO | train_inner | epoch 032:    475 / 685 loss=2.827, nll_loss=1.199, ppl=2.3, wps=72547.8, ups=4.99, wpb=14543.7, bsz=732, num_updates=21700, lr=0.000805533, gnorm=0.695, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3271
2024-02-09 22:16:55 | INFO | train_inner | epoch 032:    575 / 685 loss=2.842, nll_loss=1.215, ppl=2.32, wps=71818.9, ups=4.99, wpb=14379.5, bsz=724.3, num_updates=21800, lr=0.000803226, gnorm=0.611, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3291
2024-02-09 22:17:15 | INFO | train_inner | epoch 032:    675 / 685 loss=2.839, nll_loss=1.213, ppl=2.32, wps=72223.4, ups=4.95, wpb=14577.6, bsz=782.6, num_updates=21900, lr=0.000800908, gnorm=0.583, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3312
2024-02-09 22:17:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:17:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:17:19 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.477 | nll_loss 1.866 | ppl 3.65 | wps 153013 | wpb 3189.1 | bsz 163.4 | num_updates 21910 | best_loss 3.463
2024-02-09 22:17:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 21910 updates
2024-02-09 22:17:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt
2024-02-09 22:17:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt
2024-02-09 22:17:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt (epoch 32 @ 21910 updates, score 3.477) (writing took 2.416178818093613 seconds)
2024-02-09 22:17:21 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-02-09 22:17:21 | INFO | train | epoch 032 | loss 2.822 | nll_loss 1.192 | ppl 2.29 | wps 69914.8 | ups 4.81 | wpb 14524.4 | bsz 748.5 | num_updates 21910 | lr 0.000800675 | gnorm 0.619 | loss_scale 0.5 | train_wall 136 | gb_free 29.8 | wall 3318
2024-02-09 22:17:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:17:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:17:21 | INFO | fairseq.trainer | begin training epoch 33
2024-02-09 22:17:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:17:39 | INFO | train_inner | epoch 033:     90 / 685 loss=2.793, nll_loss=1.158, ppl=2.23, wps=58955.2, ups=4.09, wpb=14419, bsz=735.4, num_updates=22000, lr=0.000798579, gnorm=0.598, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3336
2024-02-09 22:17:59 | INFO | train_inner | epoch 033:    190 / 685 loss=2.784, nll_loss=1.148, ppl=2.22, wps=71784, ups=5.01, wpb=14317.8, bsz=725.8, num_updates=22100, lr=0.000796241, gnorm=0.58, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3356
2024-02-09 22:18:20 | INFO | train_inner | epoch 033:    290 / 685 loss=2.802, nll_loss=1.17, ppl=2.25, wps=72892.5, ups=4.93, wpb=14796.3, bsz=758.1, num_updates=22200, lr=0.000793893, gnorm=0.595, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3376
2024-02-09 22:18:40 | INFO | train_inner | epoch 033:    390 / 685 loss=2.811, nll_loss=1.181, ppl=2.27, wps=72192.5, ups=4.95, wpb=14580.6, bsz=751.5, num_updates=22300, lr=0.000791534, gnorm=0.619, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3396
2024-02-09 22:19:00 | INFO | train_inner | epoch 033:    490 / 685 loss=2.8, nll_loss=1.169, ppl=2.25, wps=71105.5, ups=4.91, wpb=14491.1, bsz=777.4, num_updates=22400, lr=0.000789166, gnorm=0.597, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3417
2024-02-09 22:19:20 | INFO | train_inner | epoch 033:    590 / 685 loss=2.812, nll_loss=1.181, ppl=2.27, wps=70379.3, ups=4.93, wpb=14261.5, bsz=727.3, num_updates=22500, lr=0.000786788, gnorm=0.592, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=3437
2024-02-09 22:19:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:19:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:19:41 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.473 | nll_loss 1.862 | ppl 3.64 | wps 152307 | wpb 3189.1 | bsz 163.4 | num_updates 22595 | best_loss 3.463
2024-02-09 22:19:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 22595 updates
2024-02-09 22:19:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt
2024-02-09 22:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt
2024-02-09 22:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt (epoch 33 @ 22595 updates, score 3.473) (writing took 2.4498218467924744 seconds)
2024-02-09 22:19:44 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-02-09 22:19:44 | INFO | train | epoch 033 | loss 2.803 | nll_loss 1.171 | ppl 2.25 | wps 69658.2 | ups 4.8 | wpb 14524.4 | bsz 748.5 | num_updates 22595 | lr 0.00078452 | gnorm 0.596 | loss_scale 0.5 | train_wall 136 | gb_free 29.7 | wall 3461
2024-02-09 22:19:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:19:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:19:44 | INFO | fairseq.trainer | begin training epoch 34
2024-02-09 22:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:19:45 | INFO | train_inner | epoch 034:      5 / 685 loss=2.817, nll_loss=1.189, ppl=2.28, wps=59444.7, ups=4.06, wpb=14657.7, bsz=756.6, num_updates=22600, lr=0.000784401, gnorm=0.6, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3462
2024-02-09 22:20:05 | INFO | train_inner | epoch 034:    105 / 685 loss=2.757, nll_loss=1.118, ppl=2.17, wps=71491.2, ups=4.96, wpb=14418.1, bsz=743.2, num_updates=22700, lr=0.000782003, gnorm=0.604, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=3482
2024-02-09 22:20:25 | INFO | train_inner | epoch 034:    205 / 685 loss=2.775, nll_loss=1.141, ppl=2.2, wps=72987.7, ups=5.01, wpb=14579.7, bsz=740.9, num_updates=22800, lr=0.000779596, gnorm=0.566, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3502
2024-02-09 22:20:45 | INFO | train_inner | epoch 034:    305 / 685 loss=2.795, nll_loss=1.163, ppl=2.24, wps=72539.1, ups=4.97, wpb=14590.3, bsz=752.3, num_updates=22900, lr=0.00077718, gnorm=0.741, loss_scale=0.5, train_wall=20, gb_free=29.6, wall=3522
2024-02-09 22:21:06 | INFO | train_inner | epoch 034:    405 / 685 loss=2.795, nll_loss=1.163, ppl=2.24, wps=72452.4, ups=4.94, wpb=14652, bsz=731.9, num_updates=23000, lr=0.000774754, gnorm=0.609, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3542
2024-02-09 22:21:26 | INFO | train_inner | epoch 034:    505 / 685 loss=2.785, nll_loss=1.153, ppl=2.22, wps=72162, ups=4.95, wpb=14579.5, bsz=761.9, num_updates=23100, lr=0.00077232, gnorm=0.593, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3562
2024-02-09 22:21:46 | INFO | train_inner | epoch 034:    605 / 685 loss=2.802, nll_loss=1.173, ppl=2.25, wps=72611.1, ups=4.98, wpb=14572.3, bsz=761.6, num_updates=23200, lr=0.000769875, gnorm=0.611, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3582
2024-02-09 22:22:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:22:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:22:04 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.453 | nll_loss 1.85 | ppl 3.61 | wps 155794 | wpb 3189.1 | bsz 163.4 | num_updates 23280 | best_loss 3.453
2024-02-09 22:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 23280 updates
2024-02-09 22:22:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt
2024-02-09 22:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt
2024-02-09 22:22:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt (epoch 34 @ 23280 updates, score 3.453) (writing took 3.9290547650307417 seconds)
2024-02-09 22:22:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-02-09 22:22:08 | INFO | train | epoch 034 | loss 2.786 | nll_loss 1.154 | ppl 2.22 | wps 69261.8 | ups 4.77 | wpb 14524.4 | bsz 748.5 | num_updates 23280 | lr 0.000767913 | gnorm 0.628 | loss_scale 0.5 | train_wall 136 | gb_free 29.7 | wall 3604
2024-02-09 22:22:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:22:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:22:09 | INFO | fairseq.trainer | begin training epoch 35
2024-02-09 22:22:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:22:15 | INFO | train_inner | epoch 035:     20 / 685 loss=2.787, nll_loss=1.155, ppl=2.23, wps=48957.5, ups=3.43, wpb=14267.3, bsz=743.7, num_updates=23300, lr=0.000767422, gnorm=0.661, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3612
2024-02-09 22:22:35 | INFO | train_inner | epoch 035:    120 / 685 loss=2.751, nll_loss=1.113, ppl=2.16, wps=72981.7, ups=5, wpb=14592.6, bsz=744, num_updates=23400, lr=0.00076496, gnorm=0.629, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3632
2024-02-09 22:22:55 | INFO | train_inner | epoch 035:    220 / 685 loss=2.76, nll_loss=1.124, ppl=2.18, wps=72481.1, ups=5, wpb=14499.3, bsz=738.6, num_updates=23500, lr=0.000762488, gnorm=0.596, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3652
2024-02-09 22:23:15 | INFO | train_inner | epoch 035:    320 / 685 loss=2.77, nll_loss=1.136, ppl=2.2, wps=73167.5, ups=5.01, wpb=14611.2, bsz=751, num_updates=23600, lr=0.000760008, gnorm=0.621, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3672
2024-02-09 22:23:35 | INFO | train_inner | epoch 035:    420 / 685 loss=2.771, nll_loss=1.136, ppl=2.2, wps=71760.2, ups=4.99, wpb=14381.3, bsz=751.4, num_updates=23700, lr=0.000757519, gnorm=0.613, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3692
2024-02-09 22:23:55 | INFO | train_inner | epoch 035:    520 / 685 loss=2.784, nll_loss=1.153, ppl=2.22, wps=72363.3, ups=4.95, wpb=14630.5, bsz=746.6, num_updates=23800, lr=0.000755021, gnorm=0.596, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3712
2024-02-09 22:24:15 | INFO | train_inner | epoch 035:    620 / 685 loss=2.779, nll_loss=1.148, ppl=2.22, wps=72007.5, ups=4.95, wpb=14556.3, bsz=754.9, num_updates=23900, lr=0.000752515, gnorm=0.601, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3732
2024-02-09 22:24:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:24:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:24:30 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.436 | nll_loss 1.829 | ppl 3.55 | wps 155151 | wpb 3189.1 | bsz 163.4 | num_updates 23965 | best_loss 3.436
2024-02-09 22:24:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 23965 updates
2024-02-09 22:24:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt
2024-02-09 22:24:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt
2024-02-09 22:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt (epoch 35 @ 23965 updates, score 3.436) (writing took 3.909066765801981 seconds)
2024-02-09 22:24:34 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-02-09 22:24:34 | INFO | train | epoch 035 | loss 2.769 | nll_loss 1.135 | ppl 2.2 | wps 67801.2 | ups 4.67 | wpb 14524.4 | bsz 748.5 | num_updates 23965 | lr 0.000750881 | gnorm 0.61 | loss_scale 0.5 | train_wall 136 | gb_free 29.6 | wall 3751
2024-02-09 22:24:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:24:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:24:34 | INFO | fairseq.trainer | begin training epoch 36
2024-02-09 22:24:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:24:42 | INFO | train_inner | epoch 036:     35 / 685 loss=2.757, nll_loss=1.122, ppl=2.18, wps=54896.5, ups=3.82, wpb=14361, bsz=760.6, num_updates=24000, lr=0.00075, gnorm=0.604, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3758
2024-02-09 22:25:07 | INFO | train_inner | epoch 036:    135 / 685 loss=2.722, nll_loss=1.081, ppl=2.12, wps=56267.9, ups=3.86, wpb=14565, bsz=763.8, num_updates=24100, lr=0.000747477, gnorm=0.683, loss_scale=0.5, train_wall=20, gb_free=29.5, wall=3784
2024-02-09 22:25:28 | INFO | train_inner | epoch 036:    235 / 685 loss=2.751, nll_loss=1.115, ppl=2.17, wps=71469.6, ups=4.95, wpb=14446.9, bsz=733.4, num_updates=24200, lr=0.000744945, gnorm=0.609, loss_scale=1, train_wall=20, gb_free=29.6, wall=3804
2024-02-09 22:25:48 | INFO | train_inner | epoch 036:    335 / 685 loss=2.754, nll_loss=1.119, ppl=2.17, wps=71723.8, ups=4.92, wpb=14570.8, bsz=747.9, num_updates=24300, lr=0.000742405, gnorm=0.686, loss_scale=1, train_wall=20, gb_free=29.5, wall=3825
2024-02-09 22:26:08 | INFO | train_inner | epoch 036:    435 / 685 loss=2.764, nll_loss=1.131, ppl=2.19, wps=72571.4, ups=4.97, wpb=14615.7, bsz=738.6, num_updates=24400, lr=0.000739857, gnorm=0.58, loss_scale=1, train_wall=20, gb_free=29.5, wall=3845
2024-02-09 22:26:28 | INFO | train_inner | epoch 036:    535 / 685 loss=2.759, nll_loss=1.124, ppl=2.18, wps=72883.5, ups=5.02, wpb=14518.5, bsz=763.3, num_updates=24500, lr=0.0007373, gnorm=0.604, loss_scale=1, train_wall=20, gb_free=29.5, wall=3865
2024-02-09 22:26:55 | INFO | train_inner | epoch 036:    635 / 685 loss=2.766, nll_loss=1.133, ppl=2.19, wps=53558.2, ups=3.68, wpb=14550.3, bsz=746.2, num_updates=24600, lr=0.000734736, gnorm=0.627, loss_scale=1, train_wall=20, gb_free=29.5, wall=3892
2024-02-09 22:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:27:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:27:09 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.44 | nll_loss 1.831 | ppl 3.56 | wps 144336 | wpb 3189.1 | bsz 163.4 | num_updates 24650 | best_loss 3.436
2024-02-09 22:27:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 24650 updates
2024-02-09 22:27:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt
2024-02-09 22:27:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt
2024-02-09 22:27:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt (epoch 36 @ 24650 updates, score 3.44) (writing took 2.3912211710121483 seconds)
2024-02-09 22:27:11 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-02-09 22:27:11 | INFO | train | epoch 036 | loss 2.753 | nll_loss 1.117 | ppl 2.17 | wps 63512.3 | ups 4.37 | wpb 14524.4 | bsz 748.5 | num_updates 24650 | lr 0.000733451 | gnorm 0.627 | loss_scale 1 | train_wall 137 | gb_free 29.6 | wall 3908
2024-02-09 22:27:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:27:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:27:11 | INFO | fairseq.trainer | begin training epoch 37
2024-02-09 22:27:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:27:22 | INFO | train_inner | epoch 037:     50 / 685 loss=2.739, nll_loss=1.102, ppl=2.15, wps=53688.5, ups=3.71, wpb=14489.6, bsz=732.2, num_updates=24700, lr=0.000732163, gnorm=0.595, loss_scale=1, train_wall=21, gb_free=29.5, wall=3919
2024-02-09 22:27:43 | INFO | train_inner | epoch 037:    150 / 685 loss=2.715, nll_loss=1.074, ppl=2.11, wps=70823, ups=4.9, wpb=14444.9, bsz=753.8, num_updates=24800, lr=0.000729583, gnorm=0.584, loss_scale=1, train_wall=20, gb_free=29.5, wall=3939
2024-02-09 22:28:02 | INFO | train_inner | epoch 037:    250 / 685 loss=2.722, nll_loss=1.084, ppl=2.12, wps=73137.3, ups=5.07, wpb=14426.2, bsz=750.7, num_updates=24900, lr=0.000726995, gnorm=0.575, loss_scale=1, train_wall=19, gb_free=29.5, wall=3959
2024-02-09 22:28:22 | INFO | train_inner | epoch 037:    350 / 685 loss=2.732, nll_loss=1.095, ppl=2.14, wps=73853.4, ups=5.1, wpb=14490, bsz=738.3, num_updates=25000, lr=0.0007244, gnorm=0.58, loss_scale=1, train_wall=19, gb_free=29.5, wall=3979
2024-02-09 22:28:42 | INFO | train_inner | epoch 037:    450 / 685 loss=2.748, nll_loss=1.113, ppl=2.16, wps=74235.5, ups=5.07, wpb=14631, bsz=758.1, num_updates=25100, lr=0.000721796, gnorm=0.623, loss_scale=1, train_wall=19, gb_free=29.5, wall=3998
2024-02-09 22:29:02 | INFO | train_inner | epoch 037:    550 / 685 loss=2.749, nll_loss=1.115, ppl=2.17, wps=73244.9, ups=5.01, wpb=14620.3, bsz=748, num_updates=25200, lr=0.000719186, gnorm=0.657, loss_scale=1, train_wall=20, gb_free=29.5, wall=4018
2024-02-09 22:29:21 | INFO | train_inner | epoch 037:    650 / 685 loss=2.751, nll_loss=1.118, ppl=2.17, wps=73398.7, ups=5.03, wpb=14601.1, bsz=753.1, num_updates=25300, lr=0.000716567, gnorm=0.586, loss_scale=1, train_wall=20, gb_free=29.6, wall=4038
2024-02-09 22:29:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:29:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:29:30 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.43 | nll_loss 1.825 | ppl 3.54 | wps 157373 | wpb 3189.1 | bsz 163.4 | num_updates 25335 | best_loss 3.43
2024-02-09 22:29:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 25335 updates
2024-02-09 22:29:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt
2024-02-09 22:29:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt
2024-02-09 22:29:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt (epoch 37 @ 25335 updates, score 3.43) (writing took 3.879871548851952 seconds)
2024-02-09 22:29:34 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-02-09 22:29:34 | INFO | train | epoch 037 | loss 2.735 | nll_loss 1.098 | ppl 2.14 | wps 69467.1 | ups 4.78 | wpb 14524.4 | bsz 748.5 | num_updates 25335 | lr 0.000715649 | gnorm 0.599 | loss_scale 1 | train_wall 135 | gb_free 29.6 | wall 4051
2024-02-09 22:29:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:29:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:29:34 | INFO | fairseq.trainer | begin training epoch 38
2024-02-09 22:29:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:29:47 | INFO | train_inner | epoch 038:     65 / 685 loss=2.706, nll_loss=1.065, ppl=2.09, wps=56045.5, ups=3.87, wpb=14500.3, bsz=749.8, num_updates=25400, lr=0.000713942, gnorm=0.573, loss_scale=1, train_wall=20, gb_free=29.5, wall=4064
2024-02-09 22:30:07 | INFO | train_inner | epoch 038:    165 / 685 loss=2.699, nll_loss=1.058, ppl=2.08, wps=73616, ups=5.01, wpb=14687.8, bsz=755.8, num_updates=25500, lr=0.000711309, gnorm=0.574, loss_scale=1, train_wall=20, gb_free=29.5, wall=4084
2024-02-09 22:30:27 | INFO | train_inner | epoch 038:    265 / 685 loss=2.718, nll_loss=1.079, ppl=2.11, wps=73204.9, ups=5.03, wpb=14565.7, bsz=740.3, num_updates=25600, lr=0.000708669, gnorm=0.589, loss_scale=1, train_wall=20, gb_free=29.5, wall=4104
2024-02-09 22:30:47 | INFO | train_inner | epoch 038:    365 / 685 loss=2.724, nll_loss=1.086, ppl=2.12, wps=72256.5, ups=5.03, wpb=14357.9, bsz=738.2, num_updates=25700, lr=0.000706022, gnorm=0.585, loss_scale=1, train_wall=20, gb_free=29.5, wall=4124
2024-02-09 22:31:07 | INFO | train_inner | epoch 038:    465 / 685 loss=2.724, nll_loss=1.087, ppl=2.12, wps=73315.3, ups=5.04, wpb=14557.9, bsz=744.6, num_updates=25800, lr=0.000703368, gnorm=0.586, loss_scale=1, train_wall=20, gb_free=29.5, wall=4144
2024-02-09 22:31:27 | INFO | train_inner | epoch 038:    565 / 685 loss=2.73, nll_loss=1.094, ppl=2.13, wps=72288.2, ups=5.02, wpb=14405.9, bsz=745.9, num_updates=25900, lr=0.000700707, gnorm=0.614, loss_scale=1, train_wall=20, gb_free=29.5, wall=4164
2024-02-09 22:31:47 | INFO | train_inner | epoch 038:    665 / 685 loss=2.728, nll_loss=1.093, ppl=2.13, wps=72984.8, ups=5, wpb=14603.6, bsz=775.8, num_updates=26000, lr=0.00069804, gnorm=0.598, loss_scale=1, train_wall=20, gb_free=29.5, wall=4184
2024-02-09 22:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:31:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:31:53 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.398 | nll_loss 1.793 | ppl 3.47 | wps 156495 | wpb 3189.1 | bsz 163.4 | num_updates 26020 | best_loss 3.398
2024-02-09 22:31:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 26020 updates
2024-02-09 22:31:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt
2024-02-09 22:31:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt
2024-02-09 22:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt (epoch 38 @ 26020 updates, score 3.398) (writing took 3.9786532851867378 seconds)
2024-02-09 22:31:57 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-02-09 22:31:57 | INFO | train | epoch 038 | loss 2.718 | nll_loss 1.079 | ppl 2.11 | wps 69839.9 | ups 4.81 | wpb 14524.4 | bsz 748.5 | num_updates 26020 | lr 0.000697506 | gnorm 0.589 | loss_scale 1 | train_wall 135 | gb_free 29.7 | wall 4193
2024-02-09 22:31:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:31:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:31:57 | INFO | fairseq.trainer | begin training epoch 39
2024-02-09 22:31:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:32:13 | INFO | train_inner | epoch 039:     80 / 685 loss=2.696, nll_loss=1.055, ppl=2.08, wps=55206.2, ups=3.84, wpb=14391.1, bsz=746.1, num_updates=26100, lr=0.000695366, gnorm=0.577, loss_scale=1, train_wall=20, gb_free=29.6, wall=4210
2024-02-09 22:32:33 | INFO | train_inner | epoch 039:    180 / 685 loss=2.689, nll_loss=1.046, ppl=2.06, wps=72244.8, ups=4.96, wpb=14571.2, bsz=750.8, num_updates=26200, lr=0.000692685, gnorm=0.564, loss_scale=1, train_wall=20, gb_free=29.5, wall=4230
2024-02-09 22:32:53 | INFO | train_inner | epoch 039:    280 / 685 loss=2.7, nll_loss=1.059, ppl=2.08, wps=71661.4, ups=4.96, wpb=14439.9, bsz=747.3, num_updates=26300, lr=0.000689997, gnorm=0.593, loss_scale=1, train_wall=20, gb_free=29.6, wall=4250
2024-02-09 22:33:13 | INFO | train_inner | epoch 039:    380 / 685 loss=2.705, nll_loss=1.067, ppl=2.1, wps=72097.3, ups=4.95, wpb=14568, bsz=746.3, num_updates=26400, lr=0.000687303, gnorm=0.57, loss_scale=1, train_wall=20, gb_free=29.6, wall=4270
2024-02-09 22:33:34 | INFO | train_inner | epoch 039:    480 / 685 loss=2.704, nll_loss=1.065, ppl=2.09, wps=72253.4, ups=4.97, wpb=14525.4, bsz=747.2, num_updates=26500, lr=0.000684603, gnorm=0.584, loss_scale=1, train_wall=20, gb_free=29.5, wall=4290
2024-02-09 22:33:54 | INFO | train_inner | epoch 039:    580 / 685 loss=2.717, nll_loss=1.08, ppl=2.11, wps=72794.6, ups=4.95, wpb=14716.7, bsz=761.2, num_updates=26600, lr=0.000681897, gnorm=0.636, loss_scale=1, train_wall=20, gb_free=29.6, wall=4310
2024-02-09 22:34:14 | INFO | train_inner | epoch 039:    680 / 685 loss=2.714, nll_loss=1.077, ppl=2.11, wps=72366.1, ups=4.99, wpb=14491.2, bsz=741.2, num_updates=26700, lr=0.000679184, gnorm=0.6, loss_scale=1, train_wall=20, gb_free=29.5, wall=4331
2024-02-09 22:34:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:34:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:34:17 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.425 | nll_loss 1.82 | ppl 3.53 | wps 157716 | wpb 3189.1 | bsz 163.4 | num_updates 26705 | best_loss 3.398
2024-02-09 22:34:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 26705 updates
2024-02-09 22:34:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt
2024-02-09 22:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt
2024-02-09 22:34:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt (epoch 39 @ 26705 updates, score 3.425) (writing took 2.4934180218260735 seconds)
2024-02-09 22:34:19 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-02-09 22:34:19 | INFO | train | epoch 039 | loss 2.703 | nll_loss 1.063 | ppl 2.09 | wps 69843.1 | ups 4.81 | wpb 14524.4 | bsz 748.5 | num_updates 26705 | lr 0.000679048 | gnorm 0.59 | loss_scale 1 | train_wall 136 | gb_free 29.6 | wall 4336
2024-02-09 22:34:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:34:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:34:19 | INFO | fairseq.trainer | begin training epoch 40
2024-02-09 22:34:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:34:38 | INFO | train_inner | epoch 040:     95 / 685 loss=2.662, nll_loss=1.016, ppl=2.02, wps=59256.3, ups=4.14, wpb=14328.4, bsz=742.2, num_updates=26800, lr=0.000676465, gnorm=0.56, loss_scale=1, train_wall=19, gb_free=29.5, wall=4355
2024-02-09 22:34:58 | INFO | train_inner | epoch 040:    195 / 685 loss=2.669, nll_loss=1.025, ppl=2.03, wps=72587.8, ups=5, wpb=14518.3, bsz=756, num_updates=26900, lr=0.000673741, gnorm=0.565, loss_scale=1, train_wall=20, gb_free=29.5, wall=4375
2024-02-09 22:35:18 | INFO | train_inner | epoch 040:    295 / 685 loss=2.684, nll_loss=1.043, ppl=2.06, wps=73540, ups=5.02, wpb=14649.6, bsz=746.2, num_updates=27000, lr=0.00067101, gnorm=0.594, loss_scale=1, train_wall=20, gb_free=29.5, wall=4395
2024-02-09 22:35:38 | INFO | train_inner | epoch 040:    395 / 685 loss=2.692, nll_loss=1.053, ppl=2.07, wps=74013.2, ups=5.06, wpb=14621.6, bsz=757.3, num_updates=27100, lr=0.000668274, gnorm=0.574, loss_scale=1, train_wall=20, gb_free=29.5, wall=4414
2024-02-09 22:35:57 | INFO | train_inner | epoch 040:    495 / 685 loss=2.702, nll_loss=1.064, ppl=2.09, wps=74297.8, ups=5.06, wpb=14670.4, bsz=734, num_updates=27200, lr=0.000665532, gnorm=0.578, loss_scale=1, train_wall=19, gb_free=29.5, wall=4434
2024-02-09 22:36:17 | INFO | train_inner | epoch 040:    595 / 685 loss=2.702, nll_loss=1.064, ppl=2.09, wps=72820.1, ups=5.07, wpb=14359.6, bsz=743.3, num_updates=27300, lr=0.000662784, gnorm=0.575, loss_scale=1, train_wall=19, gb_free=29.5, wall=4454
2024-02-09 22:36:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:36:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:36:37 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.415 | nll_loss 1.81 | ppl 3.51 | wps 162380 | wpb 3189.1 | bsz 163.4 | num_updates 27390 | best_loss 3.398
2024-02-09 22:36:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 27390 updates
2024-02-09 22:36:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt
2024-02-09 22:36:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt
2024-02-09 22:36:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt (epoch 40 @ 27390 updates, score 3.415) (writing took 2.479186027077958 seconds)
2024-02-09 22:36:39 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-02-09 22:36:39 | INFO | train | epoch 040 | loss 2.687 | nll_loss 1.046 | ppl 2.07 | wps 71084.2 | ups 4.89 | wpb 14524.4 | bsz 748.5 | num_updates 27390 | lr 0.000660306 | gnorm 0.576 | loss_scale 1 | train_wall 134 | gb_free 29.6 | wall 4476
2024-02-09 22:36:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:36:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:36:39 | INFO | fairseq.trainer | begin training epoch 41
2024-02-09 22:36:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:36:41 | INFO | train_inner | epoch 041:     10 / 685 loss=2.696, nll_loss=1.057, ppl=2.08, wps=59882.8, ups=4.17, wpb=14352.8, bsz=744, num_updates=27400, lr=0.000660031, gnorm=0.591, loss_scale=1, train_wall=19, gb_free=29.5, wall=4478
2024-02-09 22:37:01 | INFO | train_inner | epoch 041:    110 / 685 loss=2.644, nll_loss=0.998, ppl=2, wps=73818, ups=5.06, wpb=14592.8, bsz=767.2, num_updates=27500, lr=0.000657272, gnorm=0.585, loss_scale=1, train_wall=20, gb_free=29.5, wall=4498
2024-02-09 22:37:21 | INFO | train_inner | epoch 041:    210 / 685 loss=2.666, nll_loss=1.023, ppl=2.03, wps=74366.7, ups=5.07, wpb=14654.5, bsz=739.5, num_updates=27600, lr=0.000654508, gnorm=0.557, loss_scale=1, train_wall=19, gb_free=29.5, wall=4517
2024-02-09 22:37:41 | INFO | train_inner | epoch 041:    310 / 685 loss=2.667, nll_loss=1.024, ppl=2.03, wps=73037.3, ups=5, wpb=14617.5, bsz=737.4, num_updates=27700, lr=0.000651739, gnorm=0.571, loss_scale=1, train_wall=20, gb_free=29.5, wall=4537
2024-02-09 22:38:01 | INFO | train_inner | epoch 041:    410 / 685 loss=2.689, nll_loss=1.05, ppl=2.07, wps=72096.2, ups=4.99, wpb=14440.2, bsz=739.7, num_updates=27800, lr=0.000648965, gnorm=0.666, loss_scale=1, train_wall=20, gb_free=29.5, wall=4557
2024-02-09 22:38:21 | INFO | train_inner | epoch 041:    510 / 685 loss=2.679, nll_loss=1.039, ppl=2.06, wps=71708.1, ups=4.96, wpb=14451.4, bsz=771.8, num_updates=27900, lr=0.000646186, gnorm=0.605, loss_scale=1, train_wall=20, gb_free=29.5, wall=4577
2024-02-09 22:38:41 | INFO | train_inner | epoch 041:    610 / 685 loss=2.677, nll_loss=1.037, ppl=2.05, wps=72187.9, ups=4.97, wpb=14516.9, bsz=765.5, num_updates=28000, lr=0.000643402, gnorm=0.66, loss_scale=1, train_wall=20, gb_free=29.5, wall=4598
2024-02-09 22:38:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:38:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:38:58 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.44 | nll_loss 1.844 | ppl 3.59 | wps 158828 | wpb 3189.1 | bsz 163.4 | num_updates 28075 | best_loss 3.398
2024-02-09 22:38:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 28075 updates
2024-02-09 22:38:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt
2024-02-09 22:38:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt
2024-02-09 22:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt (epoch 41 @ 28075 updates, score 3.44) (writing took 2.468896796926856 seconds)
2024-02-09 22:39:00 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-02-09 22:39:00 | INFO | train | epoch 041 | loss 2.673 | nll_loss 1.031 | ppl 2.04 | wps 70516.4 | ups 4.86 | wpb 14524.4 | bsz 748.5 | num_updates 28075 | lr 0.00064131 | gnorm 0.604 | loss_scale 1 | train_wall 135 | gb_free 29.8 | wall 4617
2024-02-09 22:39:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:39:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:39:00 | INFO | fairseq.trainer | begin training epoch 42
2024-02-09 22:39:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:39:05 | INFO | train_inner | epoch 042:     25 / 685 loss=2.678, nll_loss=1.039, ppl=2.05, wps=59495.9, ups=4.1, wpb=14509.1, bsz=722, num_updates=28100, lr=0.000640613, gnorm=0.575, loss_scale=1, train_wall=20, gb_free=29.5, wall=4622
2024-02-09 22:39:25 | INFO | train_inner | epoch 042:    125 / 685 loss=2.635, nll_loss=0.988, ppl=1.98, wps=72967.5, ups=5, wpb=14597, bsz=743.8, num_updates=28200, lr=0.000637819, gnorm=0.545, loss_scale=1, train_wall=20, gb_free=29.6, wall=4642
2024-02-09 22:39:45 | INFO | train_inner | epoch 042:    225 / 685 loss=2.654, nll_loss=1.011, ppl=2.01, wps=73467.5, ups=4.97, wpb=14775.3, bsz=756.4, num_updates=28300, lr=0.00063502, gnorm=0.567, loss_scale=2, train_wall=20, gb_free=29.5, wall=4662
2024-02-09 22:40:05 | INFO | train_inner | epoch 042:    325 / 685 loss=2.652, nll_loss=1.007, ppl=2.01, wps=70994.4, ups=5.04, wpb=14080.1, bsz=738.4, num_updates=28400, lr=0.000632217, gnorm=0.582, loss_scale=2, train_wall=20, gb_free=29.5, wall=4682
2024-02-09 22:40:25 | INFO | train_inner | epoch 042:    425 / 685 loss=2.671, nll_loss=1.031, ppl=2.04, wps=73002.7, ups=4.98, wpb=14658.5, bsz=746.1, num_updates=28500, lr=0.00062941, gnorm=0.581, loss_scale=2, train_wall=20, gb_free=29.6, wall=4702
2024-02-09 22:40:45 | INFO | train_inner | epoch 042:    525 / 685 loss=2.664, nll_loss=1.022, ppl=2.03, wps=72293, ups=5, wpb=14461.1, bsz=750.8, num_updates=28600, lr=0.000626598, gnorm=0.581, loss_scale=2, train_wall=20, gb_free=29.5, wall=4722
2024-02-09 22:41:05 | INFO | train_inner | epoch 042:    625 / 685 loss=2.665, nll_loss=1.023, ppl=2.03, wps=72239.8, ups=4.98, wpb=14491.7, bsz=740.8, num_updates=28700, lr=0.000623781, gnorm=0.58, loss_scale=2, train_wall=20, gb_free=29.5, wall=4742
2024-02-09 22:41:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:41:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:41:19 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.409 | nll_loss 1.811 | ppl 3.51 | wps 156898 | wpb 3189.1 | bsz 163.4 | num_updates 28760 | best_loss 3.398
2024-02-09 22:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 28760 updates
2024-02-09 22:41:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt
2024-02-09 22:41:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt
2024-02-09 22:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt (epoch 42 @ 28760 updates, score 3.409) (writing took 2.4573569390922785 seconds)
2024-02-09 22:41:22 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-02-09 22:41:22 | INFO | train | epoch 042 | loss 2.657 | nll_loss 1.014 | ppl 2.02 | wps 70273.2 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 28760 | lr 0.00062209 | gnorm 0.572 | loss_scale 2 | train_wall 135 | gb_free 29.7 | wall 4758
2024-02-09 22:41:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:41:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:41:22 | INFO | fairseq.trainer | begin training epoch 43
2024-02-09 22:41:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:41:30 | INFO | train_inner | epoch 043:     40 / 685 loss=2.651, nll_loss=1.008, ppl=2.01, wps=59462.1, ups=4.09, wpb=14528.5, bsz=766.8, num_updates=28800, lr=0.000620961, gnorm=0.568, loss_scale=2, train_wall=20, gb_free=29.5, wall=4766
2024-02-09 22:41:50 | INFO | train_inner | epoch 043:    140 / 685 loss=2.616, nll_loss=0.967, ppl=1.96, wps=72145.4, ups=4.99, wpb=14457.2, bsz=751.3, num_updates=28900, lr=0.000618136, gnorm=0.551, loss_scale=2, train_wall=20, gb_free=29.5, wall=4787
2024-02-09 22:42:10 | INFO | train_inner | epoch 043:    240 / 685 loss=2.639, nll_loss=0.994, ppl=1.99, wps=73075.7, ups=4.99, wpb=14657.8, bsz=744.5, num_updates=29000, lr=0.000615308, gnorm=0.57, loss_scale=2, train_wall=20, gb_free=29.6, wall=4807
2024-02-09 22:42:30 | INFO | train_inner | epoch 043:    340 / 685 loss=2.651, nll_loss=1.007, ppl=2.01, wps=73085.9, ups=5.05, wpb=14482.8, bsz=737.9, num_updates=29100, lr=0.000612476, gnorm=0.56, loss_scale=2, train_wall=20, gb_free=29.5, wall=4826
2024-02-09 22:42:50 | INFO | train_inner | epoch 043:    440 / 685 loss=2.648, nll_loss=1.006, ppl=2.01, wps=73433.4, ups=5.04, wpb=14577.6, bsz=745.8, num_updates=29200, lr=0.000609639, gnorm=0.549, loss_scale=2, train_wall=20, gb_free=29.5, wall=4846
2024-02-09 22:43:09 | INFO | train_inner | epoch 043:    540 / 685 loss=2.645, nll_loss=1.002, ppl=2, wps=72642.2, ups=5.02, wpb=14459.1, bsz=740.7, num_updates=29300, lr=0.000606799, gnorm=0.541, loss_scale=2, train_wall=20, gb_free=29.5, wall=4866
2024-02-09 22:43:29 | INFO | train_inner | epoch 043:    640 / 685 loss=2.654, nll_loss=1.012, ppl=2.02, wps=73272.8, ups=5.06, wpb=14491.3, bsz=763, num_updates=29400, lr=0.000603956, gnorm=0.58, loss_scale=2, train_wall=20, gb_free=29.5, wall=4886
2024-02-09 22:43:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:43:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:43:40 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.434 | nll_loss 1.835 | ppl 3.57 | wps 156220 | wpb 3189.1 | bsz 163.4 | num_updates 29445 | best_loss 3.398
2024-02-09 22:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 29445 updates
2024-02-09 22:43:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt
2024-02-09 22:43:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt
2024-02-09 22:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt (epoch 43 @ 29445 updates, score 3.434) (writing took 2.5279659391380847 seconds)
2024-02-09 22:43:43 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-02-09 22:43:43 | INFO | train | epoch 043 | loss 2.642 | nll_loss 0.998 | ppl 2 | wps 70647.1 | ups 4.86 | wpb 14524.4 | bsz 748.5 | num_updates 29445 | lr 0.000602675 | gnorm 0.558 | loss_scale 2 | train_wall 134 | gb_free 29.6 | wall 4899
2024-02-09 22:43:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:43:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:43:43 | INFO | fairseq.trainer | begin training epoch 44
2024-02-09 22:43:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:43:54 | INFO | train_inner | epoch 044:     55 / 685 loss=2.629, nll_loss=0.984, ppl=1.98, wps=59776.5, ups=4.12, wpb=14500.5, bsz=768.6, num_updates=29500, lr=0.000601109, gnorm=0.565, loss_scale=2, train_wall=19, gb_free=29.5, wall=4910
2024-02-09 22:44:13 | INFO | train_inner | epoch 044:    155 / 685 loss=2.622, nll_loss=0.975, ppl=1.97, wps=73817.4, ups=5.06, wpb=14579.1, bsz=730.9, num_updates=29600, lr=0.000598258, gnorm=0.535, loss_scale=2, train_wall=19, gb_free=29.6, wall=4930
2024-02-09 22:44:33 | INFO | train_inner | epoch 044:    255 / 685 loss=2.618, nll_loss=0.971, ppl=1.96, wps=72832, ups=5.06, wpb=14393.9, bsz=758.1, num_updates=29700, lr=0.000595404, gnorm=0.55, loss_scale=2, train_wall=20, gb_free=29.5, wall=4950
2024-02-09 22:44:53 | INFO | train_inner | epoch 044:    355 / 685 loss=2.631, nll_loss=0.987, ppl=1.98, wps=73478.4, ups=5.06, wpb=14533.5, bsz=723, num_updates=29800, lr=0.000592547, gnorm=0.586, loss_scale=2, train_wall=20, gb_free=29.5, wall=4969
2024-02-09 22:45:13 | INFO | train_inner | epoch 044:    455 / 685 loss=2.633, nll_loss=0.989, ppl=1.99, wps=73951.3, ups=5.06, wpb=14621.9, bsz=759.3, num_updates=29900, lr=0.000589687, gnorm=0.555, loss_scale=2, train_wall=20, gb_free=29.5, wall=4989
2024-02-09 22:45:32 | INFO | train_inner | epoch 044:    555 / 685 loss=2.636, nll_loss=0.993, ppl=1.99, wps=73940.5, ups=5.05, wpb=14646.4, bsz=766.3, num_updates=30000, lr=0.000586824, gnorm=0.534, loss_scale=2, train_wall=20, gb_free=29.5, wall=5009
2024-02-09 22:45:52 | INFO | train_inner | epoch 044:    655 / 685 loss=2.64, nll_loss=0.998, ppl=2, wps=74441, ups=5.08, wpb=14650.8, bsz=749.8, num_updates=30100, lr=0.000583958, gnorm=0.564, loss_scale=2, train_wall=19, gb_free=29.5, wall=5029
2024-02-09 22:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:45:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:46:00 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.44 | nll_loss 1.832 | ppl 3.56 | wps 160736 | wpb 3189.1 | bsz 163.4 | num_updates 30130 | best_loss 3.398
2024-02-09 22:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 30130 updates
2024-02-09 22:46:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt
2024-02-09 22:46:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt
2024-02-09 22:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt (epoch 44 @ 30130 updates, score 3.44) (writing took 2.486714060185477 seconds)
2024-02-09 22:46:02 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-02-09 22:46:02 | INFO | train | epoch 044 | loss 2.628 | nll_loss 0.983 | ppl 1.98 | wps 71239.2 | ups 4.9 | wpb 14524.4 | bsz 748.5 | num_updates 30130 | lr 0.000583098 | gnorm 0.559 | loss_scale 2 | train_wall 133 | gb_free 29.6 | wall 5039
2024-02-09 22:46:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:46:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:46:02 | INFO | fairseq.trainer | begin training epoch 45
2024-02-09 22:46:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:46:16 | INFO | train_inner | epoch 045:     70 / 685 loss=2.604, nll_loss=0.955, ppl=1.94, wps=59405, ups=4.13, wpb=14369, bsz=765.5, num_updates=30200, lr=0.000581089, gnorm=0.557, loss_scale=2, train_wall=19, gb_free=29.5, wall=5053
2024-02-09 22:46:36 | INFO | train_inner | epoch 045:    170 / 685 loss=2.594, nll_loss=0.944, ppl=1.92, wps=72584.9, ups=5.02, wpb=14466.7, bsz=739.4, num_updates=30300, lr=0.000578217, gnorm=0.536, loss_scale=2, train_wall=20, gb_free=29.5, wall=5073
2024-02-09 22:46:56 | INFO | train_inner | epoch 045:    270 / 685 loss=2.601, nll_loss=0.953, ppl=1.94, wps=72728.9, ups=5.04, wpb=14417.4, bsz=754.6, num_updates=30400, lr=0.000575343, gnorm=0.561, loss_scale=2, train_wall=20, gb_free=29.5, wall=5093
2024-02-09 22:47:16 | INFO | train_inner | epoch 045:    370 / 685 loss=2.613, nll_loss=0.966, ppl=1.95, wps=73104.4, ups=5.04, wpb=14503.9, bsz=755, num_updates=30500, lr=0.000572466, gnorm=0.534, loss_scale=2, train_wall=20, gb_free=29.6, wall=5113
2024-02-09 22:47:36 | INFO | train_inner | epoch 045:    470 / 685 loss=2.63, nll_loss=0.987, ppl=1.98, wps=72925.5, ups=4.99, wpb=14600, bsz=742.1, num_updates=30600, lr=0.000569587, gnorm=0.547, loss_scale=2, train_wall=20, gb_free=29.5, wall=5133
2024-02-09 22:47:56 | INFO | train_inner | epoch 045:    570 / 685 loss=2.628, nll_loss=0.985, ppl=1.98, wps=72990.2, ups=5, wpb=14601.4, bsz=737.5, num_updates=30700, lr=0.000566705, gnorm=0.549, loss_scale=2, train_wall=20, gb_free=29.5, wall=5153
2024-02-09 22:48:16 | INFO | train_inner | epoch 045:    670 / 685 loss=2.63, nll_loss=0.988, ppl=1.98, wps=73375.5, ups=5.04, wpb=14560.4, bsz=742.6, num_updates=30800, lr=0.000563821, gnorm=0.541, loss_scale=2, train_wall=20, gb_free=29.6, wall=5172
2024-02-09 22:48:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:48:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:48:20 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.393 | nll_loss 1.785 | ppl 3.45 | wps 164260 | wpb 3189.1 | bsz 163.4 | num_updates 30815 | best_loss 3.393
2024-02-09 22:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 30815 updates
2024-02-09 22:48:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt
2024-02-09 22:48:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt
2024-02-09 22:48:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt (epoch 45 @ 30815 updates, score 3.393) (writing took 3.8158367131836712 seconds)
2024-02-09 22:48:24 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-02-09 22:48:24 | INFO | train | epoch 045 | loss 2.614 | nll_loss 0.968 | ppl 1.96 | wps 70044.1 | ups 4.82 | wpb 14524.4 | bsz 748.5 | num_updates 30815 | lr 0.000563388 | gnorm 0.544 | loss_scale 2 | train_wall 134 | gb_free 29.7 | wall 5181
2024-02-09 22:48:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:48:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:48:24 | INFO | fairseq.trainer | begin training epoch 46
2024-02-09 22:48:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:48:41 | INFO | train_inner | epoch 046:     85 / 685 loss=2.586, nll_loss=0.935, ppl=1.91, wps=56323.1, ups=3.89, wpb=14486.1, bsz=752.2, num_updates=30900, lr=0.000560935, gnorm=0.558, loss_scale=2, train_wall=20, gb_free=29.5, wall=5198
2024-02-09 22:49:02 | INFO | train_inner | epoch 046:    185 / 685 loss=2.584, nll_loss=0.934, ppl=1.91, wps=72562.1, ups=4.98, wpb=14558.5, bsz=757.7, num_updates=31000, lr=0.000558046, gnorm=0.538, loss_scale=2, train_wall=20, gb_free=29.5, wall=5218
2024-02-09 22:49:21 | INFO | train_inner | epoch 046:    285 / 685 loss=2.6, nll_loss=0.953, ppl=1.94, wps=72829.6, ups=5.03, wpb=14488, bsz=750.9, num_updates=31100, lr=0.000555156, gnorm=0.538, loss_scale=2, train_wall=20, gb_free=29.5, wall=5238
2024-02-09 22:49:41 | INFO | train_inner | epoch 046:    385 / 685 loss=2.602, nll_loss=0.955, ppl=1.94, wps=73209.8, ups=5, wpb=14648.8, bsz=740.8, num_updates=31200, lr=0.000552264, gnorm=0.588, loss_scale=2, train_wall=20, gb_free=29.5, wall=5258
2024-02-09 22:50:01 | INFO | train_inner | epoch 046:    485 / 685 loss=2.605, nll_loss=0.96, ppl=1.94, wps=73278.3, ups=5.01, wpb=14634, bsz=738.8, num_updates=31300, lr=0.00054937, gnorm=0.536, loss_scale=2, train_wall=20, gb_free=29.5, wall=5278
2024-02-09 22:50:21 | INFO | train_inner | epoch 046:    585 / 685 loss=2.613, nll_loss=0.969, ppl=1.96, wps=72217.4, ups=5, wpb=14451.7, bsz=742.4, num_updates=31400, lr=0.000546475, gnorm=0.545, loss_scale=2, train_wall=20, gb_free=29.5, wall=5298
2024-02-09 22:50:41 | INFO | train_inner | epoch 046:    685 / 685 loss=2.609, nll_loss=0.965, ppl=1.95, wps=72262, ups=5.04, wpb=14329.3, bsz=753.6, num_updates=31500, lr=0.000543578, gnorm=0.547, loss_scale=2, train_wall=20, gb_free=29.9, wall=5318
2024-02-09 22:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:50:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:50:43 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.416 | nll_loss 1.814 | ppl 3.52 | wps 156906 | wpb 3189.1 | bsz 163.4 | num_updates 31500 | best_loss 3.393
2024-02-09 22:50:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 31500 updates
2024-02-09 22:50:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt
2024-02-09 22:50:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt
2024-02-09 22:50:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt (epoch 46 @ 31500 updates, score 3.416) (writing took 2.4185511227697134 seconds)
2024-02-09 22:50:46 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-02-09 22:50:46 | INFO | train | epoch 046 | loss 2.599 | nll_loss 0.952 | ppl 1.93 | wps 70373.8 | ups 4.85 | wpb 14524.4 | bsz 748.5 | num_updates 31500 | lr 0.000543578 | gnorm 0.55 | loss_scale 2 | train_wall 135 | gb_free 29.9 | wall 5322
2024-02-09 22:50:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:50:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:50:46 | INFO | fairseq.trainer | begin training epoch 47
2024-02-09 22:50:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:51:06 | INFO | train_inner | epoch 047:    100 / 685 loss=2.569, nll_loss=0.918, ppl=1.89, wps=59641.3, ups=4.11, wpb=14510.6, bsz=740.2, num_updates=31600, lr=0.000540679, gnorm=0.53, loss_scale=2, train_wall=20, gb_free=29.5, wall=5342
2024-02-09 22:51:26 | INFO | train_inner | epoch 047:    200 / 685 loss=2.58, nll_loss=0.93, ppl=1.91, wps=72519.4, ups=4.99, wpb=14538.6, bsz=742.7, num_updates=31700, lr=0.000537779, gnorm=0.555, loss_scale=2, train_wall=20, gb_free=29.5, wall=5362
2024-02-09 22:51:46 | INFO | train_inner | epoch 047:    300 / 685 loss=2.58, nll_loss=0.93, ppl=1.91, wps=71190.5, ups=4.97, wpb=14331.8, bsz=756.4, num_updates=31800, lr=0.000534878, gnorm=0.536, loss_scale=2, train_wall=20, gb_free=29.5, wall=5382
2024-02-09 22:52:06 | INFO | train_inner | epoch 047:    400 / 685 loss=2.583, nll_loss=0.935, ppl=1.91, wps=72425.6, ups=4.93, wpb=14691.4, bsz=753.1, num_updates=31900, lr=0.000531976, gnorm=0.516, loss_scale=2, train_wall=20, gb_free=29.5, wall=5403
2024-02-09 22:52:26 | INFO | train_inner | epoch 047:    500 / 685 loss=2.595, nll_loss=0.948, ppl=1.93, wps=71853, ups=4.98, wpb=14434.8, bsz=733.5, num_updates=32000, lr=0.000529072, gnorm=0.536, loss_scale=2, train_wall=20, gb_free=29.5, wall=5423
2024-02-09 22:52:46 | INFO | train_inner | epoch 047:    600 / 685 loss=2.599, nll_loss=0.953, ppl=1.94, wps=72589.2, ups=4.97, wpb=14593.3, bsz=761.8, num_updates=32100, lr=0.000526168, gnorm=0.539, loss_scale=2, train_wall=20, gb_free=29.5, wall=5443
2024-02-09 22:53:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:53:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:53:05 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.415 | nll_loss 1.816 | ppl 3.52 | wps 144440 | wpb 3189.1 | bsz 163.4 | num_updates 32185 | best_loss 3.393
2024-02-09 22:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 32185 updates
2024-02-09 22:53:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt
2024-02-09 22:53:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt
2024-02-09 22:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt (epoch 47 @ 32185 updates, score 3.415) (writing took 2.4249796860385686 seconds)
2024-02-09 22:53:08 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-02-09 22:53:08 | INFO | train | epoch 047 | loss 2.586 | nll_loss 0.938 | ppl 1.92 | wps 69994.5 | ups 4.82 | wpb 14524.4 | bsz 748.5 | num_updates 32185 | lr 0.000523699 | gnorm 0.533 | loss_scale 2 | train_wall 136 | gb_free 29.6 | wall 5464
2024-02-09 22:53:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:53:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:53:08 | INFO | fairseq.trainer | begin training epoch 48
2024-02-09 22:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:53:11 | INFO | train_inner | epoch 048:     15 / 685 loss=2.59, nll_loss=0.945, ppl=1.93, wps=59077.2, ups=4.05, wpb=14599.2, bsz=751.9, num_updates=32200, lr=0.000523263, gnorm=0.513, loss_scale=2, train_wall=20, gb_free=29.5, wall=5468
2024-02-09 22:53:31 | INFO | train_inner | epoch 048:    115 / 685 loss=2.559, nll_loss=0.907, ppl=1.88, wps=72363.1, ups=4.98, wpb=14527.1, bsz=728.8, num_updates=32300, lr=0.000520357, gnorm=0.515, loss_scale=2, train_wall=20, gb_free=29.5, wall=5488
2024-02-09 22:53:51 | INFO | train_inner | epoch 048:    215 / 685 loss=2.56, nll_loss=0.909, ppl=1.88, wps=71758.6, ups=4.95, wpb=14495.7, bsz=756.9, num_updates=32400, lr=0.00051745, gnorm=0.531, loss_scale=4, train_wall=20, gb_free=29.5, wall=5508
2024-02-09 22:54:11 | INFO | train_inner | epoch 048:    315 / 685 loss=2.566, nll_loss=0.916, ppl=1.89, wps=73046.8, ups=5.01, wpb=14566.1, bsz=760.6, num_updates=32500, lr=0.000514542, gnorm=0.525, loss_scale=4, train_wall=20, gb_free=29.5, wall=5528
2024-02-09 22:54:31 | INFO | train_inner | epoch 048:    415 / 685 loss=2.582, nll_loss=0.935, ppl=1.91, wps=72438.6, ups=5, wpb=14494.3, bsz=754.8, num_updates=32600, lr=0.000511634, gnorm=0.521, loss_scale=4, train_wall=20, gb_free=29.5, wall=5548
2024-02-09 22:54:51 | INFO | train_inner | epoch 048:    515 / 685 loss=2.582, nll_loss=0.934, ppl=1.91, wps=72932.9, ups=5.05, wpb=14454.4, bsz=738.6, num_updates=32700, lr=0.000508726, gnorm=0.53, loss_scale=4, train_wall=20, gb_free=29.6, wall=5568
2024-02-09 22:55:11 | INFO | train_inner | epoch 048:    615 / 685 loss=2.585, nll_loss=0.94, ppl=1.92, wps=72658.2, ups=4.97, wpb=14614.2, bsz=755.7, num_updates=32800, lr=0.000505818, gnorm=0.527, loss_scale=4, train_wall=20, gb_free=29.5, wall=5588
2024-02-09 22:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:55:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:55:27 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.403 | nll_loss 1.805 | ppl 3.49 | wps 158237 | wpb 3189.1 | bsz 163.4 | num_updates 32870 | best_loss 3.393
2024-02-09 22:55:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 32870 updates
2024-02-09 22:55:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt
2024-02-09 22:55:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt
2024-02-09 22:55:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt (epoch 48 @ 32870 updates, score 3.403) (writing took 2.527567256009206 seconds)
2024-02-09 22:55:29 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-02-09 22:55:29 | INFO | train | epoch 048 | loss 2.573 | nll_loss 0.924 | ppl 1.9 | wps 70230.9 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 32870 | lr 0.000503782 | gnorm 0.526 | loss_scale 4 | train_wall 135 | gb_free 29.7 | wall 5606
2024-02-09 22:55:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:55:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:55:29 | INFO | fairseq.trainer | begin training epoch 49
2024-02-09 22:55:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:55:36 | INFO | train_inner | epoch 049:     30 / 685 loss=2.563, nll_loss=0.912, ppl=1.88, wps=59494.1, ups=4.08, wpb=14579.2, bsz=739.8, num_updates=32900, lr=0.000502909, gnorm=0.552, loss_scale=4, train_wall=20, gb_free=29.5, wall=5612
2024-02-09 22:55:56 | INFO | train_inner | epoch 049:    130 / 685 loss=2.541, nll_loss=0.887, ppl=1.85, wps=71486.6, ups=4.96, wpb=14413.8, bsz=760.8, num_updates=33000, lr=0.0005, gnorm=0.534, loss_scale=4, train_wall=20, gb_free=29.6, wall=5632
2024-02-09 22:56:16 | INFO | train_inner | epoch 049:    230 / 685 loss=2.553, nll_loss=0.901, ppl=1.87, wps=71927.5, ups=4.96, wpb=14515.8, bsz=746.5, num_updates=33100, lr=0.000497091, gnorm=0.569, loss_scale=4, train_wall=20, gb_free=29.5, wall=5653
2024-02-09 22:56:36 | INFO | train_inner | epoch 049:    330 / 685 loss=2.557, nll_loss=0.907, ppl=1.87, wps=72794, ups=5, wpb=14557.5, bsz=751.8, num_updates=33200, lr=0.000494182, gnorm=0.516, loss_scale=4, train_wall=20, gb_free=29.5, wall=5673
2024-02-09 22:56:56 | INFO | train_inner | epoch 049:    430 / 685 loss=2.567, nll_loss=0.919, ppl=1.89, wps=73058.1, ups=5, wpb=14625.8, bsz=757, num_updates=33300, lr=0.000491274, gnorm=0.52, loss_scale=4, train_wall=20, gb_free=29.5, wall=5693
2024-02-09 22:57:16 | INFO | train_inner | epoch 049:    530 / 685 loss=2.573, nll_loss=0.926, ppl=1.9, wps=73130.1, ups=4.96, wpb=14739.9, bsz=755, num_updates=33400, lr=0.000488366, gnorm=0.533, loss_scale=4, train_wall=20, gb_free=29.5, wall=5713
2024-02-09 22:57:36 | INFO | train_inner | epoch 049:    630 / 685 loss=2.568, nll_loss=0.919, ppl=1.89, wps=71347.9, ups=4.99, wpb=14293.3, bsz=731.2, num_updates=33500, lr=0.000485458, gnorm=0.529, loss_scale=4, train_wall=20, gb_free=29.5, wall=5733
2024-02-09 22:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 22:57:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:57:49 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.406 | nll_loss 1.803 | ppl 3.49 | wps 157214 | wpb 3189.1 | bsz 163.4 | num_updates 33555 | best_loss 3.393
2024-02-09 22:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 33555 updates
2024-02-09 22:57:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt
2024-02-09 22:57:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt
2024-02-09 22:57:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt (epoch 49 @ 33555 updates, score 3.406) (writing took 2.487097254022956 seconds)
2024-02-09 22:57:51 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-02-09 22:57:51 | INFO | train | epoch 049 | loss 2.559 | nll_loss 0.909 | ppl 1.88 | wps 70053.4 | ups 4.82 | wpb 14524.4 | bsz 748.5 | num_updates 33555 | lr 0.000483859 | gnorm 0.535 | loss_scale 4 | train_wall 136 | gb_free 29.6 | wall 5748
2024-02-09 22:57:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 22:57:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 22:57:51 | INFO | fairseq.trainer | begin training epoch 50
2024-02-09 22:57:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 22:58:01 | INFO | train_inner | epoch 050:     45 / 685 loss=2.548, nll_loss=0.897, ppl=1.86, wps=59073.2, ups=4.08, wpb=14463.1, bsz=746.1, num_updates=33600, lr=0.00048255, gnorm=0.533, loss_scale=4, train_wall=20, gb_free=29.6, wall=5757
2024-02-09 22:58:21 | INFO | train_inner | epoch 050:    145 / 685 loss=2.534, nll_loss=0.88, ppl=1.84, wps=72781, ups=5.01, wpb=14519.8, bsz=731.2, num_updates=33700, lr=0.000479643, gnorm=0.512, loss_scale=4, train_wall=20, gb_free=29.5, wall=5777
2024-02-09 22:58:41 | INFO | train_inner | epoch 050:    245 / 685 loss=2.539, nll_loss=0.887, ppl=1.85, wps=72389.1, ups=4.99, wpb=14507, bsz=758.4, num_updates=33800, lr=0.000476737, gnorm=0.514, loss_scale=4, train_wall=20, gb_free=29.5, wall=5797
2024-02-09 22:59:01 | INFO | train_inner | epoch 050:    345 / 685 loss=2.55, nll_loss=0.9, ppl=1.87, wps=72787.5, ups=5.02, wpb=14495.5, bsz=732.7, num_updates=33900, lr=0.000473832, gnorm=0.519, loss_scale=4, train_wall=20, gb_free=29.5, wall=5817
2024-02-09 22:59:20 | INFO | train_inner | epoch 050:    445 / 685 loss=2.546, nll_loss=0.897, ppl=1.86, wps=73135.9, ups=5.01, wpb=14602, bsz=750.6, num_updates=34000, lr=0.000470928, gnorm=0.508, loss_scale=4, train_wall=20, gb_free=29.5, wall=5837
2024-02-09 22:59:40 | INFO | train_inner | epoch 050:    545 / 685 loss=2.562, nll_loss=0.914, ppl=1.88, wps=74306.1, ups=5.07, wpb=14646.4, bsz=755.1, num_updates=34100, lr=0.000468024, gnorm=0.514, loss_scale=4, train_wall=19, gb_free=29.5, wall=5857
2024-02-09 23:00:00 | INFO | train_inner | epoch 050:    645 / 685 loss=2.557, nll_loss=0.907, ppl=1.88, wps=72222.4, ups=4.96, wpb=14553, bsz=757.4, num_updates=34200, lr=0.000465122, gnorm=0.535, loss_scale=4, train_wall=20, gb_free=29.6, wall=5877
2024-02-09 23:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:00:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:00:10 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.401 | nll_loss 1.802 | ppl 3.49 | wps 159336 | wpb 3189.1 | bsz 163.4 | num_updates 34240 | best_loss 3.393
2024-02-09 23:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 34240 updates
2024-02-09 23:00:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt
2024-02-09 23:00:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt
2024-02-09 23:00:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt (epoch 50 @ 34240 updates, score 3.401) (writing took 2.477962266886607 seconds)
2024-02-09 23:00:13 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-02-09 23:00:13 | INFO | train | epoch 050 | loss 2.546 | nll_loss 0.896 | ppl 1.86 | wps 70392.1 | ups 4.85 | wpb 14524.4 | bsz 748.5 | num_updates 34240 | lr 0.000463961 | gnorm 0.52 | loss_scale 4 | train_wall 135 | gb_free 29.6 | wall 5889
2024-02-09 23:00:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:00:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:00:13 | INFO | fairseq.trainer | begin training epoch 51
2024-02-09 23:00:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:00:25 | INFO | train_inner | epoch 051:     60 / 685 loss=2.528, nll_loss=0.875, ppl=1.83, wps=58446.1, ups=4.06, wpb=14406.4, bsz=752.4, num_updates=34300, lr=0.000462221, gnorm=0.513, loss_scale=4, train_wall=20, gb_free=29.5, wall=5902
2024-02-09 23:00:45 | INFO | train_inner | epoch 051:    160 / 685 loss=2.513, nll_loss=0.857, ppl=1.81, wps=72553.8, ups=4.98, wpb=14562, bsz=769.7, num_updates=34400, lr=0.000459321, gnorm=0.51, loss_scale=4, train_wall=20, gb_free=29.5, wall=5922
2024-02-09 23:01:05 | INFO | train_inner | epoch 051:    260 / 685 loss=2.53, nll_loss=0.878, ppl=1.84, wps=72633.5, ups=4.98, wpb=14583.9, bsz=750.3, num_updates=34500, lr=0.000456422, gnorm=0.499, loss_scale=4, train_wall=20, gb_free=29.5, wall=5942
2024-02-09 23:01:25 | INFO | train_inner | epoch 051:    360 / 685 loss=2.532, nll_loss=0.88, ppl=1.84, wps=72611.9, ups=5, wpb=14526.7, bsz=755.3, num_updates=34600, lr=0.000453525, gnorm=0.513, loss_scale=4, train_wall=20, gb_free=29.5, wall=5962
2024-02-09 23:01:45 | INFO | train_inner | epoch 051:    460 / 685 loss=2.546, nll_loss=0.897, ppl=1.86, wps=72621.5, ups=4.99, wpb=14558.9, bsz=732, num_updates=34700, lr=0.00045063, gnorm=0.528, loss_scale=4, train_wall=20, gb_free=29.6, wall=5982
2024-02-09 23:02:05 | INFO | train_inner | epoch 051:    560 / 685 loss=2.547, nll_loss=0.898, ppl=1.86, wps=72291.2, ups=5, wpb=14453.5, bsz=741, num_updates=34800, lr=0.000447736, gnorm=0.528, loss_scale=4, train_wall=20, gb_free=29.6, wall=6002
2024-02-09 23:02:25 | INFO | train_inner | epoch 051:    660 / 685 loss=2.543, nll_loss=0.894, ppl=1.86, wps=73170.6, ups=5.06, wpb=14456.7, bsz=755.1, num_updates=34900, lr=0.000444844, gnorm=0.51, loss_scale=4, train_wall=20, gb_free=29.5, wall=6022
2024-02-09 23:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:02:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:02:32 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.415 | nll_loss 1.813 | ppl 3.51 | wps 156652 | wpb 3189.1 | bsz 163.4 | num_updates 34925 | best_loss 3.393
2024-02-09 23:02:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 34925 updates
2024-02-09 23:02:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt
2024-02-09 23:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt
2024-02-09 23:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt (epoch 51 @ 34925 updates, score 3.415) (writing took 2.4053697381168604 seconds)
2024-02-09 23:02:34 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-02-09 23:02:34 | INFO | train | epoch 051 | loss 2.534 | nll_loss 0.882 | ppl 1.84 | wps 70392.3 | ups 4.85 | wpb 14524.4 | bsz 748.5 | num_updates 34925 | lr 0.000444121 | gnorm 0.513 | loss_scale 4 | train_wall 135 | gb_free 29.6 | wall 6031
2024-02-09 23:02:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:02:34 | INFO | fairseq.trainer | begin training epoch 52
2024-02-09 23:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:02:49 | INFO | train_inner | epoch 052:     75 / 685 loss=2.517, nll_loss=0.862, ppl=1.82, wps=59450, ups=4.13, wpb=14411.1, bsz=749.4, num_updates=35000, lr=0.000441954, gnorm=0.508, loss_scale=4, train_wall=20, gb_free=29.6, wall=6046
2024-02-09 23:03:09 | INFO | train_inner | epoch 052:    175 / 685 loss=2.513, nll_loss=0.858, ppl=1.81, wps=72105, ups=5.03, wpb=14348.4, bsz=724.6, num_updates=35100, lr=0.000439065, gnorm=0.511, loss_scale=4, train_wall=20, gb_free=29.5, wall=6066
2024-02-09 23:03:29 | INFO | train_inner | epoch 052:    275 / 685 loss=2.521, nll_loss=0.868, ppl=1.83, wps=73856.2, ups=4.99, wpb=14792.5, bsz=743.8, num_updates=35200, lr=0.000436179, gnorm=0.489, loss_scale=4, train_wall=20, gb_free=29.5, wall=6086
2024-02-09 23:03:49 | INFO | train_inner | epoch 052:    375 / 685 loss=2.522, nll_loss=0.87, ppl=1.83, wps=73425.7, ups=5.01, wpb=14661.6, bsz=756.1, num_updates=35300, lr=0.000433295, gnorm=0.495, loss_scale=4, train_wall=20, gb_free=29.5, wall=6106
2024-02-09 23:04:09 | INFO | train_inner | epoch 052:    475 / 685 loss=2.528, nll_loss=0.877, ppl=1.84, wps=72764.7, ups=5.03, wpb=14479.4, bsz=747.4, num_updates=35400, lr=0.000430413, gnorm=0.514, loss_scale=4, train_wall=20, gb_free=29.5, wall=6126
2024-02-09 23:04:29 | INFO | train_inner | epoch 052:    575 / 685 loss=2.529, nll_loss=0.879, ppl=1.84, wps=73100.1, ups=5, wpb=14624.8, bsz=746.2, num_updates=35500, lr=0.000427534, gnorm=0.489, loss_scale=4, train_wall=20, gb_free=29.5, wall=6146
2024-02-09 23:04:49 | INFO | train_inner | epoch 052:    675 / 685 loss=2.526, nll_loss=0.874, ppl=1.83, wps=72069.7, ups=5.02, wpb=14344.3, bsz=753.2, num_updates=35600, lr=0.000424657, gnorm=0.511, loss_scale=4, train_wall=20, gb_free=29.5, wall=6166
2024-02-09 23:04:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:04:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:04:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.407 | nll_loss 1.813 | ppl 3.51 | wps 160340 | wpb 3189.1 | bsz 163.4 | num_updates 35610 | best_loss 3.393
2024-02-09 23:04:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 35610 updates
2024-02-09 23:04:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt
2024-02-09 23:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt
2024-02-09 23:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt (epoch 52 @ 35610 updates, score 3.407) (writing took 2.4589091329835355 seconds)
2024-02-09 23:04:55 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-02-09 23:04:55 | INFO | train | epoch 052 | loss 2.521 | nll_loss 0.869 | ppl 1.83 | wps 70561.9 | ups 4.86 | wpb 14524.4 | bsz 748.5 | num_updates 35610 | lr 0.00042437 | gnorm 0.502 | loss_scale 4 | train_wall 135 | gb_free 29.7 | wall 6172
2024-02-09 23:04:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:04:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:04:55 | INFO | fairseq.trainer | begin training epoch 53
2024-02-09 23:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:05:13 | INFO | train_inner | epoch 053:     90 / 685 loss=2.491, nll_loss=0.834, ppl=1.78, wps=59839.1, ups=4.14, wpb=14440, bsz=781.1, num_updates=35700, lr=0.000421783, gnorm=0.487, loss_scale=4, train_wall=19, gb_free=29.5, wall=6190
2024-02-09 23:05:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-09 23:05:33 | INFO | train_inner | epoch 053:    191 / 685 loss=2.502, nll_loss=0.847, ppl=1.8, wps=72695.3, ups=4.97, wpb=14633.6, bsz=739.5, num_updates=35800, lr=0.000418911, gnorm=0.495, loss_scale=2, train_wall=20, gb_free=29.5, wall=6210
2024-02-09 23:05:53 | INFO | train_inner | epoch 053:    291 / 685 loss=2.511, nll_loss=0.858, ppl=1.81, wps=73172.1, ups=4.99, wpb=14666.6, bsz=745.7, num_updates=35900, lr=0.000416042, gnorm=0.489, loss_scale=2, train_wall=20, gb_free=29.5, wall=6230
2024-02-09 23:06:13 | INFO | train_inner | epoch 053:    391 / 685 loss=2.509, nll_loss=0.856, ppl=1.81, wps=72930, ups=5.03, wpb=14509.6, bsz=732.6, num_updates=36000, lr=0.000413176, gnorm=0.499, loss_scale=2, train_wall=20, gb_free=29.5, wall=6250
2024-02-09 23:06:33 | INFO | train_inner | epoch 053:    491 / 685 loss=2.509, nll_loss=0.855, ppl=1.81, wps=72583.5, ups=5.02, wpb=14452.7, bsz=736.9, num_updates=36100, lr=0.000410313, gnorm=0.498, loss_scale=2, train_wall=20, gb_free=29.6, wall=6270
2024-02-09 23:06:53 | INFO | train_inner | epoch 053:    591 / 685 loss=2.519, nll_loss=0.868, ppl=1.82, wps=72471.8, ups=5.02, wpb=14439.6, bsz=761.8, num_updates=36200, lr=0.000407453, gnorm=0.5, loss_scale=2, train_wall=20, gb_free=29.5, wall=6290
2024-02-09 23:07:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:07:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:07:13 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.411 | nll_loss 1.812 | ppl 3.51 | wps 161527 | wpb 3189.1 | bsz 163.4 | num_updates 36294 | best_loss 3.393
2024-02-09 23:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 36294 updates
2024-02-09 23:07:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt
2024-02-09 23:07:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt
2024-02-09 23:07:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt (epoch 53 @ 36294 updates, score 3.411) (writing took 2.4441601999569684 seconds)
2024-02-09 23:07:16 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-02-09 23:07:16 | INFO | train | epoch 053 | loss 2.509 | nll_loss 0.855 | ppl 1.81 | wps 70574.4 | ups 4.86 | wpb 14526.1 | bsz 748.8 | num_updates 36294 | lr 0.000404767 | gnorm 0.495 | loss_scale 2 | train_wall 135 | gb_free 29.6 | wall 6313
2024-02-09 23:07:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:07:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:07:16 | INFO | fairseq.trainer | begin training epoch 54
2024-02-09 23:07:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:07:17 | INFO | train_inner | epoch 054:      6 / 685 loss=2.52, nll_loss=0.869, ppl=1.83, wps=59435.7, ups=4.12, wpb=14421.5, bsz=752.6, num_updates=36300, lr=0.000404596, gnorm=0.499, loss_scale=2, train_wall=20, gb_free=29.5, wall=6314
2024-02-09 23:07:37 | INFO | train_inner | epoch 054:    106 / 685 loss=2.479, nll_loss=0.821, ppl=1.77, wps=73242.3, ups=4.98, wpb=14717, bsz=759.8, num_updates=36400, lr=0.000401742, gnorm=0.486, loss_scale=2, train_wall=20, gb_free=29.5, wall=6334
2024-02-09 23:07:57 | INFO | train_inner | epoch 054:    206 / 685 loss=2.492, nll_loss=0.836, ppl=1.78, wps=72636.1, ups=5.06, wpb=14353.3, bsz=735.6, num_updates=36500, lr=0.000398891, gnorm=0.504, loss_scale=2, train_wall=20, gb_free=29.5, wall=6354
2024-02-09 23:08:17 | INFO | train_inner | epoch 054:    306 / 685 loss=2.492, nll_loss=0.837, ppl=1.79, wps=73367.4, ups=5.03, wpb=14594.9, bsz=737.8, num_updates=36600, lr=0.000396044, gnorm=0.482, loss_scale=2, train_wall=20, gb_free=29.5, wall=6374
2024-02-09 23:08:37 | INFO | train_inner | epoch 054:    406 / 685 loss=2.498, nll_loss=0.845, ppl=1.8, wps=73947.2, ups=5.03, wpb=14700.3, bsz=782.4, num_updates=36700, lr=0.000393201, gnorm=0.499, loss_scale=2, train_wall=20, gb_free=29.5, wall=6394
2024-02-09 23:08:57 | INFO | train_inner | epoch 054:    506 / 685 loss=2.503, nll_loss=0.849, ppl=1.8, wps=72141.7, ups=5.04, wpb=14312.3, bsz=756.6, num_updates=36800, lr=0.000390361, gnorm=0.493, loss_scale=2, train_wall=20, gb_free=29.5, wall=6413
2024-02-09 23:09:16 | INFO | train_inner | epoch 054:    606 / 685 loss=2.511, nll_loss=0.86, ppl=1.81, wps=73840.4, ups=5.06, wpb=14583.5, bsz=740.1, num_updates=36900, lr=0.000387524, gnorm=0.489, loss_scale=2, train_wall=19, gb_free=29.5, wall=6433
2024-02-09 23:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:09:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:09:34 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.382 | nll_loss 1.786 | ppl 3.45 | wps 160538 | wpb 3189.1 | bsz 163.4 | num_updates 36979 | best_loss 3.382
2024-02-09 23:09:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 36979 updates
2024-02-09 23:09:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt
2024-02-09 23:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt
2024-02-09 23:09:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt (epoch 54 @ 36979 updates, score 3.382) (writing took 3.7703629229217768 seconds)
2024-02-09 23:09:37 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-02-09 23:09:37 | INFO | train | epoch 054 | loss 2.498 | nll_loss 0.844 | ppl 1.79 | wps 70285.7 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 36979 | lr 0.000385287 | gnorm 0.493 | loss_scale 2 | train_wall 134 | gb_free 29.7 | wall 6454
2024-02-09 23:09:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:09:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:09:37 | INFO | fairseq.trainer | begin training epoch 55
2024-02-09 23:09:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:09:42 | INFO | train_inner | epoch 055:     21 / 685 loss=2.507, nll_loss=0.853, ppl=1.81, wps=56970.7, ups=3.96, wpb=14380.1, bsz=716.9, num_updates=37000, lr=0.000384692, gnorm=0.496, loss_scale=2, train_wall=19, gb_free=29.5, wall=6458
2024-02-09 23:10:01 | INFO | train_inner | epoch 055:    121 / 685 loss=2.475, nll_loss=0.816, ppl=1.76, wps=73819.8, ups=5.05, wpb=14619.4, bsz=739, num_updates=37100, lr=0.000381864, gnorm=0.475, loss_scale=2, train_wall=20, gb_free=29.5, wall=6478
2024-02-09 23:10:21 | INFO | train_inner | epoch 055:    221 / 685 loss=2.47, nll_loss=0.813, ppl=1.76, wps=72359.9, ups=5.01, wpb=14452.5, bsz=776.2, num_updates=37200, lr=0.000379039, gnorm=0.473, loss_scale=2, train_wall=20, gb_free=29.5, wall=6498
2024-02-09 23:10:41 | INFO | train_inner | epoch 055:    321 / 685 loss=2.477, nll_loss=0.821, ppl=1.77, wps=73434.7, ups=5.03, wpb=14593.3, bsz=756.8, num_updates=37300, lr=0.000376219, gnorm=0.491, loss_scale=2, train_wall=20, gb_free=29.6, wall=6518
2024-02-09 23:11:01 | INFO | train_inner | epoch 055:    421 / 685 loss=2.495, nll_loss=0.841, ppl=1.79, wps=73190.3, ups=5.06, wpb=14477.2, bsz=741.5, num_updates=37400, lr=0.000373402, gnorm=0.491, loss_scale=2, train_wall=20, gb_free=29.5, wall=6538
2024-02-09 23:11:21 | INFO | train_inner | epoch 055:    521 / 685 loss=2.501, nll_loss=0.849, ppl=1.8, wps=73794.5, ups=5.03, wpb=14656.4, bsz=737.4, num_updates=37500, lr=0.00037059, gnorm=0.575, loss_scale=2, train_wall=20, gb_free=29.5, wall=6558
2024-02-09 23:11:41 | INFO | train_inner | epoch 055:    621 / 685 loss=2.497, nll_loss=0.844, ppl=1.8, wps=73512.7, ups=5.06, wpb=14528.5, bsz=758.8, num_updates=37600, lr=0.000367783, gnorm=0.482, loss_scale=2, train_wall=20, gb_free=29.5, wall=6577
2024-02-09 23:11:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:11:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:11:55 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.392 | nll_loss 1.792 | ppl 3.46 | wps 162729 | wpb 3189.1 | bsz 163.4 | num_updates 37664 | best_loss 3.382
2024-02-09 23:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 37664 updates
2024-02-09 23:11:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt
2024-02-09 23:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt
2024-02-09 23:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt (epoch 55 @ 37664 updates, score 3.392) (writing took 2.3897948060184717 seconds)
2024-02-09 23:11:58 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-02-09 23:11:58 | INFO | train | epoch 055 | loss 2.486 | nll_loss 0.831 | ppl 1.78 | wps 71041.7 | ups 4.89 | wpb 14524.4 | bsz 748.5 | num_updates 37664 | lr 0.000365988 | gnorm 0.496 | loss_scale 2 | train_wall 134 | gb_free 29.8 | wall 6594
2024-02-09 23:11:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:11:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:11:58 | INFO | fairseq.trainer | begin training epoch 56
2024-02-09 23:11:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:12:05 | INFO | train_inner | epoch 056:     36 / 685 loss=2.481, nll_loss=0.825, ppl=1.77, wps=59811.3, ups=4.17, wpb=14358.8, bsz=732.2, num_updates=37700, lr=0.00036498, gnorm=0.527, loss_scale=2, train_wall=19, gb_free=29.5, wall=6601
2024-02-09 23:12:25 | INFO | train_inner | epoch 056:    136 / 685 loss=2.461, nll_loss=0.803, ppl=1.74, wps=73544.7, ups=5.04, wpb=14589.4, bsz=763.6, num_updates=37800, lr=0.000362181, gnorm=0.487, loss_scale=2, train_wall=20, gb_free=29.5, wall=6621
2024-02-09 23:12:44 | INFO | train_inner | epoch 056:    236 / 685 loss=2.47, nll_loss=0.813, ppl=1.76, wps=73067.6, ups=5.06, wpb=14446.3, bsz=724.2, num_updates=37900, lr=0.000359387, gnorm=0.504, loss_scale=2, train_wall=20, gb_free=29.5, wall=6641
2024-02-09 23:13:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2024-02-09 23:13:04 | INFO | train_inner | epoch 056:    337 / 685 loss=2.483, nll_loss=0.829, ppl=1.78, wps=73395.3, ups=5, wpb=14681.2, bsz=770.2, num_updates=38000, lr=0.000356598, gnorm=0.539, loss_scale=1, train_wall=20, gb_free=29.5, wall=6661
2024-02-09 23:13:24 | INFO | train_inner | epoch 056:    437 / 685 loss=2.477, nll_loss=0.821, ppl=1.77, wps=74016.4, ups=5.08, wpb=14558.1, bsz=753.4, num_updates=38100, lr=0.000353814, gnorm=0.494, loss_scale=1, train_wall=19, gb_free=29.5, wall=6681
2024-02-09 23:13:44 | INFO | train_inner | epoch 056:    537 / 685 loss=2.481, nll_loss=0.827, ppl=1.77, wps=73440, ups=5.09, wpb=14441.5, bsz=749.8, num_updates=38200, lr=0.000351035, gnorm=0.488, loss_scale=1, train_wall=19, gb_free=29.5, wall=6700
2024-02-09 23:14:04 | INFO | train_inner | epoch 056:    637 / 685 loss=2.483, nll_loss=0.829, ppl=1.78, wps=73067, ups=5.04, wpb=14489.1, bsz=740.9, num_updates=38300, lr=0.000348261, gnorm=0.473, loss_scale=1, train_wall=20, gb_free=29.6, wall=6720
2024-02-09 23:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:14:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:14:15 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.403 | nll_loss 1.812 | ppl 3.51 | wps 154336 | wpb 3189.1 | bsz 163.4 | num_updates 38348 | best_loss 3.382
2024-02-09 23:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 38348 updates
2024-02-09 23:14:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt
2024-02-09 23:14:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt
2024-02-09 23:14:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt (epoch 56 @ 38348 updates, score 3.403) (writing took 2.4112144270911813 seconds)
2024-02-09 23:14:17 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-02-09 23:14:17 | INFO | train | epoch 056 | loss 2.476 | nll_loss 0.82 | ppl 1.77 | wps 71091.2 | ups 4.89 | wpb 14526.1 | bsz 748.7 | num_updates 38348 | lr 0.000346931 | gnorm 0.502 | loss_scale 1 | train_wall 134 | gb_free 29.6 | wall 6734
2024-02-09 23:14:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:14:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:14:17 | INFO | fairseq.trainer | begin training epoch 57
2024-02-09 23:14:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:14:28 | INFO | train_inner | epoch 057:     52 / 685 loss=2.467, nll_loss=0.81, ppl=1.75, wps=60213.6, ups=4.13, wpb=14564.9, bsz=739.7, num_updates=38400, lr=0.000345492, gnorm=0.471, loss_scale=1, train_wall=19, gb_free=29.6, wall=6744
2024-02-09 23:14:48 | INFO | train_inner | epoch 057:    152 / 685 loss=2.449, nll_loss=0.79, ppl=1.73, wps=72778.2, ups=5.02, wpb=14495.7, bsz=756.6, num_updates=38500, lr=0.000342728, gnorm=0.465, loss_scale=1, train_wall=20, gb_free=29.5, wall=6764
2024-02-09 23:15:07 | INFO | train_inner | epoch 057:    252 / 685 loss=2.459, nll_loss=0.801, ppl=1.74, wps=72992, ups=5.04, wpb=14470.6, bsz=754.1, num_updates=38600, lr=0.000339969, gnorm=0.481, loss_scale=1, train_wall=20, gb_free=29.5, wall=6784
2024-02-09 23:15:27 | INFO | train_inner | epoch 057:    352 / 685 loss=2.47, nll_loss=0.814, ppl=1.76, wps=73478.1, ups=5.06, wpb=14527.3, bsz=735.8, num_updates=38700, lr=0.000337216, gnorm=0.608, loss_scale=1, train_wall=20, gb_free=29.5, wall=6804
2024-02-09 23:15:47 | INFO | train_inner | epoch 057:    452 / 685 loss=2.476, nll_loss=0.821, ppl=1.77, wps=74052.3, ups=5.06, wpb=14628.1, bsz=724.9, num_updates=38800, lr=0.000334468, gnorm=0.482, loss_scale=1, train_wall=19, gb_free=29.6, wall=6824
2024-02-09 23:16:07 | INFO | train_inner | epoch 057:    552 / 685 loss=2.464, nll_loss=0.808, ppl=1.75, wps=73353.3, ups=5.08, wpb=14435, bsz=760, num_updates=38900, lr=0.000331726, gnorm=0.482, loss_scale=1, train_wall=19, gb_free=29.5, wall=6843
2024-02-09 23:16:27 | INFO | train_inner | epoch 057:    652 / 685 loss=2.472, nll_loss=0.817, ppl=1.76, wps=73925.7, ups=5.03, wpb=14696.5, bsz=777.1, num_updates=39000, lr=0.00032899, gnorm=0.475, loss_scale=1, train_wall=20, gb_free=29.5, wall=6863
2024-02-09 23:16:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:16:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:16:35 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.385 | nll_loss 1.791 | ppl 3.46 | wps 154605 | wpb 3189.1 | bsz 163.4 | num_updates 39033 | best_loss 3.382
2024-02-09 23:16:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 39033 updates
2024-02-09 23:16:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt
2024-02-09 23:16:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt
2024-02-09 23:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt (epoch 57 @ 39033 updates, score 3.385) (writing took 2.510647224029526 seconds)
2024-02-09 23:16:38 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-02-09 23:16:38 | INFO | train | epoch 057 | loss 2.464 | nll_loss 0.808 | ppl 1.75 | wps 70924.6 | ups 4.88 | wpb 14524.4 | bsz 748.5 | num_updates 39033 | lr 0.000328088 | gnorm 0.495 | loss_scale 1 | train_wall 134 | gb_free 29.9 | wall 6874
2024-02-09 23:16:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:16:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:16:39 | INFO | fairseq.trainer | begin training epoch 58
2024-02-09 23:16:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:16:53 | INFO | train_inner | epoch 058:     67 / 685 loss=2.447, nll_loss=0.788, ppl=1.73, wps=53892.1, ups=3.71, wpb=14515.3, bsz=730.9, num_updates=39100, lr=0.000326259, gnorm=0.469, loss_scale=1, train_wall=19, gb_free=29.5, wall=6890
2024-02-09 23:17:13 | INFO | train_inner | epoch 058:    167 / 685 loss=2.45, nll_loss=0.792, ppl=1.73, wps=73695, ups=5.06, wpb=14556.1, bsz=764.7, num_updates=39200, lr=0.000323535, gnorm=0.466, loss_scale=1, train_wall=20, gb_free=29.5, wall=6910
2024-02-09 23:17:33 | INFO | train_inner | epoch 058:    267 / 685 loss=2.457, nll_loss=0.8, ppl=1.74, wps=73502.9, ups=5.08, wpb=14463.7, bsz=749, num_updates=39300, lr=0.000320816, gnorm=0.477, loss_scale=1, train_wall=19, gb_free=29.5, wall=6930
2024-02-09 23:17:53 | INFO | train_inner | epoch 058:    367 / 685 loss=2.456, nll_loss=0.798, ppl=1.74, wps=73692.1, ups=5.05, wpb=14589.1, bsz=746.4, num_updates=39400, lr=0.000318103, gnorm=0.477, loss_scale=1, train_wall=20, gb_free=29.5, wall=6949
2024-02-09 23:18:12 | INFO | train_inner | epoch 058:    467 / 685 loss=2.452, nll_loss=0.794, ppl=1.73, wps=73244.3, ups=5.1, wpb=14358.8, bsz=746.2, num_updates=39500, lr=0.000315397, gnorm=0.496, loss_scale=1, train_wall=19, gb_free=29.5, wall=6969
2024-02-09 23:18:32 | INFO | train_inner | epoch 058:    567 / 685 loss=2.461, nll_loss=0.805, ppl=1.75, wps=73493.4, ups=5.09, wpb=14425.7, bsz=744.5, num_updates=39600, lr=0.000312697, gnorm=0.509, loss_scale=1, train_wall=19, gb_free=29.5, wall=6989
2024-02-09 23:18:52 | INFO | train_inner | epoch 058:    667 / 685 loss=2.463, nll_loss=0.808, ppl=1.75, wps=74936.1, ups=5.09, wpb=14727.7, bsz=748.7, num_updates=39700, lr=0.000310003, gnorm=0.465, loss_scale=1, train_wall=19, gb_free=29.5, wall=7008
2024-02-09 23:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:18:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:18:57 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.406 | nll_loss 1.814 | ppl 3.52 | wps 159729 | wpb 3189.1 | bsz 163.4 | num_updates 39718 | best_loss 3.382
2024-02-09 23:18:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 39718 updates
2024-02-09 23:18:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt
2024-02-09 23:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt
2024-02-09 23:18:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt (epoch 58 @ 39718 updates, score 3.406) (writing took 2.5185131698381156 seconds)
2024-02-09 23:18:59 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-02-09 23:18:59 | INFO | train | epoch 058 | loss 2.454 | nll_loss 0.796 | ppl 1.74 | wps 70124.7 | ups 4.83 | wpb 14524.4 | bsz 748.5 | num_updates 39718 | lr 0.000309519 | gnorm 0.48 | loss_scale 1 | train_wall 133 | gb_free 29.9 | wall 7016
2024-02-09 23:18:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:18:59 | INFO | fairseq.trainer | begin training epoch 59
2024-02-09 23:18:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:19:16 | INFO | train_inner | epoch 059:     82 / 685 loss=2.431, nll_loss=0.77, ppl=1.7, wps=58698.6, ups=4.11, wpb=14273.1, bsz=743.2, num_updates=39800, lr=0.000307315, gnorm=0.473, loss_scale=1, train_wall=20, gb_free=29.6, wall=7033
2024-02-09 23:19:36 | INFO | train_inner | epoch 059:    182 / 685 loss=2.437, nll_loss=0.777, ppl=1.71, wps=74127.5, ups=5.03, wpb=14730, bsz=739.6, num_updates=39900, lr=0.000304634, gnorm=0.451, loss_scale=1, train_wall=20, gb_free=29.5, wall=7052
2024-02-09 23:19:56 | INFO | train_inner | epoch 059:    282 / 685 loss=2.445, nll_loss=0.788, ppl=1.73, wps=72093.5, ups=5.01, wpb=14392.6, bsz=759.4, num_updates=40000, lr=0.00030196, gnorm=0.46, loss_scale=1, train_wall=20, gb_free=29.5, wall=7072
2024-02-09 23:20:16 | INFO | train_inner | epoch 059:    382 / 685 loss=2.442, nll_loss=0.783, ppl=1.72, wps=72536.6, ups=4.99, wpb=14536.6, bsz=761.7, num_updates=40100, lr=0.000299293, gnorm=0.459, loss_scale=1, train_wall=20, gb_free=29.5, wall=7092
2024-02-09 23:20:36 | INFO | train_inner | epoch 059:    482 / 685 loss=2.45, nll_loss=0.793, ppl=1.73, wps=73203.9, ups=5.01, wpb=14608, bsz=733.4, num_updates=40200, lr=0.000296632, gnorm=0.465, loss_scale=1, train_wall=20, gb_free=29.5, wall=7112
2024-02-09 23:20:56 | INFO | train_inner | epoch 059:    582 / 685 loss=2.453, nll_loss=0.797, ppl=1.74, wps=73594.6, ups=5.02, wpb=14652.4, bsz=745.4, num_updates=40300, lr=0.000293978, gnorm=0.457, loss_scale=1, train_wall=20, gb_free=29.5, wall=7132
2024-02-09 23:21:16 | INFO | train_inner | epoch 059:    682 / 685 loss=2.449, nll_loss=0.792, ppl=1.73, wps=72786.1, ups=5.01, wpb=14517.2, bsz=763.8, num_updates=40400, lr=0.000291331, gnorm=0.461, loss_scale=1, train_wall=20, gb_free=29.5, wall=7152
2024-02-09 23:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:21:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:21:18 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.404 | nll_loss 1.81 | ppl 3.51 | wps 155355 | wpb 3189.1 | bsz 163.4 | num_updates 40403 | best_loss 3.382
2024-02-09 23:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 40403 updates
2024-02-09 23:21:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt
2024-02-09 23:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt
2024-02-09 23:21:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt (epoch 59 @ 40403 updates, score 3.404) (writing took 2.435545939952135 seconds)
2024-02-09 23:21:20 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-02-09 23:21:20 | INFO | train | epoch 059 | loss 2.444 | nll_loss 0.786 | ppl 1.72 | wps 70551.6 | ups 4.86 | wpb 14524.4 | bsz 748.5 | num_updates 40403 | lr 0.000291251 | gnorm 0.46 | loss_scale 1 | train_wall 135 | gb_free 29.6 | wall 7157
2024-02-09 23:21:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:21:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:21:20 | INFO | fairseq.trainer | begin training epoch 60
2024-02-09 23:21:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:21:40 | INFO | train_inner | epoch 060:     97 / 685 loss=2.421, nll_loss=0.759, ppl=1.69, wps=59236.6, ups=4.15, wpb=14282.8, bsz=732.4, num_updates=40500, lr=0.000288691, gnorm=0.458, loss_scale=1, train_wall=19, gb_free=29.6, wall=7176
2024-02-09 23:22:00 | INFO | train_inner | epoch 060:    197 / 685 loss=2.431, nll_loss=0.771, ppl=1.71, wps=72991.6, ups=5.01, wpb=14572.5, bsz=743, num_updates=40600, lr=0.000286058, gnorm=0.46, loss_scale=1, train_wall=20, gb_free=29.5, wall=7196
2024-02-09 23:22:20 | INFO | train_inner | epoch 060:    297 / 685 loss=2.425, nll_loss=0.765, ppl=1.7, wps=74005.2, ups=5, wpb=14812.6, bsz=781.1, num_updates=40700, lr=0.000283433, gnorm=0.447, loss_scale=1, train_wall=20, gb_free=29.5, wall=7216
2024-02-09 23:22:40 | INFO | train_inner | epoch 060:    397 / 685 loss=2.432, nll_loss=0.773, ppl=1.71, wps=72424.5, ups=5, wpb=14480.4, bsz=745, num_updates=40800, lr=0.000280814, gnorm=0.457, loss_scale=1, train_wall=20, gb_free=29.5, wall=7236
2024-02-09 23:23:00 | INFO | train_inner | epoch 060:    497 / 685 loss=2.437, nll_loss=0.778, ppl=1.71, wps=72486.8, ups=5.02, wpb=14444.6, bsz=743.3, num_updates=40900, lr=0.000278204, gnorm=0.46, loss_scale=1, train_wall=20, gb_free=29.5, wall=7256
2024-02-09 23:23:20 | INFO | train_inner | epoch 060:    597 / 685 loss=2.441, nll_loss=0.784, ppl=1.72, wps=71943.5, ups=4.98, wpb=14440.5, bsz=739.5, num_updates=41000, lr=0.0002756, gnorm=0.463, loss_scale=1, train_wall=20, gb_free=29.5, wall=7276
2024-02-09 23:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:23:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:23:39 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.393 | nll_loss 1.796 | ppl 3.47 | wps 155235 | wpb 3189.1 | bsz 163.4 | num_updates 41088 | best_loss 3.382
2024-02-09 23:23:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 41088 updates
2024-02-09 23:23:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt
2024-02-09 23:23:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt
2024-02-09 23:23:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt (epoch 60 @ 41088 updates, score 3.393) (writing took 2.450168651062995 seconds)
2024-02-09 23:23:42 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-02-09 23:23:42 | INFO | train | epoch 060 | loss 2.434 | nll_loss 0.775 | ppl 1.71 | wps 70427.5 | ups 4.85 | wpb 14524.4 | bsz 748.5 | num_updates 41088 | lr 0.000273316 | gnorm 0.478 | loss_scale 1 | train_wall 135 | gb_free 29.6 | wall 7298
2024-02-09 23:23:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:23:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:23:43 | INFO | fairseq.trainer | begin training epoch 61
2024-02-09 23:23:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:23:52 | INFO | train_inner | epoch 061:     12 / 685 loss=2.447, nll_loss=0.792, ppl=1.73, wps=45109.8, ups=3.1, wpb=14545.2, bsz=743.7, num_updates=41100, lr=0.000273005, gnorm=0.598, loss_scale=1, train_wall=20, gb_free=29.5, wall=7309
2024-02-09 23:24:12 | INFO | train_inner | epoch 061:    112 / 685 loss=2.412, nll_loss=0.75, ppl=1.68, wps=72439.5, ups=4.97, wpb=14562.1, bsz=754.2, num_updates=41200, lr=0.000270417, gnorm=0.449, loss_scale=1, train_wall=20, gb_free=29.5, wall=7329
2024-02-09 23:24:32 | INFO | train_inner | epoch 061:    212 / 685 loss=2.422, nll_loss=0.761, ppl=1.69, wps=74035.9, ups=5.06, wpb=14619.8, bsz=732.6, num_updates=41300, lr=0.000267837, gnorm=0.449, loss_scale=1, train_wall=19, gb_free=29.5, wall=7348
2024-02-09 23:24:52 | INFO | train_inner | epoch 061:    312 / 685 loss=2.424, nll_loss=0.765, ppl=1.7, wps=71868.5, ups=4.96, wpb=14495.9, bsz=757.9, num_updates=41400, lr=0.000265264, gnorm=0.443, loss_scale=1, train_wall=20, gb_free=29.6, wall=7369
2024-02-09 23:25:12 | INFO | train_inner | epoch 061:    412 / 685 loss=2.419, nll_loss=0.759, ppl=1.69, wps=72075.4, ups=4.96, wpb=14520.2, bsz=750.4, num_updates=41500, lr=0.0002627, gnorm=0.449, loss_scale=1, train_wall=20, gb_free=29.5, wall=7389
2024-02-09 23:25:32 | INFO | train_inner | epoch 061:    512 / 685 loss=2.435, nll_loss=0.778, ppl=1.71, wps=72155.4, ups=4.99, wpb=14466.7, bsz=761.8, num_updates=41600, lr=0.000260143, gnorm=0.493, loss_scale=1, train_wall=20, gb_free=29.6, wall=7409
2024-02-09 23:25:52 | INFO | train_inner | epoch 061:    612 / 685 loss=2.423, nll_loss=0.764, ppl=1.7, wps=72608.1, ups=4.99, wpb=14560.4, bsz=756.2, num_updates=41700, lr=0.000257595, gnorm=0.445, loss_scale=1, train_wall=20, gb_free=29.5, wall=7429
2024-02-09 23:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:26:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:26:09 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.4 | nll_loss 1.806 | ppl 3.5 | wps 154914 | wpb 3189.1 | bsz 163.4 | num_updates 41773 | best_loss 3.382
2024-02-09 23:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 41773 updates
2024-02-09 23:26:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt
2024-02-09 23:26:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt
2024-02-09 23:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt (epoch 61 @ 41773 updates, score 3.4) (writing took 2.415536514017731 seconds)
2024-02-09 23:26:11 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-02-09 23:26:11 | INFO | train | epoch 061 | loss 2.424 | nll_loss 0.764 | ppl 1.7 | wps 66628.3 | ups 4.59 | wpb 14524.4 | bsz 748.5 | num_updates 41773 | lr 0.00025574 | gnorm 0.454 | loss_scale 1 | train_wall 135 | gb_free 29.6 | wall 7448
2024-02-09 23:26:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:26:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:26:11 | INFO | fairseq.trainer | begin training epoch 62
2024-02-09 23:26:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:26:17 | INFO | train_inner | epoch 062:     27 / 685 loss=2.429, nll_loss=0.771, ppl=1.71, wps=59633.8, ups=4.1, wpb=14553.7, bsz=732.2, num_updates=41800, lr=0.000255055, gnorm=0.448, loss_scale=1, train_wall=20, gb_free=29.5, wall=7453
2024-02-09 23:26:36 | INFO | train_inner | epoch 062:    127 / 685 loss=2.403, nll_loss=0.74, ppl=1.67, wps=73431.4, ups=5.04, wpb=14576, bsz=748.6, num_updates=41900, lr=0.000252523, gnorm=0.44, loss_scale=1, train_wall=20, gb_free=29.5, wall=7473
2024-02-09 23:26:56 | INFO | train_inner | epoch 062:    227 / 685 loss=2.406, nll_loss=0.744, ppl=1.67, wps=71791, ups=5.02, wpb=14303, bsz=758.8, num_updates=42000, lr=0.00025, gnorm=0.447, loss_scale=1, train_wall=20, gb_free=29.5, wall=7493
2024-02-09 23:27:16 | INFO | train_inner | epoch 062:    327 / 685 loss=2.417, nll_loss=0.757, ppl=1.69, wps=73158.3, ups=5.02, wpb=14563.2, bsz=752.6, num_updates=42100, lr=0.000247485, gnorm=0.448, loss_scale=2, train_wall=20, gb_free=29.6, wall=7513
2024-02-09 23:27:36 | INFO | train_inner | epoch 062:    427 / 685 loss=2.415, nll_loss=0.755, ppl=1.69, wps=73356.2, ups=5, wpb=14656.6, bsz=768.2, num_updates=42200, lr=0.000244979, gnorm=0.436, loss_scale=2, train_wall=20, gb_free=29.5, wall=7533
2024-02-09 23:27:56 | INFO | train_inner | epoch 062:    527 / 685 loss=2.415, nll_loss=0.756, ppl=1.69, wps=72593.8, ups=4.97, wpb=14592.7, bsz=747.4, num_updates=42300, lr=0.000242481, gnorm=0.454, loss_scale=2, train_wall=20, gb_free=29.5, wall=7553
2024-02-09 23:28:16 | INFO | train_inner | epoch 062:    627 / 685 loss=2.425, nll_loss=0.766, ppl=1.7, wps=72869.5, ups=5.03, wpb=14498, bsz=733, num_updates=42400, lr=0.000239992, gnorm=0.445, loss_scale=2, train_wall=20, gb_free=29.5, wall=7573
2024-02-09 23:28:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:28:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:28:30 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.394 | nll_loss 1.799 | ppl 3.48 | wps 159079 | wpb 3189.1 | bsz 163.4 | num_updates 42458 | best_loss 3.382
2024-02-09 23:28:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 42458 updates
2024-02-09 23:28:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt
2024-02-09 23:28:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt
2024-02-09 23:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt (epoch 62 @ 42458 updates, score 3.394) (writing took 2.4633426889777184 seconds)
2024-02-09 23:28:32 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-02-09 23:28:32 | INFO | train | epoch 062 | loss 2.414 | nll_loss 0.754 | ppl 1.69 | wps 70594.3 | ups 4.86 | wpb 14524.4 | bsz 748.5 | num_updates 42458 | lr 0.000238552 | gnorm 0.446 | loss_scale 2 | train_wall 135 | gb_free 29.6 | wall 7589
2024-02-09 23:28:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:28:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:28:32 | INFO | fairseq.trainer | begin training epoch 63
2024-02-09 23:28:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:28:40 | INFO | train_inner | epoch 063:     42 / 685 loss=2.408, nll_loss=0.746, ppl=1.68, wps=59081.9, ups=4.12, wpb=14323.9, bsz=725.8, num_updates=42500, lr=0.000237512, gnorm=0.451, loss_scale=2, train_wall=19, gb_free=29.6, wall=7597
2024-02-09 23:29:00 | INFO | train_inner | epoch 063:    142 / 685 loss=2.393, nll_loss=0.73, ppl=1.66, wps=73508, ups=5.02, wpb=14630, bsz=757.6, num_updates=42600, lr=0.00023504, gnorm=0.437, loss_scale=2, train_wall=20, gb_free=29.5, wall=7617
2024-02-09 23:29:20 | INFO | train_inner | epoch 063:    242 / 685 loss=2.403, nll_loss=0.741, ppl=1.67, wps=72186.9, ups=4.99, wpb=14464, bsz=730.3, num_updates=42700, lr=0.000232578, gnorm=0.442, loss_scale=2, train_wall=20, gb_free=29.5, wall=7637
2024-02-09 23:29:40 | INFO | train_inner | epoch 063:    342 / 685 loss=2.41, nll_loss=0.75, ppl=1.68, wps=72460.9, ups=4.99, wpb=14517.3, bsz=736.3, num_updates=42800, lr=0.000230125, gnorm=0.443, loss_scale=2, train_wall=20, gb_free=29.5, wall=7657
2024-02-09 23:30:00 | INFO | train_inner | epoch 063:    442 / 685 loss=2.406, nll_loss=0.745, ppl=1.68, wps=72604.3, ups=5.06, wpb=14355.1, bsz=763.4, num_updates=42900, lr=0.00022768, gnorm=0.44, loss_scale=2, train_wall=20, gb_free=29.5, wall=7677
2024-02-09 23:30:20 | INFO | train_inner | epoch 063:    542 / 685 loss=2.405, nll_loss=0.745, ppl=1.68, wps=73047.3, ups=5.01, wpb=14586.7, bsz=766.1, num_updates=43000, lr=0.000225246, gnorm=0.437, loss_scale=2, train_wall=20, gb_free=29.5, wall=7697
2024-02-09 23:30:40 | INFO | train_inner | epoch 063:    642 / 685 loss=2.415, nll_loss=0.756, ppl=1.69, wps=73867.7, ups=5.03, wpb=14693.5, bsz=731.2, num_updates=43100, lr=0.00022282, gnorm=0.438, loss_scale=2, train_wall=20, gb_free=29.5, wall=7717
2024-02-09 23:30:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:30:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:30:51 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.399 | nll_loss 1.804 | ppl 3.49 | wps 157104 | wpb 3189.1 | bsz 163.4 | num_updates 43143 | best_loss 3.382
2024-02-09 23:30:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 43143 updates
2024-02-09 23:30:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt
2024-02-09 23:30:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt
2024-02-09 23:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt (epoch 63 @ 43143 updates, score 3.399) (writing took 2.4207217190414667 seconds)
2024-02-09 23:30:53 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-02-09 23:30:53 | INFO | train | epoch 063 | loss 2.405 | nll_loss 0.744 | ppl 1.68 | wps 70594.3 | ups 4.86 | wpb 14524.4 | bsz 748.5 | num_updates 43143 | lr 0.00022178 | gnorm 0.44 | loss_scale 2 | train_wall 135 | gb_free 29.7 | wall 7730
2024-02-09 23:30:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:30:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:30:53 | INFO | fairseq.trainer | begin training epoch 64
2024-02-09 23:30:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:31:04 | INFO | train_inner | epoch 064:     57 / 685 loss=2.402, nll_loss=0.741, ppl=1.67, wps=59070.7, ups=4.12, wpb=14332.2, bsz=751.2, num_updates=43200, lr=0.000220404, gnorm=0.443, loss_scale=2, train_wall=20, gb_free=29.6, wall=7741
2024-02-09 23:31:24 | INFO | train_inner | epoch 064:    157 / 685 loss=2.387, nll_loss=0.724, ppl=1.65, wps=72630.9, ups=5.02, wpb=14464.7, bsz=745.6, num_updates=43300, lr=0.000217997, gnorm=0.444, loss_scale=2, train_wall=20, gb_free=29.5, wall=7761
2024-02-09 23:31:44 | INFO | train_inner | epoch 064:    257 / 685 loss=2.39, nll_loss=0.728, ppl=1.66, wps=72790.3, ups=4.98, wpb=14607.3, bsz=786.7, num_updates=43400, lr=0.000215599, gnorm=0.429, loss_scale=2, train_wall=20, gb_free=29.5, wall=7781
2024-02-09 23:32:21 | INFO | train_inner | epoch 064:    357 / 685 loss=2.396, nll_loss=0.734, ppl=1.66, wps=40664.2, ups=2.77, wpb=14661, bsz=741.9, num_updates=43500, lr=0.000213212, gnorm=0.437, loss_scale=2, train_wall=21, gb_free=29.5, wall=7817
2024-02-09 23:32:41 | INFO | train_inner | epoch 064:    457 / 685 loss=2.395, nll_loss=0.734, ppl=1.66, wps=71797.3, ups=4.96, wpb=14475.7, bsz=745.6, num_updates=43600, lr=0.000210834, gnorm=0.434, loss_scale=2, train_wall=20, gb_free=29.5, wall=7837
2024-02-09 23:33:02 | INFO | train_inner | epoch 064:    557 / 685 loss=2.407, nll_loss=0.747, ppl=1.68, wps=69344, ups=4.76, wpb=14568, bsz=740.4, num_updates=43700, lr=0.000208466, gnorm=0.441, loss_scale=2, train_wall=20, gb_free=29.5, wall=7858
2024-02-09 23:33:22 | INFO | train_inner | epoch 064:    657 / 685 loss=2.404, nll_loss=0.744, ppl=1.68, wps=73298.6, ups=5.01, wpb=14623.4, bsz=743, num_updates=43800, lr=0.000206107, gnorm=0.436, loss_scale=2, train_wall=20, gb_free=29.6, wall=7878
2024-02-09 23:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:33:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:33:29 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.396 | nll_loss 1.805 | ppl 3.49 | wps 159772 | wpb 3189.1 | bsz 163.4 | num_updates 43828 | best_loss 3.382
2024-02-09 23:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 43828 updates
2024-02-09 23:33:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt
2024-02-09 23:33:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt
2024-02-09 23:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt (epoch 64 @ 43828 updates, score 3.396) (writing took 2.625160293886438 seconds)
2024-02-09 23:33:32 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-02-09 23:33:32 | INFO | train | epoch 064 | loss 2.396 | nll_loss 0.735 | ppl 1.66 | wps 62601.3 | ups 4.31 | wpb 14524.4 | bsz 748.5 | num_updates 43828 | lr 0.000205449 | gnorm 0.438 | loss_scale 2 | train_wall 137 | gb_free 29.7 | wall 7889
2024-02-09 23:33:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:33:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:33:32 | INFO | fairseq.trainer | begin training epoch 65
2024-02-09 23:33:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:33:46 | INFO | train_inner | epoch 065:     72 / 685 loss=2.385, nll_loss=0.721, ppl=1.65, wps=57878, ups=4.05, wpb=14298.9, bsz=746.4, num_updates=43900, lr=0.000203759, gnorm=0.436, loss_scale=2, train_wall=19, gb_free=29.5, wall=7903
2024-02-09 23:34:06 | INFO | train_inner | epoch 065:    172 / 685 loss=2.379, nll_loss=0.715, ppl=1.64, wps=73537.9, ups=5.05, wpb=14568.6, bsz=755.4, num_updates=44000, lr=0.000201421, gnorm=0.429, loss_scale=2, train_wall=20, gb_free=29.5, wall=7923
2024-02-09 23:34:26 | INFO | train_inner | epoch 065:    272 / 685 loss=2.386, nll_loss=0.724, ppl=1.65, wps=73282.3, ups=5.02, wpb=14591.4, bsz=757.5, num_updates=44100, lr=0.000199092, gnorm=0.428, loss_scale=2, train_wall=20, gb_free=29.6, wall=7943
2024-02-09 23:34:46 | INFO | train_inner | epoch 065:    372 / 685 loss=2.391, nll_loss=0.729, ppl=1.66, wps=73131.5, ups=5.02, wpb=14570.8, bsz=730.2, num_updates=44200, lr=0.000196774, gnorm=0.432, loss_scale=2, train_wall=20, gb_free=29.5, wall=7963
2024-02-09 23:35:08 | INFO | train_inner | epoch 065:    472 / 685 loss=2.388, nll_loss=0.727, ppl=1.66, wps=65242.9, ups=4.47, wpb=14597.5, bsz=768.6, num_updates=44300, lr=0.000194467, gnorm=0.428, loss_scale=2, train_wall=20, gb_free=29.5, wall=7985
2024-02-09 23:35:28 | INFO | train_inner | epoch 065:    572 / 685 loss=2.394, nll_loss=0.733, ppl=1.66, wps=73294.7, ups=5.04, wpb=14532.5, bsz=750.6, num_updates=44400, lr=0.000192169, gnorm=0.431, loss_scale=2, train_wall=20, gb_free=29.5, wall=8005
2024-02-09 23:35:48 | INFO | train_inner | epoch 065:    672 / 685 loss=2.397, nll_loss=0.737, ppl=1.67, wps=72435.2, ups=5, wpb=14489, bsz=731.5, num_updates=44500, lr=0.000189882, gnorm=0.437, loss_scale=2, train_wall=20, gb_free=29.5, wall=8025
2024-02-09 23:35:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:35:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:35:53 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.414 | nll_loss 1.823 | ppl 3.54 | wps 157993 | wpb 3189.1 | bsz 163.4 | num_updates 44513 | best_loss 3.382
2024-02-09 23:35:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 44513 updates
2024-02-09 23:35:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt
2024-02-09 23:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt
2024-02-09 23:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt (epoch 65 @ 44513 updates, score 3.414) (writing took 2.538108886918053 seconds)
2024-02-09 23:35:55 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-02-09 23:35:55 | INFO | train | epoch 065 | loss 2.388 | nll_loss 0.726 | ppl 1.65 | wps 69406.1 | ups 4.78 | wpb 14524.4 | bsz 748.5 | num_updates 44513 | lr 0.000189586 | gnorm 0.481 | loss_scale 2 | train_wall 135 | gb_free 29.7 | wall 8032
2024-02-09 23:35:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:35:55 | INFO | fairseq.trainer | begin training epoch 66
2024-02-09 23:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:36:13 | INFO | train_inner | epoch 066:     87 / 685 loss=2.376, nll_loss=0.712, ppl=1.64, wps=58399.2, ups=4.09, wpb=14275.2, bsz=720.8, num_updates=44600, lr=0.000187606, gnorm=0.771, loss_scale=2, train_wall=20, gb_free=29.5, wall=8049
2024-02-09 23:36:33 | INFO | train_inner | epoch 066:    187 / 685 loss=2.374, nll_loss=0.711, ppl=1.64, wps=72478.6, ups=4.96, wpb=14616.7, bsz=785.3, num_updates=44700, lr=0.00018534, gnorm=0.42, loss_scale=2, train_wall=20, gb_free=29.5, wall=8070
2024-02-09 23:36:53 | INFO | train_inner | epoch 066:    287 / 685 loss=2.377, nll_loss=0.713, ppl=1.64, wps=72987.3, ups=4.98, wpb=14651, bsz=748.6, num_updates=44800, lr=0.000183085, gnorm=0.421, loss_scale=2, train_wall=20, gb_free=29.5, wall=8090
2024-02-09 23:37:13 | INFO | train_inner | epoch 066:    387 / 685 loss=2.381, nll_loss=0.718, ppl=1.64, wps=71729.8, ups=5, wpb=14334.5, bsz=739.1, num_updates=44900, lr=0.00018084, gnorm=0.433, loss_scale=2, train_wall=20, gb_free=29.6, wall=8110
2024-02-09 23:37:33 | INFO | train_inner | epoch 066:    487 / 685 loss=2.382, nll_loss=0.721, ppl=1.65, wps=72468.5, ups=4.93, wpb=14702.6, bsz=755.8, num_updates=45000, lr=0.000178606, gnorm=0.42, loss_scale=2, train_wall=20, gb_free=29.5, wall=8130
2024-02-09 23:37:53 | INFO | train_inner | epoch 066:    587 / 685 loss=2.384, nll_loss=0.723, ppl=1.65, wps=71905.3, ups=4.95, wpb=14520.6, bsz=734.8, num_updates=45100, lr=0.000176383, gnorm=0.44, loss_scale=2, train_wall=20, gb_free=29.5, wall=8150
2024-02-09 23:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:38:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:38:15 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.403 | nll_loss 1.809 | ppl 3.51 | wps 159466 | wpb 3189.1 | bsz 163.4 | num_updates 45198 | best_loss 3.382
2024-02-09 23:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 45198 updates
2024-02-09 23:38:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt
2024-02-09 23:38:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt
2024-02-09 23:38:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt (epoch 66 @ 45198 updates, score 3.403) (writing took 2.5145783729385585 seconds)
2024-02-09 23:38:17 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-02-09 23:38:17 | INFO | train | epoch 066 | loss 2.381 | nll_loss 0.718 | ppl 1.65 | wps 70010.4 | ups 4.82 | wpb 14524.4 | bsz 748.5 | num_updates 45198 | lr 0.000174216 | gnorm 0.427 | loss_scale 2 | train_wall 136 | gb_free 29.7 | wall 8174
2024-02-09 23:38:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:38:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:38:17 | INFO | fairseq.trainer | begin training epoch 67
2024-02-09 23:38:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:38:18 | INFO | train_inner | epoch 067:      2 / 685 loss=2.392, nll_loss=0.731, ppl=1.66, wps=59181.3, ups=4.1, wpb=14441.4, bsz=753, num_updates=45200, lr=0.000174171, gnorm=0.427, loss_scale=2, train_wall=20, gb_free=29.5, wall=8175
2024-02-09 23:38:38 | INFO | train_inner | epoch 067:    102 / 685 loss=2.367, nll_loss=0.702, ppl=1.63, wps=72308.2, ups=4.98, wpb=14525.5, bsz=726.5, num_updates=45300, lr=0.00017197, gnorm=0.417, loss_scale=2, train_wall=20, gb_free=29.5, wall=8195
2024-02-09 23:38:58 | INFO | train_inner | epoch 067:    202 / 685 loss=2.371, nll_loss=0.707, ppl=1.63, wps=72518.3, ups=4.98, wpb=14572.4, bsz=752.2, num_updates=45400, lr=0.000169781, gnorm=0.419, loss_scale=2, train_wall=20, gb_free=29.6, wall=8215
2024-02-09 23:39:18 | INFO | train_inner | epoch 067:    302 / 685 loss=2.373, nll_loss=0.71, ppl=1.64, wps=72107.9, ups=4.96, wpb=14535.1, bsz=766.9, num_updates=45500, lr=0.000167602, gnorm=0.419, loss_scale=2, train_wall=20, gb_free=29.5, wall=8235
2024-02-09 23:39:38 | INFO | train_inner | epoch 067:    402 / 685 loss=2.373, nll_loss=0.711, ppl=1.64, wps=72216.6, ups=4.95, wpb=14585.7, bsz=760.8, num_updates=45600, lr=0.000165435, gnorm=0.423, loss_scale=2, train_wall=20, gb_free=29.5, wall=8255
2024-02-09 23:39:58 | INFO | train_inner | epoch 067:    502 / 685 loss=2.374, nll_loss=0.711, ppl=1.64, wps=72189.3, ups=5, wpb=14449.1, bsz=731.9, num_updates=45700, lr=0.000163279, gnorm=0.418, loss_scale=2, train_wall=20, gb_free=29.5, wall=8275
2024-02-09 23:40:19 | INFO | train_inner | epoch 067:    602 / 685 loss=2.372, nll_loss=0.709, ppl=1.64, wps=72160.7, ups=4.96, wpb=14541.4, bsz=755.5, num_updates=45800, lr=0.000161134, gnorm=0.414, loss_scale=2, train_wall=20, gb_free=29.5, wall=8295
2024-02-09 23:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:40:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:40:37 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.399 | nll_loss 1.804 | ppl 3.49 | wps 151274 | wpb 3189.1 | bsz 163.4 | num_updates 45883 | best_loss 3.382
2024-02-09 23:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 45883 updates
2024-02-09 23:40:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt
2024-02-09 23:40:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt
2024-02-09 23:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt (epoch 67 @ 45883 updates, score 3.399) (writing took 2.4583710129372776 seconds)
2024-02-09 23:40:39 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-02-09 23:40:39 | INFO | train | epoch 067 | loss 2.373 | nll_loss 0.71 | ppl 1.64 | wps 69980.9 | ups 4.82 | wpb 14524.4 | bsz 748.5 | num_updates 45883 | lr 0.000159363 | gnorm 0.419 | loss_scale 2 | train_wall 136 | gb_free 29.6 | wall 8316
2024-02-09 23:40:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:40:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:40:40 | INFO | fairseq.trainer | begin training epoch 68
2024-02-09 23:40:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:40:43 | INFO | train_inner | epoch 068:     17 / 685 loss=2.378, nll_loss=0.716, ppl=1.64, wps=59090.3, ups=4.08, wpb=14473.8, bsz=737.6, num_updates=45900, lr=0.000159001, gnorm=0.42, loss_scale=2, train_wall=20, gb_free=29.5, wall=8320
2024-02-09 23:41:03 | INFO | train_inner | epoch 068:    117 / 685 loss=2.356, nll_loss=0.691, ppl=1.61, wps=72444.7, ups=5, wpb=14486.7, bsz=747.8, num_updates=46000, lr=0.000156879, gnorm=0.419, loss_scale=2, train_wall=20, gb_free=29.5, wall=8340
2024-02-09 23:41:23 | INFO | train_inner | epoch 068:    217 / 685 loss=2.365, nll_loss=0.701, ppl=1.63, wps=72563.6, ups=4.96, wpb=14626.9, bsz=750.1, num_updates=46100, lr=0.000154769, gnorm=0.412, loss_scale=2, train_wall=20, gb_free=29.5, wall=8360
2024-02-09 23:41:43 | INFO | train_inner | epoch 068:    317 / 685 loss=2.36, nll_loss=0.696, ppl=1.62, wps=71979.8, ups=4.96, wpb=14518.5, bsz=771.4, num_updates=46200, lr=0.000152671, gnorm=0.413, loss_scale=4, train_wall=20, gb_free=29.5, wall=8380
2024-02-09 23:42:03 | INFO | train_inner | epoch 068:    417 / 685 loss=2.369, nll_loss=0.706, ppl=1.63, wps=72358.5, ups=4.98, wpb=14518, bsz=763.3, num_updates=46300, lr=0.000150584, gnorm=0.413, loss_scale=4, train_wall=20, gb_free=29.5, wall=8400
2024-02-09 23:42:24 | INFO | train_inner | epoch 068:    517 / 685 loss=2.372, nll_loss=0.71, ppl=1.64, wps=72076.7, ups=4.98, wpb=14482.7, bsz=739.6, num_updates=46400, lr=0.000148509, gnorm=0.417, loss_scale=4, train_wall=20, gb_free=29.6, wall=8420
2024-02-09 23:42:44 | INFO | train_inner | epoch 068:    617 / 685 loss=2.373, nll_loss=0.711, ppl=1.64, wps=72618.4, ups=4.96, wpb=14639.5, bsz=739.1, num_updates=46500, lr=0.000146447, gnorm=0.53, loss_scale=4, train_wall=20, gb_free=29.5, wall=8440
2024-02-09 23:42:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:42:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:42:59 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.403 | nll_loss 1.809 | ppl 3.5 | wps 148818 | wpb 3189.1 | bsz 163.4 | num_updates 46568 | best_loss 3.382
2024-02-09 23:42:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 46568 updates
2024-02-09 23:42:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt
2024-02-09 23:43:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt
2024-02-09 23:43:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt (epoch 68 @ 46568 updates, score 3.403) (writing took 2.4534642309881747 seconds)
2024-02-09 23:43:02 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-02-09 23:43:02 | INFO | train | epoch 068 | loss 2.366 | nll_loss 0.702 | ppl 1.63 | wps 69959.5 | ups 4.82 | wpb 14524.4 | bsz 748.5 | num_updates 46568 | lr 0.000145051 | gnorm 0.432 | loss_scale 4 | train_wall 136 | gb_free 29.6 | wall 8458
2024-02-09 23:43:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:43:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:43:02 | INFO | fairseq.trainer | begin training epoch 69
2024-02-09 23:43:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:43:08 | INFO | train_inner | epoch 069:     32 / 685 loss=2.359, nll_loss=0.695, ppl=1.62, wps=58435, ups=4.06, wpb=14376, bsz=735.1, num_updates=46600, lr=0.000144396, gnorm=0.415, loss_scale=4, train_wall=20, gb_free=29.5, wall=8465
2024-02-09 23:43:29 | INFO | train_inner | epoch 069:    132 / 685 loss=2.351, nll_loss=0.685, ppl=1.61, wps=71684.8, ups=4.94, wpb=14518, bsz=745.2, num_updates=46700, lr=0.000142357, gnorm=0.411, loss_scale=4, train_wall=20, gb_free=29.5, wall=8485
2024-02-09 23:43:49 | INFO | train_inner | epoch 069:    232 / 685 loss=2.355, nll_loss=0.69, ppl=1.61, wps=72870, ups=4.98, wpb=14620.9, bsz=757.8, num_updates=46800, lr=0.00014033, gnorm=0.411, loss_scale=4, train_wall=20, gb_free=29.5, wall=8505
2024-02-09 23:44:08 | INFO | train_inner | epoch 069:    332 / 685 loss=2.355, nll_loss=0.691, ppl=1.61, wps=73900.9, ups=5.1, wpb=14489.5, bsz=755.8, num_updates=46900, lr=0.000138316, gnorm=0.411, loss_scale=4, train_wall=19, gb_free=29.5, wall=8525
2024-02-09 23:44:28 | INFO | train_inner | epoch 069:    432 / 685 loss=2.361, nll_loss=0.698, ppl=1.62, wps=73792, ups=5.06, wpb=14585.3, bsz=755.4, num_updates=47000, lr=0.000136313, gnorm=0.409, loss_scale=4, train_wall=20, gb_free=29.5, wall=8545
2024-02-09 23:44:48 | INFO | train_inner | epoch 069:    532 / 685 loss=2.365, nll_loss=0.702, ppl=1.63, wps=73657.5, ups=5.04, wpb=14602.2, bsz=756.2, num_updates=47100, lr=0.000134323, gnorm=0.415, loss_scale=4, train_wall=20, gb_free=29.5, wall=8564
2024-02-09 23:45:08 | INFO | train_inner | epoch 069:    632 / 685 loss=2.37, nll_loss=0.709, ppl=1.63, wps=73618.3, ups=5.04, wpb=14616.4, bsz=732.1, num_updates=47200, lr=0.000132345, gnorm=0.413, loss_scale=4, train_wall=20, gb_free=29.5, wall=8584
2024-02-09 23:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:45:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:45:20 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.398 | nll_loss 1.807 | ppl 3.5 | wps 160202 | wpb 3189.1 | bsz 163.4 | num_updates 47253 | best_loss 3.382
2024-02-09 23:45:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 47253 updates
2024-02-09 23:45:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt
2024-02-09 23:45:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt
2024-02-09 23:45:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt (epoch 69 @ 47253 updates, score 3.398) (writing took 2.5697958839591593 seconds)
2024-02-09 23:45:22 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-02-09 23:45:22 | INFO | train | epoch 069 | loss 2.359 | nll_loss 0.695 | ppl 1.62 | wps 70694.3 | ups 4.87 | wpb 14524.4 | bsz 748.5 | num_updates 47253 | lr 0.000131302 | gnorm 0.412 | loss_scale 4 | train_wall 134 | gb_free 29.6 | wall 8599
2024-02-09 23:45:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:45:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:45:22 | INFO | fairseq.trainer | begin training epoch 70
2024-02-09 23:45:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:45:32 | INFO | train_inner | epoch 070:     47 / 685 loss=2.353, nll_loss=0.688, ppl=1.61, wps=58841.2, ups=4.13, wpb=14261.1, bsz=750, num_updates=47300, lr=0.00013038, gnorm=0.42, loss_scale=4, train_wall=19, gb_free=29.5, wall=8609
2024-02-09 23:45:52 | INFO | train_inner | epoch 070:    147 / 685 loss=2.348, nll_loss=0.682, ppl=1.6, wps=73183.8, ups=5.04, wpb=14514.3, bsz=754, num_updates=47400, lr=0.000128428, gnorm=0.407, loss_scale=4, train_wall=20, gb_free=29.5, wall=8628
2024-02-09 23:46:11 | INFO | train_inner | epoch 070:    247 / 685 loss=2.354, nll_loss=0.689, ppl=1.61, wps=73192.2, ups=5.07, wpb=14426.9, bsz=727.7, num_updates=47500, lr=0.000126487, gnorm=0.412, loss_scale=4, train_wall=19, gb_free=29.5, wall=8648
2024-02-09 23:46:31 | INFO | train_inner | epoch 070:    347 / 685 loss=2.35, nll_loss=0.685, ppl=1.61, wps=73783.2, ups=5.07, wpb=14545.8, bsz=750.6, num_updates=47600, lr=0.00012456, gnorm=0.409, loss_scale=4, train_wall=19, gb_free=29.5, wall=8668
2024-02-09 23:46:51 | INFO | train_inner | epoch 070:    447 / 685 loss=2.35, nll_loss=0.686, ppl=1.61, wps=73429.4, ups=5.03, wpb=14590.7, bsz=764.7, num_updates=47700, lr=0.000122645, gnorm=0.407, loss_scale=4, train_wall=20, gb_free=29.6, wall=8688
2024-02-09 23:47:11 | INFO | train_inner | epoch 070:    547 / 685 loss=2.356, nll_loss=0.693, ppl=1.62, wps=72546.5, ups=4.99, wpb=14530.4, bsz=740.6, num_updates=47800, lr=0.000120743, gnorm=0.415, loss_scale=4, train_wall=20, gb_free=29.5, wall=8708
2024-02-09 23:47:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-09 23:47:31 | INFO | train_inner | epoch 070:    648 / 685 loss=2.359, nll_loss=0.695, ppl=1.62, wps=72437, ups=4.92, wpb=14712.9, bsz=744.9, num_updates=47900, lr=0.000118854, gnorm=0.412, loss_scale=2, train_wall=20, gb_free=29.5, wall=8728
2024-02-09 23:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:47:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:47:41 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.396 | nll_loss 1.803 | ppl 3.49 | wps 159004 | wpb 3189.1 | bsz 163.4 | num_updates 47937 | best_loss 3.382
2024-02-09 23:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 47937 updates
2024-02-09 23:47:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt
2024-02-09 23:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt
2024-02-09 23:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt (epoch 70 @ 47937 updates, score 3.396) (writing took 2.4916271318215877 seconds)
2024-02-09 23:47:43 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-02-09 23:47:43 | INFO | train | epoch 070 | loss 2.353 | nll_loss 0.688 | ppl 1.61 | wps 70650.4 | ups 4.86 | wpb 14524.2 | bsz 748.5 | num_updates 47937 | lr 0.000118158 | gnorm 0.412 | loss_scale 2 | train_wall 134 | gb_free 29.6 | wall 8740
2024-02-09 23:47:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:47:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:47:43 | INFO | fairseq.trainer | begin training epoch 71
2024-02-09 23:47:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:47:56 | INFO | train_inner | epoch 071:     63 / 685 loss=2.346, nll_loss=0.681, ppl=1.6, wps=58691.2, ups=4.09, wpb=14343.9, bsz=744.6, num_updates=48000, lr=0.000116978, gnorm=0.41, loss_scale=2, train_wall=20, gb_free=29.6, wall=8753
2024-02-09 23:48:16 | INFO | train_inner | epoch 071:    163 / 685 loss=2.341, nll_loss=0.674, ppl=1.6, wps=72403.8, ups=4.97, wpb=14576.5, bsz=748.6, num_updates=48100, lr=0.000115114, gnorm=0.42, loss_scale=2, train_wall=20, gb_free=29.6, wall=8773
2024-02-09 23:48:36 | INFO | train_inner | epoch 071:    263 / 685 loss=2.349, nll_loss=0.684, ppl=1.61, wps=72091.1, ups=4.99, wpb=14461.4, bsz=748.6, num_updates=48200, lr=0.000113264, gnorm=0.407, loss_scale=2, train_wall=20, gb_free=29.5, wall=8793
2024-02-09 23:48:56 | INFO | train_inner | epoch 071:    363 / 685 loss=2.347, nll_loss=0.682, ppl=1.6, wps=73448.8, ups=5, wpb=14693.6, bsz=737.8, num_updates=48300, lr=0.000111427, gnorm=0.406, loss_scale=2, train_wall=20, gb_free=29.5, wall=8813
2024-02-09 23:49:16 | INFO | train_inner | epoch 071:    463 / 685 loss=2.349, nll_loss=0.684, ppl=1.61, wps=72485.9, ups=4.98, wpb=14547.6, bsz=757, num_updates=48400, lr=0.000109603, gnorm=0.408, loss_scale=2, train_wall=20, gb_free=29.5, wall=8833
2024-02-09 23:49:36 | INFO | train_inner | epoch 071:    563 / 685 loss=2.342, nll_loss=0.676, ppl=1.6, wps=72449, ups=5.04, wpb=14362.9, bsz=759.8, num_updates=48500, lr=0.000107792, gnorm=0.407, loss_scale=2, train_wall=20, gb_free=29.5, wall=8853
2024-02-09 23:49:56 | INFO | train_inner | epoch 071:    663 / 685 loss=2.354, nll_loss=0.69, ppl=1.61, wps=73577.5, ups=5.05, wpb=14556.3, bsz=739.7, num_updates=48600, lr=0.000105995, gnorm=0.452, loss_scale=2, train_wall=20, gb_free=29.5, wall=8872
2024-02-09 23:50:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:50:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:50:02 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.404 | nll_loss 1.814 | ppl 3.52 | wps 160486 | wpb 3189.1 | bsz 163.4 | num_updates 48622 | best_loss 3.382
2024-02-09 23:50:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 48622 updates
2024-02-09 23:50:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt
2024-02-09 23:50:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt
2024-02-09 23:50:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt (epoch 71 @ 48622 updates, score 3.404) (writing took 2.4962090740446 seconds)
2024-02-09 23:50:04 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-02-09 23:50:04 | INFO | train | epoch 071 | loss 2.346 | nll_loss 0.681 | ppl 1.6 | wps 70442.6 | ups 4.85 | wpb 14524.4 | bsz 748.5 | num_updates 48622 | lr 0.000105601 | gnorm 0.415 | loss_scale 2 | train_wall 135 | gb_free 29.6 | wall 8881
2024-02-09 23:50:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:50:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:50:04 | INFO | fairseq.trainer | begin training epoch 72
2024-02-09 23:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:50:20 | INFO | train_inner | epoch 072:     78 / 685 loss=2.334, nll_loss=0.668, ppl=1.59, wps=60309.8, ups=4.13, wpb=14605.2, bsz=760.5, num_updates=48700, lr=0.00010421, gnorm=0.396, loss_scale=2, train_wall=19, gb_free=29.5, wall=8897
2024-02-09 23:50:40 | INFO | train_inner | epoch 072:    178 / 685 loss=2.338, nll_loss=0.672, ppl=1.59, wps=73154.4, ups=5.07, wpb=14433.5, bsz=736.3, num_updates=48800, lr=0.00010244, gnorm=0.415, loss_scale=2, train_wall=19, gb_free=29.5, wall=8916
2024-02-09 23:51:00 | INFO | train_inner | epoch 072:    278 / 685 loss=2.337, nll_loss=0.671, ppl=1.59, wps=73302, ups=5.03, wpb=14564.9, bsz=771, num_updates=48900, lr=0.000100682, gnorm=0.396, loss_scale=2, train_wall=20, gb_free=29.5, wall=8936
2024-02-09 23:51:19 | INFO | train_inner | epoch 072:    378 / 685 loss=2.343, nll_loss=0.677, ppl=1.6, wps=73163.5, ups=5.05, wpb=14477.5, bsz=745.2, num_updates=49000, lr=9.89384e-05, gnorm=0.405, loss_scale=2, train_wall=20, gb_free=29.5, wall=8956
2024-02-09 23:51:39 | INFO | train_inner | epoch 072:    478 / 685 loss=2.348, nll_loss=0.684, ppl=1.61, wps=74098, ups=5.07, wpb=14606.7, bsz=717.4, num_updates=49100, lr=9.72081e-05, gnorm=0.402, loss_scale=2, train_wall=19, gb_free=29.6, wall=8976
2024-02-09 23:51:59 | INFO | train_inner | epoch 072:    578 / 685 loss=2.34, nll_loss=0.674, ppl=1.6, wps=72271.1, ups=5.05, wpb=14322.5, bsz=759.4, num_updates=49200, lr=9.54915e-05, gnorm=0.401, loss_scale=2, train_wall=20, gb_free=29.5, wall=8996
2024-02-09 23:52:19 | INFO | train_inner | epoch 072:    678 / 685 loss=2.35, nll_loss=0.686, ppl=1.61, wps=74142.6, ups=5.06, wpb=14664, bsz=747.1, num_updates=49300, lr=9.37886e-05, gnorm=0.408, loss_scale=2, train_wall=20, gb_free=29.5, wall=9015
2024-02-09 23:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:52:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:52:22 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.4 | nll_loss 1.807 | ppl 3.5 | wps 159562 | wpb 3189.1 | bsz 163.4 | num_updates 49307 | best_loss 3.382
2024-02-09 23:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 49307 updates
2024-02-09 23:52:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt
2024-02-09 23:52:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt
2024-02-09 23:52:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt (epoch 72 @ 49307 updates, score 3.4) (writing took 2.4643551718909293 seconds)
2024-02-09 23:52:24 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-02-09 23:52:24 | INFO | train | epoch 072 | loss 2.341 | nll_loss 0.676 | ppl 1.6 | wps 71095.7 | ups 4.89 | wpb 14524.4 | bsz 748.5 | num_updates 49307 | lr 9.36699e-05 | gnorm 0.403 | loss_scale 2 | train_wall 134 | gb_free 29.6 | wall 9021
2024-02-09 23:52:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:52:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:52:24 | INFO | fairseq.trainer | begin training epoch 73
2024-02-09 23:52:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:52:43 | INFO | train_inner | epoch 073:     93 / 685 loss=2.331, nll_loss=0.665, ppl=1.59, wps=58555.3, ups=4.09, wpb=14319.5, bsz=739.4, num_updates=49400, lr=9.20994e-05, gnorm=0.398, loss_scale=2, train_wall=20, gb_free=29.6, wall=9040
2024-02-09 23:53:03 | INFO | train_inner | epoch 073:    193 / 685 loss=2.331, nll_loss=0.664, ppl=1.58, wps=72488.7, ups=4.99, wpb=14540.3, bsz=755.4, num_updates=49500, lr=9.0424e-05, gnorm=0.396, loss_scale=2, train_wall=20, gb_free=29.5, wall=9060
2024-02-09 23:53:23 | INFO | train_inner | epoch 073:    293 / 685 loss=2.336, nll_loss=0.671, ppl=1.59, wps=72718.3, ups=5, wpb=14549.5, bsz=741.6, num_updates=49600, lr=8.87625e-05, gnorm=0.41, loss_scale=2, train_wall=20, gb_free=29.5, wall=9080
2024-02-09 23:53:43 | INFO | train_inner | epoch 073:    393 / 685 loss=2.331, nll_loss=0.664, ppl=1.58, wps=72700.5, ups=4.97, wpb=14619.7, bsz=762.5, num_updates=49700, lr=8.71148e-05, gnorm=0.395, loss_scale=2, train_wall=20, gb_free=29.5, wall=9100
2024-02-09 23:54:03 | INFO | train_inner | epoch 073:    493 / 685 loss=2.342, nll_loss=0.677, ppl=1.6, wps=73110.5, ups=4.99, wpb=14653.4, bsz=760.3, num_updates=49800, lr=8.54812e-05, gnorm=0.404, loss_scale=2, train_wall=20, gb_free=29.5, wall=9120
2024-02-09 23:54:23 | INFO | train_inner | epoch 073:    593 / 685 loss=2.338, nll_loss=0.673, ppl=1.59, wps=72430.9, ups=5, wpb=14498.6, bsz=749.3, num_updates=49900, lr=8.38616e-05, gnorm=0.401, loss_scale=2, train_wall=20, gb_free=29.5, wall=9140
2024-02-09 23:54:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:54:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:54:43 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.409 | nll_loss 1.82 | ppl 3.53 | wps 158754 | wpb 3189.1 | bsz 163.4 | num_updates 49992 | best_loss 3.382
2024-02-09 23:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 49992 updates
2024-02-09 23:54:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt
2024-02-09 23:54:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt
2024-02-09 23:54:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt (epoch 73 @ 49992 updates, score 3.409) (writing took 2.631120552076027 seconds)
2024-02-09 23:54:46 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-02-09 23:54:46 | INFO | train | epoch 073 | loss 2.336 | nll_loss 0.67 | ppl 1.59 | wps 70142.4 | ups 4.83 | wpb 14524.4 | bsz 748.5 | num_updates 49992 | lr 8.2384e-05 | gnorm 0.401 | loss_scale 2 | train_wall 135 | gb_free 29.6 | wall 9163
2024-02-09 23:54:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:54:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:54:46 | INFO | fairseq.trainer | begin training epoch 74
2024-02-09 23:54:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:54:48 | INFO | train_inner | epoch 074:      8 / 685 loss=2.339, nll_loss=0.674, ppl=1.6, wps=58906.9, ups=4.08, wpb=14425, bsz=729, num_updates=50000, lr=8.22561e-05, gnorm=0.407, loss_scale=2, train_wall=20, gb_free=29.5, wall=9164
2024-02-09 23:55:08 | INFO | train_inner | epoch 074:    108 / 685 loss=2.328, nll_loss=0.661, ppl=1.58, wps=73446.7, ups=5.04, wpb=14571.9, bsz=754.8, num_updates=50100, lr=8.06647e-05, gnorm=0.394, loss_scale=2, train_wall=20, gb_free=29.5, wall=9184
2024-02-09 23:55:28 | INFO | train_inner | epoch 074:    208 / 685 loss=2.33, nll_loss=0.663, ppl=1.58, wps=72651.2, ups=5, wpb=14519.5, bsz=747.8, num_updates=50200, lr=7.90875e-05, gnorm=0.395, loss_scale=2, train_wall=20, gb_free=29.5, wall=9204
2024-02-09 23:55:47 | INFO | train_inner | epoch 074:    308 / 685 loss=2.34, nll_loss=0.675, ppl=1.6, wps=73756.6, ups=5.03, wpb=14656, bsz=729.4, num_updates=50300, lr=7.75246e-05, gnorm=0.395, loss_scale=2, train_wall=20, gb_free=29.5, wall=9224
2024-02-09 23:56:07 | INFO | train_inner | epoch 074:    408 / 685 loss=2.33, nll_loss=0.665, ppl=1.59, wps=73065, ups=5.01, wpb=14577.4, bsz=764.2, num_updates=50400, lr=7.5976e-05, gnorm=0.397, loss_scale=2, train_wall=20, gb_free=29.5, wall=9244
2024-02-09 23:56:27 | INFO | train_inner | epoch 074:    508 / 685 loss=2.333, nll_loss=0.667, ppl=1.59, wps=73015.9, ups=5.02, wpb=14544.4, bsz=756.4, num_updates=50500, lr=7.44417e-05, gnorm=0.391, loss_scale=2, train_wall=20, gb_free=29.5, wall=9264
2024-02-09 23:56:47 | INFO | train_inner | epoch 074:    608 / 685 loss=2.327, nll_loss=0.66, ppl=1.58, wps=72685.1, ups=5.03, wpb=14437, bsz=740.4, num_updates=50600, lr=7.29218e-05, gnorm=0.397, loss_scale=2, train_wall=20, gb_free=29.5, wall=9284
2024-02-09 23:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:57:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:57:04 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.397 | nll_loss 1.805 | ppl 3.49 | wps 153092 | wpb 3189.1 | bsz 163.4 | num_updates 50677 | best_loss 3.382
2024-02-09 23:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 50677 updates
2024-02-09 23:57:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt
2024-02-09 23:57:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt
2024-02-09 23:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt (epoch 74 @ 50677 updates, score 3.397) (writing took 2.426232854137197 seconds)
2024-02-09 23:57:07 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-02-09 23:57:07 | INFO | train | epoch 074 | loss 2.331 | nll_loss 0.665 | ppl 1.59 | wps 70694.8 | ups 4.87 | wpb 14524.4 | bsz 748.5 | num_updates 50677 | lr 7.17613e-05 | gnorm 0.396 | loss_scale 2 | train_wall 134 | gb_free 29.8 | wall 9303
2024-02-09 23:57:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:57:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:57:07 | INFO | fairseq.trainer | begin training epoch 75
2024-02-09 23:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:57:12 | INFO | train_inner | epoch 075:     23 / 685 loss=2.326, nll_loss=0.66, ppl=1.58, wps=59349.2, ups=4.12, wpb=14413.9, bsz=739.2, num_updates=50700, lr=7.14163e-05, gnorm=0.398, loss_scale=2, train_wall=19, gb_free=29.5, wall=9308
2024-02-09 23:57:31 | INFO | train_inner | epoch 075:    123 / 685 loss=2.323, nll_loss=0.656, ppl=1.58, wps=72855.4, ups=5.03, wpb=14472.4, bsz=729.3, num_updates=50800, lr=6.99254e-05, gnorm=0.39, loss_scale=2, train_wall=20, gb_free=29.5, wall=9328
2024-02-09 23:57:51 | INFO | train_inner | epoch 075:    223 / 685 loss=2.322, nll_loss=0.655, ppl=1.58, wps=72434.4, ups=5.05, wpb=14331.9, bsz=769.3, num_updates=50900, lr=6.84491e-05, gnorm=0.393, loss_scale=2, train_wall=20, gb_free=29.6, wall=9348
2024-02-09 23:58:11 | INFO | train_inner | epoch 075:    323 / 685 loss=2.33, nll_loss=0.664, ppl=1.58, wps=73760.5, ups=5.02, wpb=14684.1, bsz=740.5, num_updates=51000, lr=6.69873e-05, gnorm=0.393, loss_scale=2, train_wall=20, gb_free=29.5, wall=9368
2024-02-09 23:58:31 | INFO | train_inner | epoch 075:    423 / 685 loss=2.328, nll_loss=0.663, ppl=1.58, wps=74102.7, ups=5, wpb=14818.9, bsz=769.8, num_updates=51100, lr=6.55402e-05, gnorm=0.393, loss_scale=2, train_wall=20, gb_free=29.5, wall=9388
2024-02-09 23:58:51 | INFO | train_inner | epoch 075:    523 / 685 loss=2.325, nll_loss=0.659, ppl=1.58, wps=72799.6, ups=5.01, wpb=14531.1, bsz=751.1, num_updates=51200, lr=6.41078e-05, gnorm=0.428, loss_scale=2, train_wall=20, gb_free=29.5, wall=9408
2024-02-09 23:59:11 | INFO | train_inner | epoch 075:    623 / 685 loss=2.332, nll_loss=0.666, ppl=1.59, wps=72607.3, ups=5.05, wpb=14382.6, bsz=741.2, num_updates=51300, lr=6.26901e-05, gnorm=0.4, loss_scale=2, train_wall=20, gb_free=29.5, wall=9428
2024-02-09 23:59:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-09 23:59:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:59:25 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.407 | nll_loss 1.816 | ppl 3.52 | wps 160307 | wpb 3189.1 | bsz 163.4 | num_updates 51362 | best_loss 3.382
2024-02-09 23:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 51362 updates
2024-02-09 23:59:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt
2024-02-09 23:59:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt
2024-02-09 23:59:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt (epoch 75 @ 51362 updates, score 3.407) (writing took 2.4265886440407485 seconds)
2024-02-09 23:59:27 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-02-09 23:59:27 | INFO | train | epoch 075 | loss 2.326 | nll_loss 0.66 | ppl 1.58 | wps 70777.4 | ups 4.87 | wpb 14524.4 | bsz 748.5 | num_updates 51362 | lr 6.18186e-05 | gnorm 0.4 | loss_scale 2 | train_wall 134 | gb_free 29.7 | wall 9444
2024-02-09 23:59:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-09 23:59:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-09 23:59:27 | INFO | fairseq.trainer | begin training epoch 76
2024-02-09 23:59:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-09 23:59:35 | INFO | train_inner | epoch 076:     38 / 685 loss=2.32, nll_loss=0.654, ppl=1.57, wps=59014.1, ups=4.12, wpb=14337.5, bsz=750.6, num_updates=51400, lr=6.12873e-05, gnorm=0.4, loss_scale=2, train_wall=20, gb_free=29.6, wall=9452
2024-02-09 23:59:55 | INFO | train_inner | epoch 076:    138 / 685 loss=2.319, nll_loss=0.652, ppl=1.57, wps=72865.1, ups=5.02, wpb=14529.3, bsz=756.2, num_updates=51500, lr=5.98993e-05, gnorm=0.395, loss_scale=2, train_wall=20, gb_free=29.6, wall=9472
2024-02-10 00:00:15 | INFO | train_inner | epoch 076:    238 / 685 loss=2.321, nll_loss=0.654, ppl=1.57, wps=73595.4, ups=5.02, wpb=14658.5, bsz=755.2, num_updates=51600, lr=5.85262e-05, gnorm=0.383, loss_scale=2, train_wall=20, gb_free=29.5, wall=9492
2024-02-10 00:00:35 | INFO | train_inner | epoch 076:    338 / 685 loss=2.324, nll_loss=0.657, ppl=1.58, wps=73616.2, ups=5.06, wpb=14552.4, bsz=753.5, num_updates=51700, lr=5.7168e-05, gnorm=0.391, loss_scale=2, train_wall=20, gb_free=29.5, wall=9511
2024-02-10 00:00:55 | INFO | train_inner | epoch 076:    438 / 685 loss=2.323, nll_loss=0.656, ppl=1.58, wps=72695.1, ups=4.99, wpb=14568, bsz=740.6, num_updates=51800, lr=5.58249e-05, gnorm=0.388, loss_scale=2, train_wall=20, gb_free=29.5, wall=9531
2024-02-10 00:01:15 | INFO | train_inner | epoch 076:    538 / 685 loss=2.328, nll_loss=0.663, ppl=1.58, wps=72083.3, ups=4.97, wpb=14490.3, bsz=743.3, num_updates=51900, lr=5.44967e-05, gnorm=0.4, loss_scale=2, train_wall=20, gb_free=29.5, wall=9552
2024-02-10 00:01:35 | INFO | train_inner | epoch 076:    638 / 685 loss=2.32, nll_loss=0.654, ppl=1.57, wps=71810.4, ups=4.97, wpb=14442, bsz=736.1, num_updates=52000, lr=5.31837e-05, gnorm=0.393, loss_scale=4, train_wall=20, gb_free=29.5, wall=9572
2024-02-10 00:01:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 00:01:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:01:46 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.398 | nll_loss 1.809 | ppl 3.5 | wps 157590 | wpb 3189.1 | bsz 163.4 | num_updates 52047 | best_loss 3.382
2024-02-10 00:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 52047 updates
2024-02-10 00:01:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt
2024-02-10 00:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt
2024-02-10 00:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt (epoch 76 @ 52047 updates, score 3.398) (writing took 2.4576324401423335 seconds)
2024-02-10 00:01:49 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-02-10 00:01:49 | INFO | train | epoch 076 | loss 2.322 | nll_loss 0.656 | ppl 1.58 | wps 70356.8 | ups 4.84 | wpb 14524.4 | bsz 748.5 | num_updates 52047 | lr 5.25718e-05 | gnorm 0.392 | loss_scale 4 | train_wall 135 | gb_free 29.6 | wall 9585
2024-02-10 00:01:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:01:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 00:01:49 | INFO | fairseq.trainer | begin training epoch 77
2024-02-10 00:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 00:02:00 | INFO | train_inner | epoch 077:     53 / 685 loss=2.318, nll_loss=0.651, ppl=1.57, wps=58356.4, ups=4.07, wpb=14339.5, bsz=749.5, num_updates=52100, lr=5.18857e-05, gnorm=0.391, loss_scale=4, train_wall=20, gb_free=29.7, wall=9596
2024-02-10 00:02:20 | INFO | train_inner | epoch 077:    153 / 685 loss=2.314, nll_loss=0.646, ppl=1.57, wps=72117.9, ups=4.96, wpb=14533.4, bsz=767.1, num_updates=52200, lr=5.0603e-05, gnorm=0.378, loss_scale=4, train_wall=20, gb_free=29.5, wall=9616
2024-02-10 00:02:40 | INFO | train_inner | epoch 077:    253 / 685 loss=2.322, nll_loss=0.655, ppl=1.58, wps=73650.3, ups=4.99, wpb=14755.1, bsz=743.2, num_updates=52300, lr=4.93354e-05, gnorm=0.404, loss_scale=4, train_wall=20, gb_free=29.6, wall=9636
2024-02-10 00:03:00 | INFO | train_inner | epoch 077:    353 / 685 loss=2.32, nll_loss=0.653, ppl=1.57, wps=72092.2, ups=4.99, wpb=14461.6, bsz=758.1, num_updates=52400, lr=4.80831e-05, gnorm=0.397, loss_scale=4, train_wall=20, gb_free=29.6, wall=9657
2024-02-10 00:03:20 | INFO | train_inner | epoch 077:    453 / 685 loss=2.32, nll_loss=0.652, ppl=1.57, wps=73103.9, ups=5.07, wpb=14418.3, bsz=717.4, num_updates=52500, lr=4.68461e-05, gnorm=0.391, loss_scale=4, train_wall=19, gb_free=29.6, wall=9676
2024-02-10 00:03:39 | INFO | train_inner | epoch 077:    553 / 685 loss=2.318, nll_loss=0.651, ppl=1.57, wps=75739.3, ups=5.18, wpb=14612.8, bsz=766.5, num_updates=52600, lr=4.56244e-05, gnorm=0.385, loss_scale=4, train_wall=19, gb_free=29.5, wall=9696
2024-02-10 00:03:58 | INFO | train_inner | epoch 077:    653 / 685 loss=2.323, nll_loss=0.657, ppl=1.58, wps=76500.6, ups=5.21, wpb=14693, bsz=747.9, num_updates=52700, lr=4.44181e-05, gnorm=0.386, loss_scale=4, train_wall=19, gb_free=29.5, wall=9715
2024-02-10 00:04:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 00:04:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:04:06 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.409 | nll_loss 1.821 | ppl 3.53 | wps 168764 | wpb 3189.1 | bsz 163.4 | num_updates 52732 | best_loss 3.382
2024-02-10 00:04:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 52732 updates
2024-02-10 00:04:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt
2024-02-10 00:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt
2024-02-10 00:04:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt (epoch 77 @ 52732 updates, score 3.409) (writing took 2.4909910408314317 seconds)
2024-02-10 00:04:08 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-02-10 00:04:08 | INFO | train | epoch 077 | loss 2.319 | nll_loss 0.652 | ppl 1.57 | wps 71274.3 | ups 4.91 | wpb 14524.4 | bsz 748.5 | num_updates 52732 | lr 4.40354e-05 | gnorm 0.391 | loss_scale 4 | train_wall 133 | gb_free 29.7 | wall 9725
2024-02-10 00:04:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:04:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 00:04:08 | INFO | fairseq.trainer | begin training epoch 78
2024-02-10 00:04:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 00:04:21 | INFO | train_inner | epoch 078:     68 / 685 loss=2.315, nll_loss=0.647, ppl=1.57, wps=60936.9, ups=4.27, wpb=14275.4, bsz=731.6, num_updates=52800, lr=4.32273e-05, gnorm=0.391, loss_scale=4, train_wall=19, gb_free=29.5, wall=9738
2024-02-10 00:04:41 | INFO | train_inner | epoch 078:    168 / 685 loss=2.312, nll_loss=0.644, ppl=1.56, wps=75546.6, ups=5.22, wpb=14476, bsz=738.2, num_updates=52900, lr=4.20519e-05, gnorm=0.387, loss_scale=4, train_wall=19, gb_free=29.5, wall=9757
2024-02-10 00:05:00 | INFO | train_inner | epoch 078:    268 / 685 loss=2.308, nll_loss=0.64, ppl=1.56, wps=74731.1, ups=5.18, wpb=14426.4, bsz=756.1, num_updates=53000, lr=4.08919e-05, gnorm=0.379, loss_scale=4, train_wall=19, gb_free=29.5, wall=9777
2024-02-10 00:05:19 | INFO | train_inner | epoch 078:    368 / 685 loss=2.317, nll_loss=0.651, ppl=1.57, wps=76379.5, ups=5.19, wpb=14728.8, bsz=763.8, num_updates=53100, lr=3.97476e-05, gnorm=0.381, loss_scale=4, train_wall=19, gb_free=29.5, wall=9796
2024-02-10 00:05:38 | INFO | train_inner | epoch 078:    468 / 685 loss=2.317, nll_loss=0.65, ppl=1.57, wps=75305.3, ups=5.21, wpb=14449.7, bsz=741.4, num_updates=53200, lr=3.86188e-05, gnorm=0.385, loss_scale=4, train_wall=19, gb_free=29.5, wall=9815
2024-02-10 00:05:58 | INFO | train_inner | epoch 078:    568 / 685 loss=2.321, nll_loss=0.654, ppl=1.57, wps=76143.3, ups=5.19, wpb=14681.6, bsz=754.1, num_updates=53300, lr=3.75056e-05, gnorm=0.383, loss_scale=4, train_wall=19, gb_free=29.6, wall=9834
2024-02-10 00:06:18 | INFO | train_inner | epoch 078:    668 / 685 loss=2.317, nll_loss=0.65, ppl=1.57, wps=72430.4, ups=4.99, wpb=14515, bsz=750.2, num_updates=53400, lr=3.64081e-05, gnorm=0.383, loss_scale=4, train_wall=20, gb_free=29.5, wall=9854
2024-02-10 00:06:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 00:06:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:06:23 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.403 | nll_loss 1.812 | ppl 3.51 | wps 137179 | wpb 3189.1 | bsz 163.4 | num_updates 53417 | best_loss 3.382
2024-02-10 00:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 53417 updates
2024-02-10 00:06:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt
2024-02-10 00:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt
2024-02-10 00:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt (epoch 78 @ 53417 updates, score 3.403) (writing took 2.5430308380164206 seconds)
2024-02-10 00:06:26 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-02-10 00:06:26 | INFO | train | epoch 078 | loss 2.315 | nll_loss 0.648 | ppl 1.57 | wps 72376.7 | ups 4.98 | wpb 14524.4 | bsz 748.5 | num_updates 53417 | lr 3.62231e-05 | gnorm 0.384 | loss_scale 4 | train_wall 131 | gb_free 29.7 | wall 9863
2024-02-10 00:06:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:06:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 00:06:26 | INFO | fairseq.trainer | begin training epoch 79
2024-02-10 00:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 00:06:43 | INFO | train_inner | epoch 079:     83 / 685 loss=2.313, nll_loss=0.645, ppl=1.56, wps=57612.8, ups=4.03, wpb=14298.3, bsz=731.2, num_updates=53500, lr=3.53262e-05, gnorm=0.387, loss_scale=4, train_wall=20, gb_free=29.5, wall=9879
2024-02-10 00:07:02 | INFO | train_inner | epoch 079:    183 / 685 loss=2.31, nll_loss=0.642, ppl=1.56, wps=76246.1, ups=5.21, wpb=14640.9, bsz=750.9, num_updates=53600, lr=3.42601e-05, gnorm=0.386, loss_scale=4, train_wall=19, gb_free=29.5, wall=9898
2024-02-10 00:07:21 | INFO | train_inner | epoch 079:    283 / 685 loss=2.311, nll_loss=0.644, ppl=1.56, wps=75559.1, ups=5.2, wpb=14522.6, bsz=742.6, num_updates=53700, lr=3.32098e-05, gnorm=0.384, loss_scale=4, train_wall=19, gb_free=29.5, wall=9918
2024-02-10 00:07:40 | INFO | train_inner | epoch 079:    383 / 685 loss=2.308, nll_loss=0.64, ppl=1.56, wps=74982.7, ups=5.19, wpb=14455, bsz=761.8, num_updates=53800, lr=3.21752e-05, gnorm=0.389, loss_scale=4, train_wall=19, gb_free=29.5, wall=9937
2024-02-10 00:08:00 | INFO | train_inner | epoch 079:    483 / 685 loss=2.315, nll_loss=0.647, ppl=1.57, wps=74818.9, ups=5.08, wpb=14713.9, bsz=750.9, num_updates=53900, lr=3.11565e-05, gnorm=0.382, loss_scale=4, train_wall=19, gb_free=29.5, wall=9957
2024-02-10 00:08:21 | INFO | train_inner | epoch 079:    583 / 685 loss=2.313, nll_loss=0.647, ppl=1.57, wps=69784.7, ups=4.81, wpb=14515.6, bsz=758.1, num_updates=54000, lr=3.01537e-05, gnorm=0.385, loss_scale=4, train_wall=21, gb_free=29.6, wall=9977
2024-02-10 00:08:41 | INFO | train_inner | epoch 079:    683 / 685 loss=2.318, nll_loss=0.652, ppl=1.57, wps=73534.2, ups=5.05, wpb=14552.4, bsz=748.7, num_updates=54100, lr=2.91668e-05, gnorm=0.393, loss_scale=4, train_wall=20, gb_free=29.5, wall=9997
2024-02-10 00:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 00:08:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 00:08:43 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 3.404 | nll_loss 1.814 | ppl 3.52 | wps 162180 | wpb 3189.1 | bsz 163.4 | num_updates 54102 | best_loss 3.382
2024-02-10 00:08:43 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 25 runs
2024-02-10 00:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 54102 updates
2024-02-10 00:08:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt
2024-02-10 00:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt
2024-02-10 00:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt (epoch 79 @ 54102 updates, score 3.404) (writing took 2.45388473989442 seconds)
2024-02-10 00:08:45 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-02-10 00:08:45 | INFO | train | epoch 079 | loss 2.312 | nll_loss 0.645 | ppl 1.56 | wps 71429.4 | ups 4.92 | wpb 14524.4 | bsz 748.5 | num_updates 54102 | lr 2.91472e-05 | gnorm 0.387 | loss_scale 4 | train_wall 133 | gb_free 29.6 | wall 10002
2024-02-10 00:08:45 | INFO | fairseq_cli.train | done training in 9995.8 seconds
Training complete.
Finetuning complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt' are the same file
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt' are the same file
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test.en: 15516 sents, 256102 tokens, 0.0176% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test.de: 15516 sents, 269168 tokens, 0.0163% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
averaging checkpoints:  ['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt']
Finished writing averaged checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 16, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 6616 types
INFO:fairseq.tasks.translation:[de] dictionary: 8648 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/test.en-de.en
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/test.en-de.de
INFO:fairseq.tasks.translation:/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset test en-de 15516 examples
INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True
INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True
INFO:fairseq.tasks.fairseq_task:rebuild_batches = False
INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1
INFO:fairseq.logging.progress_bar::    101 / 243 wps=1745
INFO:fairseq.logging.progress_bar::    201 / 243 wps=1975
INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2
INFO:fairseq_cli.generate:Translated 15,512 sentences (260,084 tokens) in 79.5s (195.07 sentences/s, 3270.66 tokens/s)
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/hyp_mt.txt and /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/ref_mt.txt
Sampled predictions written to /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/hyp_mt.txt.sampled
Sample predictions:
Sample: Ich wei nicht.
Reference: Ich wei nicht.
Sample: Lassen Sie es mich sehen!
Reference: Zeig mal!
Sample: Aber nicht lange.
Reference: Aber nicht lange.
Sample: Wie gefiel ihm?
Reference: Wie war er?
Sample: Und der letzte?
Reference: Und zuletzt?
Sample: Mit dem Ganzen?
Reference: Mit allem?
Sample: Reichsdit
Reference: Reichstag
Sample: Arbeiter im Lebensmittelmarkt
Reference: Arbeitswelt
Sample: Ich tue es immer noch.
Reference: Das mache ich weiterhin.
Sample: Zeitverschiebung.
Reference: Die Zeit rast.
Sample: Du machst es.
Reference: Du machst es.
Sample: Sie ist einfach.
Reference: Sie ist gerecht.
Sample: Ein Flug (musikalische Noten)
Reference: Eine Sinfonie
Sample: Das Leben ist schn.
Reference: Das Leben ist schn.
Sample: Das goldene Land
Reference: Das goldene Land
Sample: Getroffene Erde
Reference: Gebackene Erde
Sample: Die Union ist strgerlich.
Reference: Die Union ist stark.
Sample: Siehe den Dienstag!
Reference: Sehen uns Dienstag!
Sample: Ein Akt des Glaubens.
Reference: Ein Akt des Glaubens
Sample: Das kann nicht tun.
Reference: Das ist nicht mglich.
WER:
Generate test with beam=16: BLEU4 = 27.42, 54.8/34.0/22.4/15.3 (BP=0.970, ratio=0.970, syslen=136183, reflen=140361)
BLEU:
{
 "name": "BLEU",
 "score": 27.4,
 "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:none|version:2.4.0",
 "verbose_score": "54.8/34.0/22.4/15.3 (BP = 0.970 ratio = 0.970 hyp_len = 136183 ref_len = 140360)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "none",
 "smooth": "none",
 "version": "2.4.0"
}