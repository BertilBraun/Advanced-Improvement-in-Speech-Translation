(base) [uxude@uc2n996 train]$ cat finetune_mt_paraphrased_covost_23138256.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...

[notice] A new release of pip is available: 23.3.2 -> 24.0
[notice] To update, run: pip install --upgrade pip
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[INFO] 13:43:26 [Dataset::Prepare Datasets]: Skipping dataset preparation, all config data already exists
Finetuning the paraphrased MT model...
rm: cannot remove '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/preprocess.log': No such file or directory
Binarizing the data...
Binarized data will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Training data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train
Validation data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train', validpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev', testpref=None, align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train.en: 512755 sents, 9934160 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev.en: 15520 sents, 288077 tokens, 0.00868% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train.de: 512755 sents, 9949297 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev.de: 15520 sents, 302966 tokens, 0.00792% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Binarization complete.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models
Training time: 6 hours
2024-02-10 13:47:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 8192, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 8192, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 100000, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 25, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='cosine', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=8192, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=8192, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=100000, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=25, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=-1, min_lr=0.0, t_mult=1.0, lr_period_updates=-1, lr_shrink=0.1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 10000, 'warmup_init_lr': -1.0, 'lr': [0.001], 'min_lr': 0.0, 't_mult': 1.0, 'lr_period_updates': -1.0, 'lr_shrink': 0.1, 'max_update': 100000}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-02-10 13:47:08 | INFO | fairseq.tasks.translation | [en] dictionary: 6616 types
2024-02-10 13:47:08 | INFO | fairseq.tasks.translation | [de] dictionary: 8648 types
2024-02-10 13:47:10 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6616, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8648, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=8648, bias=False)
  )
)
2024-02-10 13:47:10 | INFO | fairseq_cli.train | task: TranslationTask
2024-02-10 13:47:10 | INFO | fairseq_cli.train | model: TransformerModel
2024-02-10 13:47:10 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-02-10 13:47:10 | INFO | fairseq_cli.train | num. shared model params: 51,953,664 (num. trained: 51,953,664)
2024-02-10 13:47:10 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-02-10 13:47:10 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/valid.en-de.en
2024-02-10 13:47:10 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/valid.en-de.de
2024-02-10 13:47:10 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset valid en-de 15520 examples
2024-02-10 13:47:10 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-02-10 13:47:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-10 13:47:10 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-02-10 13:47:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-10 13:47:10 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-02-10 13:47:10 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = None
2024-02-10 13:47:10 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint_last.pt
2024-02-10 13:47:10 | INFO | fairseq.trainer | No existing checkpoint found /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint_last.pt
2024-02-10 13:47:10 | INFO | fairseq.trainer | loading train data for epoch 1
2024-02-10 13:47:10 | INFO | fairseq.data.data_utils | loaded 512,755 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/train.en-de.en
2024-02-10 13:47:10 | INFO | fairseq.data.data_utils | loaded 512,755 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/train.en-de.de
2024-02-10 13:47:10 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset train en-de 512755 examples
2024-02-10 13:47:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:47:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-10 13:47:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-10 13:47:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-10 13:47:13 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-02-10 13:47:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:47:13 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-10 13:47:13 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-10 13:47:13 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-10 13:47:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:47:16 | INFO | fairseq.trainer | begin training epoch 1
2024-02-10 13:47:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:47:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-02-10 13:47:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-02-10 13:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-02-10 13:47:39 | INFO | train_inner | epoch 001:    103 / 685 loss=12.55, nll_loss=12.405, ppl=5422.99, wps=86794.6, ups=6.05, wpb=14350.4, bsz=702.2, num_updates=100, lr=1e-05, gnorm=2.967, loss_scale=16, train_wall=24, gb_free=28.3, wall=29
2024-02-10 13:47:56 | INFO | train_inner | epoch 001:    203 / 685 loss=11.254, nll_loss=10.96, ppl=1992.08, wps=89011.1, ups=6.12, wpb=14556.1, bsz=744.9, num_updates=200, lr=2e-05, gnorm=1.446, loss_scale=16, train_wall=16, gb_free=28.3, wall=45
2024-02-10 13:48:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-10 13:48:12 | INFO | train_inner | epoch 001:    304 / 685 loss=10.398, nll_loss=9.976, ppl=1007.19, wps=88453.2, ups=6.1, wpb=14499, bsz=761.9, num_updates=300, lr=3e-05, gnorm=1.49, loss_scale=8, train_wall=16, gb_free=28.3, wall=62
2024-02-10 13:48:29 | INFO | train_inner | epoch 001:    404 / 685 loss=10.007, nll_loss=9.5, ppl=724.19, wps=88908.1, ups=6.14, wpb=14481.7, bsz=767.1, num_updates=400, lr=4e-05, gnorm=1.335, loss_scale=8, train_wall=16, gb_free=28.4, wall=78
2024-02-10 13:48:45 | INFO | train_inner | epoch 001:    504 / 685 loss=9.851, nll_loss=9.3, ppl=630.37, wps=88490.8, ups=6.13, wpb=14432.6, bsz=767, num_updates=500, lr=5e-05, gnorm=1.266, loss_scale=8, train_wall=16, gb_free=28.3, wall=94
2024-02-10 13:49:01 | INFO | train_inner | epoch 001:    604 / 685 loss=9.766, nll_loss=9.197, ppl=586.9, wps=90002.8, ups=6.09, wpb=14785.5, bsz=769, num_updates=600, lr=6e-05, gnorm=1.285, loss_scale=8, train_wall=16, gb_free=28.7, wall=111
2024-02-10 13:49:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 13:49:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:49:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.35 | nll_loss 8.714 | ppl 419.98 | wps 155926 | wpb 5716.3 | bsz 292.8 | num_updates 681
2024-02-10 13:49:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 681 updates
2024-02-10 13:49:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint1.pt
2024-02-10 13:49:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint1.pt
2024-02-10 13:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint1.pt (epoch 1 @ 681 updates, score 9.35) (writing took 3.6603378739673644 seconds)
2024-02-10 13:49:20 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-02-10 13:49:20 | INFO | train | epoch 001 | loss 10.514 | nll_loss 10.079 | ppl 1081.37 | wps 84434.9 | ups 5.82 | wpb 14517.8 | bsz 747.1 | num_updates 681 | lr 6.81e-05 | gnorm 1.587 | loss_scale 8 | train_wall 117 | gb_free 28.3 | wall 130
2024-02-10 13:49:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:49:20 | INFO | fairseq.trainer | begin training epoch 2
2024-02-10 13:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:49:23 | INFO | train_inner | epoch 002:     19 / 685 loss=9.617, nll_loss=9.028, ppl=522.05, wps=65572.5, ups=4.53, wpb=14473.8, bsz=711.5, num_updates=700, lr=7e-05, gnorm=1.246, loss_scale=8, train_wall=16, gb_free=28.3, wall=133
2024-02-10 13:49:40 | INFO | train_inner | epoch 002:    119 / 685 loss=9.302, nll_loss=8.666, ppl=406.2, wps=89677.7, ups=6.07, wpb=14762.8, bsz=792.4, num_updates=800, lr=8e-05, gnorm=1.412, loss_scale=8, train_wall=16, gb_free=28.4, wall=149
2024-02-10 13:49:56 | INFO | train_inner | epoch 002:    219 / 685 loss=9.079, nll_loss=8.407, ppl=339.34, wps=88841.4, ups=6.16, wpb=14417, bsz=746.9, num_updates=900, lr=9e-05, gnorm=1.367, loss_scale=8, train_wall=16, gb_free=28.2, wall=166
2024-02-10 13:50:12 | INFO | train_inner | epoch 002:    319 / 685 loss=8.861, nll_loss=8.153, ppl=284.63, wps=87597.2, ups=6.15, wpb=14233.8, bsz=757.2, num_updates=1000, lr=0.0001, gnorm=1.427, loss_scale=8, train_wall=16, gb_free=28.3, wall=182
2024-02-10 13:50:29 | INFO | train_inner | epoch 002:    419 / 685 loss=8.713, nll_loss=7.978, ppl=252.2, wps=89705.5, ups=6.16, wpb=14554.7, bsz=706.1, num_updates=1100, lr=0.00011, gnorm=1.307, loss_scale=8, train_wall=16, gb_free=28.4, wall=198
2024-02-10 13:50:45 | INFO | train_inner | epoch 002:    519 / 685 loss=8.472, nll_loss=7.698, ppl=207.59, wps=89532, ups=6.12, wpb=14621.1, bsz=747.5, num_updates=1200, lr=0.00012, gnorm=1.299, loss_scale=8, train_wall=16, gb_free=28.2, wall=214
2024-02-10 13:51:01 | INFO | train_inner | epoch 002:    619 / 685 loss=8.297, nll_loss=7.494, ppl=180.26, wps=88909.4, ups=6.15, wpb=14466, bsz=743.8, num_updates=1300, lr=0.00013, gnorm=1.309, loss_scale=8, train_wall=16, gb_free=28.3, wall=231
2024-02-10 13:51:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 13:51:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:51:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.848 | nll_loss 6.948 | ppl 123.48 | wps 155350 | wpb 5716.3 | bsz 292.8 | num_updates 1366 | best_loss 7.848
2024-02-10 13:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1366 updates
2024-02-10 13:51:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint2.pt
2024-02-10 13:51:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint2.pt
2024-02-10 13:51:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint2.pt (epoch 2 @ 1366 updates, score 7.848) (writing took 3.7211558849085122 seconds)
2024-02-10 13:51:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-02-10 13:51:18 | INFO | train | epoch 002 | loss 8.748 | nll_loss 8.02 | ppl 259.5 | wps 84616.9 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 1366 | lr 0.0001366 | gnorm 1.342 | loss_scale 8 | train_wall 110 | gb_free 28.2 | wall 247
2024-02-10 13:51:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:51:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:51:18 | INFO | fairseq.trainer | begin training epoch 3
2024-02-10 13:51:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:51:23 | INFO | train_inner | epoch 003:     34 / 685 loss=8.125, nll_loss=7.293, ppl=156.88, wps=64367.1, ups=4.49, wpb=14342.6, bsz=760.6, num_updates=1400, lr=0.00014, gnorm=1.306, loss_scale=8, train_wall=16, gb_free=28.3, wall=253
2024-02-10 13:51:40 | INFO | train_inner | epoch 003:    134 / 685 loss=7.958, nll_loss=7.1, ppl=137.21, wps=89554.2, ups=6.13, wpb=14608.9, bsz=757.9, num_updates=1500, lr=0.00015, gnorm=1.298, loss_scale=8, train_wall=16, gb_free=28.6, wall=269
2024-02-10 13:51:56 | INFO | train_inner | epoch 003:    234 / 685 loss=7.813, nll_loss=6.931, ppl=122.02, wps=88998.7, ups=6.18, wpb=14391.4, bsz=730.6, num_updates=1600, lr=0.00016, gnorm=1.229, loss_scale=8, train_wall=16, gb_free=28.5, wall=285
2024-02-10 13:52:12 | INFO | train_inner | epoch 003:    334 / 685 loss=7.648, nll_loss=6.74, ppl=106.87, wps=88808.4, ups=6.13, wpb=14482.4, bsz=748.3, num_updates=1700, lr=0.00017, gnorm=1.241, loss_scale=8, train_wall=16, gb_free=28.3, wall=302
2024-02-10 13:52:28 | INFO | train_inner | epoch 003:    434 / 685 loss=7.477, nll_loss=6.541, ppl=93.13, wps=88964.8, ups=6.14, wpb=14494, bsz=754, num_updates=1800, lr=0.00018, gnorm=1.264, loss_scale=8, train_wall=16, gb_free=28.2, wall=318
2024-02-10 13:52:45 | INFO | train_inner | epoch 003:    534 / 685 loss=7.293, nll_loss=6.328, ppl=80.34, wps=90036.5, ups=6.1, wpb=14756.7, bsz=773.7, num_updates=1900, lr=0.00019, gnorm=1.202, loss_scale=8, train_wall=16, gb_free=28.2, wall=334
2024-02-10 13:53:01 | INFO | train_inner | epoch 003:    634 / 685 loss=7.176, nll_loss=6.19, ppl=73, wps=91238, ups=6.13, wpb=14890.9, bsz=717.6, num_updates=2000, lr=0.0002, gnorm=1.116, loss_scale=8, train_wall=16, gb_free=28.4, wall=351
2024-02-10 13:53:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 13:53:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:53:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.737 | nll_loss 5.624 | ppl 49.31 | wps 154905 | wpb 5716.3 | bsz 292.8 | num_updates 2051 | best_loss 6.737
2024-02-10 13:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2051 updates
2024-02-10 13:53:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint3.pt
2024-02-10 13:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint3.pt
2024-02-10 13:53:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint3.pt (epoch 3 @ 2051 updates, score 6.737) (writing took 3.700508381938562 seconds)
2024-02-10 13:53:15 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-02-10 13:53:15 | INFO | train | epoch 003 | loss 7.544 | nll_loss 6.619 | ppl 98.29 | wps 84634.6 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 2051 | lr 0.0002051 | gnorm 1.238 | loss_scale 8 | train_wall 110 | gb_free 28.4 | wall 365
2024-02-10 13:53:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:53:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:53:15 | INFO | fairseq.trainer | begin training epoch 4
2024-02-10 13:53:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:53:23 | INFO | train_inner | epoch 004:     49 / 685 loss=7.006, nll_loss=5.993, ppl=63.7, wps=65474.2, ups=4.5, wpb=14540.7, bsz=736.2, num_updates=2100, lr=0.00021, gnorm=1.218, loss_scale=8, train_wall=16, gb_free=28.3, wall=373
2024-02-10 13:53:40 | INFO | train_inner | epoch 004:    149 / 685 loss=6.822, nll_loss=5.78, ppl=54.95, wps=88266.3, ups=6.16, wpb=14334.9, bsz=740.5, num_updates=2200, lr=0.00022, gnorm=1.17, loss_scale=8, train_wall=16, gb_free=28.2, wall=389
2024-02-10 13:53:56 | INFO | train_inner | epoch 004:    249 / 685 loss=6.7, nll_loss=5.638, ppl=49.78, wps=88577.3, ups=6.19, wpb=14319.4, bsz=712.8, num_updates=2300, lr=0.00023, gnorm=1.13, loss_scale=8, train_wall=16, gb_free=28.2, wall=405
2024-02-10 13:54:12 | INFO | train_inner | epoch 004:    349 / 685 loss=6.565, nll_loss=5.479, ppl=44.59, wps=89240.2, ups=6.19, wpb=14406.8, bsz=709.8, num_updates=2400, lr=0.00024, gnorm=1.117, loss_scale=8, train_wall=16, gb_free=28.3, wall=422
2024-02-10 13:54:28 | INFO | train_inner | epoch 004:    449 / 685 loss=6.367, nll_loss=5.251, ppl=38.07, wps=89006.3, ups=6.1, wpb=14579.4, bsz=779.1, num_updates=2500, lr=0.00025, gnorm=1.137, loss_scale=8, train_wall=16, gb_free=28.5, wall=438
2024-02-10 13:54:45 | INFO | train_inner | epoch 004:    549 / 685 loss=6.243, nll_loss=5.104, ppl=34.38, wps=89137.4, ups=6.14, wpb=14528.9, bsz=773, num_updates=2600, lr=0.00026, gnorm=1.113, loss_scale=8, train_wall=16, gb_free=28.5, wall=454
2024-02-10 13:55:01 | INFO | train_inner | epoch 004:    649 / 685 loss=6.127, nll_loss=4.967, ppl=31.28, wps=89367.1, ups=6.13, wpb=14589, bsz=762.4, num_updates=2700, lr=0.00027, gnorm=1.079, loss_scale=8, train_wall=16, gb_free=28.4, wall=471
2024-02-10 13:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 13:55:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:55:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.782 | nll_loss 4.443 | ppl 21.75 | wps 156087 | wpb 5716.3 | bsz 292.8 | num_updates 2736 | best_loss 5.782
2024-02-10 13:55:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2736 updates
2024-02-10 13:55:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint4.pt
2024-02-10 13:55:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint4.pt
2024-02-10 13:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint4.pt (epoch 4 @ 2736 updates, score 5.782) (writing took 3.7748913280665874 seconds)
2024-02-10 13:55:13 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-02-10 13:55:13 | INFO | train | epoch 004 | loss 6.477 | nll_loss 5.377 | ppl 41.56 | wps 84670.8 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 2736 | lr 0.0002736 | gnorm 1.12 | loss_scale 8 | train_wall 110 | gb_free 28.4 | wall 482
2024-02-10 13:55:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:55:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:55:13 | INFO | fairseq.trainer | begin training epoch 5
2024-02-10 13:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:55:23 | INFO | train_inner | epoch 005:     64 / 685 loss=5.94, nll_loss=4.751, ppl=26.93, wps=65707, ups=4.45, wpb=14750, bsz=780.9, num_updates=2800, lr=0.00028, gnorm=1.044, loss_scale=8, train_wall=16, gb_free=28.3, wall=493
2024-02-10 13:55:40 | INFO | train_inner | epoch 005:    164 / 685 loss=5.837, nll_loss=4.629, ppl=24.74, wps=89209.7, ups=6.12, wpb=14572.4, bsz=753.5, num_updates=2900, lr=0.00029, gnorm=1.073, loss_scale=8, train_wall=16, gb_free=28.3, wall=509
2024-02-10 13:55:56 | INFO | train_inner | epoch 005:    264 / 685 loss=5.684, nll_loss=4.45, ppl=21.86, wps=89110.4, ups=6.13, wpb=14525.5, bsz=748, num_updates=3000, lr=0.0003, gnorm=1.052, loss_scale=8, train_wall=16, gb_free=28.2, wall=526
2024-02-10 13:56:12 | INFO | train_inner | epoch 005:    364 / 685 loss=5.582, nll_loss=4.329, ppl=20.09, wps=90546.9, ups=6.13, wpb=14772.6, bsz=748.2, num_updates=3100, lr=0.00031, gnorm=1.047, loss_scale=8, train_wall=16, gb_free=28.6, wall=542
2024-02-10 13:56:29 | INFO | train_inner | epoch 005:    464 / 685 loss=5.493, nll_loss=4.224, ppl=18.69, wps=88666.7, ups=6.17, wpb=14363.4, bsz=747.7, num_updates=3200, lr=0.00032, gnorm=1.042, loss_scale=8, train_wall=16, gb_free=28.5, wall=558
2024-02-10 13:56:45 | INFO | train_inner | epoch 005:    564 / 685 loss=5.366, nll_loss=4.076, ppl=16.87, wps=88065, ups=6.15, wpb=14307.9, bsz=756.9, num_updates=3300, lr=0.00033, gnorm=1.019, loss_scale=8, train_wall=16, gb_free=28.4, wall=574
2024-02-10 13:57:01 | INFO | train_inner | epoch 005:    664 / 685 loss=5.3, nll_loss=3.997, ppl=15.97, wps=89595.4, ups=6.15, wpb=14560.5, bsz=735.8, num_updates=3400, lr=0.00034, gnorm=0.998, loss_scale=8, train_wall=16, gb_free=28.3, wall=591
2024-02-10 13:57:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 13:57:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:57:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.928 | nll_loss 3.402 | ppl 10.57 | wps 155538 | wpb 5716.3 | bsz 292.8 | num_updates 3421 | best_loss 4.928
2024-02-10 13:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3421 updates
2024-02-10 13:57:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint5.pt
2024-02-10 13:57:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint5.pt
2024-02-10 13:57:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint5.pt (epoch 5 @ 3421 updates, score 4.928) (writing took 3.744642686098814 seconds)
2024-02-10 13:57:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-02-10 13:57:10 | INFO | train | epoch 005 | loss 5.569 | nll_loss 4.314 | ppl 19.9 | wps 84653.8 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 3421 | lr 0.0003421 | gnorm 1.037 | loss_scale 8 | train_wall 110 | gb_free 28.3 | wall 600
2024-02-10 13:57:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:57:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:57:10 | INFO | fairseq.trainer | begin training epoch 6
2024-02-10 13:57:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:57:23 | INFO | train_inner | epoch 006:     79 / 685 loss=5.097, nll_loss=3.765, ppl=13.6, wps=66152.7, ups=4.48, wpb=14754.9, bsz=758, num_updates=3500, lr=0.00035, gnorm=0.954, loss_scale=8, train_wall=16, gb_free=28.3, wall=613
2024-02-10 13:57:40 | INFO | train_inner | epoch 006:    179 / 685 loss=5.051, nll_loss=3.71, ppl=13.09, wps=88970.2, ups=6.17, wpb=14424.8, bsz=733.7, num_updates=3600, lr=0.00036, gnorm=0.974, loss_scale=8, train_wall=16, gb_free=28.4, wall=629
2024-02-10 13:57:56 | INFO | train_inner | epoch 006:    279 / 685 loss=4.972, nll_loss=3.619, ppl=12.29, wps=87916.9, ups=6.09, wpb=14430.8, bsz=789.9, num_updates=3700, lr=0.00037, gnorm=0.933, loss_scale=8, train_wall=16, gb_free=28.3, wall=646
2024-02-10 13:58:12 | INFO | train_inner | epoch 006:    379 / 685 loss=4.887, nll_loss=3.518, ppl=11.46, wps=90741.2, ups=6.12, wpb=14831, bsz=733.3, num_updates=3800, lr=0.00038, gnorm=0.907, loss_scale=8, train_wall=16, gb_free=28.3, wall=662
2024-02-10 13:58:29 | INFO | train_inner | epoch 006:    479 / 685 loss=4.855, nll_loss=3.483, ppl=11.18, wps=88981.1, ups=6.17, wpb=14416.4, bsz=707, num_updates=3900, lr=0.00039, gnorm=0.909, loss_scale=8, train_wall=16, gb_free=28.2, wall=678
2024-02-10 13:58:45 | INFO | train_inner | epoch 006:    579 / 685 loss=4.797, nll_loss=3.416, ppl=10.68, wps=88248.7, ups=6.15, wpb=14343.8, bsz=780.8, num_updates=4000, lr=0.0004, gnorm=0.939, loss_scale=8, train_wall=16, gb_free=28.7, wall=694
2024-02-10 13:59:01 | INFO | train_inner | epoch 006:    679 / 685 loss=4.763, nll_loss=3.376, ppl=10.38, wps=89535, ups=6.18, wpb=14496.2, bsz=732.6, num_updates=4100, lr=0.00041, gnorm=0.858, loss_scale=8, train_wall=16, gb_free=28.3, wall=711
2024-02-10 13:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 13:59:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:59:04 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.521 | nll_loss 2.946 | ppl 7.7 | wps 156611 | wpb 5716.3 | bsz 292.8 | num_updates 4106 | best_loss 4.521
2024-02-10 13:59:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4106 updates
2024-02-10 13:59:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint6.pt
2024-02-10 13:59:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint6.pt
2024-02-10 13:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint6.pt (epoch 6 @ 4106 updates, score 4.521) (writing took 3.784339854028076 seconds)
2024-02-10 13:59:08 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-02-10 13:59:08 | INFO | train | epoch 006 | loss 4.907 | nll_loss 3.543 | ppl 11.66 | wps 84661 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 4106 | lr 0.0004106 | gnorm 0.923 | loss_scale 8 | train_wall 110 | gb_free 28.6 | wall 717
2024-02-10 13:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 13:59:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 13:59:08 | INFO | fairseq.trainer | begin training epoch 7
2024-02-10 13:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 13:59:23 | INFO | train_inner | epoch 007:     94 / 685 loss=4.586, nll_loss=3.174, ppl=9.03, wps=64771.4, ups=4.49, wpb=14432.4, bsz=750.5, num_updates=4200, lr=0.00042, gnorm=0.826, loss_scale=8, train_wall=16, gb_free=28.3, wall=733
2024-02-10 13:59:40 | INFO | train_inner | epoch 007:    194 / 685 loss=4.555, nll_loss=3.138, ppl=8.81, wps=83263.1, ups=5.84, wpb=14252.2, bsz=771.8, num_updates=4300, lr=0.00043, gnorm=0.853, loss_scale=8, train_wall=17, gb_free=28.3, wall=750
2024-02-10 13:59:58 | INFO | train_inner | epoch 007:    294 / 685 loss=4.501, nll_loss=3.074, ppl=8.42, wps=84455, ups=5.71, wpb=14778.8, bsz=764.2, num_updates=4400, lr=0.00044, gnorm=0.842, loss_scale=8, train_wall=17, gb_free=28.3, wall=767
2024-02-10 14:00:14 | INFO | train_inner | epoch 007:    394 / 685 loss=4.513, nll_loss=3.089, ppl=8.51, wps=90775.9, ups=6.2, wpb=14643.1, bsz=715.9, num_updates=4500, lr=0.00045, gnorm=0.822, loss_scale=8, train_wall=16, gb_free=28.5, wall=784
2024-02-10 14:00:30 | INFO | train_inner | epoch 007:    494 / 685 loss=4.478, nll_loss=3.049, ppl=8.28, wps=88977.8, ups=6.22, wpb=14311.7, bsz=709.4, num_updates=4600, lr=0.00046, gnorm=0.822, loss_scale=8, train_wall=16, gb_free=28.3, wall=800
2024-02-10 14:00:47 | INFO | train_inner | epoch 007:    594 / 685 loss=4.363, nll_loss=2.918, ppl=7.56, wps=85918.3, ups=5.89, wpb=14577.2, bsz=771.4, num_updates=4700, lr=0.00047, gnorm=0.766, loss_scale=8, train_wall=17, gb_free=28.5, wall=817
2024-02-10 14:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:01:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:01:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.25 | nll_loss 2.623 | ppl 6.16 | wps 130646 | wpb 5716.3 | bsz 292.8 | num_updates 4791 | best_loss 4.25
2024-02-10 14:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4791 updates
2024-02-10 14:01:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint7.pt
2024-02-10 14:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint7.pt
2024-02-10 14:01:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint7.pt (epoch 7 @ 4791 updates, score 4.25) (writing took 4.085801437031478 seconds)
2024-02-10 14:01:09 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-02-10 14:01:09 | INFO | train | epoch 007 | loss 4.483 | nll_loss 3.055 | ppl 8.31 | wps 81800.9 | ups 5.63 | wpb 14524.4 | bsz 748.5 | num_updates 4791 | lr 0.0004791 | gnorm 0.82 | loss_scale 8 | train_wall 113 | gb_free 28.2 | wall 839
2024-02-10 14:01:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:01:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:01:09 | INFO | fairseq.trainer | begin training epoch 8
2024-02-10 14:01:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:01:11 | INFO | train_inner | epoch 008:      9 / 685 loss=4.366, nll_loss=2.922, ppl=7.58, wps=61186.8, ups=4.15, wpb=14745.1, bsz=773.1, num_updates=4800, lr=0.00048, gnorm=0.799, loss_scale=8, train_wall=17, gb_free=28.2, wall=841
2024-02-10 14:01:28 | INFO | train_inner | epoch 008:    109 / 685 loss=4.272, nll_loss=2.815, ppl=7.03, wps=86871.8, ups=6.1, wpb=14245.8, bsz=723, num_updates=4900, lr=0.00049, gnorm=0.816, loss_scale=8, train_wall=16, gb_free=28.3, wall=857
2024-02-10 14:01:44 | INFO | train_inner | epoch 008:    209 / 685 loss=4.254, nll_loss=2.793, ppl=6.93, wps=87804.5, ups=6.06, wpb=14492.1, bsz=748.2, num_updates=5000, lr=0.0005, gnorm=0.788, loss_scale=8, train_wall=16, gb_free=28.2, wall=874
2024-02-10 14:02:00 | INFO | train_inner | epoch 008:    309 / 685 loss=4.208, nll_loss=2.741, ppl=6.68, wps=91231.5, ups=6.17, wpb=14775.2, bsz=743, num_updates=5100, lr=0.00051, gnorm=0.763, loss_scale=8, train_wall=16, gb_free=28.5, wall=890
2024-02-10 14:02:16 | INFO | train_inner | epoch 008:    409 / 685 loss=4.216, nll_loss=2.751, ppl=6.73, wps=89226.4, ups=6.18, wpb=14441.6, bsz=752.2, num_updates=5200, lr=0.00052, gnorm=0.794, loss_scale=8, train_wall=16, gb_free=28.3, wall=906
2024-02-10 14:02:33 | INFO | train_inner | epoch 008:    509 / 685 loss=4.165, nll_loss=2.692, ppl=6.46, wps=89844.2, ups=6.16, wpb=14580.8, bsz=773.4, num_updates=5300, lr=0.00053, gnorm=0.76, loss_scale=8, train_wall=16, gb_free=28.4, wall=922
2024-02-10 14:02:49 | INFO | train_inner | epoch 008:    609 / 685 loss=4.15, nll_loss=2.676, ppl=6.39, wps=90230.3, ups=6.14, wpb=14683.9, bsz=750.5, num_updates=5400, lr=0.00054, gnorm=0.771, loss_scale=8, train_wall=16, gb_free=28.3, wall=939
2024-02-10 14:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:03:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:03:03 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.082 | nll_loss 2.437 | ppl 5.42 | wps 157816 | wpb 5716.3 | bsz 292.8 | num_updates 5476 | best_loss 4.082
2024-02-10 14:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5476 updates
2024-02-10 14:03:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint8.pt
2024-02-10 14:03:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint8.pt
2024-02-10 14:03:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint8.pt (epoch 8 @ 5476 updates, score 4.082) (writing took 3.650509878061712 seconds)
2024-02-10 14:03:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-02-10 14:03:07 | INFO | train | epoch 008 | loss 4.204 | nll_loss 2.737 | ppl 6.67 | wps 84684.1 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 5476 | lr 0.0005476 | gnorm 0.781 | loss_scale 8 | train_wall 110 | gb_free 28.3 | wall 956
2024-02-10 14:03:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:03:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:03:07 | INFO | fairseq.trainer | begin training epoch 9
2024-02-10 14:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:03:11 | INFO | train_inner | epoch 009:     24 / 685 loss=4.119, nll_loss=2.641, ppl=6.24, wps=65398.7, ups=4.52, wpb=14462.7, bsz=749.8, num_updates=5500, lr=0.00055, gnorm=0.767, loss_scale=8, train_wall=16, gb_free=28.3, wall=961
2024-02-10 14:03:27 | INFO | train_inner | epoch 009:    124 / 685 loss=4.002, nll_loss=2.507, ppl=5.68, wps=89630.7, ups=6.16, wpb=14541.1, bsz=745.6, num_updates=5600, lr=0.00056, gnorm=0.717, loss_scale=8, train_wall=16, gb_free=28.7, wall=977
2024-02-10 14:03:43 | INFO | train_inner | epoch 009:    224 / 685 loss=4.047, nll_loss=2.557, ppl=5.89, wps=88985.1, ups=6.2, wpb=14362.8, bsz=722.2, num_updates=5700, lr=0.00057, gnorm=0.788, loss_scale=8, train_wall=16, gb_free=28.3, wall=993
2024-02-10 14:04:00 | INFO | train_inner | epoch 009:    324 / 685 loss=4.022, nll_loss=2.529, ppl=5.77, wps=89096.7, ups=6.16, wpb=14452.8, bsz=753, num_updates=5800, lr=0.00058, gnorm=0.743, loss_scale=8, train_wall=16, gb_free=28.3, wall=1009
2024-02-10 14:04:16 | INFO | train_inner | epoch 009:    424 / 685 loss=3.999, nll_loss=2.504, ppl=5.67, wps=90348.6, ups=6.19, wpb=14599.9, bsz=738.6, num_updates=5900, lr=0.00059, gnorm=0.753, loss_scale=8, train_wall=16, gb_free=28.4, wall=1025
2024-02-10 14:04:32 | INFO | train_inner | epoch 009:    524 / 685 loss=3.98, nll_loss=2.483, ppl=5.59, wps=88689.4, ups=6.04, wpb=14686.8, bsz=750.8, num_updates=6000, lr=0.0006, gnorm=0.73, loss_scale=8, train_wall=16, gb_free=28.3, wall=1042
2024-02-10 14:04:49 | INFO | train_inner | epoch 009:    624 / 685 loss=3.97, nll_loss=2.472, ppl=5.55, wps=86840, ups=5.97, wpb=14534.3, bsz=771.3, num_updates=6100, lr=0.00061, gnorm=0.748, loss_scale=8, train_wall=16, gb_free=28.3, wall=1059
2024-02-10 14:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:04:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:05:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.997 | nll_loss 2.339 | ppl 5.06 | wps 142163 | wpb 5716.3 | bsz 292.8 | num_updates 6161 | best_loss 3.997
2024-02-10 14:05:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 6161 updates
2024-02-10 14:05:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint9.pt
2024-02-10 14:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint9.pt
2024-02-10 14:05:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint9.pt (epoch 9 @ 6161 updates, score 3.997) (writing took 3.9410267621278763 seconds)
2024-02-10 14:05:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-02-10 14:05:05 | INFO | train | epoch 009 | loss 4.002 | nll_loss 2.507 | ppl 5.68 | wps 83996.4 | ups 5.78 | wpb 14524.4 | bsz 748.5 | num_updates 6161 | lr 0.0006161 | gnorm 0.746 | loss_scale 8 | train_wall 110 | gb_free 28.2 | wall 1075
2024-02-10 14:05:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:05:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:05:05 | INFO | fairseq.trainer | begin training epoch 10
2024-02-10 14:05:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:05:12 | INFO | train_inner | epoch 010:     39 / 685 loss=3.93, nll_loss=2.427, ppl=5.38, wps=63818.2, ups=4.4, wpb=14494.3, bsz=740.4, num_updates=6200, lr=0.00062, gnorm=0.74, loss_scale=8, train_wall=16, gb_free=28.3, wall=1081
2024-02-10 14:05:28 | INFO | train_inner | epoch 010:    139 / 685 loss=3.865, nll_loss=2.351, ppl=5.1, wps=86951.9, ups=6.09, wpb=14269.4, bsz=797.8, num_updates=6300, lr=0.00063, gnorm=0.758, loss_scale=8, train_wall=16, gb_free=28.3, wall=1098
2024-02-10 14:05:44 | INFO | train_inner | epoch 010:    239 / 685 loss=3.854, nll_loss=2.337, ppl=5.05, wps=89541.9, ups=6.18, wpb=14484.6, bsz=714.6, num_updates=6400, lr=0.00064, gnorm=0.709, loss_scale=8, train_wall=16, gb_free=28.3, wall=1114
2024-02-10 14:06:01 | INFO | train_inner | epoch 010:    339 / 685 loss=3.861, nll_loss=2.347, ppl=5.09, wps=89082.7, ups=6.15, wpb=14480.6, bsz=743.4, num_updates=6500, lr=0.00065, gnorm=0.718, loss_scale=8, train_wall=16, gb_free=28.4, wall=1130
2024-02-10 14:06:17 | INFO | train_inner | epoch 010:    439 / 685 loss=3.849, nll_loss=2.334, ppl=5.04, wps=90955.5, ups=6.16, wpb=14757.5, bsz=757.3, num_updates=6600, lr=0.00066, gnorm=0.7, loss_scale=8, train_wall=16, gb_free=28.4, wall=1146
2024-02-10 14:06:33 | INFO | train_inner | epoch 010:    539 / 685 loss=3.855, nll_loss=2.341, ppl=5.07, wps=90540.6, ups=6.17, wpb=14671.5, bsz=748.4, num_updates=6700, lr=0.00067, gnorm=0.722, loss_scale=8, train_wall=16, gb_free=28.3, wall=1163
2024-02-10 14:06:49 | INFO | train_inner | epoch 010:    639 / 685 loss=3.86, nll_loss=2.347, ppl=5.09, wps=89665.2, ups=6.14, wpb=14594.9, bsz=744.6, num_updates=6800, lr=0.00068, gnorm=0.73, loss_scale=8, train_wall=16, gb_free=28.3, wall=1179
2024-02-10 14:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:06:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:06:59 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.94 | nll_loss 2.293 | ppl 4.9 | wps 160204 | wpb 5716.3 | bsz 292.8 | num_updates 6846 | best_loss 3.94
2024-02-10 14:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6846 updates
2024-02-10 14:06:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt
2024-02-10 14:07:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt
2024-02-10 14:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt (epoch 10 @ 6846 updates, score 3.94) (writing took 3.728152801981196 seconds)
2024-02-10 14:07:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-02-10 14:07:02 | INFO | train | epoch 010 | loss 3.857 | nll_loss 2.343 | ppl 5.07 | wps 84935.3 | ups 5.85 | wpb 14524.4 | bsz 748.5 | num_updates 6846 | lr 0.0006846 | gnorm 0.726 | loss_scale 8 | train_wall 110 | gb_free 28.4 | wall 1192
2024-02-10 14:07:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:07:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:07:02 | INFO | fairseq.trainer | begin training epoch 11
2024-02-10 14:07:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:07:11 | INFO | train_inner | epoch 011:     54 / 685 loss=3.8, nll_loss=2.278, ppl=4.85, wps=65266.9, ups=4.54, wpb=14389.3, bsz=752, num_updates=6900, lr=0.00069, gnorm=0.734, loss_scale=8, train_wall=16, gb_free=28.4, wall=1201
2024-02-10 14:07:28 | INFO | train_inner | epoch 011:    154 / 685 loss=3.724, nll_loss=2.19, ppl=4.56, wps=88774.1, ups=6.08, wpb=14603.9, bsz=731.6, num_updates=7000, lr=0.0007, gnorm=0.718, loss_scale=8, train_wall=16, gb_free=28.5, wall=1217
2024-02-10 14:07:45 | INFO | train_inner | epoch 011:    254 / 685 loss=3.75, nll_loss=2.221, ppl=4.66, wps=86367.5, ups=5.95, wpb=14517, bsz=745.8, num_updates=7100, lr=0.00071, gnorm=0.724, loss_scale=8, train_wall=17, gb_free=28.4, wall=1234
2024-02-10 14:08:01 | INFO | train_inner | epoch 011:    354 / 685 loss=3.791, nll_loss=2.269, ppl=4.82, wps=86848, ups=6.03, wpb=14399.7, bsz=743.7, num_updates=7200, lr=0.00072, gnorm=0.761, loss_scale=8, train_wall=16, gb_free=28.3, wall=1251
2024-02-10 14:08:18 | INFO | train_inner | epoch 011:    454 / 685 loss=3.735, nll_loss=2.205, ppl=4.61, wps=89707.3, ups=6.11, wpb=14674.9, bsz=777.4, num_updates=7300, lr=0.00073, gnorm=0.703, loss_scale=8, train_wall=16, gb_free=28.6, wall=1267
2024-02-10 14:08:34 | INFO | train_inner | epoch 011:    554 / 685 loss=3.774, nll_loss=2.251, ppl=4.76, wps=88782.8, ups=6.18, wpb=14362.9, bsz=749.5, num_updates=7400, lr=0.00074, gnorm=0.741, loss_scale=8, train_wall=16, gb_free=28.5, wall=1283
2024-02-10 14:08:50 | INFO | train_inner | epoch 011:    654 / 685 loss=3.78, nll_loss=2.258, ppl=4.78, wps=89040.1, ups=6.16, wpb=14456.9, bsz=739, num_updates=7500, lr=0.00075, gnorm=0.762, loss_scale=8, train_wall=16, gb_free=28.4, wall=1300
2024-02-10 14:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:08:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:08:57 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.878 | nll_loss 2.232 | ppl 4.7 | wps 150262 | wpb 5716.3 | bsz 292.8 | num_updates 7531 | best_loss 3.878
2024-02-10 14:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 7531 updates
2024-02-10 14:08:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt
2024-02-10 14:08:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt
2024-02-10 14:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt (epoch 11 @ 7531 updates, score 3.878) (writing took 4.06404193607159 seconds)
2024-02-10 14:09:01 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-02-10 14:09:01 | INFO | train | epoch 011 | loss 3.757 | nll_loss 2.23 | ppl 4.69 | wps 83673.9 | ups 5.76 | wpb 14524.4 | bsz 748.5 | num_updates 7531 | lr 0.0007531 | gnorm 0.732 | loss_scale 8 | train_wall 111 | gb_free 28.3 | wall 1311
2024-02-10 14:09:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:09:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:09:01 | INFO | fairseq.trainer | begin training epoch 12
2024-02-10 14:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:09:13 | INFO | train_inner | epoch 012:     69 / 685 loss=3.669, nll_loss=2.128, ppl=4.37, wps=63730.5, ups=4.32, wpb=14760.7, bsz=752.2, num_updates=7600, lr=0.00076, gnorm=0.69, loss_scale=8, train_wall=17, gb_free=28.2, wall=1323
2024-02-10 14:09:30 | INFO | train_inner | epoch 012:    169 / 685 loss=3.662, nll_loss=2.122, ppl=4.35, wps=87499.8, ups=6.04, wpb=14478.4, bsz=765.8, num_updates=7700, lr=0.00077, gnorm=0.732, loss_scale=8, train_wall=16, gb_free=28.4, wall=1339
2024-02-10 14:09:46 | INFO | train_inner | epoch 012:    269 / 685 loss=3.662, nll_loss=2.121, ppl=4.35, wps=90404.1, ups=6.14, wpb=14712.8, bsz=731.4, num_updates=7800, lr=0.00078, gnorm=0.698, loss_scale=8, train_wall=16, gb_free=28.2, wall=1356
2024-02-10 14:10:02 | INFO | train_inner | epoch 012:    369 / 685 loss=3.666, nll_loss=2.126, ppl=4.37, wps=89091.1, ups=6.16, wpb=14471.7, bsz=749.4, num_updates=7900, lr=0.00079, gnorm=0.72, loss_scale=8, train_wall=16, gb_free=28.3, wall=1372
2024-02-10 14:10:18 | INFO | train_inner | epoch 012:    469 / 685 loss=3.694, nll_loss=2.16, ppl=4.47, wps=87742.9, ups=6.17, wpb=14221.3, bsz=761.5, num_updates=8000, lr=0.0008, gnorm=0.757, loss_scale=8, train_wall=16, gb_free=28.3, wall=1388
2024-02-10 14:10:35 | INFO | train_inner | epoch 012:    569 / 685 loss=3.708, nll_loss=2.176, ppl=4.52, wps=91324.9, ups=6.17, wpb=14804.6, bsz=751.4, num_updates=8100, lr=0.00081, gnorm=0.723, loss_scale=8, train_wall=16, gb_free=28.5, wall=1404
2024-02-10 14:10:51 | INFO | train_inner | epoch 012:    669 / 685 loss=3.711, nll_loss=2.18, ppl=4.53, wps=89206.6, ups=6.2, wpb=14398.7, bsz=718.7, num_updates=8200, lr=0.00082, gnorm=0.731, loss_scale=8, train_wall=16, gb_free=28.3, wall=1420
2024-02-10 14:10:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:10:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:10:55 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.811 | nll_loss 2.172 | ppl 4.51 | wps 158644 | wpb 5716.3 | bsz 292.8 | num_updates 8216 | best_loss 3.811
2024-02-10 14:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 8216 updates
2024-02-10 14:10:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt
2024-02-10 14:10:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt
2024-02-10 14:10:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt (epoch 12 @ 8216 updates, score 3.811) (writing took 3.634624009951949 seconds)
2024-02-10 14:10:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-02-10 14:10:59 | INFO | train | epoch 012 | loss 3.679 | nll_loss 2.142 | ppl 4.41 | wps 84559.2 | ups 5.82 | wpb 14524.4 | bsz 748.5 | num_updates 8216 | lr 0.0008216 | gnorm 0.722 | loss_scale 8 | train_wall 110 | gb_free 28.4 | wall 1429
2024-02-10 14:10:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:10:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:10:59 | INFO | fairseq.trainer | begin training epoch 13
2024-02-10 14:10:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:11:13 | INFO | train_inner | epoch 013:     84 / 685 loss=3.613, nll_loss=2.064, ppl=4.18, wps=65045.9, ups=4.55, wpb=14294.7, bsz=708.5, num_updates=8300, lr=0.00083, gnorm=0.727, loss_scale=8, train_wall=16, gb_free=28.6, wall=1442
2024-02-10 14:11:29 | INFO | train_inner | epoch 013:    184 / 685 loss=3.589, nll_loss=2.038, ppl=4.11, wps=90063.5, ups=6.14, wpb=14658.9, bsz=743.1, num_updates=8400, lr=0.00084, gnorm=0.707, loss_scale=8, train_wall=16, gb_free=28.4, wall=1459
2024-02-10 14:11:45 | INFO | train_inner | epoch 013:    284 / 685 loss=3.628, nll_loss=2.083, ppl=4.24, wps=89226.5, ups=6.19, wpb=14422.5, bsz=729.6, num_updates=8500, lr=0.00085, gnorm=0.744, loss_scale=16, train_wall=16, gb_free=28.4, wall=1475
2024-02-10 14:12:01 | INFO | train_inner | epoch 013:    384 / 685 loss=3.637, nll_loss=2.094, ppl=4.27, wps=90133.1, ups=6.15, wpb=14656.9, bsz=735.8, num_updates=8600, lr=0.00086, gnorm=0.757, loss_scale=16, train_wall=16, gb_free=28.5, wall=1491
2024-02-10 14:12:18 | INFO | train_inner | epoch 013:    484 / 685 loss=3.629, nll_loss=2.087, ppl=4.25, wps=89147.3, ups=6.13, wpb=14533.9, bsz=769.6, num_updates=8700, lr=0.00087, gnorm=0.71, loss_scale=16, train_wall=16, gb_free=28.4, wall=1507
2024-02-10 14:12:34 | INFO | train_inner | epoch 013:    584 / 685 loss=3.64, nll_loss=2.1, ppl=4.29, wps=89233.7, ups=6.16, wpb=14484.7, bsz=756.2, num_updates=8800, lr=0.00088, gnorm=0.728, loss_scale=16, train_wall=16, gb_free=28.3, wall=1524
2024-02-10 14:12:50 | INFO | train_inner | epoch 013:    684 / 685 loss=3.62, nll_loss=2.077, ppl=4.22, wps=89400.4, ups=6.15, wpb=14546, bsz=796.5, num_updates=8900, lr=0.00089, gnorm=0.722, loss_scale=16, train_wall=16, gb_free=28.3, wall=1540
2024-02-10 14:12:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:12:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:12:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.776 | nll_loss 2.141 | ppl 4.41 | wps 158171 | wpb 5716.3 | bsz 292.8 | num_updates 8901 | best_loss 3.776
2024-02-10 14:12:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 8901 updates
2024-02-10 14:12:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt
2024-02-10 14:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt
2024-02-10 14:12:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt (epoch 13 @ 8901 updates, score 3.776) (writing took 3.6115285239648074 seconds)
2024-02-10 14:12:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-02-10 14:12:56 | INFO | train | epoch 013 | loss 3.62 | nll_loss 2.075 | ppl 4.21 | wps 84980.8 | ups 5.85 | wpb 14524.4 | bsz 748.5 | num_updates 8901 | lr 0.0008901 | gnorm 0.727 | loss_scale 16 | train_wall 110 | gb_free 28.3 | wall 1546
2024-02-10 14:12:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:12:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:12:56 | INFO | fairseq.trainer | begin training epoch 14
2024-02-10 14:12:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:13:12 | INFO | train_inner | epoch 014:     99 / 685 loss=3.535, nll_loss=1.977, ppl=3.94, wps=65604.4, ups=4.53, wpb=14485.6, bsz=746.5, num_updates=9000, lr=0.0009, gnorm=0.736, loss_scale=16, train_wall=16, gb_free=28.4, wall=1562
2024-02-10 14:13:29 | INFO | train_inner | epoch 014:    199 / 685 loss=3.569, nll_loss=2.016, ppl=4.04, wps=88395.7, ups=6.15, wpb=14370.5, bsz=751.6, num_updates=9100, lr=0.00091, gnorm=0.753, loss_scale=16, train_wall=16, gb_free=28.3, wall=1578
2024-02-10 14:13:45 | INFO | train_inner | epoch 014:    299 / 685 loss=3.562, nll_loss=2.01, ppl=4.03, wps=88482.7, ups=6.17, wpb=14349.4, bsz=737.7, num_updates=9200, lr=0.00092, gnorm=0.728, loss_scale=16, train_wall=16, gb_free=28.3, wall=1594
2024-02-10 14:14:01 | INFO | train_inner | epoch 014:    399 / 685 loss=3.603, nll_loss=2.056, ppl=4.16, wps=86966.9, ups=6.03, wpb=14415, bsz=734.2, num_updates=9300, lr=0.00093, gnorm=0.771, loss_scale=16, train_wall=16, gb_free=28.3, wall=1611
2024-02-10 14:14:18 | INFO | train_inner | epoch 014:    499 / 685 loss=3.615, nll_loss=2.071, ppl=4.2, wps=87786.6, ups=5.98, wpb=14679.4, bsz=743.2, num_updates=9400, lr=0.00094, gnorm=0.765, loss_scale=16, train_wall=16, gb_free=28.3, wall=1628
2024-02-10 14:14:34 | INFO | train_inner | epoch 014:    599 / 685 loss=3.602, nll_loss=2.058, ppl=4.16, wps=90739.1, ups=6.15, wpb=14755, bsz=764.3, num_updates=9500, lr=0.00095, gnorm=0.725, loss_scale=16, train_wall=16, gb_free=28.3, wall=1644
2024-02-10 14:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:14:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:14:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.739 | nll_loss 2.106 | ppl 4.3 | wps 159139 | wpb 5716.3 | bsz 292.8 | num_updates 9586 | best_loss 3.739
2024-02-10 14:14:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 9586 updates
2024-02-10 14:14:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt
2024-02-10 14:14:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt
2024-02-10 14:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt (epoch 14 @ 9586 updates, score 3.739) (writing took 3.6940039570908993 seconds)
2024-02-10 14:14:54 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-02-10 14:14:54 | INFO | train | epoch 014 | loss 3.583 | nll_loss 2.034 | ppl 4.09 | wps 84361.2 | ups 5.81 | wpb 14524.4 | bsz 748.5 | num_updates 9586 | lr 0.0009586 | gnorm 0.744 | loss_scale 16 | train_wall 110 | gb_free 28.4 | wall 1664
2024-02-10 14:14:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:14:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:14:54 | INFO | fairseq.trainer | begin training epoch 15
2024-02-10 14:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:14:56 | INFO | train_inner | epoch 015:     14 / 685 loss=3.579, nll_loss=2.031, ppl=4.09, wps=66365.2, ups=4.54, wpb=14632.4, bsz=762.3, num_updates=9600, lr=0.00096, gnorm=0.73, loss_scale=16, train_wall=16, gb_free=28.5, wall=1666
2024-02-10 14:15:13 | INFO | train_inner | epoch 015:    114 / 685 loss=3.509, nll_loss=1.947, ppl=3.86, wps=89036.3, ups=6.16, wpb=14448.1, bsz=739.8, num_updates=9700, lr=0.00097, gnorm=0.756, loss_scale=16, train_wall=16, gb_free=28.4, wall=1682
2024-02-10 14:15:29 | INFO | train_inner | epoch 015:    214 / 685 loss=3.537, nll_loss=1.979, ppl=3.94, wps=87174.9, ups=6.08, wpb=14334.8, bsz=743.7, num_updates=9800, lr=0.00098, gnorm=0.76, loss_scale=16, train_wall=16, gb_free=28.4, wall=1699
2024-02-10 14:15:47 | INFO | train_inner | epoch 015:    314 / 685 loss=3.553, nll_loss=2.001, ppl=4, wps=84784.4, ups=5.74, wpb=14767, bsz=762.9, num_updates=9900, lr=0.00099, gnorm=0.748, loss_scale=16, train_wall=17, gb_free=28.2, wall=1716
2024-02-10 14:16:03 | INFO | train_inner | epoch 015:    414 / 685 loss=3.552, nll_loss=2, ppl=4, wps=87862.9, ups=5.95, wpb=14773.6, bsz=751.2, num_updates=10000, lr=0.001, gnorm=0.743, loss_scale=16, train_wall=17, gb_free=28.5, wall=1733
2024-02-10 14:16:20 | INFO | train_inner | epoch 015:    514 / 685 loss=3.561, nll_loss=2.01, ppl=4.03, wps=88504.7, ups=6.17, wpb=14340.8, bsz=740.1, num_updates=10100, lr=0.000999997, gnorm=0.751, loss_scale=16, train_wall=16, gb_free=28.2, wall=1749
2024-02-10 14:16:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-10 14:16:36 | INFO | train_inner | epoch 015:    615 / 685 loss=3.566, nll_loss=2.017, ppl=4.05, wps=88460.4, ups=6.1, wpb=14499.1, bsz=749.2, num_updates=10200, lr=0.000999988, gnorm=0.754, loss_scale=8, train_wall=16, gb_free=28.3, wall=1766
2024-02-10 14:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:16:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:16:49 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.743 | nll_loss 2.122 | ppl 4.35 | wps 151286 | wpb 5716.3 | bsz 292.8 | num_updates 10270 | best_loss 3.739
2024-02-10 14:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 10270 updates
2024-02-10 14:16:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt
2024-02-10 14:16:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt
2024-02-10 14:16:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt (epoch 15 @ 10270 updates, score 3.743) (writing took 2.5423264470882714 seconds)
2024-02-10 14:16:52 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-02-10 14:16:52 | INFO | train | epoch 015 | loss 3.549 | nll_loss 1.996 | ppl 3.99 | wps 84258 | ups 5.8 | wpb 14524.7 | bsz 748.5 | num_updates 10270 | lr 0.000999978 | gnorm 0.755 | loss_scale 8 | train_wall 111 | gb_free 28.3 | wall 1782
2024-02-10 14:16:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:16:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:16:52 | INFO | fairseq.trainer | begin training epoch 16
2024-02-10 14:16:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:16:57 | INFO | train_inner | epoch 016:     30 / 685 loss=3.55, nll_loss=1.998, ppl=4, wps=67905.1, ups=4.67, wpb=14547.4, bsz=751.5, num_updates=10300, lr=0.000999973, gnorm=0.77, loss_scale=8, train_wall=16, gb_free=28.6, wall=1787
2024-02-10 14:17:15 | INFO | train_inner | epoch 016:    130 / 685 loss=3.49, nll_loss=1.927, ppl=3.8, wps=83813.4, ups=5.82, wpb=14402, bsz=738.2, num_updates=10400, lr=0.000999951, gnorm=0.757, loss_scale=8, train_wall=17, gb_free=28.5, wall=1804
2024-02-10 14:17:32 | INFO | train_inner | epoch 016:    230 / 685 loss=3.472, nll_loss=1.91, ppl=3.76, wps=82884.7, ups=5.7, wpb=14544.4, bsz=769, num_updates=10500, lr=0.000999924, gnorm=0.741, loss_scale=8, train_wall=17, gb_free=28.3, wall=1822
2024-02-10 14:17:50 | INFO | train_inner | epoch 016:    330 / 685 loss=3.517, nll_loss=1.96, ppl=3.89, wps=83882.9, ups=5.75, wpb=14599.1, bsz=739.7, num_updates=10600, lr=0.00099989, gnorm=0.743, loss_scale=8, train_wall=17, gb_free=28.3, wall=1839
2024-02-10 14:18:06 | INFO | train_inner | epoch 016:    430 / 685 loss=3.503, nll_loss=1.946, ppl=3.85, wps=88734.5, ups=6.03, wpb=14717.4, bsz=762.5, num_updates=10700, lr=0.000999851, gnorm=0.733, loss_scale=8, train_wall=16, gb_free=28.3, wall=1856
2024-02-10 14:18:23 | INFO | train_inner | epoch 016:    530 / 685 loss=3.515, nll_loss=1.96, ppl=3.89, wps=85734.2, ups=6.02, wpb=14230.1, bsz=747.7, num_updates=10800, lr=0.000999805, gnorm=0.796, loss_scale=8, train_wall=16, gb_free=28.5, wall=1872
2024-02-10 14:18:39 | INFO | train_inner | epoch 016:    630 / 685 loss=3.497, nll_loss=1.939, ppl=3.83, wps=86578, ups=5.96, wpb=14535.4, bsz=739.5, num_updates=10900, lr=0.000999753, gnorm=0.715, loss_scale=8, train_wall=17, gb_free=28.3, wall=1889
2024-02-10 14:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:18:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:18:50 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.709 | nll_loss 2.078 | ppl 4.22 | wps 156583 | wpb 5716.3 | bsz 292.8 | num_updates 10955 | best_loss 3.709
2024-02-10 14:18:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 10955 updates
2024-02-10 14:18:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt
2024-02-10 14:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt
2024-02-10 14:18:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt (epoch 16 @ 10955 updates, score 3.709) (writing took 3.6710523508954793 seconds)
2024-02-10 14:18:54 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-02-10 14:18:54 | INFO | train | epoch 016 | loss 3.498 | nll_loss 1.939 | ppl 3.83 | wps 81413.2 | ups 5.61 | wpb 14524.4 | bsz 748.5 | num_updates 10955 | lr 0.000999722 | gnorm 0.745 | loss_scale 8 | train_wall 115 | gb_free 28.3 | wall 1904
2024-02-10 14:18:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:18:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:18:54 | INFO | fairseq.trainer | begin training epoch 17
2024-02-10 14:18:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:19:02 | INFO | train_inner | epoch 017:     45 / 685 loss=3.441, nll_loss=1.875, ppl=3.67, wps=65828.6, ups=4.51, wpb=14597, bsz=753.8, num_updates=11000, lr=0.000999695, gnorm=0.743, loss_scale=8, train_wall=16, gb_free=28.4, wall=1911
2024-02-10 14:19:18 | INFO | train_inner | epoch 017:    145 / 685 loss=3.432, nll_loss=1.863, ppl=3.64, wps=90158.5, ups=6.17, wpb=14604.1, bsz=750, num_updates=11100, lr=0.000999631, gnorm=0.773, loss_scale=8, train_wall=16, gb_free=28.2, wall=1927
2024-02-10 14:19:34 | INFO | train_inner | epoch 017:    245 / 685 loss=3.441, nll_loss=1.874, ppl=3.67, wps=90309.1, ups=6.18, wpb=14620.4, bsz=752.9, num_updates=11200, lr=0.000999561, gnorm=0.726, loss_scale=8, train_wall=16, gb_free=28.3, wall=1944
2024-02-10 14:19:50 | INFO | train_inner | epoch 017:    345 / 685 loss=3.461, nll_loss=1.898, ppl=3.73, wps=89911.7, ups=6.19, wpb=14529.1, bsz=744.9, num_updates=11300, lr=0.000999485, gnorm=0.789, loss_scale=8, train_wall=16, gb_free=28.3, wall=1960
2024-02-10 14:20:06 | INFO | train_inner | epoch 017:    445 / 685 loss=3.438, nll_loss=1.873, ppl=3.66, wps=89501.6, ups=6.17, wpb=14514.9, bsz=750, num_updates=11400, lr=0.000999403, gnorm=0.721, loss_scale=8, train_wall=16, gb_free=28.7, wall=1976
2024-02-10 14:20:22 | INFO | train_inner | epoch 017:    545 / 685 loss=3.445, nll_loss=1.88, ppl=3.68, wps=89615.3, ups=6.23, wpb=14394.6, bsz=756.6, num_updates=11500, lr=0.000999315, gnorm=0.788, loss_scale=8, train_wall=16, gb_free=28.4, wall=1992
2024-02-10 14:20:39 | INFO | train_inner | epoch 017:    645 / 685 loss=3.444, nll_loss=1.881, ppl=3.68, wps=88824.6, ups=6.2, wpb=14330, bsz=739.8, num_updates=11600, lr=0.00099922, gnorm=0.745, loss_scale=8, train_wall=16, gb_free=28.3, wall=2008
2024-02-10 14:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:20:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:20:47 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.663 | nll_loss 2.033 | ppl 4.09 | wps 158177 | wpb 5716.3 | bsz 292.8 | num_updates 11640 | best_loss 3.663
2024-02-10 14:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 11640 updates
2024-02-10 14:20:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt
2024-02-10 14:20:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt
2024-02-10 14:20:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt (epoch 17 @ 11640 updates, score 3.663) (writing took 3.885144801111892 seconds)
2024-02-10 14:20:51 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-02-10 14:20:51 | INFO | train | epoch 017 | loss 3.44 | nll_loss 1.874 | ppl 3.67 | wps 85140.1 | ups 5.86 | wpb 14524.4 | bsz 748.5 | num_updates 11640 | lr 0.000999181 | gnorm 0.754 | loss_scale 8 | train_wall 109 | gb_free 28.5 | wall 2021
2024-02-10 14:20:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:20:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:20:51 | INFO | fairseq.trainer | begin training epoch 18
2024-02-10 14:20:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:21:01 | INFO | train_inner | epoch 018:     60 / 685 loss=3.382, nll_loss=1.807, ppl=3.5, wps=65037.7, ups=4.5, wpb=14465.9, bsz=725.2, num_updates=11700, lr=0.00099912, gnorm=0.706, loss_scale=8, train_wall=16, gb_free=28.5, wall=2030
2024-02-10 14:21:17 | INFO | train_inner | epoch 018:    160 / 685 loss=3.358, nll_loss=1.779, ppl=3.43, wps=89404.3, ups=6.25, wpb=14296.2, bsz=690.8, num_updates=11800, lr=0.000999013, gnorm=0.723, loss_scale=8, train_wall=16, gb_free=28.2, wall=2046
2024-02-10 14:21:33 | INFO | train_inner | epoch 018:    260 / 685 loss=3.388, nll_loss=1.815, ppl=3.52, wps=90784, ups=6.17, wpb=14705.9, bsz=753, num_updates=11900, lr=0.000998901, gnorm=0.754, loss_scale=8, train_wall=16, gb_free=28.3, wall=2063
2024-02-10 14:21:49 | INFO | train_inner | epoch 018:    360 / 685 loss=3.39, nll_loss=1.818, ppl=3.53, wps=87690.7, ups=6.1, wpb=14375.8, bsz=761.5, num_updates=12000, lr=0.000998782, gnorm=0.713, loss_scale=8, train_wall=16, gb_free=28.3, wall=2079
2024-02-10 14:22:06 | INFO | train_inner | epoch 018:    460 / 685 loss=3.387, nll_loss=1.817, ppl=3.52, wps=88390.1, ups=5.96, wpb=14836.9, bsz=787.3, num_updates=12100, lr=0.000998657, gnorm=0.708, loss_scale=8, train_wall=17, gb_free=28.3, wall=2096
2024-02-10 14:22:23 | INFO | train_inner | epoch 018:    560 / 685 loss=3.408, nll_loss=1.84, ppl=3.58, wps=87904.5, ups=6.01, wpb=14614.6, bsz=754.2, num_updates=12200, lr=0.000998526, gnorm=0.749, loss_scale=8, train_wall=16, gb_free=28.4, wall=2112
2024-02-10 14:22:39 | INFO | train_inner | epoch 018:    660 / 685 loss=3.401, nll_loss=1.832, ppl=3.56, wps=89141.8, ups=6.21, wpb=14355.6, bsz=750.1, num_updates=12300, lr=0.000998389, gnorm=0.753, loss_scale=8, train_wall=16, gb_free=28.4, wall=2129
2024-02-10 14:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:22:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:22:45 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.639 | nll_loss 2.006 | ppl 4.02 | wps 158960 | wpb 5716.3 | bsz 292.8 | num_updates 12325 | best_loss 3.639
2024-02-10 14:22:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 12325 updates
2024-02-10 14:22:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt
2024-02-10 14:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt
2024-02-10 14:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt (epoch 18 @ 12325 updates, score 3.639) (writing took 3.6870447888504714 seconds)
2024-02-10 14:22:49 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-02-10 14:22:49 | INFO | train | epoch 018 | loss 3.383 | nll_loss 1.81 | ppl 3.51 | wps 84516.8 | ups 5.82 | wpb 14524.4 | bsz 748.5 | num_updates 12325 | lr 0.000998354 | gnorm 0.73 | loss_scale 8 | train_wall 110 | gb_free 28.3 | wall 2138
2024-02-10 14:22:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:22:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:22:49 | INFO | fairseq.trainer | begin training epoch 19
2024-02-10 14:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:23:01 | INFO | train_inner | epoch 019:     75 / 685 loss=3.319, nll_loss=1.737, ppl=3.33, wps=66925.6, ups=4.55, wpb=14716.2, bsz=748.3, num_updates=12400, lr=0.000998246, gnorm=0.684, loss_scale=8, train_wall=16, gb_free=28.3, wall=2151
2024-02-10 14:23:17 | INFO | train_inner | epoch 019:    175 / 685 loss=3.316, nll_loss=1.733, ppl=3.33, wps=89786.5, ups=6.23, wpb=14408.3, bsz=726, num_updates=12500, lr=0.000998097, gnorm=0.692, loss_scale=8, train_wall=16, gb_free=28.2, wall=2167
2024-02-10 14:23:33 | INFO | train_inner | epoch 019:    275 / 685 loss=3.329, nll_loss=1.75, ppl=3.36, wps=90355.6, ups=6.19, wpb=14603.2, bsz=750.8, num_updates=12600, lr=0.000997942, gnorm=0.77, loss_scale=8, train_wall=16, gb_free=28.3, wall=2183
2024-02-10 14:23:49 | INFO | train_inner | epoch 019:    375 / 685 loss=3.344, nll_loss=1.768, ppl=3.41, wps=90654.2, ups=6.18, wpb=14661, bsz=760.2, num_updates=12700, lr=0.000997781, gnorm=0.726, loss_scale=8, train_wall=16, gb_free=28.3, wall=2199
2024-02-10 14:24:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-10 14:24:06 | INFO | train_inner | epoch 019:    476 / 685 loss=3.349, nll_loss=1.774, ppl=3.42, wps=88275.9, ups=6.11, wpb=14441.1, bsz=758.9, num_updates=12800, lr=0.000997614, gnorm=0.776, loss_scale=4, train_wall=16, gb_free=28.3, wall=2215
2024-02-10 14:24:22 | INFO | train_inner | epoch 019:    576 / 685 loss=3.34, nll_loss=1.764, ppl=3.4, wps=87705.5, ups=6.07, wpb=14457.2, bsz=750, num_updates=12900, lr=0.00099744, gnorm=0.846, loss_scale=4, train_wall=16, gb_free=28.3, wall=2232
2024-02-10 14:24:39 | INFO | train_inner | epoch 019:    676 / 685 loss=3.346, nll_loss=1.772, ppl=3.41, wps=86928.9, ups=5.98, wpb=14532.9, bsz=761.9, num_updates=13000, lr=0.000997261, gnorm=0.691, loss_scale=4, train_wall=16, gb_free=28.3, wall=2248
2024-02-10 14:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:24:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:24:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.639 | nll_loss 2.026 | ppl 4.07 | wps 153091 | wpb 5716.3 | bsz 292.8 | num_updates 13009 | best_loss 3.639
2024-02-10 14:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 13009 updates
2024-02-10 14:24:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt
2024-02-10 14:24:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt
2024-02-10 14:24:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt (epoch 19 @ 13009 updates, score 3.639) (writing took 3.877492264844477 seconds)
2024-02-10 14:24:46 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-02-10 14:24:46 | INFO | train | epoch 019 | loss 3.333 | nll_loss 1.754 | ppl 3.37 | wps 84470.2 | ups 5.82 | wpb 14522.2 | bsz 748.8 | num_updates 13009 | lr 0.000997245 | gnorm 0.743 | loss_scale 4 | train_wall 110 | gb_free 28.4 | wall 2256
2024-02-10 14:24:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:24:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:24:46 | INFO | fairseq.trainer | begin training epoch 20
2024-02-10 14:24:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:25:01 | INFO | train_inner | epoch 020:     91 / 685 loss=3.267, nll_loss=1.677, ppl=3.2, wps=64896.3, ups=4.5, wpb=14414.6, bsz=721.4, num_updates=13100, lr=0.000997075, gnorm=0.693, loss_scale=4, train_wall=16, gb_free=28.2, wall=2271
2024-02-10 14:25:17 | INFO | train_inner | epoch 020:    191 / 685 loss=3.25, nll_loss=1.659, ppl=3.16, wps=90251.8, ups=6.2, wpb=14547.8, bsz=764.1, num_updates=13200, lr=0.000996884, gnorm=0.694, loss_scale=4, train_wall=16, gb_free=28.3, wall=2287
2024-02-10 14:25:33 | INFO | train_inner | epoch 020:    291 / 685 loss=3.292, nll_loss=1.709, ppl=3.27, wps=89210.4, ups=6.2, wpb=14399.1, bsz=754.3, num_updates=13300, lr=0.000996686, gnorm=0.727, loss_scale=4, train_wall=16, gb_free=28.3, wall=2303
2024-02-10 14:25:49 | INFO | train_inner | epoch 020:    391 / 685 loss=3.302, nll_loss=1.721, ppl=3.3, wps=90131.3, ups=6.23, wpb=14464, bsz=750.9, num_updates=13400, lr=0.000996483, gnorm=0.694, loss_scale=4, train_wall=16, gb_free=28.4, wall=2319
2024-02-10 14:26:05 | INFO | train_inner | epoch 020:    491 / 685 loss=3.301, nll_loss=1.72, ppl=3.29, wps=91654.7, ups=6.23, wpb=14720.9, bsz=745.4, num_updates=13500, lr=0.000996273, gnorm=0.701, loss_scale=4, train_wall=16, gb_free=28.2, wall=2335
2024-02-10 14:26:22 | INFO | train_inner | epoch 020:    591 / 685 loss=3.296, nll_loss=1.717, ppl=3.29, wps=89957.7, ups=6.18, wpb=14547.5, bsz=765.1, num_updates=13600, lr=0.000996057, gnorm=0.749, loss_scale=4, train_wall=16, gb_free=28.5, wall=2351
2024-02-10 14:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:26:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:26:39 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.613 | nll_loss 1.976 | ppl 3.93 | wps 159211 | wpb 5716.3 | bsz 292.8 | num_updates 13694 | best_loss 3.613
2024-02-10 14:26:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 13694 updates
2024-02-10 14:26:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt
2024-02-10 14:26:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt
2024-02-10 14:26:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt (epoch 20 @ 13694 updates, score 3.613) (writing took 3.701613743091002 seconds)
2024-02-10 14:26:42 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-02-10 14:26:42 | INFO | train | epoch 020 | loss 3.288 | nll_loss 1.705 | ppl 3.26 | wps 85681.4 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 13694 | lr 0.000995849 | gnorm 0.708 | loss_scale 4 | train_wall 109 | gb_free 28.3 | wall 2372
2024-02-10 14:26:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:26:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:26:42 | INFO | fairseq.trainer | begin training epoch 21
2024-02-10 14:26:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:26:44 | INFO | train_inner | epoch 021:      6 / 685 loss=3.308, nll_loss=1.729, ppl=3.32, wps=66443, ups=4.56, wpb=14555, bsz=721.3, num_updates=13700, lr=0.000995836, gnorm=0.701, loss_scale=4, train_wall=16, gb_free=28.3, wall=2373
2024-02-10 14:27:00 | INFO | train_inner | epoch 021:    106 / 685 loss=3.197, nll_loss=1.601, ppl=3.03, wps=90489.4, ups=6.19, wpb=14619.3, bsz=754.1, num_updates=13800, lr=0.000995608, gnorm=0.647, loss_scale=4, train_wall=16, gb_free=28.4, wall=2389
2024-02-10 14:27:16 | INFO | train_inner | epoch 021:    206 / 685 loss=3.237, nll_loss=1.646, ppl=3.13, wps=90660.9, ups=6.22, wpb=14569.5, bsz=739.9, num_updates=13900, lr=0.000995374, gnorm=0.669, loss_scale=4, train_wall=16, gb_free=28.4, wall=2405
2024-02-10 14:27:32 | INFO | train_inner | epoch 021:    306 / 685 loss=3.258, nll_loss=1.671, ppl=3.18, wps=91492.6, ups=6.25, wpb=14639.4, bsz=714, num_updates=14000, lr=0.000995134, gnorm=0.703, loss_scale=4, train_wall=16, gb_free=28.3, wall=2421
2024-02-10 14:27:48 | INFO | train_inner | epoch 021:    406 / 685 loss=3.252, nll_loss=1.664, ppl=3.17, wps=89616.5, ups=6.23, wpb=14381.6, bsz=746.6, num_updates=14100, lr=0.000994888, gnorm=0.725, loss_scale=4, train_wall=16, gb_free=28.2, wall=2437
2024-02-10 14:28:04 | INFO | train_inner | epoch 021:    506 / 685 loss=3.252, nll_loss=1.666, ppl=3.17, wps=89029.8, ups=6.21, wpb=14335.5, bsz=770.6, num_updates=14200, lr=0.000994636, gnorm=0.701, loss_scale=4, train_wall=16, gb_free=28.4, wall=2454
2024-02-10 14:28:20 | INFO | train_inner | epoch 021:    606 / 685 loss=3.25, nll_loss=1.663, ppl=3.17, wps=90510, ups=6.2, wpb=14592.6, bsz=758.8, num_updates=14300, lr=0.000994378, gnorm=0.691, loss_scale=4, train_wall=16, gb_free=28.4, wall=2470
2024-02-10 14:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:28:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:28:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.649 | nll_loss 2.039 | ppl 4.11 | wps 159636 | wpb 5716.3 | bsz 292.8 | num_updates 14379 | best_loss 3.613
2024-02-10 14:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 14379 updates
2024-02-10 14:28:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt
2024-02-10 14:28:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt
2024-02-10 14:28:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt (epoch 21 @ 14379 updates, score 3.649) (writing took 2.466026477050036 seconds)
2024-02-10 14:28:37 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-02-10 14:28:37 | INFO | train | epoch 021 | loss 3.244 | nll_loss 1.655 | ppl 3.15 | wps 86640.3 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 14379 | lr 0.00099417 | gnorm 0.703 | loss_scale 4 | train_wall 109 | gb_free 28.6 | wall 2487
2024-02-10 14:28:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:28:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:28:37 | INFO | fairseq.trainer | begin training epoch 22
2024-02-10 14:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:28:41 | INFO | train_inner | epoch 022:     21 / 685 loss=3.256, nll_loss=1.671, ppl=3.19, wps=70200.6, ups=4.81, wpb=14608, bsz=767.7, num_updates=14400, lr=0.000994114, gnorm=0.812, loss_scale=4, train_wall=16, gb_free=28.4, wall=2490
2024-02-10 14:28:57 | INFO | train_inner | epoch 022:    121 / 685 loss=3.166, nll_loss=1.566, ppl=2.96, wps=90574.3, ups=6.2, wpb=14597.8, bsz=726.9, num_updates=14500, lr=0.000993844, gnorm=0.717, loss_scale=4, train_wall=16, gb_free=28.3, wall=2507
2024-02-10 14:29:13 | INFO | train_inner | epoch 022:    221 / 685 loss=3.209, nll_loss=1.617, ppl=3.07, wps=90161.3, ups=6.2, wpb=14538.3, bsz=762, num_updates=14600, lr=0.000993568, gnorm=0.726, loss_scale=4, train_wall=16, gb_free=28.4, wall=2523
2024-02-10 14:29:29 | INFO | train_inner | epoch 022:    321 / 685 loss=3.201, nll_loss=1.606, ppl=3.04, wps=89885.1, ups=6.21, wpb=14467.7, bsz=768.2, num_updates=14700, lr=0.000993286, gnorm=0.684, loss_scale=4, train_wall=16, gb_free=28.2, wall=2539
2024-02-10 14:29:45 | INFO | train_inner | epoch 022:    421 / 685 loss=3.229, nll_loss=1.64, ppl=3.12, wps=90417.1, ups=6.23, wpb=14512.9, bsz=724.2, num_updates=14800, lr=0.000992998, gnorm=0.707, loss_scale=4, train_wall=16, gb_free=28.3, wall=2555
2024-02-10 14:29:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-10 14:30:01 | INFO | train_inner | epoch 022:    522 / 685 loss=3.222, nll_loss=1.632, ppl=3.1, wps=88504.3, ups=6.18, wpb=14311.4, bsz=737.4, num_updates=14900, lr=0.000992704, gnorm=0.76, loss_scale=2, train_wall=16, gb_free=28.3, wall=2571
2024-02-10 14:30:18 | INFO | train_inner | epoch 022:    622 / 685 loss=3.229, nll_loss=1.642, ppl=3.12, wps=89896.4, ups=6.17, wpb=14562.4, bsz=741.8, num_updates=15000, lr=0.000992404, gnorm=0.756, loss_scale=2, train_wall=16, gb_free=28.2, wall=2587
2024-02-10 14:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:30:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:30:30 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.595 | nll_loss 1.981 | ppl 3.95 | wps 158483 | wpb 5716.3 | bsz 292.8 | num_updates 15063 | best_loss 3.595
2024-02-10 14:30:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 15063 updates
2024-02-10 14:30:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt
2024-02-10 14:30:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt
2024-02-10 14:30:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt (epoch 22 @ 15063 updates, score 3.595) (writing took 3.723476696992293 seconds)
2024-02-10 14:30:34 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-02-10 14:30:34 | INFO | train | epoch 022 | loss 3.208 | nll_loss 1.616 | ppl 3.06 | wps 85421.1 | ups 5.88 | wpb 14527.1 | bsz 748.7 | num_updates 15063 | lr 0.000992212 | gnorm 0.721 | loss_scale 2 | train_wall 109 | gb_free 28.4 | wall 2603
2024-02-10 14:30:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:30:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:30:36 | INFO | fairseq.trainer | begin training epoch 23
2024-02-10 14:30:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:30:47 | INFO | train_inner | epoch 023:     37 / 685 loss=3.162, nll_loss=1.564, ppl=2.96, wps=48983, ups=3.37, wpb=14542, bsz=775, num_updates=15100, lr=0.000992098, gnorm=0.646, loss_scale=2, train_wall=16, gb_free=28.5, wall=2617
2024-02-10 14:31:04 | INFO | train_inner | epoch 023:    137 / 685 loss=3.123, nll_loss=1.519, ppl=2.87, wps=90320.7, ups=6.18, wpb=14618.5, bsz=762, num_updates=15200, lr=0.000991786, gnorm=0.739, loss_scale=2, train_wall=16, gb_free=28.4, wall=2633
2024-02-10 14:31:19 | INFO | train_inner | epoch 023:    237 / 685 loss=3.164, nll_loss=1.566, ppl=2.96, wps=90336.9, ups=6.25, wpb=14443, bsz=745.5, num_updates=15300, lr=0.000991468, gnorm=0.694, loss_scale=2, train_wall=16, gb_free=28.6, wall=2649
2024-02-10 14:31:36 | INFO | train_inner | epoch 023:    337 / 685 loss=3.178, nll_loss=1.583, ppl=3, wps=90171.6, ups=6.21, wpb=14514.7, bsz=743.7, num_updates=15400, lr=0.000991144, gnorm=0.658, loss_scale=2, train_wall=16, gb_free=28.3, wall=2665
2024-02-10 14:31:52 | INFO | train_inner | epoch 023:    437 / 685 loss=3.197, nll_loss=1.605, ppl=3.04, wps=90555.8, ups=6.21, wpb=14593.1, bsz=743, num_updates=15500, lr=0.000990814, gnorm=0.742, loss_scale=2, train_wall=16, gb_free=28.2, wall=2681
2024-02-10 14:32:08 | INFO | train_inner | epoch 023:    537 / 685 loss=3.177, nll_loss=1.582, ppl=2.99, wps=90153.1, ups=6.23, wpb=14469.9, bsz=750.9, num_updates=15600, lr=0.000990478, gnorm=0.662, loss_scale=2, train_wall=16, gb_free=28.3, wall=2697
2024-02-10 14:32:24 | INFO | train_inner | epoch 023:    637 / 685 loss=3.196, nll_loss=1.607, ppl=3.05, wps=91003.4, ups=6.23, wpb=14598.5, bsz=738.4, num_updates=15700, lr=0.000990136, gnorm=0.643, loss_scale=2, train_wall=16, gb_free=28.3, wall=2713
2024-02-10 14:32:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:32:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:32:34 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.562 | nll_loss 1.939 | ppl 3.83 | wps 151271 | wpb 5716.3 | bsz 292.8 | num_updates 15748 | best_loss 3.562
2024-02-10 14:32:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 15748 updates
2024-02-10 14:32:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt
2024-02-10 14:32:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt
2024-02-10 14:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt (epoch 23 @ 15748 updates, score 3.562) (writing took 3.662957373075187 seconds)
2024-02-10 14:32:37 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-02-10 14:32:37 | INFO | train | epoch 023 | loss 3.17 | nll_loss 1.574 | ppl 2.98 | wps 80464.9 | ups 5.54 | wpb 14524.4 | bsz 748.5 | num_updates 15748 | lr 0.000989969 | gnorm 0.684 | loss_scale 2 | train_wall 109 | gb_free 28.5 | wall 2727
2024-02-10 14:32:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:32:37 | INFO | fairseq.trainer | begin training epoch 24
2024-02-10 14:32:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:32:46 | INFO | train_inner | epoch 024:     52 / 685 loss=3.14, nll_loss=1.54, ppl=2.91, wps=65973.4, ups=4.55, wpb=14494.3, bsz=775.2, num_updates=15800, lr=0.000989788, gnorm=0.669, loss_scale=2, train_wall=16, gb_free=28.4, wall=2735
2024-02-10 14:33:02 | INFO | train_inner | epoch 024:    152 / 685 loss=3.116, nll_loss=1.511, ppl=2.85, wps=90757.8, ups=6.27, wpb=14485.2, bsz=713.9, num_updates=15900, lr=0.000989434, gnorm=0.665, loss_scale=2, train_wall=16, gb_free=28.4, wall=2751
2024-02-10 14:33:18 | INFO | train_inner | epoch 024:    252 / 685 loss=3.133, nll_loss=1.533, ppl=2.89, wps=90752.9, ups=6.2, wpb=14643.5, bsz=764.7, num_updates=16000, lr=0.000989074, gnorm=0.673, loss_scale=2, train_wall=16, gb_free=28.5, wall=2767
2024-02-10 14:33:34 | INFO | train_inner | epoch 024:    352 / 685 loss=3.15, nll_loss=1.553, ppl=2.93, wps=89810.2, ups=6.23, wpb=14426.2, bsz=744.3, num_updates=16100, lr=0.000988708, gnorm=0.665, loss_scale=2, train_wall=16, gb_free=28.3, wall=2784
2024-02-10 14:33:57 | INFO | train_inner | epoch 024:    452 / 685 loss=3.133, nll_loss=1.533, ppl=2.89, wps=63262.4, ups=4.37, wpb=14470.1, bsz=752.7, num_updates=16200, lr=0.000988336, gnorm=0.649, loss_scale=2, train_wall=16, gb_free=28.3, wall=2806
2024-02-10 14:34:15 | INFO | train_inner | epoch 024:    552 / 685 loss=3.166, nll_loss=1.571, ppl=2.97, wps=77735.8, ups=5.44, wpb=14288.6, bsz=725.4, num_updates=16300, lr=0.000987958, gnorm=0.684, loss_scale=2, train_wall=16, gb_free=28.3, wall=2825
2024-02-10 14:34:31 | INFO | train_inner | epoch 024:    652 / 685 loss=3.172, nll_loss=1.581, ppl=2.99, wps=91111.6, ups=6.18, wpb=14753.2, bsz=778.9, num_updates=16400, lr=0.000987575, gnorm=0.872, loss_scale=2, train_wall=16, gb_free=28.3, wall=2841
2024-02-10 14:34:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:34:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:34:39 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.589 | nll_loss 1.974 | ppl 3.93 | wps 159629 | wpb 5716.3 | bsz 292.8 | num_updates 16433 | best_loss 3.562
2024-02-10 14:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 16433 updates
2024-02-10 14:34:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt
2024-02-10 14:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt
2024-02-10 14:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt (epoch 24 @ 16433 updates, score 3.589) (writing took 2.3799991339910775 seconds)
2024-02-10 14:34:41 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-02-10 14:34:41 | INFO | train | epoch 024 | loss 3.143 | nll_loss 1.544 | ppl 2.92 | wps 80090.5 | ups 5.51 | wpb 14524.4 | bsz 748.5 | num_updates 16433 | lr 0.000987447 | gnorm 0.704 | loss_scale 2 | train_wall 109 | gb_free 28.4 | wall 2851
2024-02-10 14:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:34:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:34:42 | INFO | fairseq.trainer | begin training epoch 25
2024-02-10 14:34:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:34:52 | INFO | train_inner | epoch 025:     67 / 685 loss=3.106, nll_loss=1.502, ppl=2.83, wps=69125.7, ups=4.75, wpb=14554, bsz=741.8, num_updates=16500, lr=0.000987185, gnorm=0.726, loss_scale=2, train_wall=16, gb_free=28.3, wall=2862
2024-02-10 14:35:08 | INFO | train_inner | epoch 025:    167 / 685 loss=3.084, nll_loss=1.476, ppl=2.78, wps=90628.6, ups=6.23, wpb=14537.3, bsz=745.9, num_updates=16600, lr=0.000986789, gnorm=0.652, loss_scale=2, train_wall=16, gb_free=28.4, wall=2878
2024-02-10 14:35:25 | INFO | train_inner | epoch 025:    267 / 685 loss=3.098, nll_loss=1.495, ppl=2.82, wps=91468.9, ups=6.2, wpb=14744.9, bsz=745.7, num_updates=16700, lr=0.000986388, gnorm=0.673, loss_scale=2, train_wall=16, gb_free=28.5, wall=2894
2024-02-10 14:35:41 | INFO | train_inner | epoch 025:    367 / 685 loss=3.12, nll_loss=1.519, ppl=2.87, wps=89647.4, ups=6.23, wpb=14379.3, bsz=730.2, num_updates=16800, lr=0.000985981, gnorm=0.678, loss_scale=2, train_wall=16, gb_free=28.4, wall=2910
2024-02-10 14:35:57 | INFO | train_inner | epoch 025:    467 / 685 loss=3.125, nll_loss=1.525, ppl=2.88, wps=90865.2, ups=6.23, wpb=14583, bsz=743.8, num_updates=16900, lr=0.000985567, gnorm=0.655, loss_scale=2, train_wall=16, gb_free=28.4, wall=2926
2024-02-10 14:36:13 | INFO | train_inner | epoch 025:    567 / 685 loss=3.126, nll_loss=1.528, ppl=2.88, wps=91157.5, ups=6.22, wpb=14645, bsz=733.4, num_updates=17000, lr=0.000985148, gnorm=0.647, loss_scale=2, train_wall=16, gb_free=28.3, wall=2942
2024-02-10 14:36:29 | INFO | train_inner | epoch 025:    667 / 685 loss=3.121, nll_loss=1.522, ppl=2.87, wps=88790, ups=6.18, wpb=14356.5, bsz=797.5, num_updates=17100, lr=0.000984723, gnorm=0.693, loss_scale=2, train_wall=16, gb_free=28.5, wall=2959
2024-02-10 14:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:36:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:36:34 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.553 | nll_loss 1.938 | ppl 3.83 | wps 159612 | wpb 5716.3 | bsz 292.8 | num_updates 17118 | best_loss 3.553
2024-02-10 14:36:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 17118 updates
2024-02-10 14:36:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt
2024-02-10 14:36:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt
2024-02-10 14:36:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt (epoch 25 @ 17118 updates, score 3.553) (writing took 3.776496284175664 seconds)
2024-02-10 14:36:38 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-02-10 14:36:38 | INFO | train | epoch 025 | loss 3.109 | nll_loss 1.507 | ppl 2.84 | wps 85679.7 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 17118 | lr 0.000984646 | gnorm 0.669 | loss_scale 2 | train_wall 109 | gb_free 28.5 | wall 2967
2024-02-10 14:36:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:36:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:36:38 | INFO | fairseq.trainer | begin training epoch 26
2024-02-10 14:36:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:36:51 | INFO | train_inner | epoch 026:     82 / 685 loss=3.043, nll_loss=1.43, ppl=2.7, wps=65594.8, ups=4.53, wpb=14490.9, bsz=757.8, num_updates=17200, lr=0.000984292, gnorm=0.608, loss_scale=2, train_wall=16, gb_free=28.4, wall=2981
2024-02-10 14:37:07 | INFO | train_inner | epoch 026:    182 / 685 loss=3.051, nll_loss=1.441, ppl=2.72, wps=90223.2, ups=6.2, wpb=14545.8, bsz=795, num_updates=17300, lr=0.000983855, gnorm=0.647, loss_scale=2, train_wall=16, gb_free=28.2, wall=2997
2024-02-10 14:37:23 | INFO | train_inner | epoch 026:    282 / 685 loss=3.088, nll_loss=1.481, ppl=2.79, wps=90401.2, ups=6.26, wpb=14443.4, bsz=719, num_updates=17400, lr=0.000983412, gnorm=0.655, loss_scale=2, train_wall=16, gb_free=28.3, wall=3013
2024-02-10 14:37:39 | INFO | train_inner | epoch 026:    382 / 685 loss=3.09, nll_loss=1.487, ppl=2.8, wps=90386.4, ups=6.21, wpb=14564.1, bsz=752.1, num_updates=17500, lr=0.000982963, gnorm=0.696, loss_scale=2, train_wall=16, gb_free=28.5, wall=3029
2024-02-10 14:37:55 | INFO | train_inner | epoch 026:    482 / 685 loss=3.099, nll_loss=1.497, ppl=2.82, wps=89467.4, ups=6.22, wpb=14378.8, bsz=741.7, num_updates=17600, lr=0.000982508, gnorm=0.722, loss_scale=2, train_wall=16, gb_free=28.3, wall=3045
2024-02-10 14:38:11 | INFO | train_inner | epoch 026:    582 / 685 loss=3.107, nll_loss=1.508, ppl=2.84, wps=90700.2, ups=6.2, wpb=14619.2, bsz=731.4, num_updates=17700, lr=0.000982048, gnorm=0.663, loss_scale=2, train_wall=16, gb_free=28.2, wall=3061
2024-02-10 14:38:27 | INFO | train_inner | epoch 026:    682 / 685 loss=3.115, nll_loss=1.517, ppl=2.86, wps=91015.5, ups=6.25, wpb=14569.7, bsz=738.5, num_updates=17800, lr=0.000981581, gnorm=0.632, loss_scale=2, train_wall=16, gb_free=28.4, wall=3077
2024-02-10 14:38:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:38:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:38:30 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.521 | nll_loss 1.905 | ppl 3.74 | wps 159795 | wpb 5716.3 | bsz 292.8 | num_updates 17803 | best_loss 3.521
2024-02-10 14:38:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 17803 updates
2024-02-10 14:38:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt
2024-02-10 14:38:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt
2024-02-10 14:38:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt (epoch 26 @ 17803 updates, score 3.521) (writing took 3.632576639065519 seconds)
2024-02-10 14:38:34 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-02-10 14:38:34 | INFO | train | epoch 026 | loss 3.083 | nll_loss 1.479 | ppl 2.79 | wps 85804.2 | ups 5.91 | wpb 14524.4 | bsz 748.5 | num_updates 17803 | lr 0.000981567 | gnorm 0.661 | loss_scale 2 | train_wall 109 | gb_free 28.3 | wall 3083
2024-02-10 14:38:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:38:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:38:34 | INFO | fairseq.trainer | begin training epoch 27
2024-02-10 14:38:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2024-02-10 14:38:50 | INFO | train_inner | epoch 027:     98 / 685 loss=3.017, nll_loss=1.402, ppl=2.64, wps=65271.3, ups=4.52, wpb=14435.9, bsz=739, num_updates=17900, lr=0.000981109, gnorm=0.673, loss_scale=1, train_wall=16, gb_free=28.3, wall=3099
2024-02-10 14:39:06 | INFO | train_inner | epoch 027:    198 / 685 loss=3.05, nll_loss=1.441, ppl=2.72, wps=90186.5, ups=6.23, wpb=14474.9, bsz=737.4, num_updates=18000, lr=0.000980631, gnorm=0.669, loss_scale=1, train_wall=16, gb_free=28.5, wall=3115
2024-02-10 14:39:22 | INFO | train_inner | epoch 027:    298 / 685 loss=3.046, nll_loss=1.436, ppl=2.7, wps=88471.6, ups=6.21, wpb=14239.3, bsz=759.3, num_updates=18100, lr=0.000980147, gnorm=0.717, loss_scale=1, train_wall=16, gb_free=28.5, wall=3131
2024-02-10 14:39:38 | INFO | train_inner | epoch 027:    398 / 685 loss=3.069, nll_loss=1.463, ppl=2.76, wps=89773.2, ups=6.23, wpb=14405.6, bsz=740.3, num_updates=18200, lr=0.000979657, gnorm=0.667, loss_scale=1, train_wall=16, gb_free=28.2, wall=3147
2024-02-10 14:39:54 | INFO | train_inner | epoch 027:    498 / 685 loss=3.065, nll_loss=1.461, ppl=2.75, wps=91599.7, ups=6.17, wpb=14847.5, bsz=787.8, num_updates=18300, lr=0.000979161, gnorm=0.674, loss_scale=1, train_wall=16, gb_free=28.4, wall=3164
2024-02-10 14:40:10 | INFO | train_inner | epoch 027:    598 / 685 loss=3.099, nll_loss=1.499, ppl=2.83, wps=90065.5, ups=6.24, wpb=14438, bsz=738.2, num_updates=18400, lr=0.00097866, gnorm=0.778, loss_scale=1, train_wall=16, gb_free=28.3, wall=3180
2024-02-10 14:40:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:40:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:40:26 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.523 | nll_loss 1.925 | ppl 3.8 | wps 159149 | wpb 5716.3 | bsz 292.8 | num_updates 18487 | best_loss 3.521
2024-02-10 14:40:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 18487 updates
2024-02-10 14:40:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt
2024-02-10 14:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt
2024-02-10 14:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt (epoch 27 @ 18487 updates, score 3.523) (writing took 2.480564075987786 seconds)
2024-02-10 14:40:28 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-02-10 14:40:28 | INFO | train | epoch 027 | loss 3.061 | nll_loss 1.454 | ppl 2.74 | wps 86482 | ups 5.95 | wpb 14522.9 | bsz 748.7 | num_updates 18487 | lr 0.000978219 | gnorm 0.688 | loss_scale 1 | train_wall 109 | gb_free 28.4 | wall 3198
2024-02-10 14:40:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:40:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:40:28 | INFO | fairseq.trainer | begin training epoch 28
2024-02-10 14:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:40:31 | INFO | train_inner | epoch 028:     13 / 685 loss=3.068, nll_loss=1.464, ppl=2.76, wps=71554.2, ups=4.84, wpb=14778.6, bsz=726.5, num_updates=18500, lr=0.000978152, gnorm=0.629, loss_scale=1, train_wall=16, gb_free=28.5, wall=3200
2024-02-10 14:40:47 | INFO | train_inner | epoch 028:    113 / 685 loss=2.988, nll_loss=1.37, ppl=2.58, wps=90368.6, ups=6.23, wpb=14508.3, bsz=732.9, num_updates=18600, lr=0.000977639, gnorm=0.634, loss_scale=1, train_wall=16, gb_free=28.2, wall=3216
2024-02-10 14:41:03 | INFO | train_inner | epoch 028:    213 / 685 loss=3.026, nll_loss=1.415, ppl=2.67, wps=90424.8, ups=6.2, wpb=14580.8, bsz=756.6, num_updates=18700, lr=0.00097712, gnorm=0.629, loss_scale=1, train_wall=16, gb_free=28.3, wall=3232
2024-02-10 14:41:19 | INFO | train_inner | epoch 028:    313 / 685 loss=3.03, nll_loss=1.419, ppl=2.67, wps=89810.7, ups=6.22, wpb=14440.9, bsz=753.4, num_updates=18800, lr=0.000976595, gnorm=0.643, loss_scale=1, train_wall=16, gb_free=28.5, wall=3248
2024-02-10 14:41:35 | INFO | train_inner | epoch 028:    413 / 685 loss=3.053, nll_loss=1.447, ppl=2.73, wps=89774.1, ups=6.26, wpb=14349.5, bsz=713.8, num_updates=18900, lr=0.000976065, gnorm=0.661, loss_scale=1, train_wall=16, gb_free=28.4, wall=3264
2024-02-10 14:41:51 | INFO | train_inner | epoch 028:    513 / 685 loss=3.051, nll_loss=1.444, ppl=2.72, wps=90390.7, ups=6.22, wpb=14539.1, bsz=764.5, num_updates=19000, lr=0.000975528, gnorm=0.672, loss_scale=1, train_wall=16, gb_free=28.4, wall=3281
2024-02-10 14:42:07 | INFO | train_inner | epoch 028:    613 / 685 loss=3.038, nll_loss=1.43, ppl=2.7, wps=91350.6, ups=6.2, wpb=14725.8, bsz=775.8, num_updates=19100, lr=0.000974986, gnorm=0.647, loss_scale=1, train_wall=16, gb_free=28.3, wall=3297
2024-02-10 14:42:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:42:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:42:21 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.51 | nll_loss 1.896 | ppl 3.72 | wps 158426 | wpb 5716.3 | bsz 292.8 | num_updates 19172 | best_loss 3.51
2024-02-10 14:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 19172 updates
2024-02-10 14:42:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt
2024-02-10 14:42:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt
2024-02-10 14:42:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt (epoch 28 @ 19172 updates, score 3.51) (writing took 3.697527404874563 seconds)
2024-02-10 14:42:24 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-02-10 14:42:24 | INFO | train | epoch 028 | loss 3.034 | nll_loss 1.425 | ppl 2.69 | wps 85839 | ups 5.91 | wpb 14524.4 | bsz 748.5 | num_updates 19172 | lr 0.000974592 | gnorm 0.651 | loss_scale 1 | train_wall 109 | gb_free 28.5 | wall 3314
2024-02-10 14:42:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:42:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:42:24 | INFO | fairseq.trainer | begin training epoch 29
2024-02-10 14:42:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:42:29 | INFO | train_inner | epoch 029:     28 / 685 loss=3.054, nll_loss=1.449, ppl=2.73, wps=66592.1, ups=4.58, wpb=14533.2, bsz=730.8, num_updates=19200, lr=0.000974438, gnorm=0.666, loss_scale=1, train_wall=16, gb_free=28.4, wall=3318
2024-02-10 14:42:45 | INFO | train_inner | epoch 029:    128 / 685 loss=2.972, nll_loss=1.352, ppl=2.55, wps=89282.5, ups=6.18, wpb=14443.6, bsz=764.2, num_updates=19300, lr=0.000973884, gnorm=0.653, loss_scale=1, train_wall=16, gb_free=28.6, wall=3335
2024-02-10 14:43:01 | INFO | train_inner | epoch 029:    228 / 685 loss=2.997, nll_loss=1.383, ppl=2.61, wps=89770, ups=6.21, wpb=14462.7, bsz=752.7, num_updates=19400, lr=0.000973325, gnorm=0.648, loss_scale=1, train_wall=16, gb_free=28.3, wall=3351
2024-02-10 14:43:17 | INFO | train_inner | epoch 029:    328 / 685 loss=3.019, nll_loss=1.41, ppl=2.66, wps=90799.5, ups=6.21, wpb=14621.9, bsz=758.6, num_updates=19500, lr=0.000972759, gnorm=0.644, loss_scale=1, train_wall=16, gb_free=28.5, wall=3367
2024-02-10 14:43:33 | INFO | train_inner | epoch 029:    428 / 685 loss=3.023, nll_loss=1.416, ppl=2.67, wps=91962.3, ups=6.17, wpb=14893.1, bsz=781.6, num_updates=19600, lr=0.000972188, gnorm=0.685, loss_scale=1, train_wall=16, gb_free=28.4, wall=3383
2024-02-10 14:43:50 | INFO | train_inner | epoch 029:    528 / 685 loss=3.023, nll_loss=1.413, ppl=2.66, wps=90040.8, ups=6.24, wpb=14421.9, bsz=732.9, num_updates=19700, lr=0.000971611, gnorm=0.756, loss_scale=1, train_wall=16, gb_free=28.3, wall=3399
2024-02-10 14:44:06 | INFO | train_inner | epoch 029:    628 / 685 loss=3.029, nll_loss=1.421, ppl=2.68, wps=89978.2, ups=6.23, wpb=14438.9, bsz=740.6, num_updates=19800, lr=0.000971029, gnorm=0.634, loss_scale=1, train_wall=16, gb_free=28.2, wall=3415
2024-02-10 14:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:44:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:44:17 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.501 | nll_loss 1.894 | ppl 3.72 | wps 159978 | wpb 5716.3 | bsz 292.8 | num_updates 19857 | best_loss 3.501
2024-02-10 14:44:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 19857 updates
2024-02-10 14:44:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt
2024-02-10 14:44:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt
2024-02-10 14:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt (epoch 29 @ 19857 updates, score 3.501) (writing took 3.661113035166636 seconds)
2024-02-10 14:44:20 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-02-10 14:44:20 | INFO | train | epoch 029 | loss 3.011 | nll_loss 1.4 | ppl 2.64 | wps 85755.8 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 19857 | lr 0.000970694 | gnorm 0.666 | loss_scale 1 | train_wall 109 | gb_free 28.5 | wall 3430
2024-02-10 14:44:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:44:20 | INFO | fairseq.trainer | begin training epoch 30
2024-02-10 14:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:44:27 | INFO | train_inner | epoch 030:     43 / 685 loss=2.992, nll_loss=1.377, ppl=2.6, wps=66400.4, ups=4.58, wpb=14501.7, bsz=733.4, num_updates=19900, lr=0.00097044, gnorm=0.639, loss_scale=1, train_wall=16, gb_free=28.4, wall=3437
2024-02-10 14:44:43 | INFO | train_inner | epoch 030:    143 / 685 loss=2.953, nll_loss=1.333, ppl=2.52, wps=89747.7, ups=6.24, wpb=14387, bsz=744.5, num_updates=20000, lr=0.000969846, gnorm=0.65, loss_scale=1, train_wall=16, gb_free=28.4, wall=3453
2024-02-10 14:44:59 | INFO | train_inner | epoch 030:    243 / 685 loss=3.001, nll_loss=1.39, ppl=2.62, wps=92140, ups=6.24, wpb=14763.1, bsz=724.2, num_updates=20100, lr=0.000969247, gnorm=0.649, loss_scale=1, train_wall=16, gb_free=28.5, wall=3469
2024-02-10 14:45:15 | INFO | train_inner | epoch 030:    343 / 685 loss=2.998, nll_loss=1.386, ppl=2.61, wps=88471.8, ups=6.24, wpb=14172.4, bsz=760.7, num_updates=20200, lr=0.000968641, gnorm=0.886, loss_scale=1, train_wall=16, gb_free=28.4, wall=3485
2024-02-10 14:45:32 | INFO | train_inner | epoch 030:    443 / 685 loss=3.011, nll_loss=1.4, ppl=2.64, wps=90426.6, ups=6.23, wpb=14521.1, bsz=738.4, num_updates=20300, lr=0.00096803, gnorm=0.809, loss_scale=1, train_wall=16, gb_free=28.4, wall=3501
2024-02-10 14:45:48 | INFO | train_inner | epoch 030:    543 / 685 loss=3.003, nll_loss=1.393, ppl=2.63, wps=91419.5, ups=6.18, wpb=14784.9, bsz=748.4, num_updates=20400, lr=0.000967413, gnorm=0.69, loss_scale=1, train_wall=16, gb_free=28.2, wall=3517
2024-02-10 14:46:04 | INFO | train_inner | epoch 030:    643 / 685 loss=3, nll_loss=1.391, ppl=2.62, wps=90447, ups=6.19, wpb=14609.5, bsz=758.8, num_updates=20500, lr=0.00096679, gnorm=0.616, loss_scale=1, train_wall=16, gb_free=28.4, wall=3533
2024-02-10 14:46:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:46:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:46:13 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.505 | nll_loss 1.89 | ppl 3.71 | wps 159280 | wpb 5716.3 | bsz 292.8 | num_updates 20542 | best_loss 3.501
2024-02-10 14:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 20542 updates
2024-02-10 14:46:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt
2024-02-10 14:46:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt
2024-02-10 14:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt (epoch 30 @ 20542 updates, score 3.505) (writing took 2.4126198939047754 seconds)
2024-02-10 14:46:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-02-10 14:46:15 | INFO | train | epoch 030 | loss 2.992 | nll_loss 1.379 | ppl 2.6 | wps 86729 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 20542 | lr 0.000966527 | gnorm 0.707 | loss_scale 1 | train_wall 109 | gb_free 28.4 | wall 3545
2024-02-10 14:46:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:46:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:46:15 | INFO | fairseq.trainer | begin training epoch 31
2024-02-10 14:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:46:25 | INFO | train_inner | epoch 031:     58 / 685 loss=2.952, nll_loss=1.333, ppl=2.52, wps=70095.1, ups=4.82, wpb=14557.2, bsz=767, num_updates=20600, lr=0.000966162, gnorm=0.612, loss_scale=1, train_wall=16, gb_free=28.4, wall=3554
2024-02-10 14:46:41 | INFO | train_inner | epoch 031:    158 / 685 loss=2.933, nll_loss=1.312, ppl=2.48, wps=89844.5, ups=6.2, wpb=14484.9, bsz=762.2, num_updates=20700, lr=0.000965528, gnorm=0.631, loss_scale=1, train_wall=16, gb_free=28.4, wall=3570
2024-02-10 14:46:57 | INFO | train_inner | epoch 031:    258 / 685 loss=2.969, nll_loss=1.353, ppl=2.56, wps=89559.3, ups=6.22, wpb=14407.1, bsz=742.8, num_updates=20800, lr=0.000964888, gnorm=0.677, loss_scale=1, train_wall=16, gb_free=28.3, wall=3586
2024-02-10 14:47:13 | INFO | train_inner | epoch 031:    358 / 685 loss=2.977, nll_loss=1.363, ppl=2.57, wps=90834.3, ups=6.23, wpb=14579.3, bsz=732.2, num_updates=20900, lr=0.000964243, gnorm=0.65, loss_scale=1, train_wall=16, gb_free=28.4, wall=3602
2024-02-10 14:47:29 | INFO | train_inner | epoch 031:    458 / 685 loss=2.982, nll_loss=1.369, ppl=2.58, wps=89700.1, ups=6.22, wpb=14424.2, bsz=729.2, num_updates=21000, lr=0.000963592, gnorm=0.655, loss_scale=1, train_wall=16, gb_free=28.3, wall=3619
2024-02-10 14:47:45 | INFO | train_inner | epoch 031:    558 / 685 loss=2.977, nll_loss=1.364, ppl=2.57, wps=89953.1, ups=6.21, wpb=14485.1, bsz=775.6, num_updates=21100, lr=0.000962935, gnorm=0.679, loss_scale=1, train_wall=16, gb_free=28.2, wall=3635
2024-02-10 14:48:01 | INFO | train_inner | epoch 031:    658 / 685 loss=3.008, nll_loss=1.4, ppl=2.64, wps=90820.5, ups=6.25, wpb=14541.3, bsz=744.6, num_updates=21200, lr=0.000962273, gnorm=0.669, loss_scale=1, train_wall=16, gb_free=28.3, wall=3651
2024-02-10 14:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:48:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:48:07 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.505 | nll_loss 1.899 | ppl 3.73 | wps 159226 | wpb 5716.3 | bsz 292.8 | num_updates 21227 | best_loss 3.501
2024-02-10 14:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 21227 updates
2024-02-10 14:48:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt
2024-02-10 14:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt
2024-02-10 14:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt (epoch 31 @ 21227 updates, score 3.505) (writing took 2.3473141589201987 seconds)
2024-02-10 14:48:10 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-02-10 14:48:10 | INFO | train | epoch 031 | loss 2.971 | nll_loss 1.356 | ppl 2.56 | wps 86757.5 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 21227 | lr 0.000962093 | gnorm 0.654 | loss_scale 1 | train_wall 109 | gb_free 28.4 | wall 3659
2024-02-10 14:48:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:48:10 | INFO | fairseq.trainer | begin training epoch 32
2024-02-10 14:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:48:22 | INFO | train_inner | epoch 032:     73 / 685 loss=2.942, nll_loss=1.323, ppl=2.5, wps=70488.8, ups=4.89, wpb=14413.3, bsz=724.8, num_updates=21300, lr=0.000961605, gnorm=0.634, loss_scale=1, train_wall=16, gb_free=28.4, wall=3671
2024-02-10 14:48:38 | INFO | train_inner | epoch 032:    173 / 685 loss=2.926, nll_loss=1.304, ppl=2.47, wps=89762.7, ups=6.21, wpb=14457, bsz=777.5, num_updates=21400, lr=0.000960932, gnorm=0.662, loss_scale=1, train_wall=16, gb_free=28.4, wall=3687
2024-02-10 14:48:54 | INFO | train_inner | epoch 032:    273 / 685 loss=2.957, nll_loss=1.342, ppl=2.53, wps=90124.3, ups=6.16, wpb=14634.6, bsz=760.6, num_updates=21500, lr=0.000960252, gnorm=0.685, loss_scale=1, train_wall=16, gb_free=28.2, wall=3703
2024-02-10 14:49:10 | INFO | train_inner | epoch 032:    373 / 685 loss=2.975, nll_loss=1.361, ppl=2.57, wps=89890.4, ups=6.28, wpb=14316.2, bsz=713.3, num_updates=21600, lr=0.000959568, gnorm=0.7, loss_scale=1, train_wall=16, gb_free=28.5, wall=3719
2024-02-10 14:49:26 | INFO | train_inner | epoch 032:    473 / 685 loss=2.964, nll_loss=1.348, ppl=2.55, wps=90423.6, ups=6.27, wpb=14410.2, bsz=710.8, num_updates=21700, lr=0.000958877, gnorm=0.65, loss_scale=1, train_wall=16, gb_free=28.3, wall=3735
2024-02-10 14:49:42 | INFO | train_inner | epoch 032:    573 / 685 loss=2.962, nll_loss=1.349, ppl=2.55, wps=91195.6, ups=6.2, wpb=14716.1, bsz=756.6, num_updates=21800, lr=0.000958181, gnorm=0.636, loss_scale=1, train_wall=16, gb_free=28.4, wall=3751
2024-02-10 14:49:58 | INFO | train_inner | epoch 032:    673 / 685 loss=2.963, nll_loss=1.352, ppl=2.55, wps=90883.5, ups=6.16, wpb=14754.5, bsz=790.6, num_updates=21900, lr=0.00095748, gnorm=0.63, loss_scale=1, train_wall=16, gb_free=28.3, wall=3768
2024-02-10 14:50:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:50:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:50:02 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.537 | nll_loss 1.94 | ppl 3.84 | wps 159816 | wpb 5716.3 | bsz 292.8 | num_updates 21912 | best_loss 3.501
2024-02-10 14:50:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 21912 updates
2024-02-10 14:50:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt
2024-02-10 14:50:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt
2024-02-10 14:50:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt (epoch 32 @ 21912 updates, score 3.537) (writing took 2.4022015689406544 seconds)
2024-02-10 14:50:04 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-02-10 14:50:04 | INFO | train | epoch 032 | loss 2.954 | nll_loss 1.338 | ppl 2.53 | wps 86724.1 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 21912 | lr 0.000957395 | gnorm 0.658 | loss_scale 1 | train_wall 109 | gb_free 28.3 | wall 3774
2024-02-10 14:50:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:50:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:50:04 | INFO | fairseq.trainer | begin training epoch 33
2024-02-10 14:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:50:19 | INFO | train_inner | epoch 033:     88 / 685 loss=2.914, nll_loss=1.292, ppl=2.45, wps=68844.1, ups=4.85, wpb=14194.4, bsz=759.3, num_updates=22000, lr=0.000956773, gnorm=0.781, loss_scale=1, train_wall=16, gb_free=28.4, wall=3788
2024-02-10 14:50:35 | INFO | train_inner | epoch 033:    188 / 685 loss=3.007, nll_loss=1.4, ppl=2.64, wps=91566, ups=6.18, wpb=14809.4, bsz=741.5, num_updates=22100, lr=0.00095606, gnorm=1.732, loss_scale=1, train_wall=16, gb_free=28.2, wall=3804
2024-02-10 14:50:51 | INFO | train_inner | epoch 033:    288 / 685 loss=2.933, nll_loss=1.313, ppl=2.49, wps=90145.1, ups=6.23, wpb=14479.3, bsz=733.6, num_updates=22200, lr=0.000955342, gnorm=0.946, loss_scale=1, train_wall=16, gb_free=28.4, wall=3821
2024-02-10 14:50:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2024-02-10 14:51:07 | INFO | train_inner | epoch 033:    389 / 685 loss=2.954, nll_loss=1.34, ppl=2.53, wps=90255, ups=6.11, wpb=14770.4, bsz=770.9, num_updates=22300, lr=0.000954618, gnorm=0.891, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=3837
2024-02-10 14:51:23 | INFO | train_inner | epoch 033:    489 / 685 loss=2.965, nll_loss=1.353, ppl=2.55, wps=91361.6, ups=6.23, wpb=14676.2, bsz=740.6, num_updates=22400, lr=0.000953889, gnorm=0.965, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=3853
2024-02-10 14:51:39 | INFO | train_inner | epoch 033:    589 / 685 loss=2.947, nll_loss=1.331, ppl=2.51, wps=88236.3, ups=6.23, wpb=14160.9, bsz=757.1, num_updates=22500, lr=0.000953154, gnorm=0.836, loss_scale=0.5, train_wall=16, gb_free=28.1, wall=3869
2024-02-10 14:51:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:51:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:51:57 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.477 | nll_loss 1.866 | ppl 3.65 | wps 135273 | wpb 5716.3 | bsz 292.8 | num_updates 22596 | best_loss 3.477
2024-02-10 14:51:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 22596 updates
2024-02-10 14:51:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt
2024-02-10 14:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt
2024-02-10 14:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt (epoch 33 @ 22596 updates, score 3.477) (writing took 3.98847942519933 seconds)
2024-02-10 14:52:01 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-02-10 14:52:01 | INFO | train | epoch 033 | loss 2.954 | nll_loss 1.339 | ppl 2.53 | wps 84925.6 | ups 5.85 | wpb 14528.6 | bsz 749.1 | num_updates 22596 | lr 0.000952443 | gnorm 1.006 | loss_scale 0.5 | train_wall 109 | gb_free 28.4 | wall 3891
2024-02-10 14:52:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:52:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:52:02 | INFO | fairseq.trainer | begin training epoch 34
2024-02-10 14:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:52:02 | INFO | train_inner | epoch 034:      4 / 685 loss=2.954, nll_loss=1.34, ppl=2.53, wps=63505.9, ups=4.34, wpb=14648.3, bsz=748.7, num_updates=22600, lr=0.000952414, gnorm=0.839, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=3892
2024-02-10 14:52:30 | INFO | train_inner | epoch 034:    104 / 685 loss=2.879, nll_loss=1.253, ppl=2.38, wps=52867.2, ups=3.6, wpb=14693.3, bsz=742.9, num_updates=22700, lr=0.000951668, gnorm=0.642, loss_scale=0.5, train_wall=27, gb_free=28.2, wall=3920
2024-02-10 14:52:46 | INFO | train_inner | epoch 034:    204 / 685 loss=2.928, nll_loss=1.31, ppl=2.48, wps=91353.1, ups=6.21, wpb=14711.5, bsz=771.4, num_updates=22800, lr=0.000950916, gnorm=0.698, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=3936
2024-02-10 14:53:03 | INFO | train_inner | epoch 034:    304 / 685 loss=2.917, nll_loss=1.298, ppl=2.46, wps=90421, ups=6.19, wpb=14610.8, bsz=773.5, num_updates=22900, lr=0.000950159, gnorm=1.216, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=3952
2024-02-10 14:53:19 | INFO | train_inner | epoch 034:    404 / 685 loss=2.938, nll_loss=1.323, ppl=2.5, wps=90576.7, ups=6.24, wpb=14522.5, bsz=724.4, num_updates=23000, lr=0.000949397, gnorm=0.719, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=3968
2024-02-10 14:53:35 | INFO | train_inner | epoch 034:    504 / 685 loss=2.926, nll_loss=1.309, ppl=2.48, wps=89888.6, ups=6.18, wpb=14544.7, bsz=777.9, num_updates=23100, lr=0.000948629, gnorm=0.682, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=3984
2024-02-10 14:53:51 | INFO | train_inner | epoch 034:    604 / 685 loss=2.935, nll_loss=1.319, ppl=2.49, wps=88385.1, ups=6.25, wpb=14150.6, bsz=740.8, num_updates=23200, lr=0.000947856, gnorm=0.81, loss_scale=0.5, train_wall=16, gb_free=28.6, wall=4000
2024-02-10 14:54:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:54:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:54:06 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.483 | nll_loss 1.889 | ppl 3.7 | wps 159621 | wpb 5716.3 | bsz 292.8 | num_updates 23281 | best_loss 3.477
2024-02-10 14:54:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 23281 updates
2024-02-10 14:54:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt
2024-02-10 14:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt
2024-02-10 14:54:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt (epoch 34 @ 23281 updates, score 3.483) (writing took 2.5524392519146204 seconds)
2024-02-10 14:54:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-02-10 14:54:08 | INFO | train | epoch 034 | loss 2.923 | nll_loss 1.305 | ppl 2.47 | wps 78471.3 | ups 5.4 | wpb 14524.4 | bsz 748.5 | num_updates 23281 | lr 0.000947226 | gnorm 0.793 | loss_scale 0.5 | train_wall 120 | gb_free 28.8 | wall 4018
2024-02-10 14:54:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:54:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:54:08 | INFO | fairseq.trainer | begin training epoch 35
2024-02-10 14:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:54:12 | INFO | train_inner | epoch 035:     19 / 685 loss=2.934, nll_loss=1.317, ppl=2.49, wps=68995.4, ups=4.81, wpb=14354.1, bsz=702.1, num_updates=23300, lr=0.000947077, gnorm=0.763, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=4021
2024-02-10 14:54:28 | INFO | train_inner | epoch 035:    119 / 685 loss=2.872, nll_loss=1.246, ppl=2.37, wps=91339, ups=6.22, wpb=14694.1, bsz=751.4, num_updates=23400, lr=0.000946293, gnorm=0.642, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=4037
2024-02-10 14:54:44 | INFO | train_inner | epoch 035:    219 / 685 loss=2.877, nll_loss=1.252, ppl=2.38, wps=90561.9, ups=6.2, wpb=14604.8, bsz=764.4, num_updates=23500, lr=0.000945503, gnorm=0.713, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=4053
2024-02-10 14:55:00 | INFO | train_inner | epoch 035:    319 / 685 loss=2.904, nll_loss=1.284, ppl=2.43, wps=89984.8, ups=6.26, wpb=14376.4, bsz=739.9, num_updates=23600, lr=0.000944708, gnorm=0.672, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=4069
2024-02-10 14:55:16 | INFO | train_inner | epoch 035:    419 / 685 loss=2.929, nll_loss=1.312, ppl=2.48, wps=90819.8, ups=6.24, wpb=14552.1, bsz=726.2, num_updates=23700, lr=0.000943908, gnorm=0.686, loss_scale=0.5, train_wall=16, gb_free=28.6, wall=4085
2024-02-10 14:55:32 | INFO | train_inner | epoch 035:    519 / 685 loss=2.92, nll_loss=1.303, ppl=2.47, wps=91579.5, ups=6.21, wpb=14748.6, bsz=769.5, num_updates=23800, lr=0.000943102, gnorm=0.664, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=4101
2024-02-10 14:55:48 | INFO | train_inner | epoch 035:    619 / 685 loss=2.917, nll_loss=1.3, ppl=2.46, wps=89747, ups=6.18, wpb=14529.5, bsz=771.5, num_updates=23900, lr=0.00094229, gnorm=0.692, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=4118
2024-02-10 14:55:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:55:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:56:01 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.473 | nll_loss 1.876 | ppl 3.67 | wps 159015 | wpb 5716.3 | bsz 292.8 | num_updates 23966 | best_loss 3.473
2024-02-10 14:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 23966 updates
2024-02-10 14:56:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt
2024-02-10 14:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt
2024-02-10 14:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt (epoch 35 @ 23966 updates, score 3.473) (writing took 3.734530634013936 seconds)
2024-02-10 14:56:04 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-02-10 14:56:04 | INFO | train | epoch 035 | loss 2.905 | nll_loss 1.285 | ppl 2.44 | wps 85680.4 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 23966 | lr 0.000941752 | gnorm 0.688 | loss_scale 0.5 | train_wall 109 | gb_free 28.3 | wall 4134
2024-02-10 14:56:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:56:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:56:07 | INFO | fairseq.trainer | begin training epoch 36
2024-02-10 14:56:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:56:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2024-02-10 14:56:18 | INFO | train_inner | epoch 036:     35 / 685 loss=2.942, nll_loss=1.328, ppl=2.51, wps=46971.2, ups=3.3, wpb=14240.4, bsz=724.5, num_updates=24000, lr=0.000941474, gnorm=1.207, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4148
2024-02-10 14:56:34 | INFO | train_inner | epoch 036:    135 / 685 loss=2.873, nll_loss=1.247, ppl=2.37, wps=89015.2, ups=6.27, wpb=14194, bsz=722.4, num_updates=24100, lr=0.000940652, gnorm=0.682, loss_scale=0.25, train_wall=16, gb_free=28.7, wall=4164
2024-02-10 14:56:51 | INFO | train_inner | epoch 036:    235 / 685 loss=2.882, nll_loss=1.259, ppl=2.39, wps=90182.8, ups=6.16, wpb=14644.6, bsz=751.2, num_updates=24200, lr=0.000939824, gnorm=0.677, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4180
2024-02-10 14:57:07 | INFO | train_inner | epoch 036:    335 / 685 loss=2.89, nll_loss=1.268, ppl=2.41, wps=90059.5, ups=6.24, wpb=14435.9, bsz=758.4, num_updates=24300, lr=0.000938991, gnorm=0.728, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4196
2024-02-10 14:57:23 | INFO | train_inner | epoch 036:    435 / 685 loss=2.9, nll_loss=1.281, ppl=2.43, wps=90646.5, ups=6.21, wpb=14605.7, bsz=754.5, num_updates=24400, lr=0.000938153, gnorm=0.652, loss_scale=0.25, train_wall=16, gb_free=28.7, wall=4212
2024-02-10 14:57:39 | INFO | train_inner | epoch 036:    535 / 685 loss=2.898, nll_loss=1.279, ppl=2.43, wps=90621.3, ups=6.24, wpb=14519.3, bsz=737.8, num_updates=24500, lr=0.00093731, gnorm=0.654, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4228
2024-02-10 14:57:55 | INFO | train_inner | epoch 036:    635 / 685 loss=2.906, nll_loss=1.288, ppl=2.44, wps=90309.4, ups=6.2, wpb=14557.4, bsz=755.9, num_updates=24600, lr=0.000936461, gnorm=0.689, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=4244
2024-02-10 14:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:58:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:58:05 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.464 | nll_loss 1.863 | ppl 3.64 | wps 159313 | wpb 5716.3 | bsz 292.8 | num_updates 24650 | best_loss 3.464
2024-02-10 14:58:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 24650 updates
2024-02-10 14:58:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt
2024-02-10 14:58:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt
2024-02-10 14:58:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt (epoch 36 @ 24650 updates, score 3.464) (writing took 3.6742025609128177 seconds)
2024-02-10 14:58:09 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-02-10 14:58:09 | INFO | train | epoch 036 | loss 2.896 | nll_loss 1.276 | ppl 2.42 | wps 79937.6 | ups 5.5 | wpb 14522 | bsz 748.6 | num_updates 24650 | lr 0.000936035 | gnorm 0.744 | loss_scale 0.25 | train_wall 109 | gb_free 28.4 | wall 4258
2024-02-10 14:58:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 14:58:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 14:58:09 | INFO | fairseq.trainer | begin training epoch 37
2024-02-10 14:58:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 14:58:17 | INFO | train_inner | epoch 037:     50 / 685 loss=2.869, nll_loss=1.245, ppl=2.37, wps=66265.7, ups=4.54, wpb=14584.3, bsz=759, num_updates=24700, lr=0.000935607, gnorm=0.684, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4266
2024-02-10 14:58:33 | INFO | train_inner | epoch 037:    150 / 685 loss=2.854, nll_loss=1.228, ppl=2.34, wps=89959.9, ups=6.27, wpb=14352.9, bsz=712.7, num_updates=24800, lr=0.000934747, gnorm=0.648, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4282
2024-02-10 14:58:49 | INFO | train_inner | epoch 037:    250 / 685 loss=2.869, nll_loss=1.246, ppl=2.37, wps=90946.3, ups=6.19, wpb=14691.9, bsz=776.9, num_updates=24900, lr=0.000933883, gnorm=0.686, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4299
2024-02-10 14:59:05 | INFO | train_inner | epoch 037:    350 / 685 loss=2.886, nll_loss=1.266, ppl=2.4, wps=91653.7, ups=6.23, wpb=14704.5, bsz=743.4, num_updates=25000, lr=0.000933013, gnorm=0.667, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4315
2024-02-10 14:59:21 | INFO | train_inner | epoch 037:    450 / 685 loss=2.888, nll_loss=1.269, ppl=2.41, wps=92034.1, ups=6.2, wpb=14834.2, bsz=740.4, num_updates=25100, lr=0.000932137, gnorm=0.692, loss_scale=0.25, train_wall=16, gb_free=28.6, wall=4331
2024-02-10 14:59:37 | INFO | train_inner | epoch 037:    550 / 685 loss=2.891, nll_loss=1.271, ppl=2.41, wps=88636.8, ups=6.24, wpb=14194.1, bsz=754.7, num_updates=25200, lr=0.000931257, gnorm=0.716, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4347
2024-02-10 14:59:53 | INFO | train_inner | epoch 037:    650 / 685 loss=2.888, nll_loss=1.269, ppl=2.41, wps=90792.6, ups=6.19, wpb=14666.9, bsz=754.9, num_updates=25300, lr=0.000930371, gnorm=0.657, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4363
2024-02-10 14:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 14:59:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:00:01 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.491 | nll_loss 1.883 | ppl 3.69 | wps 154349 | wpb 5716.3 | bsz 292.8 | num_updates 25335 | best_loss 3.464
2024-02-10 15:00:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 25335 updates
2024-02-10 15:00:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt
2024-02-10 15:00:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt
2024-02-10 15:00:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt (epoch 37 @ 25335 updates, score 3.491) (writing took 2.3399198851548135 seconds)
2024-02-10 15:00:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-02-10 15:00:03 | INFO | train | epoch 037 | loss 2.876 | nll_loss 1.253 | ppl 2.38 | wps 86773.2 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 25335 | lr 0.00093006 | gnorm 0.681 | loss_scale 0.25 | train_wall 109 | gb_free 28.4 | wall 4373
2024-02-10 15:00:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:00:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:00:03 | INFO | fairseq.trainer | begin training epoch 38
2024-02-10 15:00:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:00:14 | INFO | train_inner | epoch 038:     65 / 685 loss=2.864, nll_loss=1.239, ppl=2.36, wps=69612.1, ups=4.87, wpb=14292.9, bsz=750.7, num_updates=25400, lr=0.00092948, gnorm=1.121, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4383
2024-02-10 15:00:30 | INFO | train_inner | epoch 038:    165 / 685 loss=2.855, nll_loss=1.23, ppl=2.35, wps=91315.7, ups=6.23, wpb=14648.1, bsz=756.8, num_updates=25500, lr=0.000928584, gnorm=0.688, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4399
2024-02-10 15:00:46 | INFO | train_inner | epoch 038:    265 / 685 loss=2.847, nll_loss=1.222, ppl=2.33, wps=90876.2, ups=6.19, wpb=14691.7, bsz=765.2, num_updates=25600, lr=0.000927682, gnorm=0.758, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4416
2024-02-10 15:01:02 | INFO | train_inner | epoch 038:    365 / 685 loss=2.858, nll_loss=1.232, ppl=2.35, wps=88951.7, ups=6.26, wpb=14200.1, bsz=744.8, num_updates=25700, lr=0.000926775, gnorm=0.67, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=4432
2024-02-10 15:01:18 | INFO | train_inner | epoch 038:    465 / 685 loss=2.857, nll_loss=1.234, ppl=2.35, wps=91071.6, ups=6.2, wpb=14686.2, bsz=757.2, num_updates=25800, lr=0.000925863, gnorm=0.675, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4448
2024-02-10 15:01:34 | INFO | train_inner | epoch 038:    565 / 685 loss=2.872, nll_loss=1.251, ppl=2.38, wps=91287.7, ups=6.22, wpb=14666.1, bsz=736.3, num_updates=25900, lr=0.000924946, gnorm=0.644, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4464
2024-02-10 15:01:50 | INFO | train_inner | epoch 038:    665 / 685 loss=2.891, nll_loss=1.272, ppl=2.42, wps=88845.3, ups=6.24, wpb=14228.1, bsz=730.8, num_updates=26000, lr=0.000924024, gnorm=0.752, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4480
2024-02-10 15:01:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:01:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:01:56 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.528 | nll_loss 1.93 | ppl 3.81 | wps 151968 | wpb 5716.3 | bsz 292.8 | num_updates 26020 | best_loss 3.464
2024-02-10 15:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 26020 updates
2024-02-10 15:01:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt
2024-02-10 15:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt
2024-02-10 15:01:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt (epoch 38 @ 26020 updates, score 3.528) (writing took 2.3791080228984356 seconds)
2024-02-10 15:01:58 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-02-10 15:01:58 | INFO | train | epoch 038 | loss 2.864 | nll_loss 1.241 | ppl 2.36 | wps 86780.4 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 26020 | lr 0.000923839 | gnorm 0.783 | loss_scale 0.25 | train_wall 108 | gb_free 28.3 | wall 4487
2024-02-10 15:01:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:01:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:01:58 | INFO | fairseq.trainer | begin training epoch 39
2024-02-10 15:01:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:02:11 | INFO | train_inner | epoch 039:     80 / 685 loss=2.84, nll_loss=1.213, ppl=2.32, wps=71115.3, ups=4.83, wpb=14715.3, bsz=750.2, num_updates=26100, lr=0.000923097, gnorm=0.866, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=4501
2024-02-10 15:02:27 | INFO | train_inner | epoch 039:    180 / 685 loss=2.829, nll_loss=1.201, ppl=2.3, wps=89999.3, ups=6.2, wpb=14506.4, bsz=733.4, num_updates=26200, lr=0.000922164, gnorm=0.665, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4517
2024-02-10 15:02:43 | INFO | train_inner | epoch 039:    280 / 685 loss=2.827, nll_loss=1.199, ppl=2.3, wps=90466.3, ups=6.18, wpb=14636.8, bsz=783.2, num_updates=26300, lr=0.000921226, gnorm=0.669, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4533
2024-02-10 15:02:59 | INFO | train_inner | epoch 039:    380 / 685 loss=2.861, nll_loss=1.239, ppl=2.36, wps=91506.3, ups=6.24, wpb=14675.8, bsz=745.1, num_updates=26400, lr=0.000920283, gnorm=0.699, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4549
2024-02-10 15:03:15 | INFO | train_inner | epoch 039:    480 / 685 loss=2.865, nll_loss=1.245, ppl=2.37, wps=91215.7, ups=6.18, wpb=14759.4, bsz=764.2, num_updates=26500, lr=0.000919335, gnorm=0.734, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4565
2024-02-10 15:03:31 | INFO | train_inner | epoch 039:    580 / 685 loss=2.862, nll_loss=1.239, ppl=2.36, wps=88587.5, ups=6.25, wpb=14181.8, bsz=746.8, num_updates=26600, lr=0.000918382, gnorm=0.701, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4581
2024-02-10 15:03:47 | INFO | train_inner | epoch 039:    680 / 685 loss=2.868, nll_loss=1.246, ppl=2.37, wps=89381.4, ups=6.28, wpb=14232.2, bsz=719, num_updates=26700, lr=0.000917424, gnorm=0.7, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4597
2024-02-10 15:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:03:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:03:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.488 | nll_loss 1.879 | ppl 3.68 | wps 158900 | wpb 5716.3 | bsz 292.8 | num_updates 26705 | best_loss 3.464
2024-02-10 15:03:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 26705 updates
2024-02-10 15:03:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt
2024-02-10 15:03:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt
2024-02-10 15:03:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt (epoch 39 @ 26705 updates, score 3.488) (writing took 2.3009324020240456 seconds)
2024-02-10 15:03:52 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-02-10 15:03:52 | INFO | train | epoch 039 | loss 2.848 | nll_loss 1.224 | ppl 2.34 | wps 86853.8 | ups 5.98 | wpb 14524.4 | bsz 748.5 | num_updates 26705 | lr 0.000917376 | gnorm 0.697 | loss_scale 0.25 | train_wall 109 | gb_free 28.4 | wall 4602
2024-02-10 15:03:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:03:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:03:53 | INFO | fairseq.trainer | begin training epoch 40
2024-02-10 15:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:04:08 | INFO | train_inner | epoch 040:     95 / 685 loss=2.815, nll_loss=1.186, ppl=2.28, wps=71539.4, ups=4.88, wpb=14647.4, bsz=729.9, num_updates=26800, lr=0.000916461, gnorm=0.699, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4617
2024-02-10 15:04:25 | INFO | train_inner | epoch 040:    195 / 685 loss=2.814, nll_loss=1.184, ppl=2.27, wps=84299, ups=5.88, wpb=14341.5, bsz=710.5, num_updates=26900, lr=0.000915492, gnorm=0.683, loss_scale=0.25, train_wall=17, gb_free=28.6, wall=4634
2024-02-10 15:04:42 | INFO | train_inner | epoch 040:    295 / 685 loss=2.831, nll_loss=1.204, ppl=2.3, wps=85671.4, ups=5.85, wpb=14637.1, bsz=733.8, num_updates=27000, lr=0.000914519, gnorm=0.709, loss_scale=0.25, train_wall=17, gb_free=28.3, wall=4652
2024-02-10 15:04:58 | INFO | train_inner | epoch 040:    395 / 685 loss=2.842, nll_loss=1.218, ppl=2.33, wps=88071.4, ups=6.11, wpb=14422.6, bsz=758.6, num_updates=27100, lr=0.00091354, gnorm=0.692, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4668
2024-02-10 15:05:15 | INFO | train_inner | epoch 040:    495 / 685 loss=2.848, nll_loss=1.226, ppl=2.34, wps=90296.7, ups=6.18, wpb=14617.4, bsz=768.2, num_updates=27200, lr=0.000912557, gnorm=0.663, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4684
2024-02-10 15:05:31 | INFO | train_inner | epoch 040:    595 / 685 loss=2.84, nll_loss=1.217, ppl=2.32, wps=90492.8, ups=6.22, wpb=14559.3, bsz=786, num_updates=27300, lr=0.000911568, gnorm=0.689, loss_scale=0.25, train_wall=16, gb_free=28.7, wall=4700
2024-02-10 15:05:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:05:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:05:47 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.458 | nll_loss 1.858 | ppl 3.62 | wps 158684 | wpb 5716.3 | bsz 292.8 | num_updates 27390 | best_loss 3.458
2024-02-10 15:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 27390 updates
2024-02-10 15:05:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt
2024-02-10 15:05:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt
2024-02-10 15:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt (epoch 40 @ 27390 updates, score 3.458) (writing took 3.7758372630923986 seconds)
2024-02-10 15:05:51 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-02-10 15:05:51 | INFO | train | epoch 040 | loss 2.835 | nll_loss 1.209 | ppl 2.31 | wps 84007.8 | ups 5.78 | wpb 14524.4 | bsz 748.5 | num_updates 27390 | lr 0.000910674 | gnorm 0.697 | loss_scale 0.25 | train_wall 111 | gb_free 28.3 | wall 4720
2024-02-10 15:05:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:05:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:05:51 | INFO | fairseq.trainer | begin training epoch 41
2024-02-10 15:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:05:53 | INFO | train_inner | epoch 041:     10 / 685 loss=2.845, nll_loss=1.223, ppl=2.33, wps=65643.8, ups=4.52, wpb=14528, bsz=768.6, num_updates=27400, lr=0.000910575, gnorm=0.738, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4722
2024-02-10 15:06:09 | INFO | train_inner | epoch 041:    110 / 685 loss=2.796, nll_loss=1.164, ppl=2.24, wps=89724.1, ups=6.18, wpb=14518.6, bsz=732.7, num_updates=27500, lr=0.000909576, gnorm=0.778, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4739
2024-02-10 15:06:25 | INFO | train_inner | epoch 041:    210 / 685 loss=2.811, nll_loss=1.182, ppl=2.27, wps=90867.9, ups=6.26, wpb=14511.1, bsz=724.7, num_updates=27600, lr=0.000908572, gnorm=0.685, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4754
2024-02-10 15:06:41 | INFO | train_inner | epoch 041:    310 / 685 loss=2.824, nll_loss=1.199, ppl=2.3, wps=90175.3, ups=6.19, wpb=14576.8, bsz=793.1, num_updates=27700, lr=0.000907564, gnorm=0.753, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4771
2024-02-10 15:06:57 | INFO | train_inner | epoch 041:    410 / 685 loss=2.825, nll_loss=1.2, ppl=2.3, wps=89632, ups=6.18, wpb=14492.6, bsz=760.8, num_updates=27800, lr=0.00090655, gnorm=0.702, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4787
2024-02-10 15:07:13 | INFO | train_inner | epoch 041:    510 / 685 loss=2.828, nll_loss=1.201, ppl=2.3, wps=90214.5, ups=6.24, wpb=14456.1, bsz=742.2, num_updates=27900, lr=0.000905532, gnorm=0.685, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4803
2024-02-10 15:07:29 | INFO | train_inner | epoch 041:    610 / 685 loss=2.834, nll_loss=1.21, ppl=2.31, wps=90368.8, ups=6.24, wpb=14493.8, bsz=729.8, num_updates=28000, lr=0.000904508, gnorm=0.678, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=4819
2024-02-10 15:07:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:07:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:07:43 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.474 | nll_loss 1.878 | ppl 3.68 | wps 159025 | wpb 5716.3 | bsz 292.8 | num_updates 28075 | best_loss 3.458
2024-02-10 15:07:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 28075 updates
2024-02-10 15:07:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt
2024-02-10 15:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt
2024-02-10 15:07:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt (epoch 41 @ 28075 updates, score 3.474) (writing took 2.33459059917368 seconds)
2024-02-10 15:07:46 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-02-10 15:07:46 | INFO | train | epoch 041 | loss 2.822 | nll_loss 1.195 | ppl 2.29 | wps 86704.7 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 28075 | lr 0.000903738 | gnorm 0.712 | loss_scale 0.25 | train_wall 109 | gb_free 28.3 | wall 4835
2024-02-10 15:07:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:07:46 | INFO | fairseq.trainer | begin training epoch 42
2024-02-10 15:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:07:50 | INFO | train_inner | epoch 042:     25 / 685 loss=2.819, nll_loss=1.193, ppl=2.29, wps=70150.4, ups=4.86, wpb=14435.1, bsz=751.6, num_updates=28100, lr=0.00090348, gnorm=0.692, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4839
2024-02-10 15:08:06 | INFO | train_inner | epoch 042:    125 / 685 loss=2.769, nll_loss=1.135, ppl=2.2, wps=90869.6, ups=6.22, wpb=14608.5, bsz=753.6, num_updates=28200, lr=0.000902447, gnorm=0.681, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4856
2024-02-10 15:08:22 | INFO | train_inner | epoch 042:    225 / 685 loss=2.795, nll_loss=1.167, ppl=2.24, wps=90731.2, ups=6.2, wpb=14643.5, bsz=757.9, num_updates=28300, lr=0.000901409, gnorm=0.696, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4872
2024-02-10 15:08:38 | INFO | train_inner | epoch 042:    325 / 685 loss=2.824, nll_loss=1.198, ppl=2.29, wps=90454.9, ups=6.25, wpb=14463.9, bsz=738.3, num_updates=28400, lr=0.000900366, gnorm=0.683, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4888
2024-02-10 15:08:54 | INFO | train_inner | epoch 042:    425 / 685 loss=2.82, nll_loss=1.195, ppl=2.29, wps=88687.9, ups=6.24, wpb=14219.2, bsz=742.3, num_updates=28500, lr=0.000899318, gnorm=0.688, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=4904
2024-02-10 15:09:10 | INFO | train_inner | epoch 042:    525 / 685 loss=2.828, nll_loss=1.204, ppl=2.3, wps=90997.2, ups=6.25, wpb=14564.3, bsz=751.4, num_updates=28600, lr=0.000898265, gnorm=0.706, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=4920
2024-02-10 15:09:26 | INFO | train_inner | epoch 042:    625 / 685 loss=2.821, nll_loss=1.197, ppl=2.29, wps=91156.5, ups=6.22, wpb=14666.1, bsz=747.3, num_updates=28700, lr=0.000897207, gnorm=0.767, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=4936
2024-02-10 15:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:09:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:09:38 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.485 | nll_loss 1.88 | ppl 3.68 | wps 159400 | wpb 5716.3 | bsz 292.8 | num_updates 28760 | best_loss 3.458
2024-02-10 15:09:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 28760 updates
2024-02-10 15:09:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt
2024-02-10 15:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt
2024-02-10 15:09:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt (epoch 42 @ 28760 updates, score 3.485) (writing took 2.3587490301579237 seconds)
2024-02-10 15:09:40 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-02-10 15:09:40 | INFO | train | epoch 042 | loss 2.816 | nll_loss 1.19 | ppl 2.28 | wps 86915.1 | ups 5.98 | wpb 14524.4 | bsz 748.5 | num_updates 28760 | lr 0.00089657 | gnorm 0.768 | loss_scale 0.25 | train_wall 108 | gb_free 28.4 | wall 4950
2024-02-10 15:09:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:09:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:09:40 | INFO | fairseq.trainer | begin training epoch 43
2024-02-10 15:09:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:09:47 | INFO | train_inner | epoch 043:     40 / 685 loss=2.842, nll_loss=1.219, ppl=2.33, wps=70423.5, ups=4.86, wpb=14476.3, bsz=740.1, num_updates=28800, lr=0.000896145, gnorm=1.174, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=4956
2024-02-10 15:10:03 | INFO | train_inner | epoch 043:    140 / 685 loss=2.769, nll_loss=1.134, ppl=2.2, wps=90741.4, ups=6.23, wpb=14568.7, bsz=737.9, num_updates=28900, lr=0.000895078, gnorm=0.674, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4972
2024-02-10 15:10:19 | INFO | train_inner | epoch 043:    240 / 685 loss=2.789, nll_loss=1.159, ppl=2.23, wps=90444.4, ups=6.23, wpb=14506.2, bsz=753.2, num_updates=29000, lr=0.000894005, gnorm=0.806, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=4988
2024-02-10 15:10:35 | INFO | train_inner | epoch 043:    340 / 685 loss=2.802, nll_loss=1.174, ppl=2.26, wps=89283.4, ups=6.26, wpb=14265, bsz=721.8, num_updates=29100, lr=0.000892928, gnorm=0.898, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=5004
2024-02-10 15:10:59 | INFO | train_inner | epoch 043:    440 / 685 loss=2.801, nll_loss=1.174, ppl=2.26, wps=60908.1, ups=4.19, wpb=14552.1, bsz=778.1, num_updates=29200, lr=0.000891847, gnorm=0.709, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=5028
2024-02-10 15:11:15 | INFO | train_inner | epoch 043:    540 / 685 loss=2.806, nll_loss=1.181, ppl=2.27, wps=91574.8, ups=6.18, wpb=14821.5, bsz=764.6, num_updates=29300, lr=0.00089076, gnorm=0.673, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5044
2024-02-10 15:11:31 | INFO | train_inner | epoch 043:    640 / 685 loss=2.827, nll_loss=1.206, ppl=2.31, wps=90910.6, ups=6.22, wpb=14627.4, bsz=735.5, num_updates=29400, lr=0.000889669, gnorm=0.723, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5061
2024-02-10 15:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:11:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:11:40 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.435 | nll_loss 1.831 | ppl 3.56 | wps 158736 | wpb 5716.3 | bsz 292.8 | num_updates 29445 | best_loss 3.435
2024-02-10 15:11:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 29445 updates
2024-02-10 15:11:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt
2024-02-10 15:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt
2024-02-10 15:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt (epoch 43 @ 29445 updates, score 3.435) (writing took 3.6756720261182636 seconds)
2024-02-10 15:11:44 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-02-10 15:11:44 | INFO | train | epoch 043 | loss 2.797 | nll_loss 1.169 | ppl 2.25 | wps 80389.5 | ups 5.53 | wpb 14524.4 | bsz 748.5 | num_updates 29445 | lr 0.000889176 | gnorm 0.743 | loss_scale 0.25 | train_wall 109 | gb_free 28.4 | wall 5073
2024-02-10 15:11:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:11:44 | INFO | fairseq.trainer | begin training epoch 44
2024-02-10 15:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:11:53 | INFO | train_inner | epoch 044:     55 / 685 loss=2.782, nll_loss=1.151, ppl=2.22, wps=66182, ups=4.56, wpb=14525.5, bsz=767.6, num_updates=29500, lr=0.000888573, gnorm=0.786, loss_scale=0.25, train_wall=16, gb_free=28.7, wall=5083
2024-02-10 15:12:09 | INFO | train_inner | epoch 044:    155 / 685 loss=2.771, nll_loss=1.139, ppl=2.2, wps=87912.7, ups=6.14, wpb=14310.6, bsz=762.2, num_updates=29600, lr=0.000887472, gnorm=0.722, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5099
2024-02-10 15:12:25 | INFO | train_inner | epoch 044:    255 / 685 loss=2.78, nll_loss=1.151, ppl=2.22, wps=89845.6, ups=6.15, wpb=14598.7, bsz=748.6, num_updates=29700, lr=0.000886367, gnorm=0.692, loss_scale=0.25, train_wall=16, gb_free=28.6, wall=5115
2024-02-10 15:12:42 | INFO | train_inner | epoch 044:    355 / 685 loss=2.775, nll_loss=1.144, ppl=2.21, wps=89722.2, ups=6.18, wpb=14529.1, bsz=788.9, num_updates=29800, lr=0.000885257, gnorm=0.697, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5131
2024-02-10 15:12:58 | INFO | train_inner | epoch 044:    455 / 685 loss=2.813, nll_loss=1.188, ppl=2.28, wps=92956, ups=6.23, wpb=14930, bsz=732.2, num_updates=29900, lr=0.000884142, gnorm=0.722, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=5147
2024-02-10 15:13:14 | INFO | train_inner | epoch 044:    555 / 685 loss=2.796, nll_loss=1.169, ppl=2.25, wps=90942.8, ups=6.23, wpb=14593, bsz=742.2, num_updates=30000, lr=0.000883022, gnorm=0.671, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5163
2024-02-10 15:13:30 | INFO | train_inner | epoch 044:    655 / 685 loss=2.811, nll_loss=1.187, ppl=2.28, wps=88850.2, ups=6.29, wpb=14121.4, bsz=682.7, num_updates=30100, lr=0.000881898, gnorm=0.73, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5179
2024-02-10 15:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:13:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:13:36 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.446 | nll_loss 1.848 | ppl 3.6 | wps 158604 | wpb 5716.3 | bsz 292.8 | num_updates 30130 | best_loss 3.435
2024-02-10 15:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 30130 updates
2024-02-10 15:13:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt
2024-02-10 15:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt
2024-02-10 15:13:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt (epoch 44 @ 30130 updates, score 3.446) (writing took 2.411320451879874 seconds)
2024-02-10 15:13:39 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-02-10 15:13:39 | INFO | train | epoch 044 | loss 2.788 | nll_loss 1.159 | ppl 2.23 | wps 86493.9 | ups 5.96 | wpb 14524.4 | bsz 748.5 | num_updates 30130 | lr 0.00088156 | gnorm 0.718 | loss_scale 0.25 | train_wall 109 | gb_free 28.3 | wall 5188
2024-02-10 15:13:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:13:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:13:39 | INFO | fairseq.trainer | begin training epoch 45
2024-02-10 15:13:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:13:50 | INFO | train_inner | epoch 045:     70 / 685 loss=2.756, nll_loss=1.122, ppl=2.18, wps=69867.5, ups=4.82, wpb=14484.9, bsz=786.8, num_updates=30200, lr=0.000880769, gnorm=0.691, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5200
2024-02-10 15:14:06 | INFO | train_inner | epoch 045:    170 / 685 loss=2.76, nll_loss=1.127, ppl=2.18, wps=91111.5, ups=6.21, wpb=14663.5, bsz=730.4, num_updates=30300, lr=0.000879636, gnorm=0.679, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5216
2024-02-10 15:14:23 | INFO | train_inner | epoch 045:    270 / 685 loss=2.774, nll_loss=1.144, ppl=2.21, wps=92151.9, ups=6.23, wpb=14791.7, bsz=753.8, num_updates=30400, lr=0.000878498, gnorm=0.662, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5232
2024-02-10 15:14:39 | INFO | train_inner | epoch 045:    370 / 685 loss=2.777, nll_loss=1.147, ppl=2.21, wps=88284.1, ups=6.23, wpb=14162.7, bsz=735.8, num_updates=30500, lr=0.000877355, gnorm=0.773, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5248
2024-02-10 15:14:55 | INFO | train_inner | epoch 045:    470 / 685 loss=2.789, nll_loss=1.162, ppl=2.24, wps=90169.8, ups=6.22, wpb=14497.1, bsz=757.6, num_updates=30600, lr=0.000876207, gnorm=0.725, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5264
2024-02-10 15:15:20 | INFO | train_inner | epoch 045:    570 / 685 loss=2.8, nll_loss=1.175, ppl=2.26, wps=59143.9, ups=4.04, wpb=14640.4, bsz=751.8, num_updates=30700, lr=0.000875056, gnorm=1.284, loss_scale=0.25, train_wall=24, gb_free=28.3, wall=5289
2024-02-10 15:15:36 | INFO | train_inner | epoch 045:    670 / 685 loss=2.792, nll_loss=1.165, ppl=2.24, wps=90508.9, ups=6.24, wpb=14493.9, bsz=757.7, num_updates=30800, lr=0.000873899, gnorm=0.728, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5305
2024-02-10 15:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:15:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:15:40 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.448 | nll_loss 1.844 | ppl 3.59 | wps 157808 | wpb 5716.3 | bsz 292.8 | num_updates 30815 | best_loss 3.435
2024-02-10 15:15:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 30815 updates
2024-02-10 15:15:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt
2024-02-10 15:15:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt
2024-02-10 15:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt (epoch 45 @ 30815 updates, score 3.448) (writing took 2.4947132819797844 seconds)
2024-02-10 15:15:43 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-02-10 15:15:43 | INFO | train | epoch 045 | loss 2.777 | nll_loss 1.148 | ppl 2.22 | wps 80213.5 | ups 5.52 | wpb 14524.4 | bsz 748.5 | num_updates 30815 | lr 0.000873725 | gnorm 0.794 | loss_scale 0.25 | train_wall 117 | gb_free 28.4 | wall 5313
2024-02-10 15:15:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:15:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:15:43 | INFO | fairseq.trainer | begin training epoch 46
2024-02-10 15:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:15:57 | INFO | train_inner | epoch 046:     85 / 685 loss=2.741, nll_loss=1.107, ppl=2.15, wps=68291.6, ups=4.7, wpb=14516.9, bsz=738.8, num_updates=30900, lr=0.000872738, gnorm=0.76, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5326
2024-02-10 15:16:13 | INFO | train_inner | epoch 046:    185 / 685 loss=2.753, nll_loss=1.12, ppl=2.17, wps=89418.1, ups=6.19, wpb=14442.6, bsz=737.4, num_updates=31000, lr=0.000871572, gnorm=0.684, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=5343
2024-02-10 15:16:29 | INFO | train_inner | epoch 046:    285 / 685 loss=2.746, nll_loss=1.113, ppl=2.16, wps=89323.1, ups=6.19, wpb=14440.9, bsz=779.8, num_updates=31100, lr=0.000870402, gnorm=0.736, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5359
2024-02-10 15:16:45 | INFO | train_inner | epoch 046:    385 / 685 loss=2.771, nll_loss=1.141, ppl=2.2, wps=90444.7, ups=6.24, wpb=14487.4, bsz=719.3, num_updates=31200, lr=0.000869228, gnorm=0.928, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5375
2024-02-10 15:17:01 | INFO | train_inner | epoch 046:    485 / 685 loss=2.783, nll_loss=1.156, ppl=2.23, wps=90208, ups=6.22, wpb=14509.5, bsz=756.1, num_updates=31300, lr=0.000868049, gnorm=0.804, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5391
2024-02-10 15:17:17 | INFO | train_inner | epoch 046:    585 / 685 loss=2.786, nll_loss=1.159, ppl=2.23, wps=91133.3, ups=6.21, wpb=14681.2, bsz=772.8, num_updates=31400, lr=0.000866865, gnorm=0.75, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5407
2024-02-10 15:17:33 | INFO | train_inner | epoch 046:    685 / 685 loss=2.797, nll_loss=1.171, ppl=2.25, wps=90985.1, ups=6.26, wpb=14543.5, bsz=718.4, num_updates=31500, lr=0.000865677, gnorm=0.726, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=5423
2024-02-10 15:17:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:17:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:17:35 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.454 | nll_loss 1.858 | ppl 3.62 | wps 160655 | wpb 5716.3 | bsz 292.8 | num_updates 31500 | best_loss 3.435
2024-02-10 15:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 31500 updates
2024-02-10 15:17:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt
2024-02-10 15:17:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt
2024-02-10 15:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt (epoch 46 @ 31500 updates, score 3.454) (writing took 2.3690691720694304 seconds)
2024-02-10 15:17:38 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-02-10 15:17:38 | INFO | train | epoch 046 | loss 2.768 | nll_loss 1.138 | ppl 2.2 | wps 86667.4 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 31500 | lr 0.000865677 | gnorm 0.769 | loss_scale 0.25 | train_wall 109 | gb_free 28.4 | wall 5427
2024-02-10 15:17:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:17:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:17:38 | INFO | fairseq.trainer | begin training epoch 47
2024-02-10 15:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:17:54 | INFO | train_inner | epoch 047:    100 / 685 loss=2.725, nll_loss=1.088, ppl=2.13, wps=70257.6, ups=4.86, wpb=14460.5, bsz=738.2, num_updates=31600, lr=0.000864484, gnorm=0.76, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5444
2024-02-10 15:18:10 | INFO | train_inner | epoch 047:    200 / 685 loss=2.735, nll_loss=1.101, ppl=2.14, wps=89635.3, ups=6.19, wpb=14478.6, bsz=769.8, num_updates=31700, lr=0.000863287, gnorm=0.782, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=5460
2024-02-10 15:18:26 | INFO | train_inner | epoch 047:    300 / 685 loss=2.748, nll_loss=1.115, ppl=2.17, wps=90067.3, ups=6.23, wpb=14461.6, bsz=717.6, num_updates=31800, lr=0.000862086, gnorm=0.724, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5476
2024-02-10 15:18:43 | INFO | train_inner | epoch 047:    400 / 685 loss=2.767, nll_loss=1.138, ppl=2.2, wps=89045.1, ups=6.06, wpb=14686, bsz=748.4, num_updates=31900, lr=0.00086088, gnorm=0.776, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=5492
2024-02-10 15:18:59 | INFO | train_inner | epoch 047:    500 / 685 loss=2.76, nll_loss=1.129, ppl=2.19, wps=89914.7, ups=6.22, wpb=14452, bsz=759, num_updates=32000, lr=0.00085967, gnorm=0.698, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5508
2024-02-10 15:19:15 | INFO | train_inner | epoch 047:    600 / 685 loss=2.775, nll_loss=1.148, ppl=2.22, wps=90551.7, ups=6.19, wpb=14622.4, bsz=756.2, num_updates=32100, lr=0.000858455, gnorm=0.795, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=5524
2024-02-10 15:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:19:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:19:30 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.481 | nll_loss 1.898 | ppl 3.73 | wps 159254 | wpb 5716.3 | bsz 292.8 | num_updates 32185 | best_loss 3.435
2024-02-10 15:19:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 32185 updates
2024-02-10 15:19:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt
2024-02-10 15:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt
2024-02-10 15:19:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt (epoch 47 @ 32185 updates, score 3.481) (writing took 2.3491265091579407 seconds)
2024-02-10 15:19:33 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-02-10 15:19:33 | INFO | train | epoch 047 | loss 2.756 | nll_loss 1.125 | ppl 2.18 | wps 86472.3 | ups 5.95 | wpb 14524.4 | bsz 748.5 | num_updates 32185 | lr 0.000857419 | gnorm 0.75 | loss_scale 0.5 | train_wall 109 | gb_free 28.4 | wall 5542
2024-02-10 15:19:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:19:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:19:33 | INFO | fairseq.trainer | begin training epoch 48
2024-02-10 15:19:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:19:35 | INFO | train_inner | epoch 048:     15 / 685 loss=2.775, nll_loss=1.147, ppl=2.21, wps=70700.1, ups=4.88, wpb=14482.5, bsz=749.8, num_updates=32200, lr=0.000857236, gnorm=0.701, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5545
2024-02-10 15:19:51 | INFO | train_inner | epoch 048:    115 / 685 loss=2.712, nll_loss=1.073, ppl=2.1, wps=91097.6, ups=6.26, wpb=14547.3, bsz=762.2, num_updates=32300, lr=0.000856013, gnorm=0.691, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5561
2024-02-10 15:20:07 | INFO | train_inner | epoch 048:    215 / 685 loss=2.732, nll_loss=1.098, ppl=2.14, wps=90927.7, ups=6.23, wpb=14587.4, bsz=734.7, num_updates=32400, lr=0.000854785, gnorm=0.702, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=5577
2024-02-10 15:20:23 | INFO | train_inner | epoch 048:    315 / 685 loss=2.742, nll_loss=1.109, ppl=2.16, wps=89590.7, ups=6.26, wpb=14313.3, bsz=728.5, num_updates=32500, lr=0.000853553, gnorm=0.774, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5593
2024-02-10 15:20:40 | INFO | train_inner | epoch 048:    415 / 685 loss=2.749, nll_loss=1.117, ppl=2.17, wps=88696.8, ups=6.13, wpb=14459.1, bsz=735.4, num_updates=32600, lr=0.000852317, gnorm=0.719, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5609
2024-02-10 15:20:56 | INFO | train_inner | epoch 048:    515 / 685 loss=2.75, nll_loss=1.122, ppl=2.18, wps=88740.6, ups=6, wpb=14783.3, bsz=794.7, num_updates=32700, lr=0.000851077, gnorm=0.709, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5626
2024-02-10 15:21:12 | INFO | train_inner | epoch 048:    615 / 685 loss=2.755, nll_loss=1.126, ppl=2.18, wps=90389.7, ups=6.2, wpb=14568.9, bsz=751.4, num_updates=32800, lr=0.000849832, gnorm=0.758, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5642
2024-02-10 15:21:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:21:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:21:26 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.454 | nll_loss 1.854 | ppl 3.62 | wps 158475 | wpb 5716.3 | bsz 292.8 | num_updates 32870 | best_loss 3.435
2024-02-10 15:21:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 32870 updates
2024-02-10 15:21:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt
2024-02-10 15:21:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt
2024-02-10 15:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt (epoch 48 @ 32870 updates, score 3.454) (writing took 2.316032962175086 seconds)
2024-02-10 15:21:28 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-02-10 15:21:28 | INFO | train | epoch 048 | loss 2.745 | nll_loss 1.113 | ppl 2.16 | wps 86452.8 | ups 5.95 | wpb 14524.4 | bsz 748.5 | num_updates 32870 | lr 0.000848958 | gnorm 0.732 | loss_scale 0.5 | train_wall 109 | gb_free 28.7 | wall 5657
2024-02-10 15:21:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:21:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:21:28 | INFO | fairseq.trainer | begin training epoch 49
2024-02-10 15:21:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:21:33 | INFO | train_inner | epoch 049:     30 / 685 loss=2.766, nll_loss=1.137, ppl=2.2, wps=70900.5, ups=4.9, wpb=14460.3, bsz=717.8, num_updates=32900, lr=0.000848583, gnorm=0.754, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5662
2024-02-10 15:21:49 | INFO | train_inner | epoch 049:    130 / 685 loss=2.696, nll_loss=1.057, ppl=2.08, wps=89991.8, ups=6.22, wpb=14479.5, bsz=774.5, num_updates=33000, lr=0.000847329, gnorm=0.763, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5679
2024-02-10 15:22:05 | INFO | train_inner | epoch 049:    230 / 685 loss=2.779, nll_loss=1.153, ppl=2.22, wps=90355, ups=6.21, wpb=14552.7, bsz=723, num_updates=33100, lr=0.000846072, gnorm=1.545, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=5695
2024-02-10 15:22:21 | INFO | train_inner | epoch 049:    330 / 685 loss=2.747, nll_loss=1.117, ppl=2.17, wps=92478.2, ups=6.19, wpb=14935.3, bsz=757.2, num_updates=33200, lr=0.00084481, gnorm=0.802, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5711
2024-02-10 15:22:37 | INFO | train_inner | epoch 049:    430 / 685 loss=2.75, nll_loss=1.12, ppl=2.17, wps=88367.9, ups=6.15, wpb=14361.3, bsz=734.9, num_updates=33300, lr=0.000843544, gnorm=0.764, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5727
2024-02-10 15:22:54 | INFO | train_inner | epoch 049:    530 / 685 loss=2.745, nll_loss=1.115, ppl=2.17, wps=89849.9, ups=6.19, wpb=14511.1, bsz=763, num_updates=33400, lr=0.000842274, gnorm=0.79, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5743
2024-02-10 15:23:10 | INFO | train_inner | epoch 049:    630 / 685 loss=2.766, nll_loss=1.138, ppl=2.2, wps=88149.2, ups=6.16, wpb=14317.2, bsz=728.6, num_updates=33500, lr=0.000840999, gnorm=0.818, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5759
2024-02-10 15:23:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:23:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:23:21 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.475 | nll_loss 1.879 | ppl 3.68 | wps 158701 | wpb 5716.3 | bsz 292.8 | num_updates 33555 | best_loss 3.435
2024-02-10 15:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 33555 updates
2024-02-10 15:23:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt
2024-02-10 15:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt
2024-02-10 15:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt (epoch 49 @ 33555 updates, score 3.475) (writing took 2.323570078937337 seconds)
2024-02-10 15:23:23 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-02-10 15:23:23 | INFO | train | epoch 049 | loss 2.746 | nll_loss 1.115 | ppl 2.17 | wps 86381.6 | ups 5.95 | wpb 14524.4 | bsz 748.5 | num_updates 33555 | lr 0.000840297 | gnorm 0.891 | loss_scale 0.5 | train_wall 109 | gb_free 28.3 | wall 5773
2024-02-10 15:23:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:23:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:23:23 | INFO | fairseq.trainer | begin training epoch 50
2024-02-10 15:23:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:23:30 | INFO | train_inner | epoch 050:     45 / 685 loss=2.715, nll_loss=1.078, ppl=2.11, wps=69269.2, ups=4.85, wpb=14286.8, bsz=771.6, num_updates=33600, lr=0.000839721, gnorm=0.739, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=5780
2024-02-10 15:23:47 | INFO | train_inner | epoch 050:    145 / 685 loss=2.698, nll_loss=1.06, ppl=2.09, wps=89641.3, ups=6.21, wpb=14444.1, bsz=775.4, num_updates=33700, lr=0.000838438, gnorm=0.717, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5796
2024-02-10 15:24:03 | INFO | train_inner | epoch 050:    245 / 685 loss=2.729, nll_loss=1.096, ppl=2.14, wps=90172.1, ups=6.2, wpb=14535.1, bsz=720.3, num_updates=33800, lr=0.000837151, gnorm=0.812, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=5812
2024-02-10 15:24:19 | INFO | train_inner | epoch 050:    345 / 685 loss=2.716, nll_loss=1.081, ppl=2.12, wps=90842.3, ups=6.21, wpb=14635.7, bsz=733.6, num_updates=33900, lr=0.00083586, gnorm=0.743, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5828
2024-02-10 15:24:35 | INFO | train_inner | epoch 050:    445 / 685 loss=2.731, nll_loss=1.099, ppl=2.14, wps=92069.8, ups=6.2, wpb=14841, bsz=761.4, num_updates=34000, lr=0.000834565, gnorm=0.706, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5845
2024-02-10 15:24:51 | INFO | train_inner | epoch 050:    545 / 685 loss=2.732, nll_loss=1.102, ppl=2.15, wps=89274, ups=6.17, wpb=14471.2, bsz=783.8, num_updates=34100, lr=0.000833266, gnorm=0.697, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5861
2024-02-10 15:25:07 | INFO | train_inner | epoch 050:    645 / 685 loss=2.756, nll_loss=1.128, ppl=2.19, wps=89908.3, ups=6.26, wpb=14354, bsz=730.6, num_updates=34200, lr=0.000831963, gnorm=0.801, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5877
2024-02-10 15:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:25:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:25:16 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.441 | nll_loss 1.839 | ppl 3.58 | wps 157958 | wpb 5716.3 | bsz 292.8 | num_updates 34240 | best_loss 3.435
2024-02-10 15:25:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 34240 updates
2024-02-10 15:25:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt
2024-02-10 15:25:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt
2024-02-10 15:25:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt (epoch 50 @ 34240 updates, score 3.441) (writing took 2.3919225581921637 seconds)
2024-02-10 15:25:18 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-02-10 15:25:18 | INFO | train | epoch 050 | loss 2.725 | nll_loss 1.092 | ppl 2.13 | wps 86517.5 | ups 5.96 | wpb 14524.4 | bsz 748.5 | num_updates 34240 | lr 0.000831441 | gnorm 0.741 | loss_scale 0.5 | train_wall 109 | gb_free 28.4 | wall 5888
2024-02-10 15:25:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:25:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:25:18 | INFO | fairseq.trainer | begin training epoch 51
2024-02-10 15:25:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:25:28 | INFO | train_inner | epoch 051:     60 / 685 loss=2.706, nll_loss=1.071, ppl=2.1, wps=70484.1, ups=4.79, wpb=14700.4, bsz=759, num_updates=34300, lr=0.000830656, gnorm=0.693, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5898
2024-02-10 15:25:44 | INFO | train_inner | epoch 051:    160 / 685 loss=2.705, nll_loss=1.069, ppl=2.1, wps=90983.9, ups=6.24, wpb=14579.2, bsz=738.9, num_updates=34400, lr=0.000829345, gnorm=0.71, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5914
2024-02-10 15:26:00 | INFO | train_inner | epoch 051:    260 / 685 loss=2.71, nll_loss=1.075, ppl=2.11, wps=90672.4, ups=6.21, wpb=14601.3, bsz=787.2, num_updates=34500, lr=0.00082803, gnorm=0.708, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=5930
2024-02-10 15:26:16 | INFO | train_inner | epoch 051:    360 / 685 loss=2.722, nll_loss=1.088, ppl=2.13, wps=89920.9, ups=6.26, wpb=14374.7, bsz=711.1, num_updates=34600, lr=0.00082671, gnorm=0.752, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5946
2024-02-10 15:26:32 | INFO | train_inner | epoch 051:    460 / 685 loss=2.723, nll_loss=1.091, ppl=2.13, wps=88124.8, ups=6.17, wpb=14282.7, bsz=730.3, num_updates=34700, lr=0.000825387, gnorm=0.778, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=5962
2024-02-10 15:26:48 | INFO | train_inner | epoch 051:    560 / 685 loss=2.72, nll_loss=1.088, ppl=2.13, wps=90817, ups=6.21, wpb=14617.9, bsz=739.8, num_updates=34800, lr=0.00082406, gnorm=0.705, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=5978
2024-02-10 15:27:05 | INFO | train_inner | epoch 051:    660 / 685 loss=2.73, nll_loss=1.1, ppl=2.14, wps=89565.5, ups=6.16, wpb=14544.3, bsz=769.4, num_updates=34900, lr=0.000822729, gnorm=0.715, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=5994
2024-02-10 15:27:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:27:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:27:11 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.453 | nll_loss 1.853 | ppl 3.61 | wps 158014 | wpb 5716.3 | bsz 292.8 | num_updates 34925 | best_loss 3.435
2024-02-10 15:27:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 34925 updates
2024-02-10 15:27:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt
2024-02-10 15:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt
2024-02-10 15:27:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt (epoch 51 @ 34925 updates, score 3.453) (writing took 2.355774015886709 seconds)
2024-02-10 15:27:13 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-02-10 15:27:13 | INFO | train | epoch 051 | loss 2.715 | nll_loss 1.081 | ppl 2.12 | wps 86586.8 | ups 5.96 | wpb 14524.4 | bsz 748.5 | num_updates 34925 | lr 0.000822395 | gnorm 0.731 | loss_scale 0.5 | train_wall 109 | gb_free 28.4 | wall 6003
2024-02-10 15:27:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:27:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:27:13 | INFO | fairseq.trainer | begin training epoch 52
2024-02-10 15:27:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:27:25 | INFO | train_inner | epoch 052:     75 / 685 loss=2.696, nll_loss=1.06, ppl=2.08, wps=71344.2, ups=4.86, wpb=14685.2, bsz=753.8, num_updates=35000, lr=0.000821394, gnorm=0.737, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6015
2024-02-10 15:27:41 | INFO | train_inner | epoch 052:    175 / 685 loss=2.689, nll_loss=1.052, ppl=2.07, wps=89951.9, ups=6.22, wpb=14459.9, bsz=714, num_updates=35100, lr=0.000820055, gnorm=0.864, loss_scale=0.5, train_wall=16, gb_free=28.6, wall=6031
2024-02-10 15:27:58 | INFO | train_inner | epoch 052:    275 / 685 loss=2.69, nll_loss=1.052, ppl=2.07, wps=88008.2, ups=6.13, wpb=14356.3, bsz=732.6, num_updates=35200, lr=0.000818712, gnorm=0.729, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6047
2024-02-10 15:28:14 | INFO | train_inner | epoch 052:    375 / 685 loss=2.715, nll_loss=1.082, ppl=2.12, wps=90371.7, ups=6.21, wpb=14550.3, bsz=758.1, num_updates=35300, lr=0.000817365, gnorm=0.74, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6063
2024-02-10 15:28:30 | INFO | train_inner | epoch 052:    475 / 685 loss=2.716, nll_loss=1.083, ppl=2.12, wps=89767.4, ups=6.23, wpb=14409.6, bsz=748, num_updates=35400, lr=0.000816015, gnorm=0.724, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=6079
2024-02-10 15:28:46 | INFO | train_inner | epoch 052:    575 / 685 loss=2.723, nll_loss=1.091, ppl=2.13, wps=90581.2, ups=6.18, wpb=14648.4, bsz=765.8, num_updates=35500, lr=0.00081466, gnorm=0.773, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6095
2024-02-10 15:29:02 | INFO | train_inner | epoch 052:    675 / 685 loss=2.717, nll_loss=1.086, ppl=2.12, wps=91086.1, ups=6.19, wpb=14718.1, bsz=774.3, num_updates=35600, lr=0.000813302, gnorm=1.038, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6112
2024-02-10 15:29:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:29:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:29:06 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.451 | nll_loss 1.856 | ppl 3.62 | wps 143428 | wpb 5716.3 | bsz 292.8 | num_updates 35610 | best_loss 3.435
2024-02-10 15:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 35610 updates
2024-02-10 15:29:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt
2024-02-10 15:29:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt
2024-02-10 15:29:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt (epoch 52 @ 35610 updates, score 3.451) (writing took 2.332782565848902 seconds)
2024-02-10 15:29:08 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-02-10 15:29:08 | INFO | train | epoch 052 | loss 2.705 | nll_loss 1.071 | ppl 2.1 | wps 86313.8 | ups 5.94 | wpb 14524.4 | bsz 748.5 | num_updates 35610 | lr 0.000813166 | gnorm 0.795 | loss_scale 0.5 | train_wall 109 | gb_free 28.4 | wall 6118
2024-02-10 15:29:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:29:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:29:08 | INFO | fairseq.trainer | begin training epoch 53
2024-02-10 15:29:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:29:23 | INFO | train_inner | epoch 053:     90 / 685 loss=2.666, nll_loss=1.025, ppl=2.03, wps=69036.4, ups=4.8, wpb=14370.9, bsz=755.2, num_updates=35700, lr=0.00081194, gnorm=0.766, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6132
2024-02-10 15:29:39 | INFO | train_inner | epoch 053:    190 / 685 loss=2.68, nll_loss=1.042, ppl=2.06, wps=90376.4, ups=6.22, wpb=14541.1, bsz=697.3, num_updates=35800, lr=0.000810574, gnorm=0.696, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=6149
2024-02-10 15:29:55 | INFO | train_inner | epoch 053:    290 / 685 loss=2.694, nll_loss=1.058, ppl=2.08, wps=91461.6, ups=6.22, wpb=14695.2, bsz=754.3, num_updates=35900, lr=0.000809204, gnorm=0.692, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6165
2024-02-10 15:30:11 | INFO | train_inner | epoch 053:    390 / 685 loss=2.693, nll_loss=1.057, ppl=2.08, wps=89693.9, ups=6.21, wpb=14446.5, bsz=742.2, num_updates=36000, lr=0.000807831, gnorm=0.7, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6181
2024-02-10 15:30:27 | INFO | train_inner | epoch 053:    490 / 685 loss=2.707, nll_loss=1.073, ppl=2.1, wps=89801.7, ups=6.26, wpb=14343.1, bsz=759.7, num_updates=36100, lr=0.000806454, gnorm=0.763, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6197
2024-02-10 15:30:43 | INFO | train_inner | epoch 053:    590 / 685 loss=2.704, nll_loss=1.071, ppl=2.1, wps=89614.3, ups=6.21, wpb=14436.1, bsz=756.6, num_updates=36200, lr=0.000805073, gnorm=0.73, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6213
2024-02-10 15:30:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:30:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:31:01 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.429 | nll_loss 1.831 | ppl 3.56 | wps 157923 | wpb 5716.3 | bsz 292.8 | num_updates 36295 | best_loss 3.429
2024-02-10 15:31:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 36295 updates
2024-02-10 15:31:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt
2024-02-10 15:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt
2024-02-10 15:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt (epoch 53 @ 36295 updates, score 3.429) (writing took 3.5865175491198897 seconds)
2024-02-10 15:31:04 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-02-10 15:31:04 | INFO | train | epoch 053 | loss 2.694 | nll_loss 1.058 | ppl 2.08 | wps 85842.5 | ups 5.91 | wpb 14524.4 | bsz 748.5 | num_updates 36295 | lr 0.000803757 | gnorm 0.726 | loss_scale 0.5 | train_wall 109 | gb_free 28.2 | wall 6234
2024-02-10 15:31:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:31:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:31:04 | INFO | fairseq.trainer | begin training epoch 54
2024-02-10 15:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:31:05 | INFO | train_inner | epoch 054:      5 / 685 loss=2.712, nll_loss=1.081, ppl=2.12, wps=67203.3, ups=4.57, wpb=14692.5, bsz=761.6, num_updates=36300, lr=0.000803688, gnorm=0.731, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6235
2024-02-10 15:31:21 | INFO | train_inner | epoch 054:    105 / 685 loss=2.654, nll_loss=1.011, ppl=2.02, wps=90298.3, ups=6.21, wpb=14544.8, bsz=767.2, num_updates=36400, lr=0.0008023, gnorm=0.719, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=6251
2024-02-10 15:31:37 | INFO | train_inner | epoch 054:    205 / 685 loss=2.678, nll_loss=1.041, ppl=2.06, wps=93089, ups=6.23, wpb=14933.9, bsz=724.2, num_updates=36500, lr=0.000800908, gnorm=0.686, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6267
2024-02-10 15:31:53 | INFO | train_inner | epoch 054:    305 / 685 loss=2.674, nll_loss=1.035, ppl=2.05, wps=88760.4, ups=6.26, wpb=14168.8, bsz=735.4, num_updates=36600, lr=0.000799512, gnorm=0.717, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6283
2024-02-10 15:32:09 | INFO | train_inner | epoch 054:    405 / 685 loss=2.696, nll_loss=1.062, ppl=2.09, wps=91016.2, ups=6.22, wpb=14621.3, bsz=758.6, num_updates=36700, lr=0.000798112, gnorm=0.796, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6299
2024-02-10 15:32:25 | INFO | train_inner | epoch 054:    505 / 685 loss=2.704, nll_loss=1.073, ppl=2.1, wps=90836.4, ups=6.18, wpb=14691.9, bsz=784.2, num_updates=36800, lr=0.000796709, gnorm=0.781, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6315
2024-02-10 15:32:41 | INFO | train_inner | epoch 054:    605 / 685 loss=2.713, nll_loss=1.08, ppl=2.11, wps=89989.1, ups=6.26, wpb=14378.5, bsz=704.6, num_updates=36900, lr=0.000795303, gnorm=0.745, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6331
2024-02-10 15:32:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:32:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:32:56 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.452 | nll_loss 1.858 | ppl 3.63 | wps 159640 | wpb 5716.3 | bsz 292.8 | num_updates 36980 | best_loss 3.429
2024-02-10 15:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 36980 updates
2024-02-10 15:32:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt
2024-02-10 15:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt
2024-02-10 15:32:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt (epoch 54 @ 36980 updates, score 3.452) (writing took 2.352299378020689 seconds)
2024-02-10 15:32:59 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-02-10 15:32:59 | INFO | train | epoch 054 | loss 2.686 | nll_loss 1.05 | ppl 2.07 | wps 86897.7 | ups 5.98 | wpb 14524.4 | bsz 748.5 | num_updates 36980 | lr 0.000794175 | gnorm 0.738 | loss_scale 0.5 | train_wall 108 | gb_free 28.3 | wall 6348
2024-02-10 15:32:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:32:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:32:59 | INFO | fairseq.trainer | begin training epoch 55
2024-02-10 15:32:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:33:02 | INFO | train_inner | epoch 055:     20 / 685 loss=2.676, nll_loss=1.039, ppl=2.06, wps=69755, ups=4.84, wpb=14407.5, bsz=771.5, num_updates=37000, lr=0.000793893, gnorm=0.718, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6352
2024-02-10 15:33:18 | INFO | train_inner | epoch 055:    120 / 685 loss=2.641, nll_loss=0.999, ppl=2, wps=89582.9, ups=6.1, wpb=14682.6, bsz=769, num_updates=37100, lr=0.000792479, gnorm=0.729, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6368
2024-02-10 15:33:35 | INFO | train_inner | epoch 055:    220 / 685 loss=2.674, nll_loss=1.037, ppl=2.05, wps=90811.3, ups=6.21, wpb=14626.8, bsz=759.5, num_updates=37200, lr=0.000791061, gnorm=0.776, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6384
2024-02-10 15:33:51 | INFO | train_inner | epoch 055:    320 / 685 loss=2.683, nll_loss=1.047, ppl=2.07, wps=91026.4, ups=6.23, wpb=14608.9, bsz=736, num_updates=37300, lr=0.000789641, gnorm=0.746, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6400
2024-02-10 15:34:07 | INFO | train_inner | epoch 055:    420 / 685 loss=2.679, nll_loss=1.042, ppl=2.06, wps=89342.5, ups=6.24, wpb=14315.7, bsz=737.4, num_updates=37400, lr=0.000788216, gnorm=0.715, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6416
2024-02-10 15:34:23 | INFO | train_inner | epoch 055:    520 / 685 loss=2.682, nll_loss=1.047, ppl=2.07, wps=90310, ups=6.21, wpb=14541.8, bsz=735.4, num_updates=37500, lr=0.000786788, gnorm=0.71, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=6432
2024-02-10 15:34:39 | INFO | train_inner | epoch 055:    620 / 685 loss=2.683, nll_loss=1.049, ppl=2.07, wps=90350, ups=6.24, wpb=14489.9, bsz=761.1, num_updates=37600, lr=0.000785357, gnorm=0.698, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6448
2024-02-10 15:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:34:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:34:51 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.412 | nll_loss 1.814 | ppl 3.52 | wps 159055 | wpb 5716.3 | bsz 292.8 | num_updates 37665 | best_loss 3.412
2024-02-10 15:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 37665 updates
2024-02-10 15:34:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt
2024-02-10 15:34:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt
2024-02-10 15:34:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt (epoch 55 @ 37665 updates, score 3.412) (writing took 3.7345304479822516 seconds)
2024-02-10 15:34:55 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-02-10 15:34:55 | INFO | train | epoch 055 | loss 2.676 | nll_loss 1.039 | ppl 2.06 | wps 85591.5 | ups 5.89 | wpb 14524.4 | bsz 748.5 | num_updates 37665 | lr 0.000784424 | gnorm 0.727 | loss_scale 0.5 | train_wall 109 | gb_free 28.4 | wall 6464
2024-02-10 15:34:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:34:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:34:55 | INFO | fairseq.trainer | begin training epoch 56
2024-02-10 15:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:35:01 | INFO | train_inner | epoch 056:     35 / 685 loss=2.685, nll_loss=1.05, ppl=2.07, wps=65809.7, ups=4.56, wpb=14430.5, bsz=730, num_updates=37700, lr=0.000783922, gnorm=0.719, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6470
2024-02-10 15:35:17 | INFO | train_inner | epoch 056:    135 / 685 loss=2.636, nll_loss=0.994, ppl=1.99, wps=90972, ups=6.19, wpb=14702.3, bsz=768, num_updates=37800, lr=0.000782484, gnorm=0.665, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6486
2024-02-10 15:35:33 | INFO | train_inner | epoch 056:    235 / 685 loss=2.65, nll_loss=1.009, ppl=2.01, wps=89930.9, ups=6.22, wpb=14448.8, bsz=776.9, num_updates=37900, lr=0.000781042, gnorm=0.709, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6503
2024-02-10 15:35:49 | INFO | train_inner | epoch 056:    335 / 685 loss=2.668, nll_loss=1.031, ppl=2.04, wps=90409.7, ups=6.23, wpb=14516.4, bsz=718.8, num_updates=38000, lr=0.000779596, gnorm=0.736, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6519
2024-02-10 15:36:05 | INFO | train_inner | epoch 056:    435 / 685 loss=2.675, nll_loss=1.04, ppl=2.06, wps=89928.7, ups=6.23, wpb=14440, bsz=735.7, num_updates=38100, lr=0.000778148, gnorm=0.705, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6535
2024-02-10 15:36:21 | INFO | train_inner | epoch 056:    535 / 685 loss=2.688, nll_loss=1.054, ppl=2.08, wps=90031.9, ups=6.23, wpb=14451.2, bsz=763.4, num_updates=38200, lr=0.000776696, gnorm=0.728, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6551
2024-02-10 15:36:37 | INFO | train_inner | epoch 056:    635 / 685 loss=2.68, nll_loss=1.045, ppl=2.06, wps=89828, ups=6.25, wpb=14372.6, bsz=734, num_updates=38300, lr=0.00077524, gnorm=0.735, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6567
2024-02-10 15:36:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:36:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:36:47 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.404 | nll_loss 1.798 | ppl 3.48 | wps 159426 | wpb 5716.3 | bsz 292.8 | num_updates 38350 | best_loss 3.404
2024-02-10 15:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 38350 updates
2024-02-10 15:36:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt
2024-02-10 15:36:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt
2024-02-10 15:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt (epoch 56 @ 38350 updates, score 3.404) (writing took 3.6418321719393134 seconds)
2024-02-10 15:36:51 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-02-10 15:36:51 | INFO | train | epoch 056 | loss 2.667 | nll_loss 1.029 | ppl 2.04 | wps 85846.8 | ups 5.91 | wpb 14524.4 | bsz 748.5 | num_updates 38350 | lr 0.000774511 | gnorm 0.713 | loss_scale 0.5 | train_wall 108 | gb_free 28.2 | wall 6580
2024-02-10 15:36:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:36:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:36:51 | INFO | fairseq.trainer | begin training epoch 57
2024-02-10 15:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:36:59 | INFO | train_inner | epoch 057:     50 / 685 loss=2.662, nll_loss=1.024, ppl=2.03, wps=67277.1, ups=4.57, wpb=14737, bsz=755, num_updates=38400, lr=0.000773782, gnorm=0.74, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6589
2024-02-10 15:37:15 | INFO | train_inner | epoch 057:    150 / 685 loss=2.632, nll_loss=0.99, ppl=1.99, wps=88806.9, ups=6.22, wpb=14284.5, bsz=749.4, num_updates=38500, lr=0.00077232, gnorm=0.788, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6605
2024-02-10 15:37:31 | INFO | train_inner | epoch 057:    250 / 685 loss=2.66, nll_loss=1.021, ppl=2.03, wps=91243.6, ups=6.22, wpb=14676.4, bsz=734.2, num_updates=38600, lr=0.000770854, gnorm=0.684, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6621
2024-02-10 15:37:47 | INFO | train_inner | epoch 057:    350 / 685 loss=2.653, nll_loss=1.014, ppl=2.02, wps=90490.4, ups=6.22, wpb=14538.5, bsz=756.2, num_updates=38700, lr=0.000769385, gnorm=0.712, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6637
2024-02-10 15:38:03 | INFO | train_inner | epoch 057:    450 / 685 loss=2.673, nll_loss=1.037, ppl=2.05, wps=89253, ups=6.27, wpb=14229.4, bsz=734.5, num_updates=38800, lr=0.000767913, gnorm=0.759, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6653
2024-02-10 15:38:19 | INFO | train_inner | epoch 057:    550 / 685 loss=2.657, nll_loss=1.019, ppl=2.03, wps=90494.5, ups=6.23, wpb=14535.4, bsz=747.7, num_updates=38900, lr=0.000766438, gnorm=0.754, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6669
2024-02-10 15:38:35 | INFO | train_inner | epoch 057:    650 / 685 loss=2.667, nll_loss=1.033, ppl=2.05, wps=91725, ups=6.18, wpb=14843.9, bsz=768, num_updates=39000, lr=0.00076496, gnorm=0.693, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6685
2024-02-10 15:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:38:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:38:43 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.398 | nll_loss 1.806 | ppl 3.5 | wps 159077 | wpb 5716.3 | bsz 292.8 | num_updates 39035 | best_loss 3.398
2024-02-10 15:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 39035 updates
2024-02-10 15:38:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt
2024-02-10 15:38:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt
2024-02-10 15:38:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt (epoch 57 @ 39035 updates, score 3.398) (writing took 3.704393788939342 seconds)
2024-02-10 15:38:47 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-02-10 15:38:47 | INFO | train | epoch 057 | loss 2.658 | nll_loss 1.02 | ppl 2.03 | wps 85823.1 | ups 5.91 | wpb 14524.4 | bsz 748.5 | num_updates 39035 | lr 0.000764441 | gnorm 0.737 | loss_scale 0.5 | train_wall 109 | gb_free 28.3 | wall 6696
2024-02-10 15:38:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:38:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:38:47 | INFO | fairseq.trainer | begin training epoch 58
2024-02-10 15:38:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:38:57 | INFO | train_inner | epoch 058:     65 / 685 loss=2.655, nll_loss=1.016, ppl=2.02, wps=66730.3, ups=4.58, wpb=14566.6, bsz=741.8, num_updates=39100, lr=0.000763478, gnorm=0.761, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6707
2024-02-10 15:39:13 | INFO | train_inner | epoch 058:    165 / 685 loss=2.632, nll_loss=0.991, ppl=1.99, wps=90557.6, ups=6.21, wpb=14592.5, bsz=758.5, num_updates=39200, lr=0.000761993, gnorm=0.854, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6723
2024-02-10 15:39:30 | INFO | train_inner | epoch 058:    265 / 685 loss=2.643, nll_loss=1.005, ppl=2.01, wps=90975.1, ups=6.2, wpb=14674, bsz=759.7, num_updates=39300, lr=0.000760505, gnorm=0.722, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6739
2024-02-10 15:39:45 | INFO | train_inner | epoch 058:    365 / 685 loss=2.644, nll_loss=1.004, ppl=2.01, wps=89387.5, ups=6.25, wpb=14291, bsz=750.9, num_updates=39400, lr=0.000759014, gnorm=0.713, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6755
2024-02-10 15:40:02 | INFO | train_inner | epoch 058:    465 / 685 loss=2.659, nll_loss=1.022, ppl=2.03, wps=90237.8, ups=6.24, wpb=14458.5, bsz=702.1, num_updates=39500, lr=0.000757519, gnorm=0.724, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6771
2024-02-10 15:40:18 | INFO | train_inner | epoch 058:    565 / 685 loss=2.653, nll_loss=1.016, ppl=2.02, wps=89438.7, ups=6.22, wpb=14381.1, bsz=751.3, num_updates=39600, lr=0.000756021, gnorm=0.697, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6787
2024-02-10 15:40:34 | INFO | train_inner | epoch 058:    665 / 685 loss=2.666, nll_loss=1.032, ppl=2.04, wps=91090.7, ups=6.19, wpb=14707, bsz=767, num_updates=39700, lr=0.000754521, gnorm=0.769, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6803
2024-02-10 15:40:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:40:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:40:39 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.429 | nll_loss 1.831 | ppl 3.56 | wps 159092 | wpb 5716.3 | bsz 292.8 | num_updates 39720 | best_loss 3.398
2024-02-10 15:40:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 39720 updates
2024-02-10 15:40:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt
2024-02-10 15:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt
2024-02-10 15:40:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt (epoch 58 @ 39720 updates, score 3.429) (writing took 2.3688906440511346 seconds)
2024-02-10 15:40:41 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-02-10 15:40:41 | INFO | train | epoch 058 | loss 2.649 | nll_loss 1.011 | ppl 2.01 | wps 86822.1 | ups 5.98 | wpb 14524.4 | bsz 748.5 | num_updates 39720 | lr 0.00075422 | gnorm 0.746 | loss_scale 0.5 | train_wall 108 | gb_free 28.3 | wall 6811
2024-02-10 15:40:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:40:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:40:41 | INFO | fairseq.trainer | begin training epoch 59
2024-02-10 15:40:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:40:54 | INFO | train_inner | epoch 059:     80 / 685 loss=2.612, nll_loss=0.968, ppl=1.96, wps=69098.5, ups=4.88, wpb=14157.8, bsz=752.5, num_updates=39800, lr=0.000753017, gnorm=0.742, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6824
2024-02-10 15:41:10 | INFO | train_inner | epoch 059:    180 / 685 loss=2.63, nll_loss=0.989, ppl=1.99, wps=90876.2, ups=6.18, wpb=14705.3, bsz=751.6, num_updates=39900, lr=0.00075151, gnorm=0.694, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=6840
2024-02-10 15:41:26 | INFO | train_inner | epoch 059:    280 / 685 loss=2.633, nll_loss=0.992, ppl=1.99, wps=89897.8, ups=6.25, wpb=14379.2, bsz=734, num_updates=40000, lr=0.00075, gnorm=0.74, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=6856
2024-02-10 15:41:43 | INFO | train_inner | epoch 059:    380 / 685 loss=2.644, nll_loss=1.005, ppl=2.01, wps=89037.2, ups=6.15, wpb=14486, bsz=727, num_updates=40100, lr=0.000748487, gnorm=0.756, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=6872
2024-02-10 15:42:00 | INFO | train_inner | epoch 059:    480 / 685 loss=2.667, nll_loss=1.033, ppl=2.05, wps=86364.5, ups=5.83, wpb=14809.5, bsz=747.3, num_updates=40200, lr=0.000746971, gnorm=0.731, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=6889
2024-02-10 15:42:17 | INFO | train_inner | epoch 059:    580 / 685 loss=2.655, nll_loss=1.018, ppl=2.03, wps=85625.6, ups=5.85, wpb=14641, bsz=782.5, num_updates=40300, lr=0.000745452, gnorm=0.722, loss_scale=0.5, train_wall=17, gb_free=28.3, wall=6907
2024-02-10 15:42:34 | INFO | train_inner | epoch 059:    680 / 685 loss=2.65, nll_loss=1.013, ppl=2.02, wps=84246.1, ups=5.86, wpb=14387.4, bsz=739.4, num_updates=40400, lr=0.00074393, gnorm=0.718, loss_scale=1, train_wall=17, gb_free=28.3, wall=6924
2024-02-10 15:42:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:42:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:42:37 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.429 | nll_loss 1.836 | ppl 3.57 | wps 134390 | wpb 5716.3 | bsz 292.8 | num_updates 40405 | best_loss 3.398
2024-02-10 15:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 40405 updates
2024-02-10 15:42:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt
2024-02-10 15:42:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt
2024-02-10 15:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt (epoch 59 @ 40405 updates, score 3.429) (writing took 2.479933074908331 seconds)
2024-02-10 15:42:40 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-02-10 15:42:40 | INFO | train | epoch 059 | loss 2.641 | nll_loss 1.002 | ppl 2 | wps 84035 | ups 5.79 | wpb 14524.4 | bsz 748.5 | num_updates 40405 | lr 0.000743854 | gnorm 0.729 | loss_scale 1 | train_wall 112 | gb_free 28.3 | wall 6929
2024-02-10 15:42:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:42:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:42:40 | INFO | fairseq.trainer | begin training epoch 60
2024-02-10 15:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:42:56 | INFO | train_inner | epoch 060:     95 / 685 loss=2.597, nll_loss=0.951, ppl=1.93, wps=68103.2, ups=4.64, wpb=14687.8, bsz=778.6, num_updates=40500, lr=0.000742405, gnorm=0.722, loss_scale=1, train_wall=16, gb_free=28.5, wall=6945
2024-02-10 15:43:12 | INFO | train_inner | epoch 060:    195 / 685 loss=2.617, nll_loss=0.975, ppl=1.97, wps=88805, ups=6.12, wpb=14503.2, bsz=752.4, num_updates=40600, lr=0.000740877, gnorm=0.686, loss_scale=1, train_wall=16, gb_free=28.2, wall=6961
2024-02-10 15:43:28 | INFO | train_inner | epoch 060:    295 / 685 loss=2.636, nll_loss=0.997, ppl=2, wps=93238.1, ups=6.24, wpb=14940.5, bsz=746.7, num_updates=40700, lr=0.000739346, gnorm=0.687, loss_scale=1, train_wall=16, gb_free=28.2, wall=6978
2024-02-10 15:43:44 | INFO | train_inner | epoch 060:    395 / 685 loss=2.63, nll_loss=0.99, ppl=1.99, wps=90097.7, ups=6.19, wpb=14548.3, bsz=710.5, num_updates=40800, lr=0.000737812, gnorm=0.69, loss_scale=1, train_wall=16, gb_free=28.3, wall=6994
2024-02-10 15:44:00 | INFO | train_inner | epoch 060:    495 / 685 loss=2.648, nll_loss=1.01, ppl=2.01, wps=87379.6, ups=6.16, wpb=14193.9, bsz=734.5, num_updates=40900, lr=0.000736275, gnorm=0.703, loss_scale=1, train_wall=16, gb_free=28.6, wall=7010
2024-02-10 15:44:17 | INFO | train_inner | epoch 060:    595 / 685 loss=2.644, nll_loss=1.007, ppl=2.01, wps=87803.2, ups=6.08, wpb=14430.9, bsz=781.7, num_updates=41000, lr=0.000734736, gnorm=0.739, loss_scale=1, train_wall=16, gb_free=28.3, wall=7026
2024-02-10 15:44:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:44:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:44:33 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.467 | nll_loss 1.877 | ppl 3.67 | wps 156757 | wpb 5716.3 | bsz 292.8 | num_updates 41090 | best_loss 3.398
2024-02-10 15:44:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 41090 updates
2024-02-10 15:44:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt
2024-02-10 15:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt
2024-02-10 15:44:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt (epoch 60 @ 41090 updates, score 3.467) (writing took 2.381217655027285 seconds)
2024-02-10 15:44:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-02-10 15:44:36 | INFO | train | epoch 060 | loss 2.631 | nll_loss 0.991 | ppl 1.99 | wps 85647.7 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 41090 | lr 0.000733348 | gnorm 0.718 | loss_scale 1 | train_wall 110 | gb_free 28.3 | wall 7045
2024-02-10 15:44:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:44:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:44:36 | INFO | fairseq.trainer | begin training epoch 61
2024-02-10 15:44:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:44:38 | INFO | train_inner | epoch 061:     10 / 685 loss=2.642, nll_loss=1.005, ppl=2.01, wps=69104.6, ups=4.79, wpb=14427.7, bsz=742.1, num_updates=41100, lr=0.000733193, gnorm=0.81, loss_scale=1, train_wall=16, gb_free=28.2, wall=7047
2024-02-10 15:44:54 | INFO | train_inner | epoch 061:    110 / 685 loss=2.582, nll_loss=0.935, ppl=1.91, wps=89267, ups=6.12, wpb=14582.7, bsz=812.6, num_updates=41200, lr=0.000731648, gnorm=0.666, loss_scale=1, train_wall=16, gb_free=28.3, wall=7064
2024-02-10 15:45:10 | INFO | train_inner | epoch 061:    210 / 685 loss=2.608, nll_loss=0.966, ppl=1.95, wps=90778.8, ups=6.18, wpb=14677.8, bsz=749, num_updates=41300, lr=0.0007301, gnorm=0.706, loss_scale=1, train_wall=16, gb_free=28.3, wall=7080
2024-02-10 15:45:26 | INFO | train_inner | epoch 061:    310 / 685 loss=2.617, nll_loss=0.975, ppl=1.97, wps=91635.8, ups=6.24, wpb=14679.9, bsz=740.3, num_updates=41400, lr=0.000728549, gnorm=0.726, loss_scale=1, train_wall=16, gb_free=28.6, wall=7096
2024-02-10 15:45:42 | INFO | train_inner | epoch 061:    410 / 685 loss=2.63, nll_loss=0.99, ppl=1.99, wps=89296.9, ups=6.3, wpb=14174.5, bsz=703.4, num_updates=41500, lr=0.000726995, gnorm=0.815, loss_scale=1, train_wall=16, gb_free=28.4, wall=7112
2024-02-10 15:45:58 | INFO | train_inner | epoch 061:    510 / 685 loss=2.634, nll_loss=0.996, ppl=1.99, wps=89431.5, ups=6.25, wpb=14307, bsz=737, num_updates=41600, lr=0.000725439, gnorm=0.718, loss_scale=1, train_wall=16, gb_free=28.3, wall=7128
2024-02-10 15:46:14 | INFO | train_inner | epoch 061:    610 / 685 loss=2.649, nll_loss=1.014, ppl=2.02, wps=90678.7, ups=6.21, wpb=14595.8, bsz=749, num_updates=41700, lr=0.00072388, gnorm=0.71, loss_scale=1, train_wall=16, gb_free=28.3, wall=7144
2024-02-10 15:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:46:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:46:28 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.412 | nll_loss 1.819 | ppl 3.53 | wps 159743 | wpb 5716.3 | bsz 292.8 | num_updates 41775 | best_loss 3.398
2024-02-10 15:46:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 41775 updates
2024-02-10 15:46:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt
2024-02-10 15:46:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt
2024-02-10 15:46:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt (epoch 61 @ 41775 updates, score 3.412) (writing took 2.4080844989512116 seconds)
2024-02-10 15:46:31 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-02-10 15:46:31 | INFO | train | epoch 061 | loss 2.622 | nll_loss 0.981 | ppl 1.97 | wps 86685.7 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 41775 | lr 0.000722708 | gnorm 0.724 | loss_scale 1 | train_wall 109 | gb_free 28.3 | wall 7160
2024-02-10 15:46:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:46:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:46:31 | INFO | fairseq.trainer | begin training epoch 62
2024-02-10 15:46:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:46:35 | INFO | train_inner | epoch 062:     25 / 685 loss=2.617, nll_loss=0.977, ppl=1.97, wps=70829.2, ups=4.82, wpb=14680, bsz=785.8, num_updates=41800, lr=0.000722318, gnorm=0.693, loss_scale=1, train_wall=16, gb_free=28.6, wall=7164
2024-02-10 15:46:51 | INFO | train_inner | epoch 062:    125 / 685 loss=2.593, nll_loss=0.948, ppl=1.93, wps=90364.3, ups=6.26, wpb=14442.3, bsz=742.6, num_updates=41900, lr=0.000720753, gnorm=0.693, loss_scale=1, train_wall=16, gb_free=28.5, wall=7180
2024-02-10 15:47:07 | INFO | train_inner | epoch 062:    225 / 685 loss=2.609, nll_loss=0.968, ppl=1.96, wps=90689.1, ups=6.2, wpb=14623.8, bsz=767.6, num_updates=42000, lr=0.000719186, gnorm=0.683, loss_scale=1, train_wall=16, gb_free=28.5, wall=7197
2024-02-10 15:47:23 | INFO | train_inner | epoch 062:    325 / 685 loss=2.608, nll_loss=0.967, ppl=1.96, wps=91250.5, ups=6.19, wpb=14731.3, bsz=760.2, num_updates=42100, lr=0.000717616, gnorm=0.682, loss_scale=1, train_wall=16, gb_free=28.3, wall=7213
2024-02-10 15:47:39 | INFO | train_inner | epoch 062:    425 / 685 loss=2.614, nll_loss=0.973, ppl=1.96, wps=89290.9, ups=6.24, wpb=14306.5, bsz=725.8, num_updates=42200, lr=0.000716043, gnorm=0.702, loss_scale=1, train_wall=16, gb_free=28.3, wall=7229
2024-02-10 15:47:55 | INFO | train_inner | epoch 062:    525 / 685 loss=2.634, nll_loss=0.997, ppl=2, wps=92311.8, ups=6.24, wpb=14783.3, bsz=715.8, num_updates=42300, lr=0.000714468, gnorm=0.691, loss_scale=1, train_wall=16, gb_free=28.4, wall=7245
2024-02-10 15:48:11 | INFO | train_inner | epoch 062:    625 / 685 loss=2.632, nll_loss=0.994, ppl=1.99, wps=88433.3, ups=6.25, wpb=14154.8, bsz=721, num_updates=42400, lr=0.00071289, gnorm=0.791, loss_scale=1, train_wall=16, gb_free=28.4, wall=7261
2024-02-10 15:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:48:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:48:23 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.46 | nll_loss 1.871 | ppl 3.66 | wps 160129 | wpb 5716.3 | bsz 292.8 | num_updates 42460 | best_loss 3.398
2024-02-10 15:48:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 42460 updates
2024-02-10 15:48:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt
2024-02-10 15:48:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt
2024-02-10 15:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt (epoch 62 @ 42460 updates, score 3.46) (writing took 2.329894782975316 seconds)
2024-02-10 15:48:25 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-02-10 15:48:25 | INFO | train | epoch 062 | loss 2.614 | nll_loss 0.973 | ppl 1.96 | wps 86900.9 | ups 5.98 | wpb 14524.4 | bsz 748.5 | num_updates 42460 | lr 0.000711942 | gnorm 0.708 | loss_scale 1 | train_wall 108 | gb_free 28.4 | wall 7275
2024-02-10 15:48:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:48:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:48:25 | INFO | fairseq.trainer | begin training epoch 63
2024-02-10 15:48:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:48:32 | INFO | train_inner | epoch 063:     40 / 685 loss=2.609, nll_loss=0.968, ppl=1.96, wps=71408.8, ups=4.87, wpb=14673.2, bsz=760.8, num_updates=42500, lr=0.000711309, gnorm=0.717, loss_scale=1, train_wall=16, gb_free=28.4, wall=7281
2024-02-10 15:48:48 | INFO | train_inner | epoch 063:    140 / 685 loss=2.58, nll_loss=0.934, ppl=1.91, wps=90102.8, ups=6.24, wpb=14434.8, bsz=730.4, num_updates=42600, lr=0.000709726, gnorm=0.709, loss_scale=1, train_wall=16, gb_free=28.4, wall=7297
2024-02-10 15:49:04 | INFO | train_inner | epoch 063:    240 / 685 loss=2.596, nll_loss=0.954, ppl=1.94, wps=87027.8, ups=6.04, wpb=14398.6, bsz=727.3, num_updates=42700, lr=0.00070814, gnorm=0.69, loss_scale=1, train_wall=16, gb_free=28.5, wall=7314
2024-02-10 15:49:21 | INFO | train_inner | epoch 063:    340 / 685 loss=2.599, nll_loss=0.957, ppl=1.94, wps=84662.7, ups=5.82, wpb=14545.6, bsz=778.7, num_updates=42800, lr=0.000706552, gnorm=0.706, loss_scale=1, train_wall=17, gb_free=28.4, wall=7331
2024-02-10 15:49:39 | INFO | train_inner | epoch 063:    440 / 685 loss=2.615, nll_loss=0.975, ppl=1.97, wps=84760.1, ups=5.83, wpb=14536, bsz=763, num_updates=42900, lr=0.000704962, gnorm=0.728, loss_scale=1, train_wall=17, gb_free=28.4, wall=7348
2024-02-10 15:49:56 | INFO | train_inner | epoch 063:    540 / 685 loss=2.619, nll_loss=0.98, ppl=1.97, wps=85195.6, ups=5.89, wpb=14473.3, bsz=740.2, num_updates=43000, lr=0.000703368, gnorm=0.71, loss_scale=1, train_wall=17, gb_free=28.3, wall=7365
2024-02-10 15:50:12 | INFO | train_inner | epoch 063:    640 / 685 loss=2.615, nll_loss=0.976, ppl=1.97, wps=88811.1, ups=6.08, wpb=14617.5, bsz=787.6, num_updates=43100, lr=0.000701773, gnorm=0.737, loss_scale=1, train_wall=16, gb_free=28.3, wall=7382
2024-02-10 15:50:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:50:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:50:21 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.422 | nll_loss 1.829 | ppl 3.55 | wps 158869 | wpb 5716.3 | bsz 292.8 | num_updates 43145 | best_loss 3.398
2024-02-10 15:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 43145 updates
2024-02-10 15:50:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt
2024-02-10 15:50:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt
2024-02-10 15:50:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt (epoch 63 @ 43145 updates, score 3.422) (writing took 2.335327177075669 seconds)
2024-02-10 15:50:24 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-02-10 15:50:24 | INFO | train | epoch 063 | loss 2.605 | nll_loss 0.963 | ppl 1.95 | wps 83992.5 | ups 5.78 | wpb 14524.4 | bsz 748.5 | num_updates 43145 | lr 0.000701054 | gnorm 0.709 | loss_scale 1 | train_wall 112 | gb_free 28.2 | wall 7393
2024-02-10 15:50:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:50:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:50:24 | INFO | fairseq.trainer | begin training epoch 64
2024-02-10 15:50:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:50:32 | INFO | train_inner | epoch 064:     55 / 685 loss=2.594, nll_loss=0.95, ppl=1.93, wps=70438.6, ups=4.9, wpb=14369.7, bsz=722.4, num_updates=43200, lr=0.000700175, gnorm=0.749, loss_scale=1, train_wall=16, gb_free=28.4, wall=7402
2024-02-10 15:50:48 | INFO | train_inner | epoch 064:    155 / 685 loss=2.586, nll_loss=0.941, ppl=1.92, wps=91785.4, ups=6.25, wpb=14678.4, bsz=741, num_updates=43300, lr=0.000698574, gnorm=0.669, loss_scale=1, train_wall=16, gb_free=28.5, wall=7418
2024-02-10 15:51:04 | INFO | train_inner | epoch 064:    255 / 685 loss=2.588, nll_loss=0.945, ppl=1.92, wps=89639.1, ups=6.24, wpb=14356.4, bsz=750.2, num_updates=43400, lr=0.000696971, gnorm=0.715, loss_scale=1, train_wall=16, gb_free=28.3, wall=7434
2024-02-10 15:51:21 | INFO | train_inner | epoch 064:    355 / 685 loss=2.603, nll_loss=0.962, ppl=1.95, wps=89730.6, ups=6.13, wpb=14632.1, bsz=762.4, num_updates=43500, lr=0.000695366, gnorm=0.723, loss_scale=1, train_wall=16, gb_free=28.2, wall=7450
2024-02-10 15:51:37 | INFO | train_inner | epoch 064:    455 / 685 loss=2.602, nll_loss=0.961, ppl=1.95, wps=88345.5, ups=6.02, wpb=14680.5, bsz=747, num_updates=43600, lr=0.000693758, gnorm=0.716, loss_scale=1, train_wall=16, gb_free=28.3, wall=7467
2024-02-10 15:51:54 | INFO | train_inner | epoch 064:    555 / 685 loss=2.614, nll_loss=0.975, ppl=1.97, wps=86693, ups=6, wpb=14443.6, bsz=719.5, num_updates=43700, lr=0.000692148, gnorm=0.768, loss_scale=1, train_wall=16, gb_free=28.2, wall=7484
2024-02-10 15:52:10 | INFO | train_inner | epoch 064:    655 / 685 loss=2.601, nll_loss=0.961, ppl=1.95, wps=89952.2, ups=6.2, wpb=14508.1, bsz=755.8, num_updates=43800, lr=0.000690535, gnorm=0.685, loss_scale=1, train_wall=16, gb_free=28.3, wall=7500
2024-02-10 15:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:52:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:52:17 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.437 | nll_loss 1.849 | ppl 3.6 | wps 159248 | wpb 5716.3 | bsz 292.8 | num_updates 43830 | best_loss 3.398
2024-02-10 15:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 43830 updates
2024-02-10 15:52:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt
2024-02-10 15:52:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt
2024-02-10 15:52:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt (epoch 64 @ 43830 updates, score 3.437) (writing took 2.323340421076864 seconds)
2024-02-10 15:52:19 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-02-10 15:52:19 | INFO | train | epoch 064 | loss 2.595 | nll_loss 0.953 | ppl 1.94 | wps 85937.9 | ups 5.92 | wpb 14524.4 | bsz 748.5 | num_updates 43830 | lr 0.000690051 | gnorm 0.718 | loss_scale 1 | train_wall 110 | gb_free 28.4 | wall 7509
2024-02-10 15:52:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:52:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:52:19 | INFO | fairseq.trainer | begin training epoch 65
2024-02-10 15:52:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:52:31 | INFO | train_inner | epoch 065:     70 / 685 loss=2.565, nll_loss=0.919, ppl=1.89, wps=71024.4, ups=4.86, wpb=14620, bsz=760.6, num_updates=43900, lr=0.00068892, gnorm=0.658, loss_scale=1, train_wall=16, gb_free=28.5, wall=7520
2024-02-10 15:52:47 | INFO | train_inner | epoch 065:    170 / 685 loss=2.578, nll_loss=0.933, ppl=1.91, wps=90316, ups=6.26, wpb=14416.6, bsz=729.3, num_updates=44000, lr=0.000687303, gnorm=0.705, loss_scale=1, train_wall=16, gb_free=28.4, wall=7536
2024-02-10 15:53:03 | INFO | train_inner | epoch 065:    270 / 685 loss=2.577, nll_loss=0.933, ppl=1.91, wps=89580.6, ups=6.17, wpb=14512, bsz=762.1, num_updates=44100, lr=0.000685684, gnorm=0.703, loss_scale=1, train_wall=16, gb_free=28.4, wall=7553
2024-02-10 15:53:19 | INFO | train_inner | epoch 065:    370 / 685 loss=2.586, nll_loss=0.944, ppl=1.92, wps=90286.3, ups=6.22, wpb=14521.1, bsz=778.4, num_updates=44200, lr=0.000684062, gnorm=0.689, loss_scale=1, train_wall=16, gb_free=28.6, wall=7569
2024-02-10 15:53:35 | INFO | train_inner | epoch 065:    470 / 685 loss=2.601, nll_loss=0.961, ppl=1.95, wps=91393.7, ups=6.18, wpb=14781, bsz=779, num_updates=44300, lr=0.000682438, gnorm=0.713, loss_scale=1, train_wall=16, gb_free=28.3, wall=7585
2024-02-10 15:53:51 | INFO | train_inner | epoch 065:    570 / 685 loss=2.608, nll_loss=0.969, ppl=1.96, wps=90763.9, ups=6.24, wpb=14542.6, bsz=728.3, num_updates=44400, lr=0.000680812, gnorm=0.707, loss_scale=1, train_wall=16, gb_free=28.4, wall=7601
2024-02-10 15:54:07 | INFO | train_inner | epoch 065:    670 / 685 loss=2.602, nll_loss=0.96, ppl=1.95, wps=89606.1, ups=6.27, wpb=14289.5, bsz=717.8, num_updates=44500, lr=0.000679184, gnorm=0.732, loss_scale=1, train_wall=16, gb_free=28.4, wall=7617
2024-02-10 15:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:54:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:54:12 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.397 | nll_loss 1.804 | ppl 3.49 | wps 158866 | wpb 5716.3 | bsz 292.8 | num_updates 44515 | best_loss 3.397
2024-02-10 15:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 44515 updates
2024-02-10 15:54:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt
2024-02-10 15:54:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt
2024-02-10 15:54:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt (epoch 65 @ 44515 updates, score 3.397) (writing took 3.7907472220249474 seconds)
2024-02-10 15:54:15 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-02-10 15:54:15 | INFO | train | epoch 065 | loss 2.588 | nll_loss 0.946 | ppl 1.93 | wps 85745.5 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 44515 | lr 0.00067894 | gnorm 0.701 | loss_scale 1 | train_wall 108 | gb_free 28.3 | wall 7625
2024-02-10 15:54:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:54:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:54:20 | INFO | fairseq.trainer | begin training epoch 66
2024-02-10 15:54:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:54:42 | INFO | train_inner | epoch 066:     85 / 685 loss=2.561, nll_loss=0.915, ppl=1.89, wps=41414.9, ups=2.84, wpb=14600.7, bsz=752.1, num_updates=44600, lr=0.000677553, gnorm=0.655, loss_scale=1, train_wall=16, gb_free=28.4, wall=7652
2024-02-10 15:54:59 | INFO | train_inner | epoch 066:    185 / 685 loss=2.572, nll_loss=0.927, ppl=1.9, wps=88785.5, ups=6.16, wpb=14416.6, bsz=759.2, num_updates=44700, lr=0.000675921, gnorm=0.709, loss_scale=1, train_wall=16, gb_free=28.4, wall=7668
2024-02-10 15:55:15 | INFO | train_inner | epoch 066:    285 / 685 loss=2.586, nll_loss=0.943, ppl=1.92, wps=90173.8, ups=6.25, wpb=14425.1, bsz=735.4, num_updates=44800, lr=0.000674286, gnorm=0.691, loss_scale=1, train_wall=16, gb_free=28.4, wall=7684
2024-02-10 15:55:31 | INFO | train_inner | epoch 066:    385 / 685 loss=2.569, nll_loss=0.924, ppl=1.9, wps=90855.8, ups=6.21, wpb=14629.6, bsz=736, num_updates=44900, lr=0.000672649, gnorm=0.666, loss_scale=1, train_wall=16, gb_free=28.2, wall=7700
2024-02-10 15:55:47 | INFO | train_inner | epoch 066:    485 / 685 loss=2.581, nll_loss=0.938, ppl=1.92, wps=88960.4, ups=6.13, wpb=14505.2, bsz=728.7, num_updates=45000, lr=0.00067101, gnorm=0.731, loss_scale=1, train_wall=16, gb_free=28.3, wall=7717
2024-02-10 15:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2024-02-10 15:56:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2024-02-10 15:56:04 | INFO | train_inner | epoch 066:    587 / 685 loss=2.643, nll_loss=1.01, ppl=2.01, wps=86091.4, ups=5.96, wpb=14451.7, bsz=761.5, num_updates=45100, lr=0.000669369, gnorm=2.183, loss_scale=0.25, train_wall=17, gb_free=28.2, wall=7733
2024-02-10 15:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:56:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:56:23 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.406 | nll_loss 1.818 | ppl 3.53 | wps 133336 | wpb 5716.3 | bsz 292.8 | num_updates 45198 | best_loss 3.397
2024-02-10 15:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 45198 updates
2024-02-10 15:56:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt
2024-02-10 15:56:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt
2024-02-10 15:56:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt (epoch 66 @ 45198 updates, score 3.406) (writing took 2.501794250914827 seconds)
2024-02-10 15:56:25 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-02-10 15:56:25 | INFO | train | epoch 066 | loss 2.589 | nll_loss 0.947 | ppl 1.93 | wps 76249.1 | ups 5.25 | wpb 14523.8 | bsz 749 | num_updates 45198 | lr 0.000667759 | gnorm 0.944 | loss_scale 0.25 | train_wall 110 | gb_free 28.3 | wall 7755
2024-02-10 15:56:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:56:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:56:26 | INFO | fairseq.trainer | begin training epoch 67
2024-02-10 15:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:56:26 | INFO | train_inner | epoch 067:      2 / 685 loss=2.613, nll_loss=0.976, ppl=1.97, wps=66349.1, ups=4.53, wpb=14662.4, bsz=764.3, num_updates=45200, lr=0.000667726, gnorm=0.924, loss_scale=0.25, train_wall=17, gb_free=28.2, wall=7756
2024-02-10 15:56:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2024-02-10 15:56:43 | INFO | train_inner | epoch 067:    103 / 685 loss=2.549, nll_loss=0.901, ppl=1.87, wps=84756.6, ups=5.78, wpb=14671.7, bsz=743.9, num_updates=45300, lr=0.000666081, gnorm=0.887, loss_scale=0.125, train_wall=17, gb_free=28.6, wall=7773
2024-02-10 15:57:00 | INFO | train_inner | epoch 067:    203 / 685 loss=2.569, nll_loss=0.924, ppl=1.9, wps=86129.8, ups=5.91, wpb=14561.7, bsz=744.4, num_updates=45400, lr=0.000664433, gnorm=0.748, loss_scale=0.125, train_wall=17, gb_free=28.3, wall=7790
2024-02-10 15:57:16 | INFO | train_inner | epoch 067:    303 / 685 loss=2.577, nll_loss=0.934, ppl=1.91, wps=89377, ups=6.17, wpb=14489.6, bsz=739.8, num_updates=45500, lr=0.000662784, gnorm=0.78, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=7806
2024-02-10 15:57:32 | INFO | train_inner | epoch 067:    403 / 685 loss=2.574, nll_loss=0.93, ppl=1.91, wps=91113.7, ups=6.23, wpb=14633.2, bsz=760.6, num_updates=45600, lr=0.000661133, gnorm=0.713, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=7822
2024-02-10 15:57:49 | INFO | train_inner | epoch 067:    503 / 685 loss=2.578, nll_loss=0.936, ppl=1.91, wps=87493.2, ups=6.17, wpb=14182.5, bsz=745.1, num_updates=45700, lr=0.00065948, gnorm=0.71, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=7838
2024-02-10 15:58:05 | INFO | train_inner | epoch 067:    603 / 685 loss=2.58, nll_loss=0.938, ppl=1.92, wps=87156.6, ups=6.06, wpb=14373.4, bsz=748.7, num_updates=45800, lr=0.000657825, gnorm=0.717, loss_scale=0.125, train_wall=16, gb_free=28.6, wall=7855
2024-02-10 15:58:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 15:58:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:58:21 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.421 | nll_loss 1.833 | ppl 3.56 | wps 145649 | wpb 5716.3 | bsz 292.8 | num_updates 45882 | best_loss 3.397
2024-02-10 15:58:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 45882 updates
2024-02-10 15:58:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt
2024-02-10 15:58:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt
2024-02-10 15:58:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt (epoch 67 @ 45882 updates, score 3.421) (writing took 2.3545302010606974 seconds)
2024-02-10 15:58:23 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-02-10 15:58:23 | INFO | train | epoch 067 | loss 2.573 | nll_loss 0.93 | ppl 1.9 | wps 84346.8 | ups 5.81 | wpb 14523 | bsz 748.3 | num_updates 45882 | lr 0.000656466 | gnorm 0.754 | loss_scale 0.125 | train_wall 111 | gb_free 28.4 | wall 7873
2024-02-10 15:58:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 15:58:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 15:58:23 | INFO | fairseq.trainer | begin training epoch 68
2024-02-10 15:58:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 15:58:26 | INFO | train_inner | epoch 068:     18 / 685 loss=2.578, nll_loss=0.936, ppl=1.91, wps=69489.9, ups=4.72, wpb=14734.6, bsz=773.1, num_updates=45900, lr=0.000656167, gnorm=0.712, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=7876
2024-02-10 15:58:42 | INFO | train_inner | epoch 068:    118 / 685 loss=2.537, nll_loss=0.889, ppl=1.85, wps=90383.6, ups=6.2, wpb=14588.8, bsz=771.2, num_updates=46000, lr=0.000654508, gnorm=0.717, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=7892
2024-02-10 15:58:59 | INFO | train_inner | epoch 068:    218 / 685 loss=2.559, nll_loss=0.914, ppl=1.88, wps=92203.4, ups=6.21, wpb=14852.8, bsz=740.7, num_updates=46100, lr=0.000652848, gnorm=0.673, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=7908
2024-02-10 15:59:15 | INFO | train_inner | epoch 068:    318 / 685 loss=2.568, nll_loss=0.925, ppl=1.9, wps=90326.1, ups=6.23, wpb=14487.4, bsz=755.3, num_updates=46200, lr=0.000651185, gnorm=0.669, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=7924
2024-02-10 15:59:31 | INFO | train_inner | epoch 068:    418 / 685 loss=2.565, nll_loss=0.922, ppl=1.89, wps=90211.5, ups=6.2, wpb=14548.7, bsz=750.9, num_updates=46300, lr=0.00064952, gnorm=0.664, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=7940
2024-02-10 15:59:47 | INFO | train_inner | epoch 068:    518 / 685 loss=2.572, nll_loss=0.929, ppl=1.9, wps=89563.1, ups=6.21, wpb=14431.1, bsz=769, num_updates=46400, lr=0.000647854, gnorm=0.722, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=7956
2024-02-10 16:00:03 | INFO | train_inner | epoch 068:    618 / 685 loss=2.581, nll_loss=0.938, ppl=1.92, wps=89974.4, ups=6.29, wpb=14309.1, bsz=706.1, num_updates=46500, lr=0.000646186, gnorm=0.693, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=7972
2024-02-10 16:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:00:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:00:15 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.445 | nll_loss 1.854 | ppl 3.62 | wps 159192 | wpb 5716.3 | bsz 292.8 | num_updates 46567 | best_loss 3.397
2024-02-10 16:00:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 46567 updates
2024-02-10 16:00:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt
2024-02-10 16:00:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt
2024-02-10 16:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt (epoch 68 @ 46567 updates, score 3.445) (writing took 2.330588382901624 seconds)
2024-02-10 16:00:18 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-02-10 16:00:18 | INFO | train | epoch 068 | loss 2.565 | nll_loss 0.921 | ppl 1.89 | wps 86869.9 | ups 5.98 | wpb 14524.4 | bsz 748.5 | num_updates 46567 | lr 0.000645067 | gnorm 0.698 | loss_scale 0.125 | train_wall 108 | gb_free 28.4 | wall 7987
2024-02-10 16:00:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:00:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:00:18 | INFO | fairseq.trainer | begin training epoch 69
2024-02-10 16:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:00:23 | INFO | train_inner | epoch 069:     33 / 685 loss=2.569, nll_loss=0.925, ppl=1.9, wps=70280.2, ups=4.87, wpb=14422.3, bsz=735.6, num_updates=46600, lr=0.000644516, gnorm=0.785, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=7993
2024-02-10 16:00:39 | INFO | train_inner | epoch 069:    133 / 685 loss=2.532, nll_loss=0.883, ppl=1.84, wps=91188.7, ups=6.22, wpb=14654.6, bsz=758.6, num_updates=46700, lr=0.000642844, gnorm=0.722, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8009
2024-02-10 16:00:55 | INFO | train_inner | epoch 069:    233 / 685 loss=2.549, nll_loss=0.904, ppl=1.87, wps=90863, ups=6.22, wpb=14598.3, bsz=747.7, num_updates=46800, lr=0.000641171, gnorm=0.694, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8025
2024-02-10 16:01:11 | INFO | train_inner | epoch 069:    333 / 685 loss=2.559, nll_loss=0.914, ppl=1.88, wps=90431.9, ups=6.23, wpb=14526.9, bsz=751.4, num_updates=46900, lr=0.000639496, gnorm=0.685, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8041
2024-02-10 16:01:28 | INFO | train_inner | epoch 069:    433 / 685 loss=2.558, nll_loss=0.913, ppl=1.88, wps=89017.4, ups=6.16, wpb=14461.2, bsz=752.9, num_updates=47000, lr=0.000637819, gnorm=0.698, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8057
2024-02-10 16:01:44 | INFO | train_inner | epoch 069:    533 / 685 loss=2.578, nll_loss=0.938, ppl=1.92, wps=91056.3, ups=6.2, wpb=14686.7, bsz=728.3, num_updates=47100, lr=0.00063614, gnorm=0.702, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8073
2024-02-10 16:02:00 | INFO | train_inner | epoch 069:    633 / 685 loss=2.562, nll_loss=0.918, ppl=1.89, wps=89536.5, ups=6.28, wpb=14267.4, bsz=757.9, num_updates=47200, lr=0.00063446, gnorm=0.707, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8089
2024-02-10 16:02:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:02:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:02:10 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.443 | nll_loss 1.859 | ppl 3.63 | wps 159414 | wpb 5716.3 | bsz 292.8 | num_updates 47252 | best_loss 3.397
2024-02-10 16:02:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 47252 updates
2024-02-10 16:02:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt
2024-02-10 16:02:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt
2024-02-10 16:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt (epoch 69 @ 47252 updates, score 3.443) (writing took 2.436114686075598 seconds)
2024-02-10 16:02:13 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-02-10 16:02:13 | INFO | train | epoch 069 | loss 2.557 | nll_loss 0.912 | ppl 1.88 | wps 86669 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 47252 | lr 0.000633586 | gnorm 0.708 | loss_scale 0.125 | train_wall 109 | gb_free 28.5 | wall 8102
2024-02-10 16:02:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:02:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:02:13 | INFO | fairseq.trainer | begin training epoch 70
2024-02-10 16:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:02:20 | INFO | train_inner | epoch 070:     48 / 685 loss=2.557, nll_loss=0.912, ppl=1.88, wps=70810.3, ups=4.85, wpb=14587.8, bsz=739.3, num_updates=47300, lr=0.000632778, gnorm=0.687, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=8110
2024-02-10 16:02:36 | INFO | train_inner | epoch 070:    148 / 685 loss=2.53, nll_loss=0.882, ppl=1.84, wps=89436.7, ups=6.22, wpb=14375.6, bsz=742.6, num_updates=47400, lr=0.000631095, gnorm=0.684, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8126
2024-02-10 16:02:53 | INFO | train_inner | epoch 070:    248 / 685 loss=2.529, nll_loss=0.88, ppl=1.84, wps=90552.4, ups=6.21, wpb=14588.2, bsz=758.6, num_updates=47500, lr=0.00062941, gnorm=0.668, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8142
2024-02-10 16:03:09 | INFO | train_inner | epoch 070:    348 / 685 loss=2.541, nll_loss=0.894, ppl=1.86, wps=89011.7, ups=6.15, wpb=14467.1, bsz=743.8, num_updates=47600, lr=0.000627723, gnorm=0.689, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8158
2024-02-10 16:03:25 | INFO | train_inner | epoch 070:    448 / 685 loss=2.559, nll_loss=0.915, ppl=1.89, wps=91195.5, ups=6.25, wpb=14580.9, bsz=753.9, num_updates=47700, lr=0.000626035, gnorm=0.67, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8174
2024-02-10 16:03:41 | INFO | train_inner | epoch 070:    548 / 685 loss=2.575, nll_loss=0.934, ppl=1.91, wps=91354.9, ups=6.23, wpb=14657.9, bsz=744.8, num_updates=47800, lr=0.000624345, gnorm=0.68, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8190
2024-02-10 16:03:57 | INFO | train_inner | epoch 070:    648 / 685 loss=2.555, nll_loss=0.91, ppl=1.88, wps=90422.7, ups=6.22, wpb=14527.7, bsz=744.7, num_updates=47900, lr=0.000622654, gnorm=0.692, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=8207
2024-02-10 16:04:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:04:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:04:05 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.422 | nll_loss 1.835 | ppl 3.57 | wps 159746 | wpb 5716.3 | bsz 292.8 | num_updates 47937 | best_loss 3.397
2024-02-10 16:04:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 47937 updates
2024-02-10 16:04:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt
2024-02-10 16:04:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt
2024-02-10 16:04:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt (epoch 70 @ 47937 updates, score 3.422) (writing took 2.374428784940392 seconds)
2024-02-10 16:04:07 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-02-10 16:04:07 | INFO | train | epoch 070 | loss 2.548 | nll_loss 0.903 | ppl 1.87 | wps 86751.4 | ups 5.97 | wpb 14524.4 | bsz 748.5 | num_updates 47937 | lr 0.000622028 | gnorm 0.679 | loss_scale 0.125 | train_wall 109 | gb_free 28.2 | wall 8217
2024-02-10 16:04:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:04:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:04:07 | INFO | fairseq.trainer | begin training epoch 71
2024-02-10 16:04:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:04:18 | INFO | train_inner | epoch 071:     63 / 685 loss=2.526, nll_loss=0.878, ppl=1.84, wps=69951.6, ups=4.85, wpb=14436.4, bsz=771.8, num_updates=48000, lr=0.000620961, gnorm=0.834, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8227
2024-02-10 16:04:34 | INFO | train_inner | epoch 071:    163 / 685 loss=2.525, nll_loss=0.877, ppl=1.84, wps=91772.4, ups=6.2, wpb=14807.2, bsz=745.7, num_updates=48100, lr=0.000619267, gnorm=0.653, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8243
2024-02-10 16:04:50 | INFO | train_inner | epoch 071:    263 / 685 loss=2.534, nll_loss=0.886, ppl=1.85, wps=88662.8, ups=6.19, wpb=14326, bsz=741.4, num_updates=48200, lr=0.000617571, gnorm=0.703, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8259
2024-02-10 16:05:06 | INFO | train_inner | epoch 071:    363 / 685 loss=2.537, nll_loss=0.891, ppl=1.85, wps=90019.2, ups=6.23, wpb=14449.6, bsz=763, num_updates=48300, lr=0.000615874, gnorm=0.709, loss_scale=0.125, train_wall=16, gb_free=28.7, wall=8276
2024-02-10 16:05:22 | INFO | train_inner | epoch 071:    463 / 685 loss=2.551, nll_loss=0.906, ppl=1.87, wps=88971.8, ups=6.14, wpb=14487.4, bsz=746.1, num_updates=48400, lr=0.000614175, gnorm=0.685, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8292
2024-02-10 16:05:39 | INFO | train_inner | epoch 071:    563 / 685 loss=2.561, nll_loss=0.918, ppl=1.89, wps=88111.1, ups=6.06, wpb=14545.8, bsz=736.4, num_updates=48500, lr=0.000612476, gnorm=0.684, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8308
2024-02-10 16:05:55 | INFO | train_inner | epoch 071:    663 / 685 loss=2.554, nll_loss=0.91, ppl=1.88, wps=88454, ups=6.09, wpb=14514.6, bsz=765.3, num_updates=48600, lr=0.000610774, gnorm=0.706, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8325
2024-02-10 16:05:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:05:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:06:01 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.395 | nll_loss 1.804 | ppl 3.49 | wps 159000 | wpb 5716.3 | bsz 292.8 | num_updates 48622 | best_loss 3.395
2024-02-10 16:06:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 48622 updates
2024-02-10 16:06:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt
2024-02-10 16:06:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt
2024-02-10 16:06:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt (epoch 71 @ 48622 updates, score 3.395) (writing took 3.6850628259126097 seconds)
2024-02-10 16:06:04 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-02-10 16:06:04 | INFO | train | epoch 071 | loss 2.541 | nll_loss 0.895 | ppl 1.86 | wps 85020.6 | ups 5.85 | wpb 14524.4 | bsz 748.5 | num_updates 48622 | lr 0.0006104 | gnorm 0.71 | loss_scale 0.125 | train_wall 110 | gb_free 28.3 | wall 8334
2024-02-10 16:06:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:06:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:06:04 | INFO | fairseq.trainer | begin training epoch 72
2024-02-10 16:06:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:06:17 | INFO | train_inner | epoch 072:     78 / 685 loss=2.523, nll_loss=0.875, ppl=1.83, wps=65834.8, ups=4.6, wpb=14319.7, bsz=717, num_updates=48700, lr=0.000609072, gnorm=0.682, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8346
2024-02-10 16:06:33 | INFO | train_inner | epoch 072:    178 / 685 loss=2.515, nll_loss=0.866, ppl=1.82, wps=88764, ups=6.18, wpb=14357.5, bsz=739.4, num_updates=48800, lr=0.000607368, gnorm=0.68, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8363
2024-02-10 16:06:49 | INFO | train_inner | epoch 072:    278 / 685 loss=2.543, nll_loss=0.898, ppl=1.86, wps=91768.6, ups=6.25, wpb=14674.5, bsz=722.7, num_updates=48900, lr=0.000605662, gnorm=0.68, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8379
2024-02-10 16:07:06 | INFO | train_inner | epoch 072:    378 / 685 loss=2.534, nll_loss=0.889, ppl=1.85, wps=86763, ups=5.91, wpb=14680.2, bsz=764.6, num_updates=49000, lr=0.000603956, gnorm=0.724, loss_scale=0.125, train_wall=17, gb_free=28.2, wall=8396
2024-02-10 16:07:23 | INFO | train_inner | epoch 072:    478 / 685 loss=2.532, nll_loss=0.885, ppl=1.85, wps=84366.2, ups=5.81, wpb=14530.8, bsz=774.2, num_updates=49100, lr=0.000602248, gnorm=0.677, loss_scale=0.125, train_wall=17, gb_free=28.4, wall=8413
2024-02-10 16:07:39 | INFO | train_inner | epoch 072:    578 / 685 loss=2.541, nll_loss=0.896, ppl=1.86, wps=89992, ups=6.15, wpb=14629.6, bsz=772.6, num_updates=49200, lr=0.000600539, gnorm=0.733, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8429
2024-02-10 16:07:55 | INFO | train_inner | epoch 072:    678 / 685 loss=2.55, nll_loss=0.906, ppl=1.87, wps=89988.7, ups=6.23, wpb=14452.3, bsz=714.1, num_updates=49300, lr=0.000598829, gnorm=0.698, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8445
2024-02-10 16:07:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:07:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:07:59 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.427 | nll_loss 1.837 | ppl 3.57 | wps 159782 | wpb 5716.3 | bsz 292.8 | num_updates 49307 | best_loss 3.395
2024-02-10 16:07:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 49307 updates
2024-02-10 16:07:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt
2024-02-10 16:08:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt
2024-02-10 16:08:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt (epoch 72 @ 49307 updates, score 3.427) (writing took 2.3402883869130164 seconds)
2024-02-10 16:08:01 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-02-10 16:08:01 | INFO | train | epoch 072 | loss 2.533 | nll_loss 0.886 | ppl 1.85 | wps 85242.1 | ups 5.87 | wpb 14524.4 | bsz 748.5 | num_updates 49307 | lr 0.000598709 | gnorm 0.696 | loss_scale 0.125 | train_wall 111 | gb_free 28.3 | wall 8451
2024-02-10 16:08:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:08:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:08:01 | INFO | fairseq.trainer | begin training epoch 73
2024-02-10 16:08:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:08:16 | INFO | train_inner | epoch 073:     93 / 685 loss=2.506, nll_loss=0.856, ppl=1.81, wps=71892.1, ups=4.84, wpb=14868, bsz=779.3, num_updates=49400, lr=0.000597117, gnorm=0.642, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8466
2024-02-10 16:08:32 | INFO | train_inner | epoch 073:    193 / 685 loss=2.513, nll_loss=0.862, ppl=1.82, wps=90108.9, ups=6.27, wpb=14370.5, bsz=706.6, num_updates=49500, lr=0.000595404, gnorm=0.664, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8482
2024-02-10 16:08:48 | INFO | train_inner | epoch 073:    293 / 685 loss=2.526, nll_loss=0.879, ppl=1.84, wps=90510.1, ups=6.22, wpb=14554.6, bsz=756.1, num_updates=49600, lr=0.000593691, gnorm=0.684, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8498
2024-02-10 16:09:04 | INFO | train_inner | epoch 073:    393 / 685 loss=2.515, nll_loss=0.867, ppl=1.82, wps=91065.7, ups=6.18, wpb=14736.8, bsz=785, num_updates=49700, lr=0.000591976, gnorm=0.65, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8514
2024-02-10 16:09:21 | INFO | train_inner | epoch 073:    493 / 685 loss=2.541, nll_loss=0.896, ppl=1.86, wps=88037.3, ups=6.2, wpb=14202.6, bsz=730.8, num_updates=49800, lr=0.00059026, gnorm=0.716, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8530
2024-02-10 16:09:38 | INFO | train_inner | epoch 073:    593 / 685 loss=2.538, nll_loss=0.894, ppl=1.86, wps=84281, ups=5.83, wpb=14462.2, bsz=770.2, num_updates=49900, lr=0.000588542, gnorm=0.678, loss_scale=0.125, train_wall=17, gb_free=28.3, wall=8547
2024-02-10 16:09:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:09:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:09:55 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.414 | nll_loss 1.825 | ppl 3.54 | wps 151679 | wpb 5716.3 | bsz 292.8 | num_updates 49992 | best_loss 3.395
2024-02-10 16:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 49992 updates
2024-02-10 16:09:55 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt
2024-02-10 16:09:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt
2024-02-10 16:09:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt (epoch 73 @ 49992 updates, score 3.414) (writing took 2.305467614205554 seconds)
2024-02-10 16:09:58 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-02-10 16:09:58 | INFO | train | epoch 073 | loss 2.525 | nll_loss 0.878 | ppl 1.84 | wps 85298.8 | ups 5.87 | wpb 14524.4 | bsz 748.5 | num_updates 49992 | lr 0.000586962 | gnorm 0.673 | loss_scale 0.125 | train_wall 110 | gb_free 28.3 | wall 8567
2024-02-10 16:09:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:09:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:09:58 | INFO | fairseq.trainer | begin training epoch 74
2024-02-10 16:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:09:59 | INFO | train_inner | epoch 074:      8 / 685 loss=2.533, nll_loss=0.888, ppl=1.85, wps=67882.9, ups=4.67, wpb=14537.2, bsz=719.6, num_updates=50000, lr=0.000586824, gnorm=0.672, loss_scale=0.125, train_wall=17, gb_free=28.3, wall=8569
2024-02-10 16:10:15 | INFO | train_inner | epoch 074:    108 / 685 loss=2.504, nll_loss=0.854, ppl=1.81, wps=90348.8, ups=6.16, wpb=14657.7, bsz=725, num_updates=50100, lr=0.000585105, gnorm=0.658, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=8585
2024-02-10 16:10:31 | INFO | train_inner | epoch 074:    208 / 685 loss=2.508, nll_loss=0.858, ppl=1.81, wps=89628.4, ups=6.21, wpb=14430.4, bsz=746.3, num_updates=50200, lr=0.000583384, gnorm=0.723, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8601
2024-02-10 16:10:48 | INFO | train_inner | epoch 074:    308 / 685 loss=2.511, nll_loss=0.863, ppl=1.82, wps=90134.8, ups=6.18, wpb=14594.4, bsz=758.9, num_updates=50300, lr=0.000581663, gnorm=0.65, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8617
2024-02-10 16:11:04 | INFO | train_inner | epoch 074:    408 / 685 loss=2.522, nll_loss=0.875, ppl=1.83, wps=90746.5, ups=6.19, wpb=14662.1, bsz=743.7, num_updates=50400, lr=0.000579941, gnorm=0.677, loss_scale=0.125, train_wall=16, gb_free=28.7, wall=8633
2024-02-10 16:11:20 | INFO | train_inner | epoch 074:    508 / 685 loss=2.521, nll_loss=0.874, ppl=1.83, wps=89685.9, ups=6.18, wpb=14514.9, bsz=758.7, num_updates=50500, lr=0.000578217, gnorm=0.773, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8650
2024-02-10 16:11:36 | INFO | train_inner | epoch 074:    608 / 685 loss=2.523, nll_loss=0.876, ppl=1.83, wps=88162, ups=6.16, wpb=14315.2, bsz=741.1, num_updates=50600, lr=0.000576493, gnorm=0.693, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=8666
2024-02-10 16:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:11:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:11:51 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.417 | nll_loss 1.822 | ppl 3.53 | wps 154549 | wpb 5716.3 | bsz 292.8 | num_updates 50677 | best_loss 3.395
2024-02-10 16:11:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 50677 updates
2024-02-10 16:11:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt
2024-02-10 16:11:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt
2024-02-10 16:11:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt (epoch 74 @ 50677 updates, score 3.417) (writing took 2.387970278970897 seconds)
2024-02-10 16:11:53 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-02-10 16:11:53 | INFO | train | epoch 074 | loss 2.517 | nll_loss 0.869 | ppl 1.83 | wps 85878 | ups 5.91 | wpb 14524.4 | bsz 748.5 | num_updates 50677 | lr 0.000575165 | gnorm 0.694 | loss_scale 0.125 | train_wall 109 | gb_free 28.2 | wall 8683
2024-02-10 16:11:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:11:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:11:54 | INFO | fairseq.trainer | begin training epoch 75
2024-02-10 16:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:11:57 | INFO | train_inner | epoch 075:     23 / 685 loss=2.519, nll_loss=0.873, ppl=1.83, wps=68342.3, ups=4.73, wpb=14440.4, bsz=755.6, num_updates=50700, lr=0.000574768, gnorm=0.673, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8687
2024-02-10 16:12:13 | INFO | train_inner | epoch 075:    123 / 685 loss=2.488, nll_loss=0.836, ppl=1.79, wps=90311.7, ups=6.24, wpb=14463.2, bsz=721.7, num_updates=50800, lr=0.000573042, gnorm=0.656, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8703
2024-02-10 16:12:30 | INFO | train_inner | epoch 075:    223 / 685 loss=2.499, nll_loss=0.849, ppl=1.8, wps=89130.3, ups=6.15, wpb=14495.3, bsz=743.8, num_updates=50900, lr=0.000571314, gnorm=0.649, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8719
2024-02-10 16:12:46 | INFO | train_inner | epoch 075:    323 / 685 loss=2.514, nll_loss=0.867, ppl=1.82, wps=87539.7, ups=6.06, wpb=14450.7, bsz=757.4, num_updates=51000, lr=0.000569587, gnorm=0.676, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=8736
2024-02-10 16:13:03 | INFO | train_inner | epoch 075:    423 / 685 loss=2.51, nll_loss=0.862, ppl=1.82, wps=86949.7, ups=5.99, wpb=14506.2, bsz=775.3, num_updates=51100, lr=0.000567858, gnorm=0.652, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8752
2024-02-10 16:13:19 | INFO | train_inner | epoch 075:    523 / 685 loss=2.532, nll_loss=0.888, ppl=1.85, wps=88525.3, ups=5.99, wpb=14774.5, bsz=756.8, num_updates=51200, lr=0.000566128, gnorm=0.674, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8769
2024-02-10 16:13:36 | INFO | train_inner | epoch 075:    623 / 685 loss=2.525, nll_loss=0.881, ppl=1.84, wps=88637.1, ups=6.02, wpb=14712.1, bsz=738.5, num_updates=51300, lr=0.000564398, gnorm=0.662, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8786
2024-02-10 16:13:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:13:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:13:49 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.42 | nll_loss 1.831 | ppl 3.56 | wps 134217 | wpb 5716.3 | bsz 292.8 | num_updates 51362 | best_loss 3.395
2024-02-10 16:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 51362 updates
2024-02-10 16:13:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt
2024-02-10 16:13:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt
2024-02-10 16:13:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt (epoch 75 @ 51362 updates, score 3.42) (writing took 2.5540248199831694 seconds)
2024-02-10 16:13:51 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-02-10 16:13:51 | INFO | train | epoch 075 | loss 2.51 | nll_loss 0.862 | ppl 1.82 | wps 84479.8 | ups 5.82 | wpb 14524.4 | bsz 748.5 | num_updates 51362 | lr 0.000563325 | gnorm 0.664 | loss_scale 0.125 | train_wall 111 | gb_free 28.4 | wall 8801
2024-02-10 16:13:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:13:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:13:51 | INFO | fairseq.trainer | begin training epoch 76
2024-02-10 16:13:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:13:58 | INFO | train_inner | epoch 076:     38 / 685 loss=2.497, nll_loss=0.847, ppl=1.8, wps=66423.9, ups=4.6, wpb=14424.6, bsz=771.4, num_updates=51400, lr=0.000562667, gnorm=0.666, loss_scale=0.125, train_wall=16, gb_free=28.6, wall=8807
2024-02-10 16:14:14 | INFO | train_inner | epoch 076:    138 / 685 loss=2.48, nll_loss=0.828, ppl=1.78, wps=86967.5, ups=6.02, wpb=14449, bsz=762.2, num_updates=51500, lr=0.000560935, gnorm=0.65, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8824
2024-02-10 16:14:31 | INFO | train_inner | epoch 076:    238 / 685 loss=2.494, nll_loss=0.844, ppl=1.8, wps=88156.2, ups=6, wpb=14689.4, bsz=759, num_updates=51600, lr=0.000559202, gnorm=0.762, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=8841
2024-02-10 16:14:48 | INFO | train_inner | epoch 076:    338 / 685 loss=2.511, nll_loss=0.863, ppl=1.82, wps=88265.5, ups=6.03, wpb=14631, bsz=728.6, num_updates=51700, lr=0.000557469, gnorm=0.684, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8857
2024-02-10 16:15:04 | INFO | train_inner | epoch 076:    438 / 685 loss=2.51, nll_loss=0.863, ppl=1.82, wps=85863.5, ups=5.98, wpb=14355.7, bsz=759.1, num_updates=51800, lr=0.000555734, gnorm=0.857, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8874
2024-02-10 16:15:21 | INFO | train_inner | epoch 076:    538 / 685 loss=2.502, nll_loss=0.853, ppl=1.81, wps=85804.2, ups=6.07, wpb=14140.9, bsz=718.6, num_updates=51900, lr=0.000554, gnorm=0.726, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8890
2024-02-10 16:15:37 | INFO | train_inner | epoch 076:    638 / 685 loss=2.52, nll_loss=0.874, ppl=1.83, wps=89391.3, ups=6.01, wpb=14875.8, bsz=762, num_updates=52000, lr=0.000552264, gnorm=0.68, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8907
2024-02-10 16:15:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:15:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:15:47 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.416 | nll_loss 1.824 | ppl 3.54 | wps 139500 | wpb 5716.3 | bsz 292.8 | num_updates 52047 | best_loss 3.395
2024-02-10 16:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 52047 updates
2024-02-10 16:15:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt
2024-02-10 16:15:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt
2024-02-10 16:15:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt (epoch 76 @ 52047 updates, score 3.416) (writing took 2.650320737157017 seconds)
2024-02-10 16:15:50 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-02-10 16:15:50 | INFO | train | epoch 076 | loss 2.503 | nll_loss 0.854 | ppl 1.81 | wps 83712 | ups 5.76 | wpb 14524.4 | bsz 748.5 | num_updates 52047 | lr 0.000551448 | gnorm 0.718 | loss_scale 0.125 | train_wall 112 | gb_free 28.3 | wall 8920
2024-02-10 16:15:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:15:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:15:50 | INFO | fairseq.trainer | begin training epoch 77
2024-02-10 16:15:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:15:59 | INFO | train_inner | epoch 077:     53 / 685 loss=2.497, nll_loss=0.847, ppl=1.8, wps=67216.4, ups=4.64, wpb=14472.2, bsz=720.9, num_updates=52100, lr=0.000550528, gnorm=0.662, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8929
2024-02-10 16:16:16 | INFO | train_inner | epoch 077:    153 / 685 loss=2.486, nll_loss=0.836, ppl=1.78, wps=88967.9, ups=6, wpb=14819.8, bsz=780.6, num_updates=52200, lr=0.000548791, gnorm=0.652, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=8945
2024-02-10 16:16:32 | INFO | train_inner | epoch 077:    253 / 685 loss=2.488, nll_loss=0.838, ppl=1.79, wps=87169.9, ups=6.02, wpb=14472.6, bsz=735.4, num_updates=52300, lr=0.000547054, gnorm=0.679, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8962
2024-02-10 16:16:49 | INFO | train_inner | epoch 077:    353 / 685 loss=2.503, nll_loss=0.854, ppl=1.81, wps=85421.4, ups=5.87, wpb=14564.1, bsz=742.6, num_updates=52400, lr=0.000545316, gnorm=0.682, loss_scale=0.125, train_wall=17, gb_free=28.4, wall=8979
2024-02-10 16:17:06 | INFO | train_inner | epoch 077:    453 / 685 loss=2.502, nll_loss=0.854, ppl=1.81, wps=87474.5, ups=6.06, wpb=14443, bsz=730.6, num_updates=52500, lr=0.000543578, gnorm=0.681, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=8995
2024-02-10 16:17:24 | INFO | train_inner | epoch 077:    553 / 685 loss=2.495, nll_loss=0.846, ppl=1.8, wps=81499.1, ups=5.65, wpb=14433.6, bsz=755.5, num_updates=52600, lr=0.000541839, gnorm=0.642, loss_scale=0.125, train_wall=17, gb_free=28.3, wall=9013
2024-02-10 16:17:41 | INFO | train_inner | epoch 077:    653 / 685 loss=2.499, nll_loss=0.851, ppl=1.8, wps=84842.8, ups=5.9, wpb=14384.4, bsz=757, num_updates=52700, lr=0.000540099, gnorm=0.677, loss_scale=0.125, train_wall=17, gb_free=28.4, wall=9030
2024-02-10 16:17:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:17:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:17:48 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.42 | nll_loss 1.83 | ppl 3.56 | wps 158101 | wpb 5716.3 | bsz 292.8 | num_updates 52732 | best_loss 3.395
2024-02-10 16:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 52732 updates
2024-02-10 16:17:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt
2024-02-10 16:17:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt
2024-02-10 16:17:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt (epoch 77 @ 52732 updates, score 3.42) (writing took 2.7022040160372853 seconds)
2024-02-10 16:17:50 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-02-10 16:17:50 | INFO | train | epoch 077 | loss 2.494 | nll_loss 0.845 | ppl 1.8 | wps 82634.8 | ups 5.69 | wpb 14524.4 | bsz 748.5 | num_updates 52732 | lr 0.000539543 | gnorm 0.667 | loss_scale 0.125 | train_wall 114 | gb_free 28.3 | wall 9040
2024-02-10 16:17:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:17:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:17:51 | INFO | fairseq.trainer | begin training epoch 78
2024-02-10 16:17:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:18:02 | INFO | train_inner | epoch 078:     68 / 685 loss=2.468, nll_loss=0.815, ppl=1.76, wps=66655.5, ups=4.65, wpb=14342.8, bsz=741.3, num_updates=52800, lr=0.00053836, gnorm=0.639, loss_scale=0.125, train_wall=16, gb_free=28.2, wall=9052
2024-02-10 16:18:19 | INFO | train_inner | epoch 078:    168 / 685 loss=2.478, nll_loss=0.827, ppl=1.77, wps=88960.9, ups=6.05, wpb=14693.3, bsz=745, num_updates=52900, lr=0.000536619, gnorm=0.672, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=9068
2024-02-10 16:18:35 | INFO | train_inner | epoch 078:    268 / 685 loss=2.49, nll_loss=0.84, ppl=1.79, wps=87859.1, ups=6.03, wpb=14578.2, bsz=734.3, num_updates=53000, lr=0.000534878, gnorm=0.662, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=9085
2024-02-10 16:18:52 | INFO | train_inner | epoch 078:    368 / 685 loss=2.484, nll_loss=0.833, ppl=1.78, wps=87323.2, ups=6.1, wpb=14319.6, bsz=743.8, num_updates=53100, lr=0.000533137, gnorm=0.642, loss_scale=0.125, train_wall=16, gb_free=28.3, wall=9101
2024-02-10 16:19:08 | INFO | train_inner | epoch 078:    468 / 685 loss=2.486, nll_loss=0.836, ppl=1.78, wps=87571.7, ups=5.98, wpb=14636, bsz=774.6, num_updates=53200, lr=0.000531395, gnorm=0.652, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=9118
2024-02-10 16:19:25 | INFO | train_inner | epoch 078:    568 / 685 loss=2.494, nll_loss=0.846, ppl=1.8, wps=87149.7, ups=6.09, wpb=14313, bsz=768.6, num_updates=53300, lr=0.000529653, gnorm=0.652, loss_scale=0.125, train_wall=16, gb_free=28.4, wall=9134
2024-02-10 16:19:41 | INFO | train_inner | epoch 078:    668 / 685 loss=2.509, nll_loss=0.864, ppl=1.82, wps=88846.6, ups=6.02, wpb=14746.9, bsz=749.8, num_updates=53400, lr=0.000527911, gnorm=0.653, loss_scale=0.125, train_wall=16, gb_free=28.5, wall=9151
2024-02-10 16:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:19:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:19:46 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.407 | nll_loss 1.82 | ppl 3.53 | wps 148397 | wpb 5716.3 | bsz 292.8 | num_updates 53417 | best_loss 3.395
2024-02-10 16:19:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 53417 updates
2024-02-10 16:19:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt
2024-02-10 16:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt
2024-02-10 16:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt (epoch 78 @ 53417 updates, score 3.407) (writing took 2.5939048449508846 seconds)
2024-02-10 16:19:49 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-02-10 16:19:49 | INFO | train | epoch 078 | loss 2.487 | nll_loss 0.838 | ppl 1.79 | wps 84113.6 | ups 5.79 | wpb 14524.4 | bsz 748.5 | num_updates 53417 | lr 0.000527615 | gnorm 0.654 | loss_scale 0.125 | train_wall 112 | gb_free 28.4 | wall 9158
2024-02-10 16:19:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:19:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:19:49 | INFO | fairseq.trainer | begin training epoch 79
2024-02-10 16:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:20:04 | INFO | train_inner | epoch 079:     83 / 685 loss=2.458, nll_loss=0.804, ppl=1.75, wps=64586.9, ups=4.48, wpb=14432, bsz=750.5, num_updates=53500, lr=0.000526168, gnorm=0.644, loss_scale=0.25, train_wall=17, gb_free=28.4, wall=9173
2024-02-10 16:20:39 | INFO | train_inner | epoch 079:    183 / 685 loss=2.462, nll_loss=0.809, ppl=1.75, wps=40742.7, ups=2.82, wpb=14440.6, bsz=729.8, num_updates=53600, lr=0.000524425, gnorm=0.632, loss_scale=0.25, train_wall=20, gb_free=28.3, wall=9209
2024-02-10 16:20:56 | INFO | train_inner | epoch 079:    283 / 685 loss=2.476, nll_loss=0.825, ppl=1.77, wps=88878, ups=6.07, wpb=14646.5, bsz=745.9, num_updates=53700, lr=0.000522681, gnorm=0.62, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=9225
2024-02-10 16:21:12 | INFO | train_inner | epoch 079:    383 / 685 loss=2.485, nll_loss=0.835, ppl=1.78, wps=87494.7, ups=6, wpb=14578.4, bsz=756.7, num_updates=53800, lr=0.000520938, gnorm=0.663, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9242
2024-02-10 16:21:37 | INFO | train_inner | epoch 079:    483 / 685 loss=2.486, nll_loss=0.836, ppl=1.79, wps=58451.4, ups=4.08, wpb=14330.9, bsz=750.4, num_updates=53900, lr=0.000519194, gnorm=0.68, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9266
2024-02-10 16:21:53 | INFO | train_inner | epoch 079:    583 / 685 loss=2.5, nll_loss=0.853, ppl=1.81, wps=88511.2, ups=6.06, wpb=14596, bsz=755.6, num_updates=54000, lr=0.00051745, gnorm=0.681, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=9283
2024-02-10 16:22:10 | INFO | train_inner | epoch 079:    683 / 685 loss=2.501, nll_loss=0.855, ppl=1.81, wps=87353.5, ups=5.95, wpb=14688.3, bsz=745, num_updates=54100, lr=0.000515705, gnorm=0.675, loss_scale=0.25, train_wall=17, gb_free=28.3, wall=9300
2024-02-10 16:22:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:22:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:22:13 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 3.415 | nll_loss 1.829 | ppl 3.55 | wps 139313 | wpb 5716.3 | bsz 292.8 | num_updates 54102 | best_loss 3.395
2024-02-10 16:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 54102 updates
2024-02-10 16:22:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt
2024-02-10 16:22:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt
2024-02-10 16:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt (epoch 79 @ 54102 updates, score 3.415) (writing took 2.807012334931642 seconds)
2024-02-10 16:22:15 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-02-10 16:22:15 | INFO | train | epoch 079 | loss 2.481 | nll_loss 0.831 | ppl 1.78 | wps 67809.8 | ups 4.67 | wpb 14524.4 | bsz 748.5 | num_updates 54102 | lr 0.00051567 | gnorm 0.655 | loss_scale 0.25 | train_wall 117 | gb_free 28.2 | wall 9305
2024-02-10 16:22:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:22:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:22:16 | INFO | fairseq.trainer | begin training epoch 80
2024-02-10 16:22:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:22:33 | INFO | train_inner | epoch 080:     98 / 685 loss=2.447, nll_loss=0.792, ppl=1.73, wps=64517.2, ups=4.42, wpb=14608.4, bsz=779.4, num_updates=54200, lr=0.000513961, gnorm=0.648, loss_scale=0.25, train_wall=17, gb_free=28.5, wall=9322
2024-02-10 16:22:49 | INFO | train_inner | epoch 080:    198 / 685 loss=2.46, nll_loss=0.807, ppl=1.75, wps=86416.7, ups=6.02, wpb=14359.6, bsz=759.8, num_updates=54300, lr=0.000512216, gnorm=0.668, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9339
2024-02-10 16:23:06 | INFO | train_inner | epoch 080:    298 / 685 loss=2.474, nll_loss=0.824, ppl=1.77, wps=88309.8, ups=6.1, wpb=14488.6, bsz=741.1, num_updates=54400, lr=0.000510471, gnorm=0.623, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9355
2024-02-10 16:23:22 | INFO | train_inner | epoch 080:    398 / 685 loss=2.475, nll_loss=0.825, ppl=1.77, wps=87990.6, ups=5.99, wpb=14678.3, bsz=740.2, num_updates=54500, lr=0.000508726, gnorm=0.634, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9372
2024-02-10 16:23:39 | INFO | train_inner | epoch 080:    498 / 685 loss=2.483, nll_loss=0.833, ppl=1.78, wps=88056.4, ups=6.12, wpb=14390.6, bsz=728.6, num_updates=54600, lr=0.000506981, gnorm=0.647, loss_scale=0.25, train_wall=16, gb_free=28.7, wall=9388
2024-02-10 16:23:55 | INFO | train_inner | epoch 080:    598 / 685 loss=2.485, nll_loss=0.837, ppl=1.79, wps=87709.8, ups=6.03, wpb=14536.8, bsz=729.8, num_updates=54700, lr=0.000505236, gnorm=0.634, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9405
2024-02-10 16:24:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:24:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:24:12 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 3.397 | nll_loss 1.812 | ppl 3.51 | wps 140634 | wpb 5716.3 | bsz 292.8 | num_updates 54787 | best_loss 3.395
2024-02-10 16:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 54787 updates
2024-02-10 16:24:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt
2024-02-10 16:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt
2024-02-10 16:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt (epoch 80 @ 54787 updates, score 3.397) (writing took 2.596659299917519 seconds)
2024-02-10 16:24:14 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-02-10 16:24:14 | INFO | train | epoch 080 | loss 2.472 | nll_loss 0.822 | ppl 1.77 | wps 83725.4 | ups 5.76 | wpb 14524.4 | bsz 748.5 | num_updates 54787 | lr 0.000503718 | gnorm 0.642 | loss_scale 0.25 | train_wall 112 | gb_free 28.5 | wall 9424
2024-02-10 16:24:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:24:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:24:14 | INFO | fairseq.trainer | begin training epoch 81
2024-02-10 16:24:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:24:17 | INFO | train_inner | epoch 081:     13 / 685 loss=2.476, nll_loss=0.826, ppl=1.77, wps=67929.8, ups=4.66, wpb=14568.7, bsz=765.4, num_updates=54800, lr=0.000503491, gnorm=0.637, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9426
2024-02-10 16:24:33 | INFO | train_inner | epoch 081:    113 / 685 loss=2.443, nll_loss=0.787, ppl=1.73, wps=86413, ups=6.08, wpb=14213.6, bsz=736.7, num_updates=54900, lr=0.000501745, gnorm=0.649, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9443
2024-02-10 16:24:50 | INFO | train_inner | epoch 081:    213 / 685 loss=2.46, nll_loss=0.808, ppl=1.75, wps=88010.5, ups=6.04, wpb=14569.3, bsz=733.1, num_updates=55000, lr=0.0005, gnorm=0.631, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9459
2024-02-10 16:25:06 | INFO | train_inner | epoch 081:    313 / 685 loss=2.468, nll_loss=0.817, ppl=1.76, wps=90082, ups=6.05, wpb=14901.2, bsz=773.4, num_updates=55100, lr=0.000498255, gnorm=0.634, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9476
2024-02-10 16:25:23 | INFO | train_inner | epoch 081:    413 / 685 loss=2.465, nll_loss=0.814, ppl=1.76, wps=87364.2, ups=6.03, wpb=14486.3, bsz=731.8, num_updates=55200, lr=0.000496509, gnorm=0.677, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9492
2024-02-10 16:25:39 | INFO | train_inner | epoch 081:    513 / 685 loss=2.471, nll_loss=0.821, ppl=1.77, wps=88049.3, ups=6.09, wpb=14464.4, bsz=765.4, num_updates=55300, lr=0.000494764, gnorm=0.655, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9509
2024-02-10 16:25:56 | INFO | train_inner | epoch 081:    613 / 685 loss=2.482, nll_loss=0.834, ppl=1.78, wps=86326.7, ups=6.01, wpb=14354.1, bsz=748.8, num_updates=55400, lr=0.000493019, gnorm=0.649, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9526
2024-02-10 16:26:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:26:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:26:10 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 3.394 | nll_loss 1.801 | ppl 3.49 | wps 143615 | wpb 5716.3 | bsz 292.8 | num_updates 55472 | best_loss 3.394
2024-02-10 16:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 55472 updates
2024-02-10 16:26:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt
2024-02-10 16:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt
2024-02-10 16:26:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt (epoch 81 @ 55472 updates, score 3.394) (writing took 3.8143190641421825 seconds)
2024-02-10 16:26:14 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-02-10 16:26:14 | INFO | train | epoch 081 | loss 2.466 | nll_loss 0.815 | ppl 1.76 | wps 83308.7 | ups 5.74 | wpb 14524.4 | bsz 748.5 | num_updates 55472 | lr 0.000491762 | gnorm 0.649 | loss_scale 0.25 | train_wall 112 | gb_free 28.3 | wall 9543
2024-02-10 16:26:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:26:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:26:14 | INFO | fairseq.trainer | begin training epoch 82
2024-02-10 16:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:26:19 | INFO | train_inner | epoch 082:     28 / 685 loss=2.47, nll_loss=0.82, ppl=1.77, wps=65509.9, ups=4.42, wpb=14825.2, bsz=738.6, num_updates=55500, lr=0.000491274, gnorm=0.641, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9548
2024-02-10 16:26:35 | INFO | train_inner | epoch 082:    128 / 685 loss=2.443, nll_loss=0.789, ppl=1.73, wps=86489.3, ups=6.07, wpb=14240.4, bsz=728.3, num_updates=55600, lr=0.000489529, gnorm=0.676, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9565
2024-02-10 16:26:51 | INFO | train_inner | epoch 082:    228 / 685 loss=2.453, nll_loss=0.8, ppl=1.74, wps=88595.8, ups=6.11, wpb=14499.1, bsz=741.8, num_updates=55700, lr=0.000487784, gnorm=0.641, loss_scale=0.25, train_wall=16, gb_free=28.6, wall=9581
2024-02-10 16:27:08 | INFO | train_inner | epoch 082:    328 / 685 loss=2.461, nll_loss=0.81, ppl=1.75, wps=88226.9, ups=6.1, wpb=14462.2, bsz=731.2, num_updates=55800, lr=0.000486039, gnorm=0.641, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=9597
2024-02-10 16:27:24 | INFO | train_inner | epoch 082:    428 / 685 loss=2.47, nll_loss=0.822, ppl=1.77, wps=89624.4, ups=6.03, wpb=14864.3, bsz=746.9, num_updates=55900, lr=0.000484295, gnorm=0.666, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9614
2024-02-10 16:27:41 | INFO | train_inner | epoch 082:    528 / 685 loss=2.456, nll_loss=0.804, ppl=1.75, wps=87346.1, ups=6.06, wpb=14415.8, bsz=771.8, num_updates=56000, lr=0.00048255, gnorm=0.645, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9630
2024-02-10 16:27:57 | INFO | train_inner | epoch 082:    628 / 685 loss=2.467, nll_loss=0.817, ppl=1.76, wps=88587.8, ups=6.05, wpb=14630.9, bsz=769.2, num_updates=56100, lr=0.000480806, gnorm=0.644, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9647
2024-02-10 16:28:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:28:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:28:09 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 3.435 | nll_loss 1.851 | ppl 3.61 | wps 147066 | wpb 5716.3 | bsz 292.8 | num_updates 56157 | best_loss 3.394
2024-02-10 16:28:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 56157 updates
2024-02-10 16:28:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint82.pt
2024-02-10 16:28:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint82.pt
2024-02-10 16:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint82.pt (epoch 82 @ 56157 updates, score 3.435) (writing took 2.540460488991812 seconds)
2024-02-10 16:28:11 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-02-10 16:28:11 | INFO | train | epoch 082 | loss 2.458 | nll_loss 0.807 | ppl 1.75 | wps 84537.1 | ups 5.82 | wpb 14524.4 | bsz 748.5 | num_updates 56157 | lr 0.000479812 | gnorm 0.651 | loss_scale 0.25 | train_wall 111 | gb_free 28.4 | wall 9661
2024-02-10 16:28:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:28:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:28:11 | INFO | fairseq.trainer | begin training epoch 83
2024-02-10 16:28:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:28:19 | INFO | train_inner | epoch 083:     43 / 685 loss=2.448, nll_loss=0.795, ppl=1.74, wps=67594.7, ups=4.68, wpb=14442.4, bsz=760.3, num_updates=56200, lr=0.000479062, gnorm=0.644, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9668
2024-02-10 16:28:35 | INFO | train_inner | epoch 083:    143 / 685 loss=2.426, nll_loss=0.769, ppl=1.7, wps=87209, ups=6.07, wpb=14374.7, bsz=746.6, num_updates=56300, lr=0.000477319, gnorm=0.62, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=9685
2024-02-10 16:28:52 | INFO | train_inner | epoch 083:    243 / 685 loss=2.437, nll_loss=0.783, ppl=1.72, wps=87732.5, ups=6.08, wpb=14432.7, bsz=748.6, num_updates=56400, lr=0.000475575, gnorm=0.616, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=9701
2024-02-10 16:29:08 | INFO | train_inner | epoch 083:    343 / 685 loss=2.461, nll_loss=0.81, ppl=1.75, wps=88921, ups=6.06, wpb=14668.1, bsz=733.1, num_updates=56500, lr=0.000473832, gnorm=0.633, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9718
2024-02-10 16:29:25 | INFO | train_inner | epoch 083:    443 / 685 loss=2.454, nll_loss=0.803, ppl=1.74, wps=88196.8, ups=6.05, wpb=14578.1, bsz=759.1, num_updates=56600, lr=0.000472089, gnorm=0.639, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9734
2024-02-10 16:29:41 | INFO | train_inner | epoch 083:    543 / 685 loss=2.474, nll_loss=0.825, ppl=1.77, wps=87043.5, ups=6.08, wpb=14311, bsz=729.7, num_updates=56700, lr=0.000470347, gnorm=0.683, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9751
2024-02-10 16:29:58 | INFO | train_inner | epoch 083:    643 / 685 loss=2.466, nll_loss=0.817, ppl=1.76, wps=89239.2, ups=6.07, wpb=14708.1, bsz=765.8, num_updates=56800, lr=0.000468605, gnorm=0.619, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9767
2024-02-10 16:30:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:30:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:30:07 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 3.415 | nll_loss 1.827 | ppl 3.55 | wps 140699 | wpb 5716.3 | bsz 292.8 | num_updates 56842 | best_loss 3.394
2024-02-10 16:30:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 56842 updates
2024-02-10 16:30:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint83.pt
2024-02-10 16:30:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint83.pt
2024-02-10 16:30:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint83.pt (epoch 83 @ 56842 updates, score 3.415) (writing took 2.585319015895948 seconds)
2024-02-10 16:30:09 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-02-10 16:30:09 | INFO | train | epoch 083 | loss 2.452 | nll_loss 0.8 | ppl 1.74 | wps 84326.3 | ups 5.81 | wpb 14524.4 | bsz 748.5 | num_updates 56842 | lr 0.000467873 | gnorm 0.635 | loss_scale 0.25 | train_wall 111 | gb_free 28.3 | wall 9779
2024-02-10 16:30:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:30:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:30:09 | INFO | fairseq.trainer | begin training epoch 84
2024-02-10 16:30:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:30:19 | INFO | train_inner | epoch 084:     58 / 685 loss=2.434, nll_loss=0.78, ppl=1.72, wps=67004.7, ups=4.63, wpb=14463, bsz=731.1, num_updates=56900, lr=0.000466863, gnorm=0.628, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9789
2024-02-10 16:30:36 | INFO | train_inner | epoch 084:    158 / 685 loss=2.43, nll_loss=0.775, ppl=1.71, wps=87972.1, ups=6.06, wpb=14523.3, bsz=744.9, num_updates=57000, lr=0.000465122, gnorm=0.621, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9805
2024-02-10 16:30:52 | INFO | train_inner | epoch 084:    258 / 685 loss=2.439, nll_loss=0.785, ppl=1.72, wps=86927.7, ups=6.06, wpb=14345.3, bsz=764.8, num_updates=57100, lr=0.000463381, gnorm=0.639, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9822
2024-02-10 16:31:09 | INFO | train_inner | epoch 084:    358 / 685 loss=2.443, nll_loss=0.79, ppl=1.73, wps=88923.6, ups=6.08, wpb=14615.9, bsz=749.5, num_updates=57200, lr=0.00046164, gnorm=0.632, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9838
2024-02-10 16:31:25 | INFO | train_inner | epoch 084:    458 / 685 loss=2.46, nll_loss=0.809, ppl=1.75, wps=88112.7, ups=6.01, wpb=14660.8, bsz=759.5, num_updates=57300, lr=0.000459901, gnorm=0.661, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9855
2024-02-10 16:31:42 | INFO | train_inner | epoch 084:    558 / 685 loss=2.459, nll_loss=0.81, ppl=1.75, wps=88890.6, ups=6.03, wpb=14746, bsz=739.5, num_updates=57400, lr=0.000458161, gnorm=0.63, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9871
2024-02-10 16:31:58 | INFO | train_inner | epoch 084:    658 / 685 loss=2.452, nll_loss=0.8, ppl=1.74, wps=86637.6, ups=6.05, wpb=14322.3, bsz=748.6, num_updates=57500, lr=0.000456422, gnorm=0.634, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9888
2024-02-10 16:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:32:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:32:05 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 3.421 | nll_loss 1.83 | ppl 3.56 | wps 152946 | wpb 5716.3 | bsz 292.8 | num_updates 57527 | best_loss 3.394
2024-02-10 16:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 57527 updates
2024-02-10 16:32:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint84.pt
2024-02-10 16:32:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint84.pt
2024-02-10 16:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint84.pt (epoch 84 @ 57527 updates, score 3.421) (writing took 2.4852465980220586 seconds)
2024-02-10 16:32:07 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-02-10 16:32:07 | INFO | train | epoch 084 | loss 2.445 | nll_loss 0.792 | ppl 1.73 | wps 84321 | ups 5.81 | wpb 14524.4 | bsz 748.5 | num_updates 57527 | lr 0.000455953 | gnorm 0.634 | loss_scale 0.25 | train_wall 112 | gb_free 28.5 | wall 9897
2024-02-10 16:32:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:32:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:32:07 | INFO | fairseq.trainer | begin training epoch 85
2024-02-10 16:32:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:32:20 | INFO | train_inner | epoch 085:     73 / 685 loss=2.428, nll_loss=0.773, ppl=1.71, wps=69257.4, ups=4.68, wpb=14795.8, bsz=764.1, num_updates=57600, lr=0.000454684, gnorm=0.627, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=9909
2024-02-10 16:32:36 | INFO | train_inner | epoch 085:    173 / 685 loss=2.434, nll_loss=0.781, ppl=1.72, wps=88273.4, ups=6, wpb=14717.5, bsz=762.1, num_updates=57700, lr=0.000452946, gnorm=0.646, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=9926
2024-02-10 16:32:53 | INFO | train_inner | epoch 085:    273 / 685 loss=2.437, nll_loss=0.783, ppl=1.72, wps=85483.8, ups=5.95, wpb=14359.1, bsz=739.4, num_updates=57800, lr=0.000451209, gnorm=0.634, loss_scale=0.25, train_wall=17, gb_free=28.3, wall=9943
2024-02-10 16:33:10 | INFO | train_inner | epoch 085:    373 / 685 loss=2.439, nll_loss=0.786, ppl=1.72, wps=87777.8, ups=6.03, wpb=14558.9, bsz=743.6, num_updates=57900, lr=0.000449472, gnorm=0.62, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9959
2024-02-10 16:33:27 | INFO | train_inner | epoch 085:    473 / 685 loss=2.433, nll_loss=0.779, ppl=1.72, wps=85905.1, ups=5.95, wpb=14434.6, bsz=794.8, num_updates=58000, lr=0.000447736, gnorm=0.683, loss_scale=0.25, train_wall=17, gb_free=28.3, wall=9976
2024-02-10 16:33:43 | INFO | train_inner | epoch 085:    573 / 685 loss=2.448, nll_loss=0.796, ppl=1.74, wps=86892.8, ups=6.12, wpb=14188, bsz=716.1, num_updates=58100, lr=0.000446, gnorm=0.634, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=9993
2024-02-10 16:34:00 | INFO | train_inner | epoch 085:    673 / 685 loss=2.449, nll_loss=0.799, ppl=1.74, wps=88617.3, ups=6.04, wpb=14666.4, bsz=720.8, num_updates=58200, lr=0.000444266, gnorm=0.622, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10009
2024-02-10 16:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:34:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:34:04 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 3.403 | nll_loss 1.82 | ppl 3.53 | wps 149480 | wpb 5716.3 | bsz 292.8 | num_updates 58212 | best_loss 3.394
2024-02-10 16:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 58212 updates
2024-02-10 16:34:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint85.pt
2024-02-10 16:34:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint85.pt
2024-02-10 16:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint85.pt (epoch 85 @ 58212 updates, score 3.403) (writing took 2.458651036955416 seconds)
2024-02-10 16:34:06 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2024-02-10 16:34:06 | INFO | train | epoch 085 | loss 2.438 | nll_loss 0.785 | ppl 1.72 | wps 83862.6 | ups 5.77 | wpb 14524.4 | bsz 748.5 | num_updates 58212 | lr 0.000444057 | gnorm 0.637 | loss_scale 0.25 | train_wall 112 | gb_free 28.2 | wall 10016
2024-02-10 16:34:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:34:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:34:06 | INFO | fairseq.trainer | begin training epoch 86
2024-02-10 16:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:34:21 | INFO | train_inner | epoch 086:     88 / 685 loss=2.418, nll_loss=0.762, ppl=1.7, wps=68102.7, ups=4.73, wpb=14389.4, bsz=740.5, num_updates=58300, lr=0.000442531, gnorm=0.606, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10030
2024-02-10 16:34:37 | INFO | train_inner | epoch 086:    188 / 685 loss=2.421, nll_loss=0.765, ppl=1.7, wps=87645.1, ups=6.05, wpb=14498.8, bsz=768.3, num_updates=58400, lr=0.000440798, gnorm=0.61, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=10047
2024-02-10 16:34:54 | INFO | train_inner | epoch 086:    288 / 685 loss=2.43, nll_loss=0.776, ppl=1.71, wps=88796.7, ups=6.06, wpb=14659.4, bsz=721.6, num_updates=58500, lr=0.000439065, gnorm=0.653, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10063
2024-02-10 16:35:10 | INFO | train_inner | epoch 086:    388 / 685 loss=2.435, nll_loss=0.781, ppl=1.72, wps=88064.6, ups=6.1, wpb=14441.9, bsz=729.7, num_updates=58600, lr=0.000437333, gnorm=0.621, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10080
2024-02-10 16:35:27 | INFO | train_inner | epoch 086:    488 / 685 loss=2.428, nll_loss=0.775, ppl=1.71, wps=88398.5, ups=6.04, wpb=14638.4, bsz=760.9, num_updates=58700, lr=0.000435602, gnorm=0.646, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=10096
2024-02-10 16:35:43 | INFO | train_inner | epoch 086:    588 / 685 loss=2.443, nll_loss=0.792, ppl=1.73, wps=89104.8, ups=6.05, wpb=14734.1, bsz=756.2, num_updates=58800, lr=0.000433872, gnorm=0.621, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10113
2024-02-10 16:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:35:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:36:01 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 3.416 | nll_loss 1.827 | ppl 3.55 | wps 147919 | wpb 5716.3 | bsz 292.8 | num_updates 58897 | best_loss 3.394
2024-02-10 16:36:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 58897 updates
2024-02-10 16:36:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint86.pt
2024-02-10 16:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint86.pt
2024-02-10 16:36:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint86.pt (epoch 86 @ 58897 updates, score 3.416) (writing took 2.5285329499747604 seconds)
2024-02-10 16:36:04 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2024-02-10 16:36:04 | INFO | train | epoch 086 | loss 2.431 | nll_loss 0.777 | ppl 1.71 | wps 84412.7 | ups 5.81 | wpb 14524.4 | bsz 748.5 | num_updates 58897 | lr 0.000432194 | gnorm 0.624 | loss_scale 0.25 | train_wall 111 | gb_free 28.4 | wall 10134
2024-02-10 16:36:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:36:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:36:04 | INFO | fairseq.trainer | begin training epoch 87
2024-02-10 16:36:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:36:05 | INFO | train_inner | epoch 087:      3 / 685 loss=2.441, nll_loss=0.79, ppl=1.73, wps=66822.4, ups=4.68, wpb=14268.9, bsz=762.5, num_updates=58900, lr=0.000432142, gnorm=0.614, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10134
2024-02-10 16:36:21 | INFO | train_inner | epoch 087:    103 / 685 loss=2.4, nll_loss=0.742, ppl=1.67, wps=88653.6, ups=6.06, wpb=14628.5, bsz=771.3, num_updates=59000, lr=0.000430413, gnorm=0.593, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10151
2024-02-10 16:36:38 | INFO | train_inner | epoch 087:    203 / 685 loss=2.409, nll_loss=0.753, ppl=1.69, wps=86915.2, ups=6.01, wpb=14455.8, bsz=744.6, num_updates=59100, lr=0.000428686, gnorm=0.604, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10167
2024-02-10 16:36:54 | INFO | train_inner | epoch 087:    303 / 685 loss=2.433, nll_loss=0.78, ppl=1.72, wps=88415.4, ups=6.07, wpb=14554.4, bsz=722.2, num_updates=59200, lr=0.000426958, gnorm=0.624, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10184
2024-02-10 16:37:11 | INFO | train_inner | epoch 087:    403 / 685 loss=2.423, nll_loss=0.769, ppl=1.7, wps=87621.6, ups=6, wpb=14609.6, bsz=755.8, num_updates=59300, lr=0.000425232, gnorm=0.6, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10200
2024-02-10 16:37:27 | INFO | train_inner | epoch 087:    503 / 685 loss=2.435, nll_loss=0.782, ppl=1.72, wps=87367.2, ups=6.08, wpb=14366.9, bsz=745.7, num_updates=59400, lr=0.000423507, gnorm=0.636, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10217
2024-02-10 16:37:44 | INFO | train_inner | epoch 087:    603 / 685 loss=2.446, nll_loss=0.795, ppl=1.73, wps=88337.6, ups=6.08, wpb=14522.6, bsz=736.2, num_updates=59500, lr=0.000421783, gnorm=0.65, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=10233
2024-02-10 16:37:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:37:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:37:59 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 3.405 | nll_loss 1.818 | ppl 3.52 | wps 142786 | wpb 5716.3 | bsz 292.8 | num_updates 59582 | best_loss 3.394
2024-02-10 16:37:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 59582 updates
2024-02-10 16:37:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint87.pt
2024-02-10 16:38:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint87.pt
2024-02-10 16:38:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint87.pt (epoch 87 @ 59582 updates, score 3.405) (writing took 2.4044174479786307 seconds)
2024-02-10 16:38:02 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2024-02-10 16:38:02 | INFO | train | epoch 087 | loss 2.424 | nll_loss 0.77 | ppl 1.71 | wps 84405.5 | ups 5.81 | wpb 14524.4 | bsz 748.5 | num_updates 59582 | lr 0.00042037 | gnorm 0.616 | loss_scale 0.25 | train_wall 111 | gb_free 28.7 | wall 10251
2024-02-10 16:38:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:38:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:38:02 | INFO | fairseq.trainer | begin training epoch 88
2024-02-10 16:38:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:38:05 | INFO | train_inner | epoch 088:     18 / 685 loss=2.422, nll_loss=0.769, ppl=1.7, wps=68343.4, ups=4.69, wpb=14560.5, bsz=772.5, num_updates=59600, lr=0.000420059, gnorm=0.609, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10255
2024-02-10 16:38:21 | INFO | train_inner | epoch 088:    118 / 685 loss=2.401, nll_loss=0.743, ppl=1.67, wps=87595.8, ups=6.11, wpb=14326, bsz=727.7, num_updates=59700, lr=0.000418337, gnorm=0.602, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=10271
2024-02-10 16:38:38 | INFO | train_inner | epoch 088:    218 / 685 loss=2.407, nll_loss=0.751, ppl=1.68, wps=87620.6, ups=6.05, wpb=14475.5, bsz=780.2, num_updates=59800, lr=0.000416616, gnorm=0.599, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=10287
2024-02-10 16:38:54 | INFO | train_inner | epoch 088:    318 / 685 loss=2.417, nll_loss=0.763, ppl=1.7, wps=88935.9, ups=6.03, wpb=14749, bsz=777.3, num_updates=59900, lr=0.000414895, gnorm=0.618, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=10304
2024-02-10 16:39:11 | INFO | train_inner | epoch 088:    418 / 685 loss=2.423, nll_loss=0.769, ppl=1.7, wps=87614.4, ups=6.05, wpb=14493.5, bsz=740, num_updates=60000, lr=0.000413176, gnorm=0.61, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10321
2024-02-10 16:39:27 | INFO | train_inner | epoch 088:    518 / 685 loss=2.435, nll_loss=0.783, ppl=1.72, wps=88273.7, ups=6.08, wpb=14513.1, bsz=710.3, num_updates=60100, lr=0.000411458, gnorm=0.625, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10337
2024-02-10 16:39:44 | INFO | train_inner | epoch 088:    618 / 685 loss=2.424, nll_loss=0.77, ppl=1.71, wps=88098.6, ups=6.07, wpb=14502.2, bsz=742.6, num_updates=60200, lr=0.00040974, gnorm=0.604, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10354
2024-02-10 16:39:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:39:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:39:58 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 3.409 | nll_loss 1.82 | ppl 3.53 | wps 138964 | wpb 5716.3 | bsz 292.8 | num_updates 60267 | best_loss 3.394
2024-02-10 16:39:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 60267 updates
2024-02-10 16:39:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint88.pt
2024-02-10 16:39:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint88.pt
2024-02-10 16:40:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint88.pt (epoch 88 @ 60267 updates, score 3.409) (writing took 2.6841077799908817 seconds)
2024-02-10 16:40:00 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2024-02-10 16:40:00 | INFO | train | epoch 088 | loss 2.418 | nll_loss 0.764 | ppl 1.7 | wps 83995.7 | ups 5.78 | wpb 14524.4 | bsz 748.5 | num_updates 60267 | lr 0.000408591 | gnorm 0.61 | loss_scale 0.25 | train_wall 112 | gb_free 28.6 | wall 10370
2024-02-10 16:40:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:40:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:40:00 | INFO | fairseq.trainer | begin training epoch 89
2024-02-10 16:40:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:40:06 | INFO | train_inner | epoch 089:     33 / 685 loss=2.415, nll_loss=0.761, ppl=1.69, wps=65368.5, ups=4.49, wpb=14560.4, bsz=751.6, num_updates=60300, lr=0.000408024, gnorm=0.61, loss_scale=0.25, train_wall=17, gb_free=28.4, wall=10376
2024-02-10 16:40:23 | INFO | train_inner | epoch 089:    133 / 685 loss=2.403, nll_loss=0.746, ppl=1.68, wps=85187.1, ups=5.86, wpb=14537.4, bsz=740.5, num_updates=60400, lr=0.000406309, gnorm=0.584, loss_scale=0.25, train_wall=17, gb_free=28.5, wall=10393
2024-02-10 16:40:40 | INFO | train_inner | epoch 089:    233 / 685 loss=2.399, nll_loss=0.742, ppl=1.67, wps=89479.4, ups=6.16, wpb=14527, bsz=761.1, num_updates=60500, lr=0.000404596, gnorm=0.601, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10409
2024-02-10 16:40:55 | INFO | train_inner | epoch 089:    333 / 685 loss=2.414, nll_loss=0.759, ppl=1.69, wps=90684.3, ups=6.27, wpb=14471.5, bsz=736.3, num_updates=60600, lr=0.000402883, gnorm=0.606, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10425
2024-02-10 16:41:11 | INFO | train_inner | epoch 089:    433 / 685 loss=2.414, nll_loss=0.759, ppl=1.69, wps=89227.8, ups=6.25, wpb=14276.1, bsz=725.2, num_updates=60700, lr=0.000401171, gnorm=0.621, loss_scale=0.25, train_wall=16, gb_free=28.7, wall=10441
2024-02-10 16:41:28 | INFO | train_inner | epoch 089:    533 / 685 loss=2.423, nll_loss=0.77, ppl=1.71, wps=91527.7, ups=6.2, wpb=14752.9, bsz=764.7, num_updates=60800, lr=0.000399461, gnorm=0.638, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=10457
2024-02-10 16:41:44 | INFO | train_inner | epoch 089:    633 / 685 loss=2.422, nll_loss=0.769, ppl=1.7, wps=91323.8, ups=6.23, wpb=14663.6, bsz=736.6, num_updates=60900, lr=0.000397752, gnorm=0.625, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10473
2024-02-10 16:41:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:41:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:41:54 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 3.404 | nll_loss 1.817 | ppl 3.52 | wps 160455 | wpb 5716.3 | bsz 292.8 | num_updates 60952 | best_loss 3.394
2024-02-10 16:41:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 60952 updates
2024-02-10 16:41:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint89.pt
2024-02-10 16:41:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint89.pt
2024-02-10 16:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint89.pt (epoch 89 @ 60952 updates, score 3.404) (writing took 2.3688510041683912 seconds)
2024-02-10 16:41:56 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2024-02-10 16:41:56 | INFO | train | epoch 089 | loss 2.411 | nll_loss 0.756 | ppl 1.69 | wps 85691.9 | ups 5.9 | wpb 14524.4 | bsz 748.5 | num_updates 60952 | lr 0.000396864 | gnorm 0.612 | loss_scale 0.25 | train_wall 110 | gb_free 28.6 | wall 10486
2024-02-10 16:41:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:41:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:41:56 | INFO | fairseq.trainer | begin training epoch 90
2024-02-10 16:41:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:42:04 | INFO | train_inner | epoch 090:     48 / 685 loss=2.401, nll_loss=0.744, ppl=1.67, wps=70658.2, ups=4.86, wpb=14535.8, bsz=774.4, num_updates=61000, lr=0.000396044, gnorm=0.608, loss_scale=0.25, train_wall=16, gb_free=28.4, wall=10494
2024-02-10 16:42:20 | INFO | train_inner | epoch 090:    148 / 685 loss=2.398, nll_loss=0.741, ppl=1.67, wps=90760.4, ups=6.24, wpb=14538.4, bsz=745.8, num_updates=61100, lr=0.000394338, gnorm=0.601, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=10510
2024-02-10 16:42:36 | INFO | train_inner | epoch 090:    248 / 685 loss=2.4, nll_loss=0.743, ppl=1.67, wps=90959.9, ups=6.24, wpb=14586.4, bsz=728.1, num_updates=61200, lr=0.000392632, gnorm=0.591, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10526
2024-02-10 16:42:52 | INFO | train_inner | epoch 090:    348 / 685 loss=2.406, nll_loss=0.752, ppl=1.68, wps=90461.4, ups=6.21, wpb=14569.3, bsz=770.6, num_updates=61300, lr=0.000390928, gnorm=0.604, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=10542
2024-02-10 16:43:08 | INFO | train_inner | epoch 090:    448 / 685 loss=2.406, nll_loss=0.752, ppl=1.68, wps=91244.7, ups=6.21, wpb=14695.1, bsz=763.9, num_updates=61400, lr=0.000389226, gnorm=0.599, loss_scale=0.25, train_wall=16, gb_free=28.3, wall=10558
2024-02-10 16:43:24 | INFO | train_inner | epoch 090:    548 / 685 loss=2.414, nll_loss=0.76, ppl=1.69, wps=89255.8, ups=6.25, wpb=14275, bsz=730.4, num_updates=61500, lr=0.000387524, gnorm=0.623, loss_scale=0.25, train_wall=16, gb_free=28.5, wall=10574
2024-02-10 16:43:41 | INFO | train_inner | epoch 090:    648 / 685 loss=2.406, nll_loss=0.75, ppl=1.68, wps=88335.8, ups=6.13, wpb=14398.8, bsz=764, num_updates=61600, lr=0.000385825, gnorm=0.6, loss_scale=0.25, train_wall=16, gb_free=28.2, wall=10590
2024-02-10 16:43:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:43:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:43:49 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 3.395 | nll_loss 1.807 | ppl 3.5 | wps 136196 | wpb 5716.3 | bsz 292.8 | num_updates 61637 | best_loss 3.394
2024-02-10 16:43:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 61637 updates
2024-02-10 16:43:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint90.pt
2024-02-10 16:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint90.pt
2024-02-10 16:43:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint90.pt (epoch 90 @ 61637 updates, score 3.395) (writing took 2.5916351689957082 seconds)
2024-02-10 16:43:52 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2024-02-10 16:43:52 | INFO | train | epoch 090 | loss 2.405 | nll_loss 0.749 | ppl 1.68 | wps 86070.2 | ups 5.93 | wpb 14524.4 | bsz 748.5 | num_updates 61637 | lr 0.000385196 | gnorm 0.603 | loss_scale 0.5 | train_wall 109 | gb_free 28.5 | wall 10602
2024-02-10 16:43:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:43:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:43:52 | INFO | fairseq.trainer | begin training epoch 91
2024-02-10 16:43:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:44:03 | INFO | train_inner | epoch 091:     63 / 685 loss=2.395, nll_loss=0.738, ppl=1.67, wps=64839.6, ups=4.51, wpb=14362.3, bsz=748.6, num_updates=61700, lr=0.000384126, gnorm=0.586, loss_scale=0.5, train_wall=17, gb_free=28.5, wall=10613
2024-02-10 16:44:20 | INFO | train_inner | epoch 091:    163 / 685 loss=2.391, nll_loss=0.733, ppl=1.66, wps=87069.6, ups=5.96, wpb=14605.3, bsz=744.2, num_updates=61800, lr=0.000382429, gnorm=0.602, loss_scale=0.5, train_wall=17, gb_free=28.2, wall=10629
2024-02-10 16:44:36 | INFO | train_inner | epoch 091:    263 / 685 loss=2.387, nll_loss=0.729, ppl=1.66, wps=86220.6, ups=5.98, wpb=14424.2, bsz=752.2, num_updates=61900, lr=0.000380733, gnorm=0.581, loss_scale=0.5, train_wall=16, gb_free=28.7, wall=10646
2024-02-10 16:44:53 | INFO | train_inner | epoch 091:    363 / 685 loss=2.398, nll_loss=0.742, ppl=1.67, wps=86796.4, ups=5.95, wpb=14582, bsz=764, num_updates=62000, lr=0.000379039, gnorm=0.616, loss_scale=0.5, train_wall=17, gb_free=28.3, wall=10663
2024-02-10 16:45:10 | INFO | train_inner | epoch 091:    463 / 685 loss=2.397, nll_loss=0.74, ppl=1.67, wps=86771.4, ups=6.01, wpb=14439.5, bsz=753, num_updates=62100, lr=0.000377346, gnorm=0.601, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10679
2024-02-10 16:45:26 | INFO | train_inner | epoch 091:    563 / 685 loss=2.409, nll_loss=0.755, ppl=1.69, wps=86987.6, ups=6.01, wpb=14465.8, bsz=720.2, num_updates=62200, lr=0.000375655, gnorm=0.586, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=10696
2024-02-10 16:45:43 | INFO | train_inner | epoch 091:    663 / 685 loss=2.415, nll_loss=0.763, ppl=1.7, wps=87833.7, ups=5.96, wpb=14748.4, bsz=748.7, num_updates=62300, lr=0.000373965, gnorm=0.608, loss_scale=0.5, train_wall=17, gb_free=28.3, wall=10713
2024-02-10 16:45:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:45:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:45:49 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 3.425 | nll_loss 1.841 | ppl 3.58 | wps 149240 | wpb 5716.3 | bsz 292.8 | num_updates 62322 | best_loss 3.394
2024-02-10 16:45:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 62322 updates
2024-02-10 16:45:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint91.pt
2024-02-10 16:45:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint91.pt
2024-02-10 16:45:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint91.pt (epoch 91 @ 62322 updates, score 3.425) (writing took 2.4721547591034323 seconds)
2024-02-10 16:45:52 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2024-02-10 16:45:52 | INFO | train | epoch 091 | loss 2.398 | nll_loss 0.742 | ppl 1.67 | wps 83178.4 | ups 5.73 | wpb 14524.4 | bsz 748.5 | num_updates 62322 | lr 0.000373594 | gnorm 0.597 | loss_scale 0.5 | train_wall 113 | gb_free 28.3 | wall 10721
2024-02-10 16:45:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:45:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:45:52 | INFO | fairseq.trainer | begin training epoch 92
2024-02-10 16:45:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:46:05 | INFO | train_inner | epoch 092:     78 / 685 loss=2.383, nll_loss=0.725, ppl=1.65, wps=67786.5, ups=4.67, wpb=14515.7, bsz=721.1, num_updates=62400, lr=0.000372277, gnorm=0.582, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=10734
2024-02-10 16:46:21 | INFO | train_inner | epoch 092:    178 / 685 loss=2.392, nll_loss=0.735, ppl=1.66, wps=86776.8, ups=6.01, wpb=14445.7, bsz=730, num_updates=62500, lr=0.00037059, gnorm=0.594, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=10751
2024-02-10 16:46:38 | INFO | train_inner | epoch 092:    278 / 685 loss=2.383, nll_loss=0.726, ppl=1.65, wps=86948.3, ups=5.97, wpb=14572.8, bsz=763.8, num_updates=62600, lr=0.000368905, gnorm=0.61, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10768
2024-02-10 16:46:55 | INFO | train_inner | epoch 092:    378 / 685 loss=2.386, nll_loss=0.729, ppl=1.66, wps=86415, ups=5.95, wpb=14517.8, bsz=790.1, num_updates=62700, lr=0.000367222, gnorm=0.599, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=10784
2024-02-10 16:47:12 | INFO | train_inner | epoch 092:    478 / 685 loss=2.397, nll_loss=0.742, ppl=1.67, wps=88292.1, ups=5.98, wpb=14768.3, bsz=758.2, num_updates=62800, lr=0.00036554, gnorm=0.594, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10801
2024-02-10 16:47:28 | INFO | train_inner | epoch 092:    578 / 685 loss=2.411, nll_loss=0.758, ppl=1.69, wps=87770.7, ups=6, wpb=14637.8, bsz=718.3, num_updates=62900, lr=0.00036386, gnorm=0.636, loss_scale=0.5, train_wall=16, gb_free=28.7, wall=10818
2024-02-10 16:47:45 | INFO | train_inner | epoch 092:    678 / 685 loss=2.393, nll_loss=0.737, ppl=1.67, wps=85308.9, ups=5.97, wpb=14289, bsz=765, num_updates=63000, lr=0.000362181, gnorm=0.599, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=10835
2024-02-10 16:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:47:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:47:48 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 3.409 | nll_loss 1.827 | ppl 3.55 | wps 148434 | wpb 5716.3 | bsz 292.8 | num_updates 63007 | best_loss 3.394
2024-02-10 16:47:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 63007 updates
2024-02-10 16:47:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint92.pt
2024-02-10 16:47:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint92.pt
2024-02-10 16:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint92.pt (epoch 92 @ 63007 updates, score 3.409) (writing took 2.358615829842165 seconds)
2024-02-10 16:47:51 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2024-02-10 16:47:51 | INFO | train | epoch 092 | loss 2.392 | nll_loss 0.736 | ppl 1.67 | wps 83512.4 | ups 5.75 | wpb 14524.4 | bsz 748.5 | num_updates 63007 | lr 0.000362064 | gnorm 0.602 | loss_scale 0.5 | train_wall 113 | gb_free 28.4 | wall 10840
2024-02-10 16:47:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:47:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:47:51 | INFO | fairseq.trainer | begin training epoch 93
2024-02-10 16:47:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:48:06 | INFO | train_inner | epoch 093:     93 / 685 loss=2.373, nll_loss=0.714, ppl=1.64, wps=67274, ups=4.69, wpb=14335, bsz=738.4, num_updates=63100, lr=0.000360504, gnorm=0.587, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10856
2024-02-10 16:48:23 | INFO | train_inner | epoch 093:    193 / 685 loss=2.386, nll_loss=0.729, ppl=1.66, wps=89081.7, ups=5.98, wpb=14894.6, bsz=729, num_updates=63200, lr=0.000358829, gnorm=0.589, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=10873
2024-02-10 16:48:40 | INFO | train_inner | epoch 093:    293 / 685 loss=2.381, nll_loss=0.724, ppl=1.65, wps=87149.6, ups=5.98, wpb=14575, bsz=765.7, num_updates=63300, lr=0.000357156, gnorm=0.629, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10889
2024-02-10 16:48:57 | INFO | train_inner | epoch 093:    393 / 685 loss=2.383, nll_loss=0.726, ppl=1.65, wps=87053.7, ups=5.96, wpb=14604.5, bsz=777.8, num_updates=63400, lr=0.000355484, gnorm=0.582, loss_scale=0.5, train_wall=17, gb_free=28.8, wall=10906
2024-02-10 16:49:13 | INFO | train_inner | epoch 093:    493 / 685 loss=2.387, nll_loss=0.731, ppl=1.66, wps=86511.4, ups=5.99, wpb=14444.1, bsz=725, num_updates=63500, lr=0.000353814, gnorm=0.587, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=10923
2024-02-10 16:49:30 | INFO | train_inner | epoch 093:    593 / 685 loss=2.394, nll_loss=0.738, ppl=1.67, wps=86843.2, ups=5.97, wpb=14541.9, bsz=746.9, num_updates=63600, lr=0.000352146, gnorm=0.598, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=10940
2024-02-10 16:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:49:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:49:47 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 3.427 | nll_loss 1.844 | ppl 3.59 | wps 149781 | wpb 5716.3 | bsz 292.8 | num_updates 63692 | best_loss 3.394
2024-02-10 16:49:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 63692 updates
2024-02-10 16:49:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint93.pt
2024-02-10 16:49:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint93.pt
2024-02-10 16:49:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint93.pt (epoch 93 @ 63692 updates, score 3.427) (writing took 2.385796586982906 seconds)
2024-02-10 16:49:50 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2024-02-10 16:49:50 | INFO | train | epoch 093 | loss 2.386 | nll_loss 0.729 | ppl 1.66 | wps 83601.7 | ups 5.76 | wpb 14524.4 | bsz 748.5 | num_updates 63692 | lr 0.000350613 | gnorm 0.597 | loss_scale 0.5 | train_wall 112 | gb_free 28.3 | wall 10959
2024-02-10 16:49:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:49:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:49:50 | INFO | fairseq.trainer | begin training epoch 94
2024-02-10 16:49:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:49:51 | INFO | train_inner | epoch 094:      8 / 685 loss=2.397, nll_loss=0.742, ppl=1.67, wps=67516.4, ups=4.72, wpb=14313.1, bsz=755.2, num_updates=63700, lr=0.00035048, gnorm=0.606, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10961
2024-02-10 16:50:08 | INFO | train_inner | epoch 094:    108 / 685 loss=2.374, nll_loss=0.715, ppl=1.64, wps=87404.6, ups=6, wpb=14560.7, bsz=706.3, num_updates=63800, lr=0.000348815, gnorm=0.578, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=10977
2024-02-10 16:50:25 | INFO | train_inner | epoch 094:    208 / 685 loss=2.374, nll_loss=0.716, ppl=1.64, wps=86577.7, ups=5.96, wpb=14517.4, bsz=758.1, num_updates=63900, lr=0.000347152, gnorm=0.581, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=10994
2024-02-10 16:50:41 | INFO | train_inner | epoch 094:    308 / 685 loss=2.374, nll_loss=0.715, ppl=1.64, wps=86491.1, ups=5.99, wpb=14446.9, bsz=752.9, num_updates=64000, lr=0.000345492, gnorm=0.579, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11011
2024-02-10 16:50:58 | INFO | train_inner | epoch 094:    408 / 685 loss=2.388, nll_loss=0.732, ppl=1.66, wps=86624, ups=5.94, wpb=14575.9, bsz=753, num_updates=64100, lr=0.000343833, gnorm=0.582, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=11028
2024-02-10 16:51:15 | INFO | train_inner | epoch 094:    508 / 685 loss=2.385, nll_loss=0.729, ppl=1.66, wps=84384.4, ups=5.83, wpb=14474.6, bsz=730.5, num_updates=64200, lr=0.000342175, gnorm=0.579, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=11045
2024-02-10 16:51:33 | INFO | train_inner | epoch 094:    608 / 685 loss=2.394, nll_loss=0.74, ppl=1.67, wps=85209.9, ups=5.76, wpb=14796.1, bsz=780.7, num_updates=64300, lr=0.00034052, gnorm=0.592, loss_scale=0.5, train_wall=17, gb_free=28.5, wall=11062
2024-02-10 16:51:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:51:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:51:48 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 3.428 | nll_loss 1.847 | ppl 3.6 | wps 149362 | wpb 5716.3 | bsz 292.8 | num_updates 64377 | best_loss 3.394
2024-02-10 16:51:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 64377 updates
2024-02-10 16:51:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint94.pt
2024-02-10 16:51:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint94.pt
2024-02-10 16:51:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint94.pt (epoch 94 @ 64377 updates, score 3.428) (writing took 2.4278592159971595 seconds)
2024-02-10 16:51:50 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2024-02-10 16:51:50 | INFO | train | epoch 094 | loss 2.38 | nll_loss 0.723 | ppl 1.65 | wps 82704.1 | ups 5.69 | wpb 14524.4 | bsz 748.5 | num_updates 64377 | lr 0.000339247 | gnorm 0.582 | loss_scale 0.5 | train_wall 114 | gb_free 28.5 | wall 11080
2024-02-10 16:51:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:51:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:51:50 | INFO | fairseq.trainer | begin training epoch 95
2024-02-10 16:51:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:51:54 | INFO | train_inner | epoch 095:     23 / 685 loss=2.362, nll_loss=0.702, ppl=1.63, wps=66309.5, ups=4.7, wpb=14116.3, bsz=750.6, num_updates=64400, lr=0.000338867, gnorm=0.579, loss_scale=0.5, train_wall=16, gb_free=28.6, wall=11084
2024-02-10 16:52:11 | INFO | train_inner | epoch 095:    123 / 685 loss=2.355, nll_loss=0.694, ppl=1.62, wps=84648.4, ups=5.98, wpb=14144.3, bsz=755.8, num_updates=64500, lr=0.000337216, gnorm=0.573, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11100
2024-02-10 16:52:27 | INFO | train_inner | epoch 095:    223 / 685 loss=2.369, nll_loss=0.711, ppl=1.64, wps=87240.9, ups=5.97, wpb=14609.6, bsz=752.2, num_updates=64600, lr=0.000335567, gnorm=0.584, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11117
2024-02-10 16:52:44 | INFO | train_inner | epoch 095:    323 / 685 loss=2.37, nll_loss=0.712, ppl=1.64, wps=86665.7, ups=5.98, wpb=14499.8, bsz=766.1, num_updates=64700, lr=0.000333919, gnorm=0.565, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11134
2024-02-10 16:53:01 | INFO | train_inner | epoch 095:    423 / 685 loss=2.381, nll_loss=0.726, ppl=1.65, wps=88510.6, ups=5.97, wpb=14828.5, bsz=753.5, num_updates=64800, lr=0.000332274, gnorm=0.567, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11151
2024-02-10 16:53:18 | INFO | train_inner | epoch 095:    523 / 685 loss=2.373, nll_loss=0.715, ppl=1.64, wps=86962, ups=6.01, wpb=14473.9, bsz=742.6, num_updates=64900, lr=0.000330631, gnorm=0.564, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11167
2024-02-10 16:53:34 | INFO | train_inner | epoch 095:    623 / 685 loss=2.393, nll_loss=0.738, ppl=1.67, wps=88207.9, ups=6.03, wpb=14629.9, bsz=728.3, num_updates=65000, lr=0.00032899, gnorm=0.594, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11184
2024-02-10 16:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:53:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:53:47 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 3.413 | nll_loss 1.826 | ppl 3.55 | wps 141867 | wpb 5716.3 | bsz 292.8 | num_updates 65062 | best_loss 3.394
2024-02-10 16:53:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 65062 updates
2024-02-10 16:53:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint95.pt
2024-02-10 16:53:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint95.pt
2024-02-10 16:53:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint95.pt (epoch 95 @ 65062 updates, score 3.413) (writing took 2.3790935399010777 seconds)
2024-02-10 16:53:49 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2024-02-10 16:53:49 | INFO | train | epoch 095 | loss 2.374 | nll_loss 0.716 | ppl 1.64 | wps 83537.5 | ups 5.75 | wpb 14524.4 | bsz 748.5 | num_updates 65062 | lr 0.000327973 | gnorm 0.577 | loss_scale 0.5 | train_wall 112 | gb_free 28.4 | wall 11199
2024-02-10 16:53:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:53:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:53:49 | INFO | fairseq.trainer | begin training epoch 96
2024-02-10 16:53:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:53:56 | INFO | train_inner | epoch 096:     38 / 685 loss=2.375, nll_loss=0.717, ppl=1.64, wps=67632.2, ups=4.67, wpb=14489.3, bsz=753.5, num_updates=65100, lr=0.000327351, gnorm=0.597, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11205
2024-02-10 16:54:12 | INFO | train_inner | epoch 096:    138 / 685 loss=2.366, nll_loss=0.707, ppl=1.63, wps=86729.6, ups=5.99, wpb=14482.4, bsz=731, num_updates=65200, lr=0.000325714, gnorm=0.576, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11222
2024-02-10 16:54:29 | INFO | train_inner | epoch 096:    238 / 685 loss=2.367, nll_loss=0.71, ppl=1.64, wps=88697.1, ups=5.97, wpb=14850.4, bsz=769.4, num_updates=65300, lr=0.000324079, gnorm=0.562, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11239
2024-02-10 16:54:46 | INFO | train_inner | epoch 096:    338 / 685 loss=2.363, nll_loss=0.705, ppl=1.63, wps=87440.9, ups=5.96, wpb=14664.3, bsz=766, num_updates=65400, lr=0.000322447, gnorm=0.584, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11255
2024-02-10 16:55:02 | INFO | train_inner | epoch 096:    438 / 685 loss=2.373, nll_loss=0.716, ppl=1.64, wps=87425.9, ups=6.01, wpb=14545.3, bsz=724.8, num_updates=65500, lr=0.000320816, gnorm=0.578, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11272
2024-02-10 16:55:19 | INFO | train_inner | epoch 096:    538 / 685 loss=2.364, nll_loss=0.706, ppl=1.63, wps=87026.1, ups=6, wpb=14516, bsz=744.3, num_updates=65600, lr=0.000319188, gnorm=0.626, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11289
2024-02-10 16:55:36 | INFO | train_inner | epoch 096:    638 / 685 loss=2.375, nll_loss=0.718, ppl=1.65, wps=86585.9, ups=6.03, wpb=14353.1, bsz=738.3, num_updates=65700, lr=0.000317562, gnorm=0.58, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11305
2024-02-10 16:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:55:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:55:46 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 3.441 | nll_loss 1.861 | ppl 3.63 | wps 149826 | wpb 5716.3 | bsz 292.8 | num_updates 65747 | best_loss 3.394
2024-02-10 16:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 65747 updates
2024-02-10 16:55:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint96.pt
2024-02-10 16:55:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint96.pt
2024-02-10 16:55:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint96.pt (epoch 96 @ 65747 updates, score 3.441) (writing took 2.4047884999308735 seconds)
2024-02-10 16:55:48 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2024-02-10 16:55:48 | INFO | train | epoch 096 | loss 2.368 | nll_loss 0.71 | ppl 1.64 | wps 83632.7 | ups 5.76 | wpb 14524.4 | bsz 748.5 | num_updates 65747 | lr 0.000316798 | gnorm 0.583 | loss_scale 0.5 | train_wall 112 | gb_free 28.6 | wall 11318
2024-02-10 16:55:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:55:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:55:48 | INFO | fairseq.trainer | begin training epoch 97
2024-02-10 16:55:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:55:57 | INFO | train_inner | epoch 097:     53 / 685 loss=2.361, nll_loss=0.702, ppl=1.63, wps=68277.4, ups=4.66, wpb=14648.1, bsz=776.6, num_updates=65800, lr=0.000315938, gnorm=0.561, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=11327
2024-02-10 16:56:14 | INFO | train_inner | epoch 097:    153 / 685 loss=2.354, nll_loss=0.694, ppl=1.62, wps=87295.5, ups=5.97, wpb=14632.7, bsz=777.7, num_updates=65900, lr=0.000314316, gnorm=0.566, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11344
2024-02-10 16:56:31 | INFO | train_inner | epoch 097:    253 / 685 loss=2.356, nll_loss=0.696, ppl=1.62, wps=86159.3, ups=6.02, wpb=14315.5, bsz=730.1, num_updates=66000, lr=0.000312697, gnorm=0.56, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11360
2024-02-10 16:56:47 | INFO | train_inner | epoch 097:    353 / 685 loss=2.365, nll_loss=0.708, ppl=1.63, wps=87134.3, ups=5.99, wpb=14548.5, bsz=761.4, num_updates=66100, lr=0.00031108, gnorm=0.565, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11377
2024-02-10 16:57:05 | INFO | train_inner | epoch 097:    453 / 685 loss=2.366, nll_loss=0.709, ppl=1.63, wps=82410.5, ups=5.64, wpb=14617.6, bsz=738, num_updates=66200, lr=0.000309465, gnorm=0.568, loss_scale=0.5, train_wall=17, gb_free=28.5, wall=11395
2024-02-10 16:57:22 | INFO | train_inner | epoch 097:    553 / 685 loss=2.365, nll_loss=0.707, ppl=1.63, wps=85460.6, ups=6.04, wpb=14154.6, bsz=729.5, num_updates=66300, lr=0.000307852, gnorm=0.621, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11411
2024-02-10 16:57:38 | INFO | train_inner | epoch 097:    653 / 685 loss=2.372, nll_loss=0.715, ppl=1.64, wps=86888.6, ups=5.99, wpb=14500.9, bsz=729, num_updates=66400, lr=0.000306242, gnorm=0.574, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11428
2024-02-10 16:57:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:57:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:57:46 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 3.416 | nll_loss 1.831 | ppl 3.56 | wps 152114 | wpb 5716.3 | bsz 292.8 | num_updates 66432 | best_loss 3.394
2024-02-10 16:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 66432 updates
2024-02-10 16:57:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint97.pt
2024-02-10 16:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint97.pt
2024-02-10 16:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint97.pt (epoch 97 @ 66432 updates, score 3.416) (writing took 2.4559146089013666 seconds)
2024-02-10 16:57:48 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2024-02-10 16:57:48 | INFO | train | epoch 097 | loss 2.362 | nll_loss 0.704 | ppl 1.63 | wps 82806.1 | ups 5.7 | wpb 14524.4 | bsz 748.5 | num_updates 66432 | lr 0.000305727 | gnorm 0.577 | loss_scale 0.5 | train_wall 113 | gb_free 28.3 | wall 11438
2024-02-10 16:57:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:57:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:57:48 | INFO | fairseq.trainer | begin training epoch 98
2024-02-10 16:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 16:58:00 | INFO | train_inner | epoch 098:     68 / 685 loss=2.357, nll_loss=0.699, ppl=1.62, wps=67580.4, ups=4.57, wpb=14772.4, bsz=743, num_updates=66500, lr=0.000304634, gnorm=0.584, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=11450
2024-02-10 16:58:17 | INFO | train_inner | epoch 098:    168 / 685 loss=2.339, nll_loss=0.678, ppl=1.6, wps=85354.3, ups=5.82, wpb=14675.9, bsz=781, num_updates=66600, lr=0.000303029, gnorm=0.55, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=11467
2024-02-10 16:58:34 | INFO | train_inner | epoch 098:    268 / 685 loss=2.356, nll_loss=0.698, ppl=1.62, wps=92009.5, ups=6.16, wpb=14938, bsz=788.3, num_updates=66700, lr=0.000301426, gnorm=0.562, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11483
2024-02-10 16:58:50 | INFO | train_inner | epoch 098:    368 / 685 loss=2.363, nll_loss=0.705, ppl=1.63, wps=89893.3, ups=6.25, wpb=14388.5, bsz=713.4, num_updates=66800, lr=0.000299825, gnorm=0.564, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11499
2024-02-10 16:59:06 | INFO | train_inner | epoch 098:    468 / 685 loss=2.36, nll_loss=0.701, ppl=1.63, wps=90536.2, ups=6.24, wpb=14516.8, bsz=769.2, num_updates=66900, lr=0.000298227, gnorm=0.561, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11515
2024-02-10 16:59:22 | INFO | train_inner | epoch 098:    568 / 685 loss=2.364, nll_loss=0.708, ppl=1.63, wps=89738.5, ups=6.24, wpb=14371.4, bsz=720.6, num_updates=67000, lr=0.000296632, gnorm=0.562, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11531
2024-02-10 16:59:38 | INFO | train_inner | epoch 098:    668 / 685 loss=2.359, nll_loss=0.701, ppl=1.63, wps=88408.8, ups=6.21, wpb=14241.9, bsz=734.7, num_updates=67100, lr=0.000295038, gnorm=0.573, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11547
2024-02-10 16:59:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 16:59:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:59:43 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 3.42 | nll_loss 1.83 | ppl 3.55 | wps 133792 | wpb 5716.3 | bsz 292.8 | num_updates 67117 | best_loss 3.394
2024-02-10 16:59:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 67117 updates
2024-02-10 16:59:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint98.pt
2024-02-10 16:59:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint98.pt
2024-02-10 16:59:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint98.pt (epoch 98 @ 67117 updates, score 3.42) (writing took 2.531799343181774 seconds)
2024-02-10 16:59:45 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2024-02-10 16:59:45 | INFO | train | epoch 098 | loss 2.356 | nll_loss 0.698 | ppl 1.62 | wps 84899.6 | ups 5.85 | wpb 14524.4 | bsz 748.5 | num_updates 67117 | lr 0.000294768 | gnorm 0.562 | loss_scale 0.5 | train_wall 110 | gb_free 28.4 | wall 11555
2024-02-10 16:59:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 16:59:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 16:59:45 | INFO | fairseq.trainer | begin training epoch 99
2024-02-10 16:59:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:00:00 | INFO | train_inner | epoch 099:     83 / 685 loss=2.342, nll_loss=0.682, ppl=1.6, wps=65520.9, ups=4.48, wpb=14620.6, bsz=779.4, num_updates=67200, lr=0.000293448, gnorm=0.544, loss_scale=0.5, train_wall=17, gb_free=28.4, wall=11570
2024-02-10 17:00:17 | INFO | train_inner | epoch 099:    183 / 685 loss=2.346, nll_loss=0.686, ppl=1.61, wps=87139.4, ups=6, wpb=14516.2, bsz=758.8, num_updates=67300, lr=0.00029186, gnorm=0.552, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11586
2024-02-10 17:00:33 | INFO | train_inner | epoch 099:    283 / 685 loss=2.349, nll_loss=0.69, ppl=1.61, wps=89738.6, ups=6.09, wpb=14723.9, bsz=743, num_updates=67400, lr=0.000290274, gnorm=0.545, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11603
2024-02-10 17:00:50 | INFO | train_inner | epoch 099:    383 / 685 loss=2.351, nll_loss=0.692, ppl=1.62, wps=86336.6, ups=5.91, wpb=14597.6, bsz=736.6, num_updates=67500, lr=0.000288691, gnorm=0.586, loss_scale=0.5, train_wall=17, gb_free=28.3, wall=11620
2024-02-10 17:01:06 | INFO | train_inner | epoch 099:    483 / 685 loss=2.35, nll_loss=0.691, ppl=1.61, wps=87707.8, ups=6.07, wpb=14444.6, bsz=724.6, num_updates=67600, lr=0.00028711, gnorm=0.567, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11636
2024-02-10 17:01:23 | INFO | train_inner | epoch 099:    583 / 685 loss=2.357, nll_loss=0.699, ppl=1.62, wps=86956.3, ups=6.18, wpb=14079.6, bsz=746.5, num_updates=67700, lr=0.000285532, gnorm=0.6, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11652
2024-02-10 17:01:39 | INFO | train_inner | epoch 099:    683 / 685 loss=2.36, nll_loss=0.702, ppl=1.63, wps=91160.8, ups=6.25, wpb=14576.5, bsz=746.4, num_updates=67800, lr=0.000283957, gnorm=0.559, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11668
2024-02-10 17:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:01:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:01:41 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 3.444 | nll_loss 1.868 | ppl 3.65 | wps 158707 | wpb 5716.3 | bsz 292.8 | num_updates 67802 | best_loss 3.394
2024-02-10 17:01:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 67802 updates
2024-02-10 17:01:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint99.pt
2024-02-10 17:01:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint99.pt
2024-02-10 17:01:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint99.pt (epoch 99 @ 67802 updates, score 3.444) (writing took 2.3313730519730598 seconds)
2024-02-10 17:01:43 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2024-02-10 17:01:43 | INFO | train | epoch 099 | loss 2.351 | nll_loss 0.692 | ppl 1.62 | wps 84393.3 | ups 5.81 | wpb 14524.4 | bsz 748.5 | num_updates 67802 | lr 0.000283926 | gnorm 0.565 | loss_scale 0.5 | train_wall 112 | gb_free 28.3 | wall 11673
2024-02-10 17:01:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:01:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:01:43 | INFO | fairseq.trainer | begin training epoch 100
2024-02-10 17:01:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:01:59 | INFO | train_inner | epoch 100:     98 / 685 loss=2.336, nll_loss=0.675, ppl=1.6, wps=70493.8, ups=4.84, wpb=14579.2, bsz=772.5, num_updates=67900, lr=0.000282384, gnorm=0.547, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11689
2024-02-10 17:02:15 | INFO | train_inner | epoch 100:    198 / 685 loss=2.342, nll_loss=0.681, ppl=1.6, wps=90943.1, ups=6.22, wpb=14628.4, bsz=756.1, num_updates=68000, lr=0.000280814, gnorm=0.561, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11705
2024-02-10 17:02:32 | INFO | train_inner | epoch 100:    298 / 685 loss=2.348, nll_loss=0.689, ppl=1.61, wps=90947.4, ups=6.19, wpb=14683.5, bsz=765.7, num_updates=68100, lr=0.000279247, gnorm=0.602, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11721
2024-02-10 17:02:48 | INFO | train_inner | epoch 100:    398 / 685 loss=2.35, nll_loss=0.692, ppl=1.62, wps=90524.7, ups=6.22, wpb=14550.5, bsz=732, num_updates=68200, lr=0.000277682, gnorm=0.565, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11737
2024-02-10 17:03:04 | INFO | train_inner | epoch 100:    498 / 685 loss=2.349, nll_loss=0.689, ppl=1.61, wps=88166, ups=6.07, wpb=14531.2, bsz=719.8, num_updates=68300, lr=0.00027612, gnorm=0.553, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11754
2024-02-10 17:03:21 | INFO | train_inner | epoch 100:    598 / 685 loss=2.341, nll_loss=0.681, ppl=1.6, wps=86506, ups=6.07, wpb=14241, bsz=729.9, num_updates=68400, lr=0.000274561, gnorm=0.556, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11770
2024-02-10 17:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:03:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:03:37 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 3.407 | nll_loss 1.823 | ppl 3.54 | wps 143849 | wpb 5716.3 | bsz 292.8 | num_updates 68487 | best_loss 3.394
2024-02-10 17:03:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 68487 updates
2024-02-10 17:03:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint100.pt
2024-02-10 17:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint100.pt
2024-02-10 17:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint100.pt (epoch 100 @ 68487 updates, score 3.407) (writing took 2.4573433720506728 seconds)
2024-02-10 17:03:40 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2024-02-10 17:03:40 | INFO | train | epoch 100 | loss 2.345 | nll_loss 0.686 | ppl 1.61 | wps 85538.4 | ups 5.89 | wpb 14524.4 | bsz 748.5 | num_updates 68487 | lr 0.000273207 | gnorm 0.563 | loss_scale 0.5 | train_wall 110 | gb_free 28.4 | wall 11789
2024-02-10 17:03:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:03:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:03:40 | INFO | fairseq.trainer | begin training epoch 101
2024-02-10 17:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:03:42 | INFO | train_inner | epoch 101:     13 / 685 loss=2.352, nll_loss=0.694, ppl=1.62, wps=67930.2, ups=4.67, wpb=14553.4, bsz=773.3, num_updates=68500, lr=0.000273005, gnorm=0.551, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11792
2024-02-10 17:03:58 | INFO | train_inner | epoch 101:    113 / 685 loss=2.335, nll_loss=0.673, ppl=1.59, wps=88755.5, ups=6.08, wpb=14590.1, bsz=732.9, num_updates=68600, lr=0.000271451, gnorm=0.555, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11808
2024-02-10 17:04:15 | INFO | train_inner | epoch 101:    213 / 685 loss=2.334, nll_loss=0.673, ppl=1.59, wps=86762.4, ups=6.04, wpb=14362.5, bsz=758.7, num_updates=68700, lr=0.0002699, gnorm=0.559, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11825
2024-02-10 17:04:32 | INFO | train_inner | epoch 101:    313 / 685 loss=2.334, nll_loss=0.673, ppl=1.59, wps=87765, ups=6.04, wpb=14524.8, bsz=742.5, num_updates=68800, lr=0.000268352, gnorm=0.545, loss_scale=0.5, train_wall=16, gb_free=29, wall=11841
2024-02-10 17:04:48 | INFO | train_inner | epoch 101:    413 / 685 loss=2.338, nll_loss=0.678, ppl=1.6, wps=86425.5, ups=6.07, wpb=14249, bsz=737.2, num_updates=68900, lr=0.000266807, gnorm=0.558, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11858
2024-02-10 17:05:05 | INFO | train_inner | epoch 101:    513 / 685 loss=2.344, nll_loss=0.685, ppl=1.61, wps=88417.5, ups=6.03, wpb=14659.2, bsz=751, num_updates=69000, lr=0.000265264, gnorm=0.552, loss_scale=0.5, train_wall=16, gb_free=28.5, wall=11874
2024-02-10 17:05:21 | INFO | train_inner | epoch 101:    613 / 685 loss=2.356, nll_loss=0.7, ppl=1.62, wps=89409.3, ups=6.05, wpb=14777.6, bsz=754.1, num_updates=69100, lr=0.000263725, gnorm=0.549, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11891
2024-02-10 17:05:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:05:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:05:35 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 3.411 | nll_loss 1.826 | ppl 3.55 | wps 141102 | wpb 5716.3 | bsz 292.8 | num_updates 69172 | best_loss 3.394
2024-02-10 17:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 69172 updates
2024-02-10 17:05:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint101.pt
2024-02-10 17:05:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint101.pt
2024-02-10 17:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint101.pt (epoch 101 @ 69172 updates, score 3.411) (writing took 2.4718692051246762 seconds)
2024-02-10 17:05:38 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2024-02-10 17:05:38 | INFO | train | epoch 101 | loss 2.34 | nll_loss 0.68 | ppl 1.6 | wps 84260 | ups 5.8 | wpb 14524.4 | bsz 748.5 | num_updates 69172 | lr 0.000262618 | gnorm 0.554 | loss_scale 0.5 | train_wall 112 | gb_free 28.3 | wall 11907
2024-02-10 17:05:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:05:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:05:38 | INFO | fairseq.trainer | begin training epoch 102
2024-02-10 17:05:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:05:43 | INFO | train_inner | epoch 102:     28 / 685 loss=2.332, nll_loss=0.671, ppl=1.59, wps=67901.7, ups=4.68, wpb=14523.8, bsz=774.2, num_updates=69200, lr=0.000262188, gnorm=0.561, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11912
2024-02-10 17:05:59 | INFO | train_inner | epoch 102:    128 / 685 loss=2.325, nll_loss=0.664, ppl=1.58, wps=87934.8, ups=6.07, wpb=14489.9, bsz=777, num_updates=69300, lr=0.000260654, gnorm=0.536, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11929
2024-02-10 17:06:15 | INFO | train_inner | epoch 102:    228 / 685 loss=2.326, nll_loss=0.664, ppl=1.58, wps=87972.9, ups=6.1, wpb=14418.5, bsz=741.9, num_updates=69400, lr=0.000259123, gnorm=0.551, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=11945
2024-02-10 17:06:32 | INFO | train_inner | epoch 102:    328 / 685 loss=2.33, nll_loss=0.67, ppl=1.59, wps=88701.5, ups=6.07, wpb=14615, bsz=759.7, num_updates=69500, lr=0.000257595, gnorm=0.533, loss_scale=0.5, train_wall=16, gb_free=28.4, wall=11961
2024-02-10 17:06:48 | INFO | train_inner | epoch 102:    428 / 685 loss=2.341, nll_loss=0.681, ppl=1.6, wps=88030.9, ups=6.12, wpb=14392, bsz=723.8, num_updates=69600, lr=0.00025607, gnorm=0.552, loss_scale=0.5, train_wall=16, gb_free=28.2, wall=11978
2024-02-10 17:07:05 | INFO | train_inner | epoch 102:    528 / 685 loss=2.334, nll_loss=0.675, ppl=1.6, wps=86158.4, ups=5.95, wpb=14476.5, bsz=756.2, num_updates=69700, lr=0.000254548, gnorm=0.543, loss_scale=0.5, train_wall=17, gb_free=28.3, wall=11995
2024-02-10 17:07:22 | INFO | train_inner | epoch 102:    628 / 685 loss=2.348, nll_loss=0.692, ppl=1.62, wps=88148.4, ups=5.99, wpb=14708.4, bsz=734.6, num_updates=69800, lr=0.000253029, gnorm=0.554, loss_scale=0.5, train_wall=16, gb_free=28.3, wall=12011
2024-02-10 17:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:07:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:07:33 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 3.429 | nll_loss 1.847 | ppl 3.6 | wps 139050 | wpb 5716.3 | bsz 292.8 | num_updates 69857 | best_loss 3.394
2024-02-10 17:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 69857 updates
2024-02-10 17:07:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint102.pt
2024-02-10 17:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint102.pt
2024-02-10 17:07:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint102.pt (epoch 102 @ 69857 updates, score 3.429) (writing took 2.645173011114821 seconds)
2024-02-10 17:07:36 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2024-02-10 17:07:36 | INFO | train | epoch 102 | loss 2.334 | nll_loss 0.674 | ppl 1.6 | wps 83991.1 | ups 5.78 | wpb 14524.4 | bsz 748.5 | num_updates 69857 | lr 0.000252165 | gnorm 0.548 | loss_scale 1 | train_wall 112 | gb_free 28.3 | wall 12026
2024-02-10 17:07:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:07:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:07:36 | INFO | fairseq.trainer | begin training epoch 103
2024-02-10 17:07:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:07:43 | INFO | train_inner | epoch 103:     43 / 685 loss=2.33, nll_loss=0.67, ppl=1.59, wps=66241, ups=4.61, wpb=14383.7, bsz=743.7, num_updates=69900, lr=0.000251513, gnorm=0.567, loss_scale=1, train_wall=16, gb_free=28.3, wall=12033
2024-02-10 17:08:00 | INFO | train_inner | epoch 103:    143 / 685 loss=2.325, nll_loss=0.663, ppl=1.58, wps=88799.5, ups=6.08, wpb=14615, bsz=758, num_updates=70000, lr=0.00025, gnorm=0.56, loss_scale=1, train_wall=16, gb_free=28.5, wall=12049
2024-02-10 17:08:16 | INFO | train_inner | epoch 103:    243 / 685 loss=2.329, nll_loss=0.669, ppl=1.59, wps=88772.8, ups=6.08, wpb=14612.5, bsz=720.6, num_updates=70100, lr=0.00024849, gnorm=0.543, loss_scale=1, train_wall=16, gb_free=28.4, wall=12066
2024-02-10 17:08:33 | INFO | train_inner | epoch 103:    343 / 685 loss=2.329, nll_loss=0.669, ppl=1.59, wps=88637.6, ups=6.02, wpb=14721.9, bsz=767.4, num_updates=70200, lr=0.000246983, gnorm=0.53, loss_scale=1, train_wall=16, gb_free=28.3, wall=12083
2024-02-10 17:08:49 | INFO | train_inner | epoch 103:    443 / 685 loss=2.325, nll_loss=0.664, ppl=1.58, wps=87369.5, ups=6.1, wpb=14318.8, bsz=736.2, num_updates=70300, lr=0.000245479, gnorm=0.541, loss_scale=1, train_wall=16, gb_free=28.3, wall=12099
2024-02-10 17:09:06 | INFO | train_inner | epoch 103:    543 / 685 loss=2.337, nll_loss=0.678, ppl=1.6, wps=87750.7, ups=6.07, wpb=14460.9, bsz=735.4, num_updates=70400, lr=0.000243979, gnorm=0.55, loss_scale=1, train_wall=16, gb_free=28.3, wall=12115
2024-02-10 17:09:22 | INFO | train_inner | epoch 103:    643 / 685 loss=2.336, nll_loss=0.677, ppl=1.6, wps=88838.5, ups=6.05, wpb=14688.9, bsz=758.6, num_updates=70500, lr=0.000242481, gnorm=0.566, loss_scale=1, train_wall=16, gb_free=28.7, wall=12132
2024-02-10 17:09:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:09:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:09:31 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 3.422 | nll_loss 1.841 | ppl 3.58 | wps 151483 | wpb 5716.3 | bsz 292.8 | num_updates 70542 | best_loss 3.394
2024-02-10 17:09:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 70542 updates
2024-02-10 17:09:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint103.pt
2024-02-10 17:09:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint103.pt
2024-02-10 17:09:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint103.pt (epoch 103 @ 70542 updates, score 3.422) (writing took 2.4063893710263073 seconds)
2024-02-10 17:09:34 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2024-02-10 17:09:34 | INFO | train | epoch 103 | loss 2.33 | nll_loss 0.669 | ppl 1.59 | wps 84658.1 | ups 5.83 | wpb 14524.4 | bsz 748.5 | num_updates 70542 | lr 0.000241853 | gnorm 0.556 | loss_scale 1 | train_wall 111 | gb_free 28.2 | wall 12143
2024-02-10 17:09:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:09:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:09:34 | INFO | fairseq.trainer | begin training epoch 104
2024-02-10 17:09:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:09:44 | INFO | train_inner | epoch 104:     58 / 685 loss=2.32, nll_loss=0.658, ppl=1.58, wps=67401.2, ups=4.72, wpb=14273, bsz=754.7, num_updates=70600, lr=0.000240986, gnorm=0.584, loss_scale=1, train_wall=16, gb_free=28.5, wall=12153
2024-02-10 17:10:00 | INFO | train_inner | epoch 104:    158 / 685 loss=2.319, nll_loss=0.657, ppl=1.58, wps=88107.9, ups=6.09, wpb=14463.8, bsz=739.9, num_updates=70700, lr=0.000239495, gnorm=0.529, loss_scale=1, train_wall=16, gb_free=28.4, wall=12170
2024-02-10 17:10:16 | INFO | train_inner | epoch 104:    258 / 685 loss=2.322, nll_loss=0.661, ppl=1.58, wps=87972.1, ups=6.04, wpb=14554.9, bsz=764.4, num_updates=70800, lr=0.000238007, gnorm=0.532, loss_scale=1, train_wall=16, gb_free=28.4, wall=12186
2024-02-10 17:10:33 | INFO | train_inner | epoch 104:    358 / 685 loss=2.322, nll_loss=0.661, ppl=1.58, wps=88429.1, ups=6.06, wpb=14583.8, bsz=755.8, num_updates=70900, lr=0.000236522, gnorm=0.539, loss_scale=1, train_wall=16, gb_free=28.3, wall=12203
2024-02-10 17:10:49 | INFO | train_inner | epoch 104:    458 / 685 loss=2.324, nll_loss=0.663, ppl=1.58, wps=88309.7, ups=6.05, wpb=14590.2, bsz=771.4, num_updates=71000, lr=0.00023504, gnorm=0.559, loss_scale=1, train_wall=16, gb_free=28.2, wall=12219
2024-02-10 17:11:06 | INFO | train_inner | epoch 104:    558 / 685 loss=2.334, nll_loss=0.675, ppl=1.6, wps=88097.4, ups=6.08, wpb=14483.2, bsz=721.3, num_updates=71100, lr=0.000233562, gnorm=0.56, loss_scale=1, train_wall=16, gb_free=28.4, wall=12236
2024-02-10 17:11:23 | INFO | train_inner | epoch 104:    658 / 685 loss=2.333, nll_loss=0.673, ppl=1.59, wps=86963.4, ups=5.98, wpb=14530.4, bsz=751, num_updates=71200, lr=0.000232087, gnorm=0.533, loss_scale=1, train_wall=16, gb_free=28.6, wall=12252
2024-02-10 17:11:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:11:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:11:29 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 3.432 | nll_loss 1.85 | ppl 3.61 | wps 144544 | wpb 5716.3 | bsz 292.8 | num_updates 71227 | best_loss 3.394
2024-02-10 17:11:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 71227 updates
2024-02-10 17:11:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint104.pt
2024-02-10 17:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint104.pt
2024-02-10 17:11:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint104.pt (epoch 104 @ 71227 updates, score 3.432) (writing took 2.443423510994762 seconds)
2024-02-10 17:11:32 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2024-02-10 17:11:32 | INFO | train | epoch 104 | loss 2.325 | nll_loss 0.664 | ppl 1.58 | wps 84276.4 | ups 5.8 | wpb 14524.4 | bsz 748.5 | num_updates 71227 | lr 0.000231689 | gnorm 0.541 | loss_scale 1 | train_wall 112 | gb_free 28.3 | wall 12261
2024-02-10 17:11:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:11:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:11:32 | INFO | fairseq.trainer | begin training epoch 105
2024-02-10 17:11:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:11:44 | INFO | train_inner | epoch 105:     73 / 685 loss=2.322, nll_loss=0.661, ppl=1.58, wps=68603.2, ups=4.67, wpb=14680.4, bsz=736.1, num_updates=71300, lr=0.000230615, gnorm=0.54, loss_scale=1, train_wall=16, gb_free=28.4, wall=12274
2024-02-10 17:12:01 | INFO | train_inner | epoch 105:    173 / 685 loss=2.313, nll_loss=0.651, ppl=1.57, wps=86639.3, ups=5.99, wpb=14452.1, bsz=770.2, num_updates=71400, lr=0.000229146, gnorm=0.537, loss_scale=1, train_wall=16, gb_free=28.3, wall=12290
2024-02-10 17:12:17 | INFO | train_inner | epoch 105:    273 / 685 loss=2.317, nll_loss=0.655, ppl=1.57, wps=86548.7, ups=6.08, wpb=14244.3, bsz=714.3, num_updates=71500, lr=0.00022768, gnorm=0.542, loss_scale=1, train_wall=16, gb_free=28.4, wall=12307
2024-02-10 17:12:34 | INFO | train_inner | epoch 105:    373 / 685 loss=2.325, nll_loss=0.665, ppl=1.59, wps=88391.6, ups=6.04, wpb=14641.8, bsz=732.4, num_updates=71600, lr=0.000226218, gnorm=0.531, loss_scale=1, train_wall=16, gb_free=28.5, wall=12323
2024-02-10 17:12:50 | INFO | train_inner | epoch 105:    473 / 685 loss=2.326, nll_loss=0.666, ppl=1.59, wps=88719.7, ups=6.02, wpb=14726.1, bsz=769.4, num_updates=71700, lr=0.00022476, gnorm=0.529, loss_scale=1, train_wall=16, gb_free=28.4, wall=12340
2024-02-10 17:13:07 | INFO | train_inner | epoch 105:    573 / 685 loss=2.317, nll_loss=0.656, ppl=1.58, wps=88533, ups=6.07, wpb=14586.6, bsz=747, num_updates=71800, lr=0.000223304, gnorm=0.53, loss_scale=1, train_wall=16, gb_free=28.3, wall=12356
2024-02-10 17:13:23 | INFO | train_inner | epoch 105:    673 / 685 loss=2.322, nll_loss=0.661, ppl=1.58, wps=86671.6, ups=6.05, wpb=14322.4, bsz=746.9, num_updates=71900, lr=0.000221852, gnorm=0.54, loss_scale=1, train_wall=16, gb_free=28.3, wall=12373
2024-02-10 17:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:13:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:13:27 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 3.413 | nll_loss 1.83 | ppl 3.56 | wps 147829 | wpb 5716.3 | bsz 292.8 | num_updates 71912 | best_loss 3.394
2024-02-10 17:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 71912 updates
2024-02-10 17:13:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint105.pt
2024-02-10 17:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint105.pt
2024-02-10 17:13:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint105.pt (epoch 105 @ 71912 updates, score 3.413) (writing took 2.3894780799746513 seconds)
2024-02-10 17:13:30 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2024-02-10 17:13:30 | INFO | train | epoch 105 | loss 2.32 | nll_loss 0.659 | ppl 1.58 | wps 84237.5 | ups 5.8 | wpb 14524.4 | bsz 748.5 | num_updates 71912 | lr 0.000221678 | gnorm 0.534 | loss_scale 1 | train_wall 112 | gb_free 28.3 | wall 12379
2024-02-10 17:13:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:13:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 17:13:30 | INFO | fairseq.trainer | begin training epoch 106
2024-02-10 17:13:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 17:13:44 | INFO | train_inner | epoch 106:     88 / 685 loss=2.308, nll_loss=0.645, ppl=1.56, wps=67842, ups=4.74, wpb=14305, bsz=726.4, num_updates=72000, lr=0.000220404, gnorm=0.519, loss_scale=1, train_wall=16, gb_free=28.7, wall=12394
2024-02-10 17:14:01 | INFO | train_inner | epoch 106:    188 / 685 loss=2.312, nll_loss=0.65, ppl=1.57, wps=87921.3, ups=6.01, wpb=14621.3, bsz=772, num_updates=72100, lr=0.000218958, gnorm=0.54, loss_scale=1, train_wall=16, gb_free=28.9, wall=12411
2024-02-10 17:14:18 | INFO | train_inner | epoch 106:    288 / 685 loss=2.311, nll_loss=0.65, ppl=1.57, wps=87891.1, ups=6.03, wpb=14586.5, bsz=763.3, num_updates=72200, lr=0.000217516, gnorm=0.525, loss_scale=1, train_wall=16, gb_free=28.5, wall=12427
2024-02-10 17:14:34 | INFO | train_inner | epoch 106:    388 / 685 loss=2.312, nll_loss=0.65, ppl=1.57, wps=87552.2, ups=6.03, wpb=14517.2, bsz=734.8, num_updates=72300, lr=0.000216078, gnorm=0.532, loss_scale=1, train_wall=16, gb_free=28.2, wall=12444
2024-02-10 17:14:51 | INFO | train_inner | epoch 106:    488 / 685 loss=2.316, nll_loss=0.655, ppl=1.57, wps=87937.2, ups=6.04, wpb=14562.5, bsz=753.2, num_updates=72400, lr=0.000214643, gnorm=0.523, loss_scale=1, train_wall=16, gb_free=28.3, wall=12460
2024-02-10 17:15:07 | INFO | train_inner | epoch 106:    588 / 685 loss=2.325, nll_loss=0.664, ppl=1.58, wps=87269.9, ups=6.09, wpb=14320.1, bsz=726.6, num_updates=72500, lr=0.000213212, gnorm=0.548, loss_scale=1, train_wall=16, gb_free=28.6, wall=12477
2024-02-10 17:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 17:15:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 17:15:25 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 3.431 | nll_loss 1.85 | ppl 3.6 | wps 142321 | wpb 5716.3 | bsz 292.8 | num_updates 72597 | best_loss 3.394
2024-02-10 17:15:25 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 25 runs
2024-02-10 17:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 72597 updates
2024-02-10 17:15:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint106.pt
2024-02-10 17:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint106.pt
2024-02-10 17:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint106.pt (epoch 106 @ 72597 updates, score 3.431) (writing took 2.4341238199267536 seconds)
2024-02-10 17:15:28 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2024-02-10 17:15:28 | INFO | train | epoch 106 | loss 2.315 | nll_loss 0.653 | ppl 1.57 | wps 84262.6 | ups 5.8 | wpb 14524.4 | bsz 748.5 | num_updates 72597 | lr 0.000211827 | gnorm 0.531 | loss_scale 1 | train_wall 112 | gb_free 28.3 | wall 12497
2024-02-10 17:15:28 | INFO | fairseq_cli.train | done training in 12492.4 seconds
Training complete.
Finetuning complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt' are the same file
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt' are the same file
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test.en: 15516 sents, 256102 tokens, 0.0176% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test.de: 15516 sents, 269168 tokens, 0.0163% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
averaging checkpoints:  ['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint106.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint105.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint104.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint103.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint102.pt']
Finished writing averaged checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 16, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 6616 types
INFO:fairseq.tasks.translation:[de] dictionary: 8648 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/test.en-de.en
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/test.en-de.de
INFO:fairseq.tasks.translation:/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset test en-de 15516 examples
INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True
INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True
INFO:fairseq.tasks.fairseq_task:rebuild_batches = False
INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1
INFO:fairseq.logging.progress_bar::    101 / 243 wps=1358
INFO:fairseq.logging.progress_bar::    201 / 243 wps=1607
INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2
INFO:fairseq_cli.generate:Translated 15,512 sentences (259,506 tokens) in 93.4s (166.09 sentences/s, 2778.53 tokens/s)
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/hyp_mt.txt and /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/ref_mt.txt
Sampled predictions written to /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/hyp_mt.txt.sampled
Sample predictions:
Sample: Ich wei es nicht.
Reference: Ich wei nicht.
Sample: Lass mich es sehen!
Reference: Zeig mal!
Sample: Aber nicht lange.
Reference: Aber nicht lange.
Sample: Wie sieht er aus?
Reference: Wie war er?
Sample: Und der letzte?
Reference: Und zuletzt?
Sample: Mit all dem?
Reference: Mit allem?
Sample: Kaiserliche Reichsdit.
Reference: Reichstag
Sample: Arbeiter aus der ganzen Welt
Reference: Arbeitswelt
Sample: Das tue ich immer noch.
Reference: Das mache ich weiterhin.
Sample: Time acceleration.
Reference: Die Zeit rast.
Sample: Du machst es.
Reference: Du machst es.
Sample: Sie ist einfach so.
Reference: Sie ist gerecht.
Sample: Flugendes Musikstck
Reference: Eine Sinfonie
Sample: Das Leben ist wunderschn.
Reference: Das Leben ist schn.
Sample: Das goldene Land.
Reference: Das goldene Land
Sample: Verdammte Erde
Reference: Gebackene Erde
Sample: Die Union ist strkt.
Reference: Die Union ist stark.
Sample: Sehen Sie jeden Dienstag!
Reference: Sehen uns Dienstag!
Sample: Ein Akt des Glaubens.
Reference: Ein Akt des Glaubens
Sample: Das kann nicht funktionieren.
Reference: Das ist nicht mglich.
WER:
Generate test with beam=16: BLEU4 = 27.17, 54.6/33.8/22.3/15.2 (BP=0.967, ratio=0.968, syslen=135835, reflen=140361)
BLEU:
{
 "name": "BLEU",
 "score": 27.2,
 "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:none|version:2.4.0",
 "verbose_score": "54.6/33.8/22.3/15.2 (BP = 0.967 ratio = 0.968 hyp_len = 135835 ref_len = 140360)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "none",
 "smooth": "none",
 "version": "2.4.0"
}
Submitted batch job 23138680