(base) [uxude@uc2n994 train]$ cat finetune_mt_covost_23095538.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[2024-01-31 00:18:36] [INFO] [Dataset::Prepare Datasets]: Preparing CoVoST...
Loading CoVoST train: 100%|██████████| 289430/289430 [00:02<00:00, 124050.27it/s]
Loading CoVoST dev: 100%|██████████| 15531/15531 [00:00<00:00, 218545.82it/s]
Loading CoVoST test: 100%|██████████| 15531/15531 [00:00<00:00, 216215.88it/s]
cp: cannot stat '/pfs/work7/workspace/scratch/uxude-ASR/spm.asr.model': No such file or directory
[2024-01-31 00:43:14] [INFO] [Dataset::Prepare Datasets]: Creating ASR configs for CoVoST...
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Processing dataset train to encoding...
[2024-01-31 00:43:14] [INFO] [ASR::MelEncoding]: Finished processing mel spectrogram for all samples.
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Processing dataset dev to encoding...
[2024-01-31 00:43:14] [INFO] [ASR::MelEncoding]: Finished processing mel spectrogram for all samples.
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Processing dataset test to encoding...
[2024-01-31 00:43:14] [INFO] [ASR::MelEncoding]: Finished processing mel spectrogram for all samples.
[2024-01-31 00:43:14] [INFO] [ASR::Config]: Fetching manifest from train...
Manifest train:  76%|███████▌  | 2187/2895 [1:48:51<39:32,  3.35s/it][2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411967!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411975!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411976!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411984!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411985!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411986!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411987!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411989!
[2024-01-31 02:32:08] [WARNING] [ASR::Config]: Missing audio file for dc3db1a53620d85b9664bf89a66c12905ad39c9c4cd2bf52b2a9c731b0935ec25efa72329932cda761f94c235839ecdd57623571630451a40704ca86cf75b218-common_voice_en_19411990!
Manifest train: 100%|██████████| 2895/2895 [2:26:33<00:00,  3.04s/it]
[2024-01-31 03:09:48] [INFO] [ASR::Config]: Saving manifest for train...
[2024-01-31 03:09:53] [INFO] [ASR::Config]: Fetching manifest from dev...
Manifest dev: 100%|██████████| 156/156 [09:03<00:00,  3.48s/it]
[2024-01-31 03:18:56] [INFO] [ASR::Config]: Saving manifest for dev...
[2024-01-31 03:18:56] [INFO] [ASR::Config]: Fetching manifest from test...
Manifest test: 100%|██████████| 156/156 [08:56<00:00,  3.44s/it]
[2024-01-31 03:27:53] [INFO] [ASR::Config]: Saving manifest for test...
[2024-01-31 03:27:53] [INFO] [ASR::Config]: Generating config yaml...
[2024-01-31 03:27:53] [INFO] [ASR::Config]: Generated config yaml...
[2024-01-31 03:27:53] [INFO] [Dataset::Prepare Datasets]: Preparing MT CoVoST...
Loading CoVoST train: 100%|██████████| 289430/289430 [00:02<00:00, 125849.53it/s]
100%|██████████| 2895/2895 [00:06<00:00, 482.08it/s]
Processing dataset: 100%|██████████| 289430/289430 [00:02<00:00, 118904.61it/s]
Loading CoVoST dev: 100%|██████████| 15531/15531 [00:00<00:00, 151785.39it/s]
100%|██████████| 156/156 [00:00<00:00, 237.67it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 116840.08it/s]
Loading CoVoST test: 100%|██████████| 15531/15531 [00:00<00:00, 215195.90it/s]
100%|██████████| 156/156 [00:00<00:00, 897.08it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 131155.92it/s]
[2024-01-31 04:40:24] [INFO] [MT::Config]: Writing MT Dataset train to disk...
Writing to files: 289024it [00:00, 906715.10it/s]
[2024-01-31 04:40:25] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:40:26] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:40:26] [INFO] [MT::Config]: Encoding split train...
[2024-01-31 04:40:26] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/train.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train.en already exists.
[2024-01-31 04:40:26] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/train.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/train.de already exists.
[2024-01-31 04:40:26] [INFO] [MT::Config]: Writing MT Dataset dev to disk...
Writing to files: 15520it [00:00, 1637172.06it/s]
[2024-01-31 04:40:27] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding split dev...
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/dev.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev.en already exists.
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/dev.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/dev.de already exists.
[2024-01-31 04:40:27] [INFO] [MT::Config]: Writing MT Dataset test to disk...
Writing to files: 15516it [00:00, 1628639.88it/s]
[2024-01-31 04:40:27] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:40:27] [INFO] [MT::Config]: Encoding split test...
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/test.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.en already exists.
[2024-01-31 04:40:27] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/data/test.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.de already exists.
[2024-01-31 04:40:27] [INFO] [Dataset::Prepare Datasets]: Preparing MT CoVoST with punctuation...
Loading CoVoST train: 100%|██████████| 289430/289430 [00:01<00:00, 157105.41it/s]
100%|██████████| 2895/2895 [00:05<00:00, 525.37it/s]
Processing dataset: 100%|██████████| 289430/289430 [00:02<00:00, 128023.86it/s]
Loading CoVoST dev: 100%|██████████| 15531/15531 [00:00<00:00, 218888.77it/s]
100%|██████████| 156/156 [00:00<00:00, 602.17it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 126705.81it/s]
Loading CoVoST test: 100%|██████████| 15531/15531 [00:00<00:00, 211068.78it/s]
100%|██████████| 156/156 [00:00<00:00, 585.05it/s]
Processing dataset: 100%|██████████| 15531/15531 [00:00<00:00, 142362.66it/s]
[2024-01-31 04:41:15] [INFO] [MT::Config]: Writing MT Dataset train to disk...
Writing to files: 289365it [00:00, 1305878.49it/s]
[2024-01-31 04:41:16] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding split train...
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/train.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train.en already exists.
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/train.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/train.de already exists.
[2024-01-31 04:41:16] [INFO] [MT::Config]: Writing MT Dataset dev to disk...
Writing to files: 15530it [00:00, 1773753.26it/s]
[2024-01-31 04:41:16] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding split dev...
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/dev.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev.en already exists.
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/dev.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/dev.de already exists.
[2024-01-31 04:41:16] [INFO] [MT::Config]: Writing MT Dataset test to disk...
Writing to files: 15529it [00:00, 1795741.69it/s]
[2024-01-31 04:41:16] [INFO] [MT::Config]: Loading MT BPE...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding MT Dataset with sentencepiece...
[2024-01-31 04:41:16] [INFO] [MT::Config]: Encoding split test...
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/test.en because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.en already exists.
[2024-01-31 04:41:16] [INFO] [MachineTranslation::BPE]: Skipping Encoding of /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/data/test.de because /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_punctuation/spm/test.de already exists.
[2024-01-31 04:41:16] [INFO] [Dataset::Prepare Datasets]: Done!
Finetuning the MT model...
Binarized data already exists. Skipping binarization.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Training time: 6 hours
2024-01-31 04:43:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=50000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-01-31 04:43:38 | INFO | fairseq.tasks.translation | [en] dictionary: 6520 types
2024-01-31 04:43:38 | INFO | fairseq.tasks.translation | [de] dictionary: 7560 types
2024-01-31 04:43:41 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7560, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=7560, bias=False)
  )
)
2024-01-31 04:43:41 | INFO | fairseq_cli.train | task: TranslationTask
2024-01-31 04:43:41 | INFO | fairseq_cli.train | model: TransformerModel
2024-01-31 04:43:41 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-31 04:43:41 | INFO | fairseq_cli.train | num. shared model params: 51,347,456 (num. trained: 51,347,456)
2024-01-31 04:43:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-31 04:43:41 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.en
2024-01-31 04:43:41 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.de
2024-01-31 04:43:41 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset valid en-de 15520 examples
2024-01-31 04:43:41 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-31 04:43:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 04:43:41 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-01-31 04:43:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 04:43:41 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-31 04:43:41 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-01-31 04:43:41 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-01-31 04:43:45 | INFO | fairseq.trainer | Loaded checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt (epoch 101 @ 147490 updates)
2024-01-31 04:43:45 | INFO | fairseq.trainer | loading train data for epoch 101
2024-01-31 04:43:45 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.en
2024-01-31 04:43:45 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.de
2024-01-31 04:43:45 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset train en-de 289024 examples
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 04:43:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 101
2024-01-31 04:43:46 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 04:43:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-31 04:43:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:43:48 | INFO | fairseq.trainer | begin training epoch 101
2024-01-31 04:43:48 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-01-31 04:43:56 | INFO | train_inner | epoch 101:     10 / 1475 loss=2.232, nll_loss=0.579, ppl=1.49, wps=52310.5, ups=13.44, wpb=3902.2, bsz=229.6, num_updates=147500, lr=8.23387e-05, gnorm=0.843, loss_scale=16, train_wall=8, gb_free=29.7, wall=15
2024-01-31 04:44:03 | INFO | train_inner | epoch 101:    110 / 1475 loss=2.277, nll_loss=0.629, ppl=1.55, wps=58097.4, ups=15.22, wpb=3817.9, bsz=197.9, num_updates=147600, lr=8.23108e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=22
2024-01-31 04:44:09 | INFO | train_inner | epoch 101:    210 / 1475 loss=2.284, nll_loss=0.637, ppl=1.56, wps=58032.2, ups=15.3, wpb=3794.1, bsz=193.5, num_updates=147700, lr=8.22829e-05, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.7, wall=28
2024-01-31 04:44:16 | INFO | train_inner | epoch 101:    310 / 1475 loss=2.276, nll_loss=0.629, ppl=1.55, wps=58261.2, ups=15.23, wpb=3825.8, bsz=201.8, num_updates=147800, lr=8.22551e-05, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.8, wall=35
2024-01-31 04:44:23 | INFO | train_inner | epoch 101:    410 / 1475 loss=2.293, nll_loss=0.647, ppl=1.57, wps=57460.8, ups=15.31, wpb=3752.8, bsz=189.9, num_updates=147900, lr=8.22273e-05, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.7, wall=41
2024-01-31 04:44:29 | INFO | train_inner | epoch 101:    510 / 1475 loss=2.285, nll_loss=0.638, ppl=1.56, wps=57923.7, ups=15.32, wpb=3781.6, bsz=192.2, num_updates=148000, lr=8.21995e-05, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.7, wall=48
2024-01-31 04:44:36 | INFO | train_inner | epoch 101:    610 / 1475 loss=2.281, nll_loss=0.634, ppl=1.55, wps=56855.7, ups=15.32, wpb=3710.7, bsz=198.2, num_updates=148100, lr=8.21717e-05, gnorm=0.952, loss_scale=16, train_wall=6, gb_free=29.7, wall=55
2024-01-31 04:44:42 | INFO | train_inner | epoch 101:    710 / 1475 loss=2.299, nll_loss=0.655, ppl=1.57, wps=58057.4, ups=15.33, wpb=3788.3, bsz=188.4, num_updates=148200, lr=8.2144e-05, gnorm=0.962, loss_scale=16, train_wall=6, gb_free=29.6, wall=61
2024-01-31 04:44:49 | INFO | train_inner | epoch 101:    810 / 1475 loss=2.294, nll_loss=0.649, ppl=1.57, wps=56713.3, ups=15.23, wpb=3724.7, bsz=191, num_updates=148300, lr=8.21163e-05, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.8, wall=68
2024-01-31 04:44:55 | INFO | train_inner | epoch 101:    910 / 1475 loss=2.285, nll_loss=0.64, ppl=1.56, wps=57570.2, ups=15.26, wpb=3771.6, bsz=202.7, num_updates=148400, lr=8.20886e-05, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.7, wall=74
2024-01-31 04:45:02 | INFO | train_inner | epoch 101:   1010 / 1475 loss=2.298, nll_loss=0.654, ppl=1.57, wps=58060.3, ups=15.29, wpb=3797.1, bsz=197.3, num_updates=148500, lr=8.2061e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=81
2024-01-31 04:45:08 | INFO | train_inner | epoch 101:   1110 / 1475 loss=2.292, nll_loss=0.648, ppl=1.57, wps=56622.4, ups=15.22, wpb=3720.9, bsz=197.7, num_updates=148600, lr=8.20334e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.8, wall=87
2024-01-31 04:45:15 | INFO | train_inner | epoch 101:   1210 / 1475 loss=2.294, nll_loss=0.65, ppl=1.57, wps=57451.3, ups=15.23, wpb=3772.7, bsz=201.7, num_updates=148700, lr=8.20058e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=94
2024-01-31 04:45:22 | INFO | train_inner | epoch 101:   1310 / 1475 loss=2.3, nll_loss=0.656, ppl=1.58, wps=57314.4, ups=15.25, wpb=3757.2, bsz=193.3, num_updates=148800, lr=8.19782e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.8, wall=100
2024-01-31 04:45:28 | INFO | train_inner | epoch 101:   1410 / 1475 loss=2.299, nll_loss=0.656, ppl=1.58, wps=57694.5, ups=15.28, wpb=3776.8, bsz=195.7, num_updates=148900, lr=8.19507e-05, gnorm=0.961, loss_scale=16, train_wall=6, gb_free=29.7, wall=107
2024-01-31 04:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:45:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:45:35 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 3.487 | nll_loss 1.875 | ppl 3.67 | wps 130730 | wpb 3189.1 | bsz 163.4 | num_updates 148965 | best_loss 3.385
2024-01-31 04:45:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 148965 updates
2024-01-31 04:45:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint101.pt
2024-01-31 04:45:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint101.pt
2024-01-31 04:45:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint101.pt (epoch 101 @ 148965 updates, score 3.487) (writing took 2.4888894020114094 seconds)
2024-01-31 04:45:37 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2024-01-31 04:45:37 | INFO | train | epoch 101 | loss 2.29 | nll_loss 0.645 | ppl 1.56 | wps 54832.3 | ups 14.54 | wpb 3771.6 | bsz 195.9 | num_updates 148965 | lr 8.19328e-05 | gnorm 0.947 | loss_scale 16 | train_wall 100 | gb_free 29.7 | wall 116
2024-01-31 04:45:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:45:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:45:37 | INFO | fairseq.trainer | begin training epoch 102
2024-01-31 04:45:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:45:39 | INFO | train_inner | epoch 102:     35 / 1475 loss=2.299, nll_loss=0.655, ppl=1.57, wps=33153.9, ups=8.75, wpb=3788.1, bsz=190.5, num_updates=149000, lr=8.19232e-05, gnorm=0.956, loss_scale=16, train_wall=6, gb_free=29.7, wall=118
2024-01-31 04:45:46 | INFO | train_inner | epoch 102:    135 / 1475 loss=2.266, nll_loss=0.618, ppl=1.53, wps=57248.5, ups=15.23, wpb=3759.7, bsz=195.4, num_updates=149100, lr=8.18957e-05, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.7, wall=125
2024-01-31 04:45:53 | INFO | train_inner | epoch 102:    235 / 1475 loss=2.261, nll_loss=0.612, ppl=1.53, wps=56530.6, ups=15.17, wpb=3727.2, bsz=208.8, num_updates=149200, lr=8.18683e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=132
2024-01-31 04:45:59 | INFO | train_inner | epoch 102:    335 / 1475 loss=2.276, nll_loss=0.629, ppl=1.55, wps=57777.9, ups=15.28, wpb=3781, bsz=198.2, num_updates=149300, lr=8.18408e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=138
2024-01-31 04:46:06 | INFO | train_inner | epoch 102:    435 / 1475 loss=2.277, nll_loss=0.631, ppl=1.55, wps=57930.6, ups=15.31, wpb=3783.2, bsz=206.8, num_updates=149400, lr=8.18134e-05, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=145
2024-01-31 04:46:12 | INFO | train_inner | epoch 102:    535 / 1475 loss=2.295, nll_loss=0.649, ppl=1.57, wps=57407.5, ups=15.39, wpb=3730.6, bsz=187, num_updates=149500, lr=8.17861e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=151
2024-01-31 04:46:19 | INFO | train_inner | epoch 102:    635 / 1475 loss=2.303, nll_loss=0.659, ppl=1.58, wps=57709.5, ups=15.34, wpb=3762.5, bsz=183.7, num_updates=149600, lr=8.17587e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=30, wall=158
2024-01-31 04:46:25 | INFO | train_inner | epoch 102:    735 / 1475 loss=2.284, nll_loss=0.639, ppl=1.56, wps=57883.9, ups=15.29, wpb=3784.8, bsz=199.8, num_updates=149700, lr=8.17314e-05, gnorm=0.954, loss_scale=16, train_wall=6, gb_free=29.7, wall=164
2024-01-31 04:46:32 | INFO | train_inner | epoch 102:    835 / 1475 loss=2.283, nll_loss=0.637, ppl=1.55, wps=58231.4, ups=15.26, wpb=3816.6, bsz=200.7, num_updates=149800, lr=8.17041e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=171
2024-01-31 04:46:38 | INFO | train_inner | epoch 102:    935 / 1475 loss=2.292, nll_loss=0.648, ppl=1.57, wps=57587.4, ups=15.32, wpb=3760.1, bsz=197.7, num_updates=149900, lr=8.16769e-05, gnorm=0.955, loss_scale=16, train_wall=6, gb_free=29.7, wall=177
2024-01-31 04:46:45 | INFO | train_inner | epoch 102:   1035 / 1475 loss=2.303, nll_loss=0.659, ppl=1.58, wps=58993.4, ups=15.36, wpb=3840.6, bsz=189.8, num_updates=150000, lr=8.16497e-05, gnorm=0.943, loss_scale=16, train_wall=6, gb_free=29.7, wall=184
2024-01-31 04:46:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:46:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:46:47 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 3.493 | nll_loss 1.881 | ppl 3.68 | wps 129244 | wpb 3189.1 | bsz 163.4 | num_updates 150000 | best_loss 3.385
2024-01-31 04:46:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 150000 updates
2024-01-31 04:46:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_102_150000.pt
2024-01-31 04:46:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_102_150000.pt
2024-01-31 04:46:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_102_150000.pt (epoch 102 @ 150000 updates, score 3.493) (writing took 2.3441237680381164 seconds)
2024-01-31 04:46:56 | INFO | train_inner | epoch 102:   1135 / 1475 loss=2.279, nll_loss=0.633, ppl=1.55, wps=33483.1, ups=8.83, wpb=3791.5, bsz=206.3, num_updates=150100, lr=8.16225e-05, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.7, wall=195
2024-01-31 04:47:03 | INFO | train_inner | epoch 102:   1235 / 1475 loss=2.31, nll_loss=0.668, ppl=1.59, wps=57866.7, ups=15.32, wpb=3777.3, bsz=195, num_updates=150200, lr=8.15953e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.6, wall=202
2024-01-31 04:47:09 | INFO | train_inner | epoch 102:   1335 / 1475 loss=2.302, nll_loss=0.659, ppl=1.58, wps=58252.8, ups=15.3, wpb=3807.4, bsz=192.2, num_updates=150300, lr=8.15681e-05, gnorm=0.968, loss_scale=16, train_wall=6, gb_free=29.7, wall=208
2024-01-31 04:47:16 | INFO | train_inner | epoch 102:   1435 / 1475 loss=2.311, nll_loss=0.668, ppl=1.59, wps=55905.9, ups=15.3, wpb=3653, bsz=184, num_updates=150400, lr=8.1541e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.6, wall=215
2024-01-31 04:47:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:47:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:47:21 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 3.494 | nll_loss 1.879 | ppl 3.68 | wps 130519 | wpb 3189.1 | bsz 163.4 | num_updates 150440 | best_loss 3.385
2024-01-31 04:47:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 150440 updates
2024-01-31 04:47:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint102.pt
2024-01-31 04:47:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint102.pt
2024-01-31 04:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint102.pt (epoch 102 @ 150440 updates, score 3.494) (writing took 2.3791731520323083 seconds)
2024-01-31 04:47:23 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2024-01-31 04:47:23 | INFO | train | epoch 102 | loss 2.288 | nll_loss 0.643 | ppl 1.56 | wps 52466.2 | ups 13.91 | wpb 3771.6 | bsz 195.9 | num_updates 150440 | lr 8.15302e-05 | gnorm 0.951 | loss_scale 16 | train_wall 93 | gb_free 29.7 | wall 222
2024-01-31 04:47:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:47:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:47:23 | INFO | fairseq.trainer | begin training epoch 103
2024-01-31 04:47:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:47:27 | INFO | train_inner | epoch 103:     60 / 1475 loss=2.274, nll_loss=0.627, ppl=1.54, wps=33159.8, ups=8.78, wpb=3778.6, bsz=194.6, num_updates=150500, lr=8.15139e-05, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.6, wall=226
2024-01-31 04:47:34 | INFO | train_inner | epoch 103:    160 / 1475 loss=2.254, nll_loss=0.604, ppl=1.52, wps=57130.4, ups=15.22, wpb=3753, bsz=216.9, num_updates=150600, lr=8.14868e-05, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.6, wall=233
2024-01-31 04:47:40 | INFO | train_inner | epoch 103:    260 / 1475 loss=2.269, nll_loss=0.621, ppl=1.54, wps=57702.8, ups=15.29, wpb=3774.9, bsz=202.5, num_updates=150700, lr=8.14598e-05, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=239
2024-01-31 04:47:47 | INFO | train_inner | epoch 103:    360 / 1475 loss=2.275, nll_loss=0.627, ppl=1.54, wps=58033.6, ups=15.37, wpb=3776.8, bsz=191.8, num_updates=150800, lr=8.14328e-05, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.9, wall=246
2024-01-31 04:47:53 | INFO | train_inner | epoch 103:    460 / 1475 loss=2.284, nll_loss=0.639, ppl=1.56, wps=58119.1, ups=15.31, wpb=3795.4, bsz=201.8, num_updates=150900, lr=8.14058e-05, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=252
2024-01-31 04:48:00 | INFO | train_inner | epoch 103:    560 / 1475 loss=2.282, nll_loss=0.636, ppl=1.55, wps=58516.7, ups=15.32, wpb=3820.4, bsz=194.4, num_updates=151000, lr=8.13788e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.7, wall=259
2024-01-31 04:48:06 | INFO | train_inner | epoch 103:    660 / 1475 loss=2.295, nll_loss=0.65, ppl=1.57, wps=58788.9, ups=15.42, wpb=3811.9, bsz=185.4, num_updates=151100, lr=8.13519e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=29.7, wall=265
2024-01-31 04:48:13 | INFO | train_inner | epoch 103:    760 / 1475 loss=2.29, nll_loss=0.645, ppl=1.56, wps=58255, ups=15.33, wpb=3801.2, bsz=199, num_updates=151200, lr=8.1325e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.7, wall=272
2024-01-31 04:48:19 | INFO | train_inner | epoch 103:    860 / 1475 loss=2.291, nll_loss=0.647, ppl=1.57, wps=58104.5, ups=15.44, wpb=3764.1, bsz=188.2, num_updates=151300, lr=8.12981e-05, gnorm=0.968, loss_scale=16, train_wall=6, gb_free=29.8, wall=278
2024-01-31 04:48:26 | INFO | train_inner | epoch 103:    960 / 1475 loss=2.29, nll_loss=0.645, ppl=1.56, wps=57752.9, ups=15.41, wpb=3747.4, bsz=193.6, num_updates=151400, lr=8.12713e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=29.6, wall=285
2024-01-31 04:48:32 | INFO | train_inner | epoch 103:   1060 / 1475 loss=2.294, nll_loss=0.65, ppl=1.57, wps=57457.8, ups=15.38, wpb=3736.1, bsz=192.4, num_updates=151500, lr=8.12444e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.7, wall=291
2024-01-31 04:48:39 | INFO | train_inner | epoch 103:   1160 / 1475 loss=2.307, nll_loss=0.664, ppl=1.58, wps=58296.4, ups=15.5, wpb=3760.2, bsz=189.7, num_updates=151600, lr=8.12176e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.6, wall=298
2024-01-31 04:48:45 | INFO | train_inner | epoch 103:   1260 / 1475 loss=2.305, nll_loss=0.662, ppl=1.58, wps=59334.5, ups=15.48, wpb=3832.5, bsz=187.9, num_updates=151700, lr=8.11909e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.6, wall=304
2024-01-31 04:48:52 | INFO | train_inner | epoch 103:   1360 / 1475 loss=2.286, nll_loss=0.64, ppl=1.56, wps=56951.7, ups=15.38, wpb=3703.8, bsz=204.2, num_updates=151800, lr=8.11641e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.7, wall=311
2024-01-31 04:48:58 | INFO | train_inner | epoch 103:   1460 / 1475 loss=2.3, nll_loss=0.656, ppl=1.58, wps=57709.2, ups=15.41, wpb=3744.4, bsz=194.5, num_updates=151900, lr=8.11374e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.6, wall=317
2024-01-31 04:48:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:48:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:49:01 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 3.492 | nll_loss 1.881 | ppl 3.68 | wps 131646 | wpb 3189.1 | bsz 163.4 | num_updates 151915 | best_loss 3.385
2024-01-31 04:49:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 151915 updates
2024-01-31 04:49:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint103.pt
2024-01-31 04:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint103.pt
2024-01-31 04:49:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint103.pt (epoch 103 @ 151915 updates, score 3.492) (writing took 2.4132039500400424 seconds)
2024-01-31 04:49:04 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2024-01-31 04:49:04 | INFO | train | epoch 103 | loss 2.286 | nll_loss 0.64 | ppl 1.56 | wps 55187.4 | ups 14.63 | wpb 3771.6 | bsz 195.9 | num_updates 151915 | lr 8.11334e-05 | gnorm 0.95 | loss_scale 16 | train_wall 93 | gb_free 29.7 | wall 323
2024-01-31 04:49:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:49:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:49:04 | INFO | fairseq.trainer | begin training epoch 104
2024-01-31 04:49:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:49:10 | INFO | train_inner | epoch 104:     85 / 1475 loss=2.252, nll_loss=0.602, ppl=1.52, wps=32919.7, ups=8.83, wpb=3729.5, bsz=211.3, num_updates=152000, lr=8.11107e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=328
2024-01-31 04:49:16 | INFO | train_inner | epoch 104:    185 / 1475 loss=2.279, nll_loss=0.632, ppl=1.55, wps=58973.4, ups=15.44, wpb=3818.7, bsz=189.6, num_updates=152100, lr=8.1084e-05, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=335
2024-01-31 04:49:23 | INFO | train_inner | epoch 104:    285 / 1475 loss=2.277, nll_loss=0.63, ppl=1.55, wps=57693.5, ups=15.23, wpb=3788.7, bsz=196, num_updates=152200, lr=8.10574e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=342
2024-01-31 04:49:29 | INFO | train_inner | epoch 104:    385 / 1475 loss=2.284, nll_loss=0.639, ppl=1.56, wps=58455.9, ups=15.44, wpb=3786.2, bsz=190.1, num_updates=152300, lr=8.10308e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=348
2024-01-31 04:49:36 | INFO | train_inner | epoch 104:    485 / 1475 loss=2.275, nll_loss=0.628, ppl=1.55, wps=58353.8, ups=15.44, wpb=3778.6, bsz=201, num_updates=152400, lr=8.10042e-05, gnorm=0.935, loss_scale=16, train_wall=6, gb_free=29.8, wall=354
2024-01-31 04:49:42 | INFO | train_inner | epoch 104:    585 / 1475 loss=2.268, nll_loss=0.621, ppl=1.54, wps=57981.6, ups=15.28, wpb=3793.4, bsz=209.4, num_updates=152500, lr=8.09776e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.6, wall=361
2024-01-31 04:49:49 | INFO | train_inner | epoch 104:    685 / 1475 loss=2.293, nll_loss=0.648, ppl=1.57, wps=58040.4, ups=15.45, wpb=3757.1, bsz=185.8, num_updates=152600, lr=8.09511e-05, gnorm=0.964, loss_scale=16, train_wall=6, gb_free=29.7, wall=367
2024-01-31 04:49:55 | INFO | train_inner | epoch 104:    785 / 1475 loss=2.28, nll_loss=0.635, ppl=1.55, wps=57873, ups=15.44, wpb=3747.7, bsz=197.8, num_updates=152700, lr=8.09246e-05, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.7, wall=374
2024-01-31 04:50:02 | INFO | train_inner | epoch 104:    885 / 1475 loss=2.292, nll_loss=0.649, ppl=1.57, wps=58699.8, ups=15.47, wpb=3794.2, bsz=190.6, num_updates=152800, lr=8.08981e-05, gnorm=0.975, loss_scale=16, train_wall=6, gb_free=29.7, wall=380
2024-01-31 04:50:08 | INFO | train_inner | epoch 104:    985 / 1475 loss=2.282, nll_loss=0.636, ppl=1.55, wps=58397.9, ups=15.39, wpb=3793.5, bsz=201.7, num_updates=152900, lr=8.08716e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.7, wall=387
2024-01-31 04:50:14 | INFO | train_inner | epoch 104:   1085 / 1475 loss=2.285, nll_loss=0.639, ppl=1.56, wps=57045.6, ups=15.53, wpb=3672.6, bsz=189.8, num_updates=153000, lr=8.08452e-05, gnorm=0.996, loss_scale=16, train_wall=6, gb_free=29.6, wall=393
2024-01-31 04:50:21 | INFO | train_inner | epoch 104:   1185 / 1475 loss=2.288, nll_loss=0.644, ppl=1.56, wps=56999.5, ups=15.33, wpb=3717.2, bsz=196.2, num_updates=153100, lr=8.08188e-05, gnorm=0.964, loss_scale=16, train_wall=6, gb_free=29.8, wall=400
2024-01-31 04:50:27 | INFO | train_inner | epoch 104:   1285 / 1475 loss=2.296, nll_loss=0.652, ppl=1.57, wps=57887.9, ups=15.46, wpb=3743.7, bsz=189.1, num_updates=153200, lr=8.07924e-05, gnorm=0.966, loss_scale=16, train_wall=6, gb_free=29.7, wall=406
2024-01-31 04:50:34 | INFO | train_inner | epoch 104:   1385 / 1475 loss=2.291, nll_loss=0.646, ppl=1.56, wps=59424.6, ups=15.32, wpb=3879.5, bsz=205.8, num_updates=153300, lr=8.07661e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.7, wall=413
2024-01-31 04:50:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 04:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:50:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:50:42 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 3.5 | nll_loss 1.887 | ppl 3.7 | wps 132468 | wpb 3189.1 | bsz 163.4 | num_updates 153389 | best_loss 3.385
2024-01-31 04:50:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 153389 updates
2024-01-31 04:50:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint104.pt
2024-01-31 04:50:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint104.pt
2024-01-31 04:50:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint104.pt (epoch 104 @ 153389 updates, score 3.5) (writing took 2.381781760021113 seconds)
2024-01-31 04:50:44 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2024-01-31 04:50:44 | INFO | train | epoch 104 | loss 2.284 | nll_loss 0.638 | ppl 1.56 | wps 55294.9 | ups 14.66 | wpb 3771.4 | bsz 196 | num_updates 153389 | lr 8.07426e-05 | gnorm 0.953 | loss_scale 8 | train_wall 92 | gb_free 29.9 | wall 423
2024-01-31 04:50:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:50:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:50:45 | INFO | fairseq.trainer | begin training epoch 105
2024-01-31 04:50:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:50:45 | INFO | train_inner | epoch 105:     11 / 1475 loss=2.31, nll_loss=0.667, ppl=1.59, wps=33090.2, ups=8.84, wpb=3745.3, bsz=187.8, num_updates=153400, lr=8.07397e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.6, wall=424
2024-01-31 04:50:52 | INFO | train_inner | epoch 105:    111 / 1475 loss=2.253, nll_loss=0.603, ppl=1.52, wps=57347.6, ups=15.4, wpb=3724.9, bsz=200.6, num_updates=153500, lr=8.07134e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.7, wall=431
2024-01-31 04:50:58 | INFO | train_inner | epoch 105:    211 / 1475 loss=2.258, nll_loss=0.609, ppl=1.53, wps=57512.4, ups=15.39, wpb=3736.4, bsz=205.2, num_updates=153600, lr=8.06872e-05, gnorm=0.906, loss_scale=8, train_wall=6, gb_free=29.8, wall=437
2024-01-31 04:51:05 | INFO | train_inner | epoch 105:    311 / 1475 loss=2.274, nll_loss=0.626, ppl=1.54, wps=58803.5, ups=15.46, wpb=3803.8, bsz=195.8, num_updates=153700, lr=8.06609e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.6, wall=444
2024-01-31 04:51:11 | INFO | train_inner | epoch 105:    411 / 1475 loss=2.261, nll_loss=0.612, ppl=1.53, wps=56803.5, ups=15.42, wpb=3683.4, bsz=196.1, num_updates=153800, lr=8.06347e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.8, wall=450
2024-01-31 04:51:18 | INFO | train_inner | epoch 105:    511 / 1475 loss=2.291, nll_loss=0.646, ppl=1.56, wps=59265.6, ups=15.36, wpb=3857.6, bsz=189, num_updates=153900, lr=8.06085e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=457
2024-01-31 04:51:24 | INFO | train_inner | epoch 105:    611 / 1475 loss=2.291, nll_loss=0.645, ppl=1.56, wps=58171.6, ups=15.53, wpb=3745.2, bsz=183.4, num_updates=154000, lr=8.05823e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=463
2024-01-31 04:51:31 | INFO | train_inner | epoch 105:    711 / 1475 loss=2.279, nll_loss=0.632, ppl=1.55, wps=57676.9, ups=15.37, wpb=3751.6, bsz=199.6, num_updates=154100, lr=8.05561e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.7, wall=470
2024-01-31 04:51:37 | INFO | train_inner | epoch 105:    811 / 1475 loss=2.292, nll_loss=0.648, ppl=1.57, wps=59359.9, ups=15.38, wpb=3859.1, bsz=191.5, num_updates=154200, lr=8.053e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.7, wall=476
2024-01-31 04:51:44 | INFO | train_inner | epoch 105:    911 / 1475 loss=2.271, nll_loss=0.625, ppl=1.54, wps=58101.2, ups=15.3, wpb=3796.4, bsz=205, num_updates=154300, lr=8.05039e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=483
2024-01-31 04:51:50 | INFO | train_inner | epoch 105:   1011 / 1475 loss=2.289, nll_loss=0.644, ppl=1.56, wps=57445.6, ups=15.32, wpb=3750, bsz=198.7, num_updates=154400, lr=8.04778e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=489
2024-01-31 04:51:57 | INFO | train_inner | epoch 105:   1111 / 1475 loss=2.292, nll_loss=0.648, ppl=1.57, wps=57821, ups=15.45, wpb=3741.4, bsz=191.4, num_updates=154500, lr=8.04518e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.7, wall=496
2024-01-31 04:52:03 | INFO | train_inner | epoch 105:   1211 / 1475 loss=2.293, nll_loss=0.649, ppl=1.57, wps=58693.4, ups=15.41, wpb=3809.4, bsz=195.6, num_updates=154600, lr=8.04258e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=502
2024-01-31 04:52:10 | INFO | train_inner | epoch 105:   1311 / 1475 loss=2.28, nll_loss=0.635, ppl=1.55, wps=57715.6, ups=15.39, wpb=3751, bsz=204.8, num_updates=154700, lr=8.03998e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.7, wall=509
2024-01-31 04:52:16 | INFO | train_inner | epoch 105:   1411 / 1475 loss=2.303, nll_loss=0.659, ppl=1.58, wps=57672.8, ups=15.46, wpb=3729.9, bsz=189.1, num_updates=154800, lr=8.03738e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.6, wall=515
2024-01-31 04:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:52:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:52:23 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 3.491 | nll_loss 1.883 | ppl 3.69 | wps 132185 | wpb 3189.1 | bsz 163.4 | num_updates 154864 | best_loss 3.385
2024-01-31 04:52:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 154864 updates
2024-01-31 04:52:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint105.pt
2024-01-31 04:52:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint105.pt
2024-01-31 04:52:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint105.pt (epoch 105 @ 154864 updates, score 3.491) (writing took 2.4214005609974265 seconds)
2024-01-31 04:52:25 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2024-01-31 04:52:25 | INFO | train | epoch 105 | loss 2.281 | nll_loss 0.635 | ppl 1.55 | wps 55281.9 | ups 14.66 | wpb 3771.6 | bsz 195.9 | num_updates 154864 | lr 8.03572e-05 | gnorm 0.956 | loss_scale 8 | train_wall 92 | gb_free 29.6 | wall 524
2024-01-31 04:52:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:52:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:52:25 | INFO | fairseq.trainer | begin training epoch 106
2024-01-31 04:52:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:52:28 | INFO | train_inner | epoch 106:     36 / 1475 loss=2.277, nll_loss=0.631, ppl=1.55, wps=33654.3, ups=8.79, wpb=3829.6, bsz=199.8, num_updates=154900, lr=8.03479e-05, gnorm=0.935, loss_scale=8, train_wall=6, gb_free=29.8, wall=526
2024-01-31 04:52:34 | INFO | train_inner | epoch 106:    136 / 1475 loss=2.264, nll_loss=0.615, ppl=1.53, wps=59144.6, ups=15.4, wpb=3840.2, bsz=199.8, num_updates=155000, lr=8.03219e-05, gnorm=0.917, loss_scale=8, train_wall=6, gb_free=29.7, wall=533
2024-01-31 04:52:41 | INFO | train_inner | epoch 106:    236 / 1475 loss=2.276, nll_loss=0.628, ppl=1.55, wps=59516.7, ups=15.42, wpb=3858.5, bsz=187.3, num_updates=155100, lr=8.0296e-05, gnorm=0.924, loss_scale=8, train_wall=6, gb_free=29.7, wall=539
2024-01-31 04:52:47 | INFO | train_inner | epoch 106:    336 / 1475 loss=2.277, nll_loss=0.631, ppl=1.55, wps=58540.7, ups=15.37, wpb=3809.7, bsz=188.2, num_updates=155200, lr=8.02702e-05, gnorm=0.931, loss_scale=8, train_wall=6, gb_free=29.8, wall=546
2024-01-31 04:52:54 | INFO | train_inner | epoch 106:    436 / 1475 loss=2.263, nll_loss=0.615, ppl=1.53, wps=58410.1, ups=15.41, wpb=3789.3, bsz=203, num_updates=155300, lr=8.02443e-05, gnorm=0.924, loss_scale=8, train_wall=6, gb_free=29.8, wall=552
2024-01-31 04:53:00 | INFO | train_inner | epoch 106:    536 / 1475 loss=2.273, nll_loss=0.626, ppl=1.54, wps=57787.6, ups=15.46, wpb=3736.9, bsz=199.8, num_updates=155400, lr=8.02185e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.8, wall=559
2024-01-31 04:53:07 | INFO | train_inner | epoch 106:    636 / 1475 loss=2.274, nll_loss=0.628, ppl=1.55, wps=58258, ups=15.41, wpb=3780.7, bsz=199.9, num_updates=155500, lr=8.01927e-05, gnorm=0.931, loss_scale=8, train_wall=6, gb_free=29.8, wall=565
2024-01-31 04:53:13 | INFO | train_inner | epoch 106:    736 / 1475 loss=2.278, nll_loss=0.631, ppl=1.55, wps=56743.1, ups=15.41, wpb=3683.3, bsz=194.3, num_updates=155600, lr=8.01669e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=572
2024-01-31 04:53:20 | INFO | train_inner | epoch 106:    836 / 1475 loss=2.285, nll_loss=0.639, ppl=1.56, wps=57787.3, ups=15.28, wpb=3782.1, bsz=191.8, num_updates=155700, lr=8.01412e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.8, wall=578
2024-01-31 04:53:26 | INFO | train_inner | epoch 106:    936 / 1475 loss=2.294, nll_loss=0.65, ppl=1.57, wps=57957.9, ups=15.51, wpb=3737.1, bsz=183, num_updates=155800, lr=8.01154e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.8, wall=585
2024-01-31 04:53:32 | INFO | train_inner | epoch 106:   1036 / 1475 loss=2.276, nll_loss=0.629, ppl=1.55, wps=56912.3, ups=15.36, wpb=3705.6, bsz=203.7, num_updates=155900, lr=8.00898e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.8, wall=591
2024-01-31 04:53:39 | INFO | train_inner | epoch 106:   1136 / 1475 loss=2.281, nll_loss=0.637, ppl=1.55, wps=58078.4, ups=15.4, wpb=3772.2, bsz=205, num_updates=156000, lr=8.00641e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=598
2024-01-31 04:53:45 | INFO | train_inner | epoch 106:   1236 / 1475 loss=2.305, nll_loss=0.663, ppl=1.58, wps=59151.3, ups=15.48, wpb=3820.9, bsz=191.8, num_updates=156100, lr=8.00384e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.7, wall=604
2024-01-31 04:53:52 | INFO | train_inner | epoch 106:   1336 / 1475 loss=2.278, nll_loss=0.633, ppl=1.55, wps=57314.9, ups=15.31, wpb=3743.8, bsz=199.4, num_updates=156200, lr=8.00128e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.6, wall=611
2024-01-31 04:53:58 | INFO | train_inner | epoch 106:   1436 / 1475 loss=2.295, nll_loss=0.651, ppl=1.57, wps=57659.9, ups=15.52, wpb=3714.8, bsz=192.7, num_updates=156300, lr=7.99872e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=617
2024-01-31 04:54:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:54:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:54:03 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 3.493 | nll_loss 1.884 | ppl 3.69 | wps 133910 | wpb 3189.1 | bsz 163.4 | num_updates 156339 | best_loss 3.385
2024-01-31 04:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 156339 updates
2024-01-31 04:54:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint106.pt
2024-01-31 04:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint106.pt
2024-01-31 04:54:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint106.pt (epoch 106 @ 156339 updates, score 3.493) (writing took 2.463958861073479 seconds)
2024-01-31 04:54:06 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2024-01-31 04:54:06 | INFO | train | epoch 106 | loss 2.279 | nll_loss 0.633 | ppl 1.55 | wps 55240.1 | ups 14.65 | wpb 3771.6 | bsz 195.9 | num_updates 156339 | lr 7.99772e-05 | gnorm 0.956 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 625
2024-01-31 04:54:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:54:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:54:06 | INFO | fairseq.trainer | begin training epoch 107
2024-01-31 04:54:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:54:10 | INFO | train_inner | epoch 107:     61 / 1475 loss=2.271, nll_loss=0.624, ppl=1.54, wps=32738.1, ups=8.75, wpb=3743, bsz=190.1, num_updates=156400, lr=7.99616e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.8, wall=629
2024-01-31 04:54:16 | INFO | train_inner | epoch 107:    161 / 1475 loss=2.272, nll_loss=0.624, ppl=1.54, wps=59944.2, ups=15.45, wpb=3879.5, bsz=187.7, num_updates=156500, lr=7.99361e-05, gnorm=0.938, loss_scale=8, train_wall=6, gb_free=29.6, wall=635
2024-01-31 04:54:23 | INFO | train_inner | epoch 107:    261 / 1475 loss=2.273, nll_loss=0.625, ppl=1.54, wps=58575, ups=15.42, wpb=3799.1, bsz=189.5, num_updates=156600, lr=7.99106e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=642
2024-01-31 04:54:29 | INFO | train_inner | epoch 107:    361 / 1475 loss=2.273, nll_loss=0.625, ppl=1.54, wps=58791.1, ups=15.42, wpb=3811.6, bsz=191.8, num_updates=156700, lr=7.9885e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.7, wall=648
2024-01-31 04:54:36 | INFO | train_inner | epoch 107:    461 / 1475 loss=2.261, nll_loss=0.613, ppl=1.53, wps=58645.5, ups=15.32, wpb=3827.6, bsz=206.4, num_updates=156800, lr=7.98596e-05, gnorm=0.919, loss_scale=8, train_wall=6, gb_free=29.7, wall=655
2024-01-31 04:54:42 | INFO | train_inner | epoch 107:    561 / 1475 loss=2.281, nll_loss=0.635, ppl=1.55, wps=58043.6, ups=15.35, wpb=3780.8, bsz=195.4, num_updates=156900, lr=7.98341e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.7, wall=661
2024-01-31 04:54:49 | INFO | train_inner | epoch 107:    661 / 1475 loss=2.258, nll_loss=0.611, ppl=1.53, wps=56395.1, ups=15.34, wpb=3675.9, bsz=208.7, num_updates=157000, lr=7.98087e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=668
2024-01-31 04:54:55 | INFO | train_inner | epoch 107:    761 / 1475 loss=2.279, nll_loss=0.633, ppl=1.55, wps=56561.7, ups=15.5, wpb=3648.2, bsz=189, num_updates=157100, lr=7.97833e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.6, wall=674
2024-01-31 04:55:02 | INFO | train_inner | epoch 107:    861 / 1475 loss=2.294, nll_loss=0.65, ppl=1.57, wps=58757.1, ups=15.52, wpb=3784.9, bsz=185.9, num_updates=157200, lr=7.97579e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.8, wall=681
2024-01-31 04:55:08 | INFO | train_inner | epoch 107:    961 / 1475 loss=2.278, nll_loss=0.632, ppl=1.55, wps=58355.9, ups=15.37, wpb=3795.5, bsz=203.5, num_updates=157300, lr=7.97325e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.8, wall=687
2024-01-31 04:55:15 | INFO | train_inner | epoch 107:   1061 / 1475 loss=2.271, nll_loss=0.624, ppl=1.54, wps=57426.3, ups=15.38, wpb=3734, bsz=201.1, num_updates=157400, lr=7.97072e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=694
2024-01-31 04:55:21 | INFO | train_inner | epoch 107:   1161 / 1475 loss=2.286, nll_loss=0.642, ppl=1.56, wps=58344.6, ups=15.27, wpb=3819.8, bsz=195.8, num_updates=157500, lr=7.96819e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.6, wall=700
2024-01-31 04:55:28 | INFO | train_inner | epoch 107:   1261 / 1475 loss=2.283, nll_loss=0.638, ppl=1.56, wps=58408.6, ups=15.37, wpb=3799.5, bsz=207.1, num_updates=157600, lr=7.96566e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.7, wall=707
2024-01-31 04:55:34 | INFO | train_inner | epoch 107:   1361 / 1475 loss=2.294, nll_loss=0.651, ppl=1.57, wps=58489.9, ups=15.44, wpb=3787.8, bsz=193.6, num_updates=157700, lr=7.96314e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.6, wall=713
2024-01-31 04:55:41 | INFO | train_inner | epoch 107:   1461 / 1475 loss=2.281, nll_loss=0.635, ppl=1.55, wps=57550, ups=15.39, wpb=3738.9, bsz=193.4, num_updates=157800, lr=7.96061e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.8, wall=720
2024-01-31 04:55:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:55:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:55:44 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 3.496 | nll_loss 1.886 | ppl 3.7 | wps 134441 | wpb 3189.1 | bsz 163.4 | num_updates 157814 | best_loss 3.385
2024-01-31 04:55:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 157814 updates
2024-01-31 04:55:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint107.pt
2024-01-31 04:55:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint107.pt
2024-01-31 04:55:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint107.pt (epoch 107 @ 157814 updates, score 3.496) (writing took 2.4265160290524364 seconds)
2024-01-31 04:55:46 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2024-01-31 04:55:46 | INFO | train | epoch 107 | loss 2.277 | nll_loss 0.63 | ppl 1.55 | wps 55258.9 | ups 14.65 | wpb 3771.6 | bsz 195.9 | num_updates 157814 | lr 7.96026e-05 | gnorm 0.963 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 725
2024-01-31 04:55:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:55:47 | INFO | fairseq.trainer | begin training epoch 108
2024-01-31 04:55:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:55:52 | INFO | train_inner | epoch 108:     86 / 1475 loss=2.248, nll_loss=0.598, ppl=1.51, wps=31819.8, ups=8.77, wpb=3626.9, bsz=199.2, num_updates=157900, lr=7.95809e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.8, wall=731
2024-01-31 04:55:59 | INFO | train_inner | epoch 108:    186 / 1475 loss=2.258, nll_loss=0.609, ppl=1.52, wps=57988.2, ups=15.47, wpb=3747.8, bsz=185.8, num_updates=158000, lr=7.95557e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=738
2024-01-31 04:56:05 | INFO | train_inner | epoch 108:    286 / 1475 loss=2.274, nll_loss=0.626, ppl=1.54, wps=57946.2, ups=15.37, wpb=3770.2, bsz=191.6, num_updates=158100, lr=7.95306e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.7, wall=744
2024-01-31 04:56:12 | INFO | train_inner | epoch 108:    386 / 1475 loss=2.272, nll_loss=0.625, ppl=1.54, wps=58184.6, ups=15.44, wpb=3769, bsz=194.9, num_updates=158200, lr=7.95054e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=751
2024-01-31 04:56:18 | INFO | train_inner | epoch 108:    486 / 1475 loss=2.292, nll_loss=0.646, ppl=1.56, wps=58764.1, ups=15.47, wpb=3798.4, bsz=177.8, num_updates=158300, lr=7.94803e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.7, wall=757
2024-01-31 04:56:25 | INFO | train_inner | epoch 108:    586 / 1475 loss=2.272, nll_loss=0.625, ppl=1.54, wps=57980.1, ups=15.39, wpb=3766.4, bsz=197.4, num_updates=158400, lr=7.94552e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.9, wall=763
2024-01-31 04:56:31 | INFO | train_inner | epoch 108:    686 / 1475 loss=2.269, nll_loss=0.623, ppl=1.54, wps=57950, ups=15.34, wpb=3777.4, bsz=198, num_updates=158500, lr=7.94301e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.9, wall=770
2024-01-31 04:56:38 | INFO | train_inner | epoch 108:    786 / 1475 loss=2.277, nll_loss=0.631, ppl=1.55, wps=58644.7, ups=15.38, wpb=3812.9, bsz=195.6, num_updates=158600, lr=7.94051e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=777
2024-01-31 04:56:44 | INFO | train_inner | epoch 108:    886 / 1475 loss=2.283, nll_loss=0.638, ppl=1.56, wps=58071.8, ups=15.48, wpb=3751.7, bsz=194.2, num_updates=158700, lr=7.93801e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.6, wall=783
2024-01-31 04:56:51 | INFO | train_inner | epoch 108:    986 / 1475 loss=2.277, nll_loss=0.63, ppl=1.55, wps=59524.7, ups=15.42, wpb=3861.3, bsz=200.9, num_updates=158800, lr=7.93551e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=789
2024-01-31 04:56:57 | INFO | train_inner | epoch 108:   1086 / 1475 loss=2.285, nll_loss=0.64, ppl=1.56, wps=57587.1, ups=15.47, wpb=3722.3, bsz=195.8, num_updates=158900, lr=7.93301e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.6, wall=796
2024-01-31 04:57:04 | INFO | train_inner | epoch 108:   1186 / 1475 loss=2.289, nll_loss=0.644, ppl=1.56, wps=58728.8, ups=15.43, wpb=3806.4, bsz=192.2, num_updates=159000, lr=7.93052e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.8, wall=802
2024-01-31 04:57:10 | INFO | train_inner | epoch 108:   1286 / 1475 loss=2.285, nll_loss=0.641, ppl=1.56, wps=58573.1, ups=15.36, wpb=3812.8, bsz=200.2, num_updates=159100, lr=7.92802e-05, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.7, wall=809
2024-01-31 04:57:17 | INFO | train_inner | epoch 108:   1386 / 1475 loss=2.271, nll_loss=0.625, ppl=1.54, wps=56765.1, ups=15.38, wpb=3690.6, bsz=202.4, num_updates=159200, lr=7.92553e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.7, wall=815
2024-01-31 04:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:57:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:57:25 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 3.504 | nll_loss 1.894 | ppl 3.72 | wps 134138 | wpb 3189.1 | bsz 163.4 | num_updates 159289 | best_loss 3.385
2024-01-31 04:57:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 159289 updates
2024-01-31 04:57:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint108.pt
2024-01-31 04:57:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint108.pt
2024-01-31 04:57:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint108.pt (epoch 108 @ 159289 updates, score 3.504) (writing took 2.511260764906183 seconds)
2024-01-31 04:57:27 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2024-01-31 04:57:27 | INFO | train | epoch 108 | loss 2.275 | nll_loss 0.628 | ppl 1.55 | wps 55219.4 | ups 14.64 | wpb 3771.6 | bsz 195.9 | num_updates 159289 | lr 7.92332e-05 | gnorm 0.957 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 826
2024-01-31 04:57:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:57:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:57:27 | INFO | fairseq.trainer | begin training epoch 109
2024-01-31 04:57:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:57:28 | INFO | train_inner | epoch 109:     11 / 1475 loss=2.272, nll_loss=0.626, ppl=1.54, wps=33378, ups=8.68, wpb=3845.2, bsz=209.4, num_updates=159300, lr=7.92304e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.8, wall=827
2024-01-31 04:57:34 | INFO | train_inner | epoch 109:    111 / 1475 loss=2.26, nll_loss=0.61, ppl=1.53, wps=57479.6, ups=15.59, wpb=3687.3, bsz=181.7, num_updates=159400, lr=7.92056e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=833
2024-01-31 04:57:41 | INFO | train_inner | epoch 109:    211 / 1475 loss=2.255, nll_loss=0.606, ppl=1.52, wps=58416.8, ups=15.39, wpb=3795.2, bsz=199.4, num_updates=159500, lr=7.91808e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=840
2024-01-31 04:57:47 | INFO | train_inner | epoch 109:    311 / 1475 loss=2.26, nll_loss=0.611, ppl=1.53, wps=57784.2, ups=15.43, wpb=3746, bsz=195.1, num_updates=159600, lr=7.91559e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=846
2024-01-31 04:57:54 | INFO | train_inner | epoch 109:    411 / 1475 loss=2.255, nll_loss=0.606, ppl=1.52, wps=57040.2, ups=15.38, wpb=3709.5, bsz=197.3, num_updates=159700, lr=7.91312e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.8, wall=853
2024-01-31 04:58:00 | INFO | train_inner | epoch 109:    511 / 1475 loss=2.281, nll_loss=0.636, ppl=1.55, wps=58939.3, ups=15.48, wpb=3807.7, bsz=190.2, num_updates=159800, lr=7.91064e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.9, wall=859
2024-01-31 04:58:07 | INFO | train_inner | epoch 109:    611 / 1475 loss=2.27, nll_loss=0.623, ppl=1.54, wps=57439.7, ups=15.44, wpb=3719.7, bsz=191.6, num_updates=159900, lr=7.90817e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=866
2024-01-31 04:58:13 | INFO | train_inner | epoch 109:    711 / 1475 loss=2.281, nll_loss=0.635, ppl=1.55, wps=57993.4, ups=15.4, wpb=3764.9, bsz=188.2, num_updates=160000, lr=7.90569e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.7, wall=872
2024-01-31 04:58:20 | INFO | train_inner | epoch 109:    811 / 1475 loss=2.267, nll_loss=0.619, ppl=1.54, wps=57960, ups=15.34, wpb=3778, bsz=198.7, num_updates=160100, lr=7.90322e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=879
2024-01-31 04:58:26 | INFO | train_inner | epoch 109:    911 / 1475 loss=2.268, nll_loss=0.622, ppl=1.54, wps=58660, ups=15.34, wpb=3824.6, bsz=209.8, num_updates=160200, lr=7.90076e-05, gnorm=0.94, loss_scale=8, train_wall=6, gb_free=29.6, wall=885
2024-01-31 04:58:33 | INFO | train_inner | epoch 109:   1011 / 1475 loss=2.276, nll_loss=0.63, ppl=1.55, wps=58690.6, ups=15.31, wpb=3832.4, bsz=209.4, num_updates=160300, lr=7.89829e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.6, wall=892
2024-01-31 04:58:39 | INFO | train_inner | epoch 109:   1111 / 1475 loss=2.28, nll_loss=0.634, ppl=1.55, wps=58211.9, ups=15.43, wpb=3772.7, bsz=193.9, num_updates=160400, lr=7.89583e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.7, wall=898
2024-01-31 04:58:46 | INFO | train_inner | epoch 109:   1211 / 1475 loss=2.289, nll_loss=0.645, ppl=1.56, wps=57933, ups=15.37, wpb=3768.6, bsz=191.1, num_updates=160500, lr=7.89337e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=905
2024-01-31 04:58:52 | INFO | train_inner | epoch 109:   1311 / 1475 loss=2.275, nll_loss=0.629, ppl=1.55, wps=58489, ups=15.45, wpb=3786.6, bsz=200, num_updates=160600, lr=7.89091e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.8, wall=911
2024-01-31 04:58:59 | INFO | train_inner | epoch 109:   1411 / 1475 loss=2.29, nll_loss=0.647, ppl=1.57, wps=57938.2, ups=15.36, wpb=3772.1, bsz=200.7, num_updates=160700, lr=7.88846e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.6, wall=918
2024-01-31 04:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 04:59:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:59:05 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 3.5 | nll_loss 1.889 | ppl 3.7 | wps 133454 | wpb 3189.1 | bsz 163.4 | num_updates 160764 | best_loss 3.385
2024-01-31 04:59:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 160764 updates
2024-01-31 04:59:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint109.pt
2024-01-31 04:59:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint109.pt
2024-01-31 04:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint109.pt (epoch 109 @ 160764 updates, score 3.5) (writing took 2.4783600689843297 seconds)
2024-01-31 04:59:08 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2024-01-31 04:59:08 | INFO | train | epoch 109 | loss 2.273 | nll_loss 0.626 | ppl 1.54 | wps 55240 | ups 14.65 | wpb 3771.6 | bsz 195.9 | num_updates 160764 | lr 7.88689e-05 | gnorm 0.959 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 927
2024-01-31 04:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 04:59:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 04:59:08 | INFO | fairseq.trainer | begin training epoch 110
2024-01-31 04:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 04:59:10 | INFO | train_inner | epoch 110:     36 / 1475 loss=2.277, nll_loss=0.631, ppl=1.55, wps=33038.6, ups=8.73, wpb=3785.8, bsz=192.3, num_updates=160800, lr=7.886e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.9, wall=929
2024-01-31 04:59:17 | INFO | train_inner | epoch 110:    136 / 1475 loss=2.262, nll_loss=0.614, ppl=1.53, wps=57832.6, ups=15.38, wpb=3760.3, bsz=195.4, num_updates=160900, lr=7.88355e-05, gnorm=0.922, loss_scale=8, train_wall=6, gb_free=29.7, wall=936
2024-01-31 04:59:23 | INFO | train_inner | epoch 110:    236 / 1475 loss=2.247, nll_loss=0.598, ppl=1.51, wps=57969.9, ups=15.38, wpb=3768.6, bsz=197.5, num_updates=161000, lr=7.8811e-05, gnorm=0.923, loss_scale=8, train_wall=6, gb_free=29.7, wall=942
2024-01-31 04:59:30 | INFO | train_inner | epoch 110:    336 / 1475 loss=2.271, nll_loss=0.623, ppl=1.54, wps=58757.6, ups=15.4, wpb=3815.9, bsz=188.5, num_updates=161100, lr=7.87866e-05, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.7, wall=949
2024-01-31 04:59:36 | INFO | train_inner | epoch 110:    436 / 1475 loss=2.249, nll_loss=0.599, ppl=1.52, wps=57551.6, ups=15.38, wpb=3740.9, bsz=199.9, num_updates=161200, lr=7.87621e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.8, wall=955
2024-01-31 04:59:43 | INFO | train_inner | epoch 110:    536 / 1475 loss=2.271, nll_loss=0.624, ppl=1.54, wps=57157.1, ups=15.13, wpb=3778.1, bsz=189.8, num_updates=161300, lr=7.87377e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.7, wall=962
2024-01-31 04:59:49 | INFO | train_inner | epoch 110:    636 / 1475 loss=2.264, nll_loss=0.615, ppl=1.53, wps=58181.7, ups=15.45, wpb=3766.9, bsz=192.8, num_updates=161400, lr=7.87133e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=968
2024-01-31 04:59:56 | INFO | train_inner | epoch 110:    736 / 1475 loss=2.274, nll_loss=0.627, ppl=1.54, wps=58009.9, ups=15.49, wpb=3745.1, bsz=196.2, num_updates=161500, lr=7.86889e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.6, wall=975
2024-01-31 05:00:02 | INFO | train_inner | epoch 110:    836 / 1475 loss=2.264, nll_loss=0.617, ppl=1.53, wps=58410.6, ups=15.41, wpb=3790.1, bsz=202, num_updates=161600, lr=7.86646e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=981
2024-01-31 05:00:09 | INFO | train_inner | epoch 110:    936 / 1475 loss=2.268, nll_loss=0.621, ppl=1.54, wps=57192.2, ups=15.49, wpb=3693.1, bsz=192.6, num_updates=161700, lr=7.86403e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.7, wall=988
2024-01-31 05:00:15 | INFO | train_inner | epoch 110:   1036 / 1475 loss=2.282, nll_loss=0.637, ppl=1.55, wps=58853, ups=15.41, wpb=3818.6, bsz=203.3, num_updates=161800, lr=7.8616e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=994
2024-01-31 05:00:22 | INFO | train_inner | epoch 110:   1136 / 1475 loss=2.287, nll_loss=0.642, ppl=1.56, wps=58630.3, ups=15.44, wpb=3796.8, bsz=191.4, num_updates=161900, lr=7.85917e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=1001
2024-01-31 05:00:28 | INFO | train_inner | epoch 110:   1236 / 1475 loss=2.272, nll_loss=0.626, ppl=1.54, wps=58557.9, ups=15.44, wpb=3793.7, bsz=206.3, num_updates=162000, lr=7.85674e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=1007
2024-01-31 05:00:35 | INFO | train_inner | epoch 110:   1336 / 1475 loss=2.288, nll_loss=0.644, ppl=1.56, wps=57463.5, ups=15.4, wpb=3732.3, bsz=193.2, num_updates=162100, lr=7.85432e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.7, wall=1014
2024-01-31 05:00:41 | INFO | train_inner | epoch 110:   1436 / 1475 loss=2.279, nll_loss=0.634, ppl=1.55, wps=59070.9, ups=15.43, wpb=3828.1, bsz=200.3, num_updates=162200, lr=7.8519e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.7, wall=1020
2024-01-31 05:00:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:00:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:00:46 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 3.502 | nll_loss 1.891 | ppl 3.71 | wps 133571 | wpb 3189.1 | bsz 163.4 | num_updates 162239 | best_loss 3.385
2024-01-31 05:00:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 162239 updates
2024-01-31 05:00:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint110.pt
2024-01-31 05:00:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint110.pt
2024-01-31 05:00:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint110.pt (epoch 110 @ 162239 updates, score 3.502) (writing took 2.5294412749353796 seconds)
2024-01-31 05:00:49 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2024-01-31 05:00:49 | INFO | train | epoch 110 | loss 2.27 | nll_loss 0.624 | ppl 1.54 | wps 55205.2 | ups 14.64 | wpb 3771.6 | bsz 195.9 | num_updates 162239 | lr 7.85095e-05 | gnorm 0.954 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 1028
2024-01-31 05:00:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:00:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:00:49 | INFO | fairseq.trainer | begin training epoch 111
2024-01-31 05:00:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:00:53 | INFO | train_inner | epoch 111:     61 / 1475 loss=2.262, nll_loss=0.614, ppl=1.53, wps=32890.5, ups=8.68, wpb=3790.2, bsz=199, num_updates=162300, lr=7.84948e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.7, wall=1032
2024-01-31 05:00:59 | INFO | train_inner | epoch 111:    161 / 1475 loss=2.26, nll_loss=0.611, ppl=1.53, wps=58107.6, ups=15.43, wpb=3766.6, bsz=189.5, num_updates=162400, lr=7.84706e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=1038
2024-01-31 05:01:06 | INFO | train_inner | epoch 111:    261 / 1475 loss=2.263, nll_loss=0.615, ppl=1.53, wps=58717.6, ups=15.47, wpb=3794.9, bsz=196.6, num_updates=162500, lr=7.84465e-05, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.7, wall=1045
2024-01-31 05:01:12 | INFO | train_inner | epoch 111:    361 / 1475 loss=2.251, nll_loss=0.602, ppl=1.52, wps=58046.2, ups=15.38, wpb=3773.2, bsz=206.1, num_updates=162600, lr=7.84223e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=1051
2024-01-31 05:01:19 | INFO | train_inner | epoch 111:    461 / 1475 loss=2.257, nll_loss=0.609, ppl=1.52, wps=56928.3, ups=15.38, wpb=3702.1, bsz=194.2, num_updates=162700, lr=7.83982e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=1058
2024-01-31 05:01:25 | INFO | train_inner | epoch 111:    561 / 1475 loss=2.279, nll_loss=0.632, ppl=1.55, wps=58945.3, ups=15.54, wpb=3792.7, bsz=185.1, num_updates=162800, lr=7.83741e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.6, wall=1064
2024-01-31 05:01:32 | INFO | train_inner | epoch 111:    661 / 1475 loss=2.275, nll_loss=0.629, ppl=1.55, wps=58928.9, ups=15.43, wpb=3819.8, bsz=192.3, num_updates=162900, lr=7.83501e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=1071
2024-01-31 05:01:38 | INFO | train_inner | epoch 111:    761 / 1475 loss=2.272, nll_loss=0.625, ppl=1.54, wps=57600.5, ups=15.43, wpb=3732.4, bsz=188.7, num_updates=163000, lr=7.8326e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.7, wall=1077
2024-01-31 05:01:45 | INFO | train_inner | epoch 111:    861 / 1475 loss=2.277, nll_loss=0.631, ppl=1.55, wps=57556.5, ups=15.5, wpb=3713.1, bsz=189.6, num_updates=163100, lr=7.8302e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.6, wall=1083
2024-01-31 05:01:51 | INFO | train_inner | epoch 111:    961 / 1475 loss=2.274, nll_loss=0.628, ppl=1.55, wps=57942.3, ups=15.4, wpb=3761.7, bsz=196.6, num_updates=163200, lr=7.8278e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.6, wall=1090
2024-01-31 05:01:58 | INFO | train_inner | epoch 111:   1061 / 1475 loss=2.266, nll_loss=0.62, ppl=1.54, wps=58909.7, ups=15.45, wpb=3813.7, bsz=202, num_updates=163300, lr=7.82541e-05, gnorm=0.935, loss_scale=8, train_wall=6, gb_free=29.8, wall=1096
2024-01-31 05:02:04 | INFO | train_inner | epoch 111:   1161 / 1475 loss=2.294, nll_loss=0.65, ppl=1.57, wps=57584.9, ups=15.5, wpb=3714.8, bsz=185.1, num_updates=163400, lr=7.82301e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=1103
2024-01-31 05:02:10 | INFO | train_inner | epoch 111:   1261 / 1475 loss=2.271, nll_loss=0.626, ppl=1.54, wps=58783.4, ups=15.44, wpb=3806.7, bsz=199, num_updates=163500, lr=7.82062e-05, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.8, wall=1109
2024-01-31 05:02:17 | INFO | train_inner | epoch 111:   1361 / 1475 loss=2.279, nll_loss=0.634, ppl=1.55, wps=58525.1, ups=15.36, wpb=3809.5, bsz=200.6, num_updates=163600, lr=7.81823e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=1116
2024-01-31 05:02:24 | INFO | train_inner | epoch 111:   1461 / 1475 loss=2.258, nll_loss=0.612, ppl=1.53, wps=57468.9, ups=15.29, wpb=3759.5, bsz=211.8, num_updates=163700, lr=7.81584e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.7, wall=1122
2024-01-31 05:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:02:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:02:27 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 3.504 | nll_loss 1.896 | ppl 3.72 | wps 134004 | wpb 3189.1 | bsz 163.4 | num_updates 163714 | best_loss 3.385
2024-01-31 05:02:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 163714 updates
2024-01-31 05:02:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint111.pt
2024-01-31 05:02:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint111.pt
2024-01-31 05:02:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint111.pt (epoch 111 @ 163714 updates, score 3.504) (writing took 2.4630027710227296 seconds)
2024-01-31 05:02:29 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2024-01-31 05:02:29 | INFO | train | epoch 111 | loss 2.269 | nll_loss 0.622 | ppl 1.54 | wps 55310.2 | ups 14.66 | wpb 3771.6 | bsz 195.9 | num_updates 163714 | lr 7.81551e-05 | gnorm 0.958 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 1128
2024-01-31 05:02:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:02:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:02:29 | INFO | fairseq.trainer | begin training epoch 112
2024-01-31 05:02:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:02:35 | INFO | train_inner | epoch 112:     86 / 1475 loss=2.257, nll_loss=0.608, ppl=1.52, wps=33280.6, ups=8.74, wpb=3808.2, bsz=195.6, num_updates=163800, lr=7.81345e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.7, wall=1134
2024-01-31 05:02:41 | INFO | train_inner | epoch 112:    186 / 1475 loss=2.249, nll_loss=0.599, ppl=1.51, wps=57844.3, ups=15.4, wpb=3756.6, bsz=197.7, num_updates=163900, lr=7.81107e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.7, wall=1140
2024-01-31 05:02:48 | INFO | train_inner | epoch 112:    286 / 1475 loss=2.257, nll_loss=0.609, ppl=1.53, wps=57577, ups=15.41, wpb=3736.9, bsz=195.5, num_updates=164000, lr=7.80869e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.7, wall=1147
2024-01-31 05:02:54 | INFO | train_inner | epoch 112:    386 / 1475 loss=2.266, nll_loss=0.618, ppl=1.54, wps=58877.5, ups=15.42, wpb=3817.4, bsz=192.4, num_updates=164100, lr=7.80631e-05, gnorm=0.956, loss_scale=8, train_wall=6, gb_free=29.8, wall=1153
2024-01-31 05:03:01 | INFO | train_inner | epoch 112:    486 / 1475 loss=2.26, nll_loss=0.612, ppl=1.53, wps=58359.1, ups=15.38, wpb=3793.6, bsz=202.2, num_updates=164200, lr=7.80393e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.7, wall=1160
2024-01-31 05:03:07 | INFO | train_inner | epoch 112:    586 / 1475 loss=2.268, nll_loss=0.621, ppl=1.54, wps=57891.4, ups=15.45, wpb=3747.1, bsz=194.6, num_updates=164300, lr=7.80156e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.7, wall=1166
2024-01-31 05:03:14 | INFO | train_inner | epoch 112:    686 / 1475 loss=2.277, nll_loss=0.632, ppl=1.55, wps=57669.4, ups=15.48, wpb=3724.6, bsz=185.1, num_updates=164400, lr=7.79918e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.9, wall=1173
2024-01-31 05:03:20 | INFO | train_inner | epoch 112:    786 / 1475 loss=2.276, nll_loss=0.63, ppl=1.55, wps=58118.6, ups=15.32, wpb=3793.9, bsz=188.4, num_updates=164500, lr=7.79681e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.7, wall=1179
2024-01-31 05:03:27 | INFO | train_inner | epoch 112:    886 / 1475 loss=2.263, nll_loss=0.616, ppl=1.53, wps=58428.5, ups=15.44, wpb=3784.1, bsz=197.5, num_updates=164600, lr=7.79444e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.6, wall=1186
2024-01-31 05:03:33 | INFO | train_inner | epoch 112:    986 / 1475 loss=2.279, nll_loss=0.634, ppl=1.55, wps=58849.1, ups=15.44, wpb=3811.2, bsz=189.8, num_updates=164700, lr=7.79208e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.8, wall=1192
2024-01-31 05:03:40 | INFO | train_inner | epoch 112:   1086 / 1475 loss=2.268, nll_loss=0.621, ppl=1.54, wps=57890.2, ups=15.46, wpb=3745.7, bsz=197.8, num_updates=164800, lr=7.78971e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.6, wall=1199
2024-01-31 05:03:46 | INFO | train_inner | epoch 112:   1186 / 1475 loss=2.253, nll_loss=0.605, ppl=1.52, wps=57553.5, ups=15.4, wpb=3736.8, bsz=211, num_updates=164900, lr=7.78735e-05, gnorm=0.927, loss_scale=8, train_wall=6, gb_free=29.7, wall=1205
2024-01-31 05:03:53 | INFO | train_inner | epoch 112:   1286 / 1475 loss=2.28, nll_loss=0.635, ppl=1.55, wps=58706.7, ups=15.4, wpb=3811.9, bsz=195.8, num_updates=165000, lr=7.78499e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=1212
2024-01-31 05:03:59 | INFO | train_inner | epoch 112:   1386 / 1475 loss=2.27, nll_loss=0.624, ppl=1.54, wps=58072.6, ups=15.42, wpb=3766.6, bsz=201.6, num_updates=165100, lr=7.78263e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=1218
2024-01-31 05:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:04:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:04:07 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 3.499 | nll_loss 1.891 | ppl 3.71 | wps 134539 | wpb 3189.1 | bsz 163.4 | num_updates 165189 | best_loss 3.385
2024-01-31 05:04:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 165189 updates
2024-01-31 05:04:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint112.pt
2024-01-31 05:04:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint112.pt
2024-01-31 05:04:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint112.pt (epoch 112 @ 165189 updates, score 3.499) (writing took 2.492865300970152 seconds)
2024-01-31 05:04:10 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2024-01-31 05:04:10 | INFO | train | epoch 112 | loss 2.267 | nll_loss 0.62 | ppl 1.54 | wps 55296.3 | ups 14.66 | wpb 3771.6 | bsz 195.9 | num_updates 165189 | lr 7.78053e-05 | gnorm 0.962 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 1229
2024-01-31 05:04:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:04:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:04:10 | INFO | fairseq.trainer | begin training epoch 113
2024-01-31 05:04:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:04:11 | INFO | train_inner | epoch 113:     11 / 1475 loss=2.276, nll_loss=0.629, ppl=1.55, wps=32994, ups=8.77, wpb=3763, bsz=195.5, num_updates=165200, lr=7.78028e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=1230
2024-01-31 05:04:17 | INFO | train_inner | epoch 113:    111 / 1475 loss=2.253, nll_loss=0.603, ppl=1.52, wps=58075.2, ups=15.4, wpb=3770.1, bsz=191, num_updates=165300, lr=7.77792e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.9, wall=1236
2024-01-31 05:04:24 | INFO | train_inner | epoch 113:    211 / 1475 loss=2.251, nll_loss=0.601, ppl=1.52, wps=58184.8, ups=15.4, wpb=3779.2, bsz=188.6, num_updates=165400, lr=7.77557e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.9, wall=1243
2024-01-31 05:04:30 | INFO | train_inner | epoch 113:    311 / 1475 loss=2.265, nll_loss=0.617, ppl=1.53, wps=58691.9, ups=15.55, wpb=3775.5, bsz=188.9, num_updates=165500, lr=7.77322e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.8, wall=1249
2024-01-31 05:04:37 | INFO | train_inner | epoch 113:    411 / 1475 loss=2.244, nll_loss=0.594, ppl=1.51, wps=58898.5, ups=15.37, wpb=3832.7, bsz=209.4, num_updates=165600, lr=7.77087e-05, gnorm=0.921, loss_scale=8, train_wall=6, gb_free=29.7, wall=1256
2024-01-31 05:04:43 | INFO | train_inner | epoch 113:    511 / 1475 loss=2.265, nll_loss=0.617, ppl=1.53, wps=58599.1, ups=15.47, wpb=3788.6, bsz=196, num_updates=165700, lr=7.76853e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.7, wall=1262
2024-01-31 05:04:50 | INFO | train_inner | epoch 113:    611 / 1475 loss=2.26, nll_loss=0.613, ppl=1.53, wps=57543.1, ups=15.38, wpb=3741, bsz=201.1, num_updates=165800, lr=7.76619e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=1269
2024-01-31 05:04:56 | INFO | train_inner | epoch 113:    711 / 1475 loss=2.266, nll_loss=0.62, ppl=1.54, wps=58018.7, ups=15.42, wpb=3763.5, bsz=191, num_updates=165900, lr=7.76384e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.8, wall=1275
2024-01-31 05:05:03 | INFO | train_inner | epoch 113:    811 / 1475 loss=2.249, nll_loss=0.601, ppl=1.52, wps=58046.3, ups=15.38, wpb=3775.1, bsz=212.6, num_updates=166000, lr=7.76151e-05, gnorm=0.925, loss_scale=8, train_wall=6, gb_free=29.7, wall=1282
2024-01-31 05:05:09 | INFO | train_inner | epoch 113:    911 / 1475 loss=2.269, nll_loss=0.622, ppl=1.54, wps=58072.4, ups=15.37, wpb=3778.9, bsz=195.7, num_updates=166100, lr=7.75917e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=1288
2024-01-31 05:05:16 | INFO | train_inner | epoch 113:   1011 / 1475 loss=2.269, nll_loss=0.623, ppl=1.54, wps=58154.6, ups=15.42, wpb=3770.9, bsz=196.6, num_updates=166200, lr=7.75683e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.6, wall=1294
2024-01-31 05:05:22 | INFO | train_inner | epoch 113:   1111 / 1475 loss=2.27, nll_loss=0.623, ppl=1.54, wps=58064.2, ups=15.34, wpb=3785.9, bsz=198.2, num_updates=166300, lr=7.7545e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.6, wall=1301
2024-01-31 05:05:29 | INFO | train_inner | epoch 113:   1211 / 1475 loss=2.274, nll_loss=0.629, ppl=1.55, wps=58132.8, ups=15.44, wpb=3764.9, bsz=196.3, num_updates=166400, lr=7.75217e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.8, wall=1307
2024-01-31 05:05:35 | INFO | train_inner | epoch 113:   1311 / 1475 loss=2.286, nll_loss=0.641, ppl=1.56, wps=59065.3, ups=15.55, wpb=3799.4, bsz=193, num_updates=166500, lr=7.74984e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=1314
2024-01-31 05:05:41 | INFO | train_inner | epoch 113:   1411 / 1475 loss=2.282, nll_loss=0.637, ppl=1.55, wps=57631.7, ups=15.44, wpb=3731.8, bsz=186.1, num_updates=166600, lr=7.74752e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.7, wall=1320
2024-01-31 05:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:05:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:05:48 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 3.507 | nll_loss 1.9 | ppl 3.73 | wps 133470 | wpb 3189.1 | bsz 163.4 | num_updates 166664 | best_loss 3.385
2024-01-31 05:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 166664 updates
2024-01-31 05:05:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint113.pt
2024-01-31 05:05:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint113.pt
2024-01-31 05:05:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint113.pt (epoch 113 @ 166664 updates, score 3.507) (writing took 2.500255085993558 seconds)
2024-01-31 05:05:50 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2024-01-31 05:05:50 | INFO | train | epoch 113 | loss 2.264 | nll_loss 0.617 | ppl 1.53 | wps 55287.8 | ups 14.66 | wpb 3771.6 | bsz 195.9 | num_updates 166664 | lr 7.74603e-05 | gnorm 0.958 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 1329
2024-01-31 05:05:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:05:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:05:51 | INFO | fairseq.trainer | begin training epoch 114
2024-01-31 05:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:05:53 | INFO | train_inner | epoch 114:     36 / 1475 loss=2.265, nll_loss=0.619, ppl=1.54, wps=31988.1, ups=8.74, wpb=3661.6, bsz=190.4, num_updates=166700, lr=7.74519e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.6, wall=1332
2024-01-31 05:05:59 | INFO | train_inner | epoch 114:    136 / 1475 loss=2.253, nll_loss=0.602, ppl=1.52, wps=57609, ups=15.54, wpb=3707.7, bsz=187.4, num_updates=166800, lr=7.74287e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=30, wall=1338
2024-01-31 05:06:06 | INFO | train_inner | epoch 114:    236 / 1475 loss=2.252, nll_loss=0.603, ppl=1.52, wps=58533.5, ups=15.38, wpb=3806.2, bsz=200.7, num_updates=166900, lr=7.74055e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.9, wall=1345
2024-01-31 05:06:12 | INFO | train_inner | epoch 114:    336 / 1475 loss=2.256, nll_loss=0.607, ppl=1.52, wps=58354.2, ups=15.51, wpb=3762.9, bsz=188.5, num_updates=167000, lr=7.73823e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=1351
2024-01-31 05:06:19 | INFO | train_inner | epoch 114:    436 / 1475 loss=2.258, nll_loss=0.61, ppl=1.53, wps=57596.3, ups=15.34, wpb=3754.4, bsz=192.8, num_updates=167100, lr=7.73592e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.7, wall=1358
2024-01-31 05:06:25 | INFO | train_inner | epoch 114:    536 / 1475 loss=2.265, nll_loss=0.617, ppl=1.53, wps=57695.1, ups=15.46, wpb=3732.7, bsz=187.7, num_updates=167200, lr=7.7336e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=1364
2024-01-31 05:06:32 | INFO | train_inner | epoch 114:    636 / 1475 loss=2.246, nll_loss=0.597, ppl=1.51, wps=58457.6, ups=15.36, wpb=3805, bsz=211.7, num_updates=167300, lr=7.73129e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.7, wall=1371
2024-01-31 05:06:38 | INFO | train_inner | epoch 114:    736 / 1475 loss=2.271, nll_loss=0.624, ppl=1.54, wps=58402.3, ups=15.42, wpb=3787.9, bsz=188.7, num_updates=167400, lr=7.72898e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=1377
2024-01-31 05:06:45 | INFO | train_inner | epoch 114:    836 / 1475 loss=2.264, nll_loss=0.617, ppl=1.53, wps=58114.9, ups=15.45, wpb=3760.7, bsz=191.8, num_updates=167500, lr=7.72667e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.7, wall=1384
2024-01-31 05:06:51 | INFO | train_inner | epoch 114:    936 / 1475 loss=2.279, nll_loss=0.634, ppl=1.55, wps=58660.1, ups=15.5, wpb=3784.6, bsz=181.8, num_updates=167600, lr=7.72437e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=1390
2024-01-31 05:06:58 | INFO | train_inner | epoch 114:   1036 / 1475 loss=2.263, nll_loss=0.616, ppl=1.53, wps=58061.4, ups=15.4, wpb=3769.5, bsz=197, num_updates=167700, lr=7.72207e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.8, wall=1397
2024-01-31 05:07:04 | INFO | train_inner | epoch 114:   1136 / 1475 loss=2.267, nll_loss=0.62, ppl=1.54, wps=57754, ups=15.39, wpb=3752.6, bsz=197.5, num_updates=167800, lr=7.71976e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.9, wall=1403
2024-01-31 05:07:11 | INFO | train_inner | epoch 114:   1236 / 1475 loss=2.264, nll_loss=0.617, ppl=1.53, wps=58148.3, ups=15.39, wpb=3779.1, bsz=210.7, num_updates=167900, lr=7.71746e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.6, wall=1410
2024-01-31 05:07:17 | INFO | train_inner | epoch 114:   1336 / 1475 loss=2.276, nll_loss=0.631, ppl=1.55, wps=58329.3, ups=15.37, wpb=3794.5, bsz=204.6, num_updates=168000, lr=7.71517e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.7, wall=1416
2024-01-31 05:07:24 | INFO | train_inner | epoch 114:   1436 / 1475 loss=2.27, nll_loss=0.625, ppl=1.54, wps=58350, ups=15.34, wpb=3804.2, bsz=198.5, num_updates=168100, lr=7.71287e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.8, wall=1423
2024-01-31 05:07:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-01-31 05:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:07:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:07:29 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 3.5 | nll_loss 1.894 | ppl 3.72 | wps 134238 | wpb 3189.1 | bsz 163.4 | num_updates 168138 | best_loss 3.385
2024-01-31 05:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 168138 updates
2024-01-31 05:07:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint114.pt
2024-01-31 05:07:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint114.pt
2024-01-31 05:07:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint114.pt (epoch 114 @ 168138 updates, score 3.5) (writing took 2.449500374030322 seconds)
2024-01-31 05:07:31 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2024-01-31 05:07:31 | INFO | train | epoch 114 | loss 2.263 | nll_loss 0.615 | ppl 1.53 | wps 55268.4 | ups 14.65 | wpb 3771.6 | bsz 195.9 | num_updates 168138 | lr 7.712e-05 | gnorm 0.964 | loss_scale 4 | train_wall 92 | gb_free 29.8 | wall 1430
2024-01-31 05:07:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:07:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:07:31 | INFO | fairseq.trainer | begin training epoch 115
2024-01-31 05:07:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:07:35 | INFO | train_inner | epoch 115:     62 / 1475 loss=2.234, nll_loss=0.584, ppl=1.5, wps=33572.2, ups=8.69, wpb=3862.9, bsz=217.3, num_updates=168200, lr=7.71058e-05, gnorm=0.912, loss_scale=4, train_wall=6, gb_free=29.7, wall=1434
2024-01-31 05:07:42 | INFO | train_inner | epoch 115:    162 / 1475 loss=2.25, nll_loss=0.6, ppl=1.52, wps=58277.7, ups=15.49, wpb=3761.3, bsz=187.9, num_updates=168300, lr=7.70829e-05, gnorm=0.969, loss_scale=4, train_wall=6, gb_free=29.7, wall=1441
2024-01-31 05:07:48 | INFO | train_inner | epoch 115:    262 / 1475 loss=2.233, nll_loss=0.582, ppl=1.5, wps=56741.3, ups=15.37, wpb=3692.5, bsz=207.8, num_updates=168400, lr=7.706e-05, gnorm=0.928, loss_scale=4, train_wall=6, gb_free=29.7, wall=1447
2024-01-31 05:07:55 | INFO | train_inner | epoch 115:    362 / 1475 loss=2.252, nll_loss=0.604, ppl=1.52, wps=58314.8, ups=15.41, wpb=3784.3, bsz=196.6, num_updates=168500, lr=7.70371e-05, gnorm=0.961, loss_scale=4, train_wall=6, gb_free=29.7, wall=1454
2024-01-31 05:08:01 | INFO | train_inner | epoch 115:    462 / 1475 loss=2.264, nll_loss=0.615, ppl=1.53, wps=59744.9, ups=15.51, wpb=3852.6, bsz=187.4, num_updates=168600, lr=7.70143e-05, gnorm=0.941, loss_scale=4, train_wall=6, gb_free=29.7, wall=1460
2024-01-31 05:08:08 | INFO | train_inner | epoch 115:    562 / 1475 loss=2.261, nll_loss=0.613, ppl=1.53, wps=57811.4, ups=15.44, wpb=3745.1, bsz=197.9, num_updates=168700, lr=7.69914e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=1467
2024-01-31 05:08:14 | INFO | train_inner | epoch 115:    662 / 1475 loss=2.265, nll_loss=0.617, ppl=1.53, wps=57874.5, ups=15.51, wpb=3732.2, bsz=183.9, num_updates=168800, lr=7.69686e-05, gnorm=0.963, loss_scale=4, train_wall=6, gb_free=29.8, wall=1473
2024-01-31 05:08:21 | INFO | train_inner | epoch 115:    762 / 1475 loss=2.264, nll_loss=0.617, ppl=1.53, wps=58035.9, ups=15.37, wpb=3775.6, bsz=198.4, num_updates=168900, lr=7.69458e-05, gnorm=0.977, loss_scale=4, train_wall=6, gb_free=29.7, wall=1479
2024-01-31 05:08:27 | INFO | train_inner | epoch 115:    862 / 1475 loss=2.262, nll_loss=0.616, ppl=1.53, wps=58924, ups=15.47, wpb=3809.5, bsz=198.3, num_updates=169000, lr=7.69231e-05, gnorm=0.965, loss_scale=4, train_wall=6, gb_free=29.7, wall=1486
2024-01-31 05:08:34 | INFO | train_inner | epoch 115:    962 / 1475 loss=2.278, nll_loss=0.633, ppl=1.55, wps=58277, ups=15.47, wpb=3766.4, bsz=184.3, num_updates=169100, lr=7.69003e-05, gnorm=1.012, loss_scale=4, train_wall=6, gb_free=29.7, wall=1492
2024-01-31 05:08:40 | INFO | train_inner | epoch 115:   1062 / 1475 loss=2.263, nll_loss=0.616, ppl=1.53, wps=57978.4, ups=15.57, wpb=3724.2, bsz=201, num_updates=169200, lr=7.68776e-05, gnorm=0.986, loss_scale=4, train_wall=6, gb_free=29.8, wall=1499
2024-01-31 05:08:46 | INFO | train_inner | epoch 115:   1162 / 1475 loss=2.281, nll_loss=0.635, ppl=1.55, wps=58317.1, ups=15.54, wpb=3753.6, bsz=185.5, num_updates=169300, lr=7.68549e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=1505
2024-01-31 05:08:53 | INFO | train_inner | epoch 115:   1262 / 1475 loss=2.256, nll_loss=0.609, ppl=1.53, wps=58799, ups=15.45, wpb=3805.8, bsz=201, num_updates=169400, lr=7.68322e-05, gnorm=0.955, loss_scale=4, train_wall=6, gb_free=29.7, wall=1512
2024-01-31 05:08:59 | INFO | train_inner | epoch 115:   1362 / 1475 loss=2.278, nll_loss=0.633, ppl=1.55, wps=58918.6, ups=15.51, wpb=3798.4, bsz=197.4, num_updates=169500, lr=7.68095e-05, gnorm=0.968, loss_scale=4, train_wall=6, gb_free=29.7, wall=1518
2024-01-31 05:09:06 | INFO | train_inner | epoch 115:   1462 / 1475 loss=2.262, nll_loss=0.616, ppl=1.53, wps=58272.2, ups=15.38, wpb=3787.7, bsz=204.3, num_updates=169600, lr=7.67869e-05, gnorm=0.977, loss_scale=4, train_wall=6, gb_free=29.7, wall=1525
2024-01-31 05:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:09:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:09:09 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 3.507 | nll_loss 1.899 | ppl 3.73 | wps 134112 | wpb 3189.1 | bsz 163.4 | num_updates 169613 | best_loss 3.385
2024-01-31 05:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 169613 updates
2024-01-31 05:09:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint115.pt
2024-01-31 05:09:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint115.pt
2024-01-31 05:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint115.pt (epoch 115 @ 169613 updates, score 3.507) (writing took 2.4548551249317825 seconds)
2024-01-31 05:09:11 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2024-01-31 05:09:11 | INFO | train | epoch 115 | loss 2.26 | nll_loss 0.613 | ppl 1.53 | wps 55437.2 | ups 14.7 | wpb 3771.6 | bsz 195.9 | num_updates 169613 | lr 7.67839e-05 | gnorm 0.966 | loss_scale 4 | train_wall 92 | gb_free 29.7 | wall 1530
2024-01-31 05:09:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:09:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:09:11 | INFO | fairseq.trainer | begin training epoch 116
2024-01-31 05:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:09:17 | INFO | train_inner | epoch 116:     87 / 1475 loss=2.249, nll_loss=0.599, ppl=1.51, wps=33400.7, ups=8.78, wpb=3802.8, bsz=188.2, num_updates=169700, lr=7.67643e-05, gnorm=0.968, loss_scale=4, train_wall=6, gb_free=29.7, wall=1536
2024-01-31 05:09:24 | INFO | train_inner | epoch 116:    187 / 1475 loss=2.242, nll_loss=0.592, ppl=1.51, wps=56940.5, ups=15.39, wpb=3700.5, bsz=192.6, num_updates=169800, lr=7.67417e-05, gnorm=0.967, loss_scale=4, train_wall=6, gb_free=29.7, wall=1543
2024-01-31 05:09:30 | INFO | train_inner | epoch 116:    287 / 1475 loss=2.245, nll_loss=0.595, ppl=1.51, wps=58969.6, ups=15.45, wpb=3816.1, bsz=201.6, num_updates=169900, lr=7.67191e-05, gnorm=0.979, loss_scale=4, train_wall=6, gb_free=29.7, wall=1549
2024-01-31 05:09:37 | INFO | train_inner | epoch 116:    387 / 1475 loss=2.246, nll_loss=0.597, ppl=1.51, wps=58658.2, ups=15.46, wpb=3793.8, bsz=196.1, num_updates=170000, lr=7.66965e-05, gnorm=0.97, loss_scale=4, train_wall=6, gb_free=29.6, wall=1556
2024-01-31 05:09:43 | INFO | train_inner | epoch 116:    487 / 1475 loss=2.252, nll_loss=0.603, ppl=1.52, wps=57920.2, ups=15.48, wpb=3741.3, bsz=195.5, num_updates=170100, lr=7.6674e-05, gnorm=0.972, loss_scale=4, train_wall=6, gb_free=29.7, wall=1562
2024-01-31 05:09:50 | INFO | train_inner | epoch 116:    587 / 1475 loss=2.26, nll_loss=0.611, ppl=1.53, wps=57743.4, ups=15.45, wpb=3736.8, bsz=188.2, num_updates=170200, lr=7.66514e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=1568
2024-01-31 05:09:56 | INFO | train_inner | epoch 116:    687 / 1475 loss=2.253, nll_loss=0.605, ppl=1.52, wps=58828.1, ups=15.43, wpb=3812.8, bsz=201.3, num_updates=170300, lr=7.66289e-05, gnorm=0.984, loss_scale=4, train_wall=6, gb_free=29.7, wall=1575
2024-01-31 05:10:03 | INFO | train_inner | epoch 116:    787 / 1475 loss=2.255, nll_loss=0.608, ppl=1.52, wps=58119, ups=15.42, wpb=3768.4, bsz=202.8, num_updates=170400, lr=7.66064e-05, gnorm=0.954, loss_scale=4, train_wall=6, gb_free=29.7, wall=1581
2024-01-31 05:10:09 | INFO | train_inner | epoch 116:    887 / 1475 loss=2.26, nll_loss=0.613, ppl=1.53, wps=58564.4, ups=15.38, wpb=3806.9, bsz=199, num_updates=170500, lr=7.6584e-05, gnorm=0.947, loss_scale=4, train_wall=6, gb_free=29.8, wall=1588
2024-01-31 05:10:15 | INFO | train_inner | epoch 116:    987 / 1475 loss=2.262, nll_loss=0.615, ppl=1.53, wps=57053.3, ups=15.5, wpb=3681.8, bsz=194.6, num_updates=170600, lr=7.65615e-05, gnorm=0.979, loss_scale=4, train_wall=6, gb_free=29.8, wall=1594
2024-01-31 05:10:22 | INFO | train_inner | epoch 116:   1087 / 1475 loss=2.277, nll_loss=0.632, ppl=1.55, wps=58900.2, ups=15.47, wpb=3806.7, bsz=190.9, num_updates=170700, lr=7.65391e-05, gnorm=0.974, loss_scale=4, train_wall=6, gb_free=29.7, wall=1601
2024-01-31 05:10:28 | INFO | train_inner | epoch 116:   1187 / 1475 loss=2.269, nll_loss=0.624, ppl=1.54, wps=58000, ups=15.52, wpb=3737.9, bsz=196.5, num_updates=170800, lr=7.65167e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=1607
2024-01-31 05:10:35 | INFO | train_inner | epoch 116:   1287 / 1475 loss=2.28, nll_loss=0.635, ppl=1.55, wps=58351.1, ups=15.6, wpb=3740, bsz=186.8, num_updates=170900, lr=7.64943e-05, gnorm=0.993, loss_scale=4, train_wall=6, gb_free=29.7, wall=1614
2024-01-31 05:10:41 | INFO | train_inner | epoch 116:   1387 / 1475 loss=2.255, nll_loss=0.608, ppl=1.52, wps=58519.7, ups=15.41, wpb=3798.7, bsz=210.9, num_updates=171000, lr=7.64719e-05, gnorm=0.954, loss_scale=4, train_wall=6, gb_free=29.7, wall=1620
2024-01-31 05:10:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:10:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:10:49 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 3.5 | nll_loss 1.892 | ppl 3.71 | wps 132452 | wpb 3189.1 | bsz 163.4 | num_updates 171088 | best_loss 3.385
2024-01-31 05:10:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 171088 updates
2024-01-31 05:10:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint116.pt
2024-01-31 05:10:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint116.pt
2024-01-31 05:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint116.pt (epoch 116 @ 171088 updates, score 3.5) (writing took 2.616559628979303 seconds)
2024-01-31 05:10:52 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2024-01-31 05:10:52 | INFO | train | epoch 116 | loss 2.259 | nll_loss 0.612 | ppl 1.53 | wps 55359.8 | ups 14.68 | wpb 3771.6 | bsz 195.9 | num_updates 171088 | lr 7.64522e-05 | gnorm 0.971 | loss_scale 4 | train_wall 92 | gb_free 29.7 | wall 1631
2024-01-31 05:10:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:10:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:10:52 | INFO | fairseq.trainer | begin training epoch 117
2024-01-31 05:10:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:10:53 | INFO | train_inner | epoch 117:     12 / 1475 loss=2.285, nll_loss=0.639, ppl=1.56, wps=33006, ups=8.68, wpb=3803.6, bsz=187.9, num_updates=171100, lr=7.64496e-05, gnorm=0.983, loss_scale=4, train_wall=6, gb_free=29.7, wall=1632
2024-01-31 05:10:59 | INFO | train_inner | epoch 117:    112 / 1475 loss=2.233, nll_loss=0.582, ppl=1.5, wps=58629.2, ups=15.47, wpb=3789.8, bsz=203.2, num_updates=171200, lr=7.64272e-05, gnorm=0.95, loss_scale=4, train_wall=6, gb_free=29.7, wall=1638
2024-01-31 05:11:06 | INFO | train_inner | epoch 117:    212 / 1475 loss=2.241, nll_loss=0.589, ppl=1.5, wps=58277.2, ups=15.54, wpb=3750.2, bsz=199.9, num_updates=171300, lr=7.64049e-05, gnorm=0.929, loss_scale=4, train_wall=6, gb_free=29.8, wall=1645
2024-01-31 05:11:12 | INFO | train_inner | epoch 117:    312 / 1475 loss=2.237, nll_loss=0.586, ppl=1.5, wps=58306.7, ups=15.49, wpb=3764.6, bsz=200.9, num_updates=171400, lr=7.63826e-05, gnorm=0.926, loss_scale=4, train_wall=6, gb_free=29.8, wall=1651
2024-01-31 05:11:19 | INFO | train_inner | epoch 117:    412 / 1475 loss=2.239, nll_loss=0.588, ppl=1.5, wps=57427.3, ups=15.39, wpb=3731, bsz=199.9, num_updates=171500, lr=7.63604e-05, gnorm=0.942, loss_scale=4, train_wall=6, gb_free=29.7, wall=1658
2024-01-31 05:11:25 | INFO | train_inner | epoch 117:    512 / 1475 loss=2.258, nll_loss=0.61, ppl=1.53, wps=59079.4, ups=15.57, wpb=3795.2, bsz=187.5, num_updates=171600, lr=7.63381e-05, gnorm=0.969, loss_scale=4, train_wall=6, gb_free=29.7, wall=1664
2024-01-31 05:11:32 | INFO | train_inner | epoch 117:    612 / 1475 loss=2.263, nll_loss=0.616, ppl=1.53, wps=58775.2, ups=15.58, wpb=3772.1, bsz=188.5, num_updates=171700, lr=7.63159e-05, gnorm=0.957, loss_scale=4, train_wall=6, gb_free=29.7, wall=1670
2024-01-31 05:11:38 | INFO | train_inner | epoch 117:    712 / 1475 loss=2.254, nll_loss=0.606, ppl=1.52, wps=58581.6, ups=15.46, wpb=3788.9, bsz=198.7, num_updates=171800, lr=7.62937e-05, gnorm=0.986, loss_scale=4, train_wall=6, gb_free=29.7, wall=1677
2024-01-31 05:11:44 | INFO | train_inner | epoch 117:    812 / 1475 loss=2.262, nll_loss=0.615, ppl=1.53, wps=58637.2, ups=15.5, wpb=3782.2, bsz=197.3, num_updates=171900, lr=7.62715e-05, gnorm=0.974, loss_scale=4, train_wall=6, gb_free=29.7, wall=1683
2024-01-31 05:11:51 | INFO | train_inner | epoch 117:    912 / 1475 loss=2.262, nll_loss=0.615, ppl=1.53, wps=58784.5, ups=15.55, wpb=3781.1, bsz=186.3, num_updates=172000, lr=7.62493e-05, gnorm=0.99, loss_scale=4, train_wall=6, gb_free=29.8, wall=1690
2024-01-31 05:11:57 | INFO | train_inner | epoch 117:   1012 / 1475 loss=2.259, nll_loss=0.613, ppl=1.53, wps=59104.3, ups=15.55, wpb=3800.8, bsz=200.1, num_updates=172100, lr=7.62271e-05, gnorm=0.976, loss_scale=4, train_wall=6, gb_free=29.7, wall=1696
2024-01-31 05:12:04 | INFO | train_inner | epoch 117:   1112 / 1475 loss=2.273, nll_loss=0.627, ppl=1.54, wps=58170.8, ups=15.59, wpb=3730.8, bsz=190.8, num_updates=172200, lr=7.6205e-05, gnorm=0.998, loss_scale=4, train_wall=6, gb_free=29.9, wall=1703
2024-01-31 05:12:10 | INFO | train_inner | epoch 117:   1212 / 1475 loss=2.266, nll_loss=0.62, ppl=1.54, wps=58799.8, ups=15.53, wpb=3786.5, bsz=193.4, num_updates=172300, lr=7.61829e-05, gnorm=0.97, loss_scale=4, train_wall=6, gb_free=29.8, wall=1709
2024-01-31 05:12:17 | INFO | train_inner | epoch 117:   1312 / 1475 loss=2.261, nll_loss=0.616, ppl=1.53, wps=59190.4, ups=15.52, wpb=3813.4, bsz=202.8, num_updates=172400, lr=7.61608e-05, gnorm=0.963, loss_scale=4, train_wall=6, gb_free=29.7, wall=1715
2024-01-31 05:12:23 | INFO | train_inner | epoch 117:   1412 / 1475 loss=2.275, nll_loss=0.629, ppl=1.55, wps=58774.2, ups=15.65, wpb=3755.4, bsz=192.8, num_updates=172500, lr=7.61387e-05, gnorm=0.992, loss_scale=4, train_wall=6, gb_free=29.7, wall=1722
2024-01-31 05:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:12:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:12:29 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 3.503 | nll_loss 1.896 | ppl 3.72 | wps 129567 | wpb 3189.1 | bsz 163.4 | num_updates 172563 | best_loss 3.385
2024-01-31 05:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 172563 updates
2024-01-31 05:12:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint117.pt
2024-01-31 05:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint117.pt
2024-01-31 05:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint117.pt (epoch 117 @ 172563 updates, score 3.503) (writing took 2.4763039940735325 seconds)
2024-01-31 05:12:32 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2024-01-31 05:12:32 | INFO | train | epoch 117 | loss 2.257 | nll_loss 0.609 | ppl 1.52 | wps 55627.8 | ups 14.75 | wpb 3771.6 | bsz 195.9 | num_updates 172563 | lr 7.61248e-05 | gnorm 0.966 | loss_scale 4 | train_wall 92 | gb_free 29.9 | wall 1731
2024-01-31 05:12:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:12:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:12:32 | INFO | fairseq.trainer | begin training epoch 118
2024-01-31 05:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:12:34 | INFO | train_inner | epoch 118:     37 / 1475 loss=2.264, nll_loss=0.616, ppl=1.53, wps=32061.6, ups=8.75, wpb=3663.6, bsz=195, num_updates=172600, lr=7.61166e-05, gnorm=0.974, loss_scale=4, train_wall=6, gb_free=29.8, wall=1733
2024-01-31 05:12:41 | INFO | train_inner | epoch 118:    137 / 1475 loss=2.231, nll_loss=0.58, ppl=1.5, wps=59558.6, ups=15.48, wpb=3846.8, bsz=203.9, num_updates=172700, lr=7.60946e-05, gnorm=0.911, loss_scale=4, train_wall=6, gb_free=29.7, wall=1740
2024-01-31 05:12:47 | INFO | train_inner | epoch 118:    237 / 1475 loss=2.253, nll_loss=0.604, ppl=1.52, wps=59358.6, ups=15.59, wpb=3808.6, bsz=191.3, num_updates=172800, lr=7.60726e-05, gnorm=0.972, loss_scale=4, train_wall=6, gb_free=29.7, wall=1746
2024-01-31 05:12:54 | INFO | train_inner | epoch 118:    337 / 1475 loss=2.236, nll_loss=0.584, ppl=1.5, wps=58623, ups=15.58, wpb=3762.8, bsz=206.6, num_updates=172900, lr=7.60506e-05, gnorm=0.944, loss_scale=4, train_wall=6, gb_free=29.8, wall=1753
2024-01-31 05:13:00 | INFO | train_inner | epoch 118:    437 / 1475 loss=2.248, nll_loss=0.599, ppl=1.51, wps=59299.7, ups=15.51, wpb=3822.4, bsz=196.9, num_updates=173000, lr=7.60286e-05, gnorm=0.937, loss_scale=4, train_wall=6, gb_free=29.8, wall=1759
2024-01-31 05:13:07 | INFO | train_inner | epoch 118:    537 / 1475 loss=2.258, nll_loss=0.609, ppl=1.53, wps=58868, ups=15.65, wpb=3760.9, bsz=186.7, num_updates=173100, lr=7.60066e-05, gnorm=0.973, loss_scale=4, train_wall=6, gb_free=29.7, wall=1765
2024-01-31 05:13:13 | INFO | train_inner | epoch 118:    637 / 1475 loss=2.254, nll_loss=0.605, ppl=1.52, wps=59473.7, ups=15.63, wpb=3804.1, bsz=195.8, num_updates=173200, lr=7.59847e-05, gnorm=0.959, loss_scale=4, train_wall=6, gb_free=29.7, wall=1772
2024-01-31 05:13:19 | INFO | train_inner | epoch 118:    737 / 1475 loss=2.257, nll_loss=0.609, ppl=1.53, wps=59504.9, ups=15.56, wpb=3824, bsz=189.2, num_updates=173300, lr=7.59628e-05, gnorm=0.96, loss_scale=4, train_wall=6, gb_free=29.7, wall=1778
2024-01-31 05:13:26 | INFO | train_inner | epoch 118:    837 / 1475 loss=2.252, nll_loss=0.604, ppl=1.52, wps=58117.3, ups=15.46, wpb=3760.3, bsz=201.7, num_updates=173400, lr=7.59408e-05, gnorm=0.999, loss_scale=4, train_wall=6, gb_free=29.8, wall=1785
2024-01-31 05:13:32 | INFO | train_inner | epoch 118:    937 / 1475 loss=2.258, nll_loss=0.611, ppl=1.53, wps=58664.1, ups=15.56, wpb=3770.6, bsz=198.6, num_updates=173500, lr=7.5919e-05, gnorm=0.984, loss_scale=4, train_wall=6, gb_free=29.7, wall=1791
2024-01-31 05:13:39 | INFO | train_inner | epoch 118:   1037 / 1475 loss=2.272, nll_loss=0.627, ppl=1.54, wps=59542.9, ups=15.5, wpb=3841.3, bsz=193.8, num_updates=173600, lr=7.58971e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=1798
2024-01-31 05:13:45 | INFO | train_inner | epoch 118:   1137 / 1475 loss=2.261, nll_loss=0.614, ppl=1.53, wps=58248.7, ups=15.58, wpb=3738.7, bsz=191.4, num_updates=173700, lr=7.58752e-05, gnorm=0.981, loss_scale=4, train_wall=6, gb_free=29.7, wall=1804
2024-01-31 05:13:52 | INFO | train_inner | epoch 118:   1237 / 1475 loss=2.257, nll_loss=0.61, ppl=1.53, wps=57262.1, ups=15.62, wpb=3666.1, bsz=197.3, num_updates=173800, lr=7.58534e-05, gnorm=0.996, loss_scale=4, train_wall=6, gb_free=29.8, wall=1810
2024-01-31 05:13:58 | INFO | train_inner | epoch 118:   1337 / 1475 loss=2.264, nll_loss=0.618, ppl=1.53, wps=58261.1, ups=15.65, wpb=3723.5, bsz=194.7, num_updates=173900, lr=7.58316e-05, gnorm=0.998, loss_scale=4, train_wall=6, gb_free=29.7, wall=1817
2024-01-31 05:14:04 | INFO | train_inner | epoch 118:   1437 / 1475 loss=2.262, nll_loss=0.616, ppl=1.53, wps=58969.2, ups=15.5, wpb=3804.3, bsz=201, num_updates=174000, lr=7.58098e-05, gnorm=0.964, loss_scale=4, train_wall=6, gb_free=29.6, wall=1823
2024-01-31 05:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:14:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:14:09 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 3.509 | nll_loss 1.902 | ppl 3.74 | wps 135534 | wpb 3189.1 | bsz 163.4 | num_updates 174038 | best_loss 3.385
2024-01-31 05:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 174038 updates
2024-01-31 05:14:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint118.pt
2024-01-31 05:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint118.pt
2024-01-31 05:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint118.pt (epoch 118 @ 174038 updates, score 3.509) (writing took 2.416628191014752 seconds)
2024-01-31 05:14:12 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2024-01-31 05:14:12 | INFO | train | epoch 118 | loss 2.255 | nll_loss 0.607 | ppl 1.52 | wps 55848.8 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 174038 | lr 7.58015e-05 | gnorm 0.97 | loss_scale 4 | train_wall 91 | gb_free 29.7 | wall 1830
2024-01-31 05:14:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:14:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:14:12 | INFO | fairseq.trainer | begin training epoch 119
2024-01-31 05:14:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:14:16 | INFO | train_inner | epoch 119:     62 / 1475 loss=2.259, nll_loss=0.61, ppl=1.53, wps=33318.1, ups=8.89, wpb=3747.7, bsz=189.8, num_updates=174100, lr=7.5788e-05, gnorm=0.981, loss_scale=4, train_wall=6, gb_free=29.8, wall=1835
2024-01-31 05:14:22 | INFO | train_inner | epoch 119:    162 / 1475 loss=2.245, nll_loss=0.595, ppl=1.51, wps=58550.3, ups=15.5, wpb=3778, bsz=188.2, num_updates=174200, lr=7.57663e-05, gnorm=0.939, loss_scale=4, train_wall=6, gb_free=29.7, wall=1841
2024-01-31 05:14:28 | INFO | train_inner | epoch 119:    262 / 1475 loss=2.24, nll_loss=0.589, ppl=1.5, wps=58761.7, ups=15.58, wpb=3771.6, bsz=200.4, num_updates=174300, lr=7.57445e-05, gnorm=0.953, loss_scale=4, train_wall=6, gb_free=29.7, wall=1847
2024-01-31 05:14:35 | INFO | train_inner | epoch 119:    362 / 1475 loss=2.243, nll_loss=0.593, ppl=1.51, wps=58335.4, ups=15.61, wpb=3737.7, bsz=198.9, num_updates=174400, lr=7.57228e-05, gnorm=0.963, loss_scale=4, train_wall=6, gb_free=29.8, wall=1854
2024-01-31 05:14:41 | INFO | train_inner | epoch 119:    462 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=57980, ups=15.56, wpb=3725.1, bsz=195.3, num_updates=174500, lr=7.57011e-05, gnorm=0.964, loss_scale=4, train_wall=6, gb_free=29.8, wall=1860
2024-01-31 05:14:48 | INFO | train_inner | epoch 119:    562 / 1475 loss=2.249, nll_loss=0.601, ppl=1.52, wps=57137.5, ups=15.6, wpb=3662.7, bsz=188.1, num_updates=174600, lr=7.56794e-05, gnorm=0.981, loss_scale=4, train_wall=6, gb_free=29.7, wall=1867
2024-01-31 05:14:54 | INFO | train_inner | epoch 119:    662 / 1475 loss=2.266, nll_loss=0.619, ppl=1.54, wps=60195.9, ups=15.56, wpb=3869.7, bsz=186.6, num_updates=174700, lr=7.56578e-05, gnorm=0.97, loss_scale=4, train_wall=6, gb_free=29.7, wall=1873
2024-01-31 05:15:01 | INFO | train_inner | epoch 119:    762 / 1475 loss=2.253, nll_loss=0.605, ppl=1.52, wps=59572.8, ups=15.53, wpb=3836.9, bsz=202, num_updates=174800, lr=7.56361e-05, gnorm=1.009, loss_scale=4, train_wall=6, gb_free=29.7, wall=1880
2024-01-31 05:15:07 | INFO | train_inner | epoch 119:    862 / 1475 loss=2.256, nll_loss=0.608, ppl=1.52, wps=59082.1, ups=15.63, wpb=3781.2, bsz=192.6, num_updates=174900, lr=7.56145e-05, gnorm=0.979, loss_scale=4, train_wall=6, gb_free=29.7, wall=1886
2024-01-31 05:15:13 | INFO | train_inner | epoch 119:    962 / 1475 loss=2.253, nll_loss=0.605, ppl=1.52, wps=58590, ups=15.57, wpb=3763.4, bsz=197.6, num_updates=175000, lr=7.55929e-05, gnorm=0.956, loss_scale=4, train_wall=6, gb_free=29.7, wall=1892
2024-01-31 05:15:20 | INFO | train_inner | epoch 119:   1062 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=58316.2, ups=15.45, wpb=3774.1, bsz=208.8, num_updates=175100, lr=7.55713e-05, gnorm=0.921, loss_scale=4, train_wall=6, gb_free=29.7, wall=1899
2024-01-31 05:15:26 | INFO | train_inner | epoch 119:   1162 / 1475 loss=2.263, nll_loss=0.616, ppl=1.53, wps=57555.1, ups=15.59, wpb=3692.8, bsz=194.2, num_updates=175200, lr=7.55497e-05, gnorm=1.021, loss_scale=4, train_wall=6, gb_free=29.8, wall=1905
2024-01-31 05:15:33 | INFO | train_inner | epoch 119:   1262 / 1475 loss=2.267, nll_loss=0.621, ppl=1.54, wps=59021.4, ups=15.55, wpb=3795.7, bsz=197.3, num_updates=175300, lr=7.55282e-05, gnorm=0.966, loss_scale=4, train_wall=6, gb_free=29.7, wall=1912
2024-01-31 05:15:39 | INFO | train_inner | epoch 119:   1362 / 1475 loss=2.254, nll_loss=0.607, ppl=1.52, wps=59077.7, ups=15.58, wpb=3791.1, bsz=205.4, num_updates=175400, lr=7.55067e-05, gnorm=0.962, loss_scale=4, train_wall=6, gb_free=29.7, wall=1918
2024-01-31 05:15:46 | INFO | train_inner | epoch 119:   1462 / 1475 loss=2.268, nll_loss=0.623, ppl=1.54, wps=59090.6, ups=15.66, wpb=3774.3, bsz=192.2, num_updates=175500, lr=7.54851e-05, gnorm=0.983, loss_scale=4, train_wall=6, gb_free=29.8, wall=1924
2024-01-31 05:15:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:15:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:15:49 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 3.514 | nll_loss 1.908 | ppl 3.75 | wps 134940 | wpb 3189.1 | bsz 163.4 | num_updates 175513 | best_loss 3.385
2024-01-31 05:15:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 175513 updates
2024-01-31 05:15:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint119.pt
2024-01-31 05:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint119.pt
2024-01-31 05:15:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint119.pt (epoch 119 @ 175513 updates, score 3.514) (writing took 2.4800317289773375 seconds)
2024-01-31 05:15:51 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2024-01-31 05:15:51 | INFO | train | epoch 119 | loss 2.253 | nll_loss 0.605 | ppl 1.52 | wps 55814.7 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 175513 | lr 7.54823e-05 | gnorm 0.969 | loss_scale 4 | train_wall 91 | gb_free 29.8 | wall 1930
2024-01-31 05:15:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:15:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:15:51 | INFO | fairseq.trainer | begin training epoch 120
2024-01-31 05:15:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:15:57 | INFO | train_inner | epoch 120:     87 / 1475 loss=2.242, nll_loss=0.592, ppl=1.51, wps=33497.5, ups=8.79, wpb=3811.1, bsz=192.3, num_updates=175600, lr=7.54636e-05, gnorm=0.943, loss_scale=4, train_wall=6, gb_free=29.7, wall=1936
2024-01-31 05:16:03 | INFO | train_inner | epoch 120:    187 / 1475 loss=2.234, nll_loss=0.583, ppl=1.5, wps=59140.2, ups=15.61, wpb=3787.5, bsz=195.8, num_updates=175700, lr=7.54422e-05, gnorm=0.932, loss_scale=4, train_wall=6, gb_free=29.7, wall=1942
2024-01-31 05:16:10 | INFO | train_inner | epoch 120:    287 / 1475 loss=2.245, nll_loss=0.596, ppl=1.51, wps=59164.6, ups=15.57, wpb=3800.1, bsz=194.5, num_updates=175800, lr=7.54207e-05, gnorm=0.935, loss_scale=4, train_wall=6, gb_free=29.7, wall=1949
2024-01-31 05:16:16 | INFO | train_inner | epoch 120:    387 / 1475 loss=2.247, nll_loss=0.598, ppl=1.51, wps=59872.6, ups=15.53, wpb=3854.6, bsz=193.8, num_updates=175900, lr=7.53993e-05, gnorm=0.942, loss_scale=4, train_wall=6, gb_free=29.7, wall=1955
2024-01-31 05:16:23 | INFO | train_inner | epoch 120:    487 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=59049.4, ups=15.53, wpb=3802.1, bsz=198.2, num_updates=176000, lr=7.53778e-05, gnorm=0.964, loss_scale=4, train_wall=6, gb_free=29.9, wall=1962
2024-01-31 05:16:29 | INFO | train_inner | epoch 120:    587 / 1475 loss=2.247, nll_loss=0.597, ppl=1.51, wps=58544.1, ups=15.59, wpb=3755.6, bsz=199.6, num_updates=176100, lr=7.53564e-05, gnorm=0.965, loss_scale=4, train_wall=6, gb_free=29.6, wall=1968
2024-01-31 05:16:35 | INFO | train_inner | epoch 120:    687 / 1475 loss=2.257, nll_loss=0.609, ppl=1.53, wps=58957.1, ups=15.62, wpb=3774.5, bsz=188.3, num_updates=176200, lr=7.5335e-05, gnorm=0.978, loss_scale=4, train_wall=6, gb_free=29.8, wall=1974
2024-01-31 05:16:42 | INFO | train_inner | epoch 120:    787 / 1475 loss=2.247, nll_loss=0.599, ppl=1.51, wps=58732.2, ups=15.59, wpb=3767, bsz=195.6, num_updates=176300, lr=7.53137e-05, gnorm=0.951, loss_scale=4, train_wall=6, gb_free=29.8, wall=1981
2024-01-31 05:16:48 | INFO | train_inner | epoch 120:    887 / 1475 loss=2.275, nll_loss=0.629, ppl=1.55, wps=59240.9, ups=15.57, wpb=3804.9, bsz=185.4, num_updates=176400, lr=7.52923e-05, gnorm=0.989, loss_scale=4, train_wall=6, gb_free=29.7, wall=1987
2024-01-31 05:16:55 | INFO | train_inner | epoch 120:    987 / 1475 loss=2.252, nll_loss=0.604, ppl=1.52, wps=57855.7, ups=15.62, wpb=3704.6, bsz=194.2, num_updates=176500, lr=7.5271e-05, gnorm=0.969, loss_scale=4, train_wall=6, gb_free=29.7, wall=1994
2024-01-31 05:17:01 | INFO | train_inner | epoch 120:   1087 / 1475 loss=2.254, nll_loss=0.606, ppl=1.52, wps=57996.8, ups=15.59, wpb=3719.7, bsz=194.9, num_updates=176600, lr=7.52497e-05, gnorm=0.972, loss_scale=4, train_wall=6, gb_free=29.7, wall=2000
2024-01-31 05:17:08 | INFO | train_inner | epoch 120:   1187 / 1475 loss=2.256, nll_loss=0.609, ppl=1.52, wps=56798.5, ups=15.64, wpb=3631.3, bsz=201.3, num_updates=176700, lr=7.52284e-05, gnorm=1.006, loss_scale=4, train_wall=6, gb_free=29.7, wall=2006
2024-01-31 05:17:14 | INFO | train_inner | epoch 120:   1287 / 1475 loss=2.258, nll_loss=0.611, ppl=1.53, wps=59622.4, ups=15.56, wpb=3831.1, bsz=196.5, num_updates=176800, lr=7.52071e-05, gnorm=0.962, loss_scale=4, train_wall=6, gb_free=29.7, wall=2013
2024-01-31 05:17:20 | INFO | train_inner | epoch 120:   1387 / 1475 loss=2.258, nll_loss=0.613, ppl=1.53, wps=59254.2, ups=15.44, wpb=3836.6, bsz=199.1, num_updates=176900, lr=7.51858e-05, gnorm=0.949, loss_scale=4, train_wall=6, gb_free=29.6, wall=2019
2024-01-31 05:17:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:17:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:17:28 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 3.514 | nll_loss 1.907 | ppl 3.75 | wps 135380 | wpb 3189.1 | bsz 163.4 | num_updates 176988 | best_loss 3.385
2024-01-31 05:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 176988 updates
2024-01-31 05:17:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint120.pt
2024-01-31 05:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint120.pt
2024-01-31 05:17:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint120.pt (epoch 120 @ 176988 updates, score 3.514) (writing took 2.461221772013232 seconds)
2024-01-31 05:17:31 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2024-01-31 05:17:31 | INFO | train | epoch 120 | loss 2.251 | nll_loss 0.602 | ppl 1.52 | wps 55833.6 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 176988 | lr 7.51672e-05 | gnorm 0.963 | loss_scale 4 | train_wall 91 | gb_free 29.7 | wall 2030
2024-01-31 05:17:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:17:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:17:31 | INFO | fairseq.trainer | begin training epoch 121
2024-01-31 05:17:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:17:32 | INFO | train_inner | epoch 121:     12 / 1475 loss=2.248, nll_loss=0.599, ppl=1.52, wps=32587.4, ups=8.81, wpb=3700.5, bsz=208.1, num_updates=177000, lr=7.51646e-05, gnorm=1.003, loss_scale=4, train_wall=6, gb_free=29.7, wall=2031
2024-01-31 05:17:38 | INFO | train_inner | epoch 121:    112 / 1475 loss=2.245, nll_loss=0.595, ppl=1.51, wps=59236.7, ups=15.66, wpb=3783.2, bsz=192.8, num_updates=177100, lr=7.51434e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=2037
2024-01-31 05:17:45 | INFO | train_inner | epoch 121:    212 / 1475 loss=2.226, nll_loss=0.573, ppl=1.49, wps=59138.2, ups=15.55, wpb=3804.2, bsz=205.7, num_updates=177200, lr=7.51222e-05, gnorm=0.926, loss_scale=4, train_wall=6, gb_free=29.7, wall=2043
2024-01-31 05:17:51 | INFO | train_inner | epoch 121:    312 / 1475 loss=2.234, nll_loss=0.584, ppl=1.5, wps=59455.3, ups=15.47, wpb=3843.3, bsz=204, num_updates=177300, lr=7.5101e-05, gnorm=0.92, loss_scale=4, train_wall=6, gb_free=29.8, wall=2050
2024-01-31 05:17:58 | INFO | train_inner | epoch 121:    412 / 1475 loss=2.234, nll_loss=0.585, ppl=1.5, wps=57751, ups=15.47, wpb=3732.8, bsz=198.6, num_updates=177400, lr=7.50798e-05, gnorm=0.957, loss_scale=4, train_wall=6, gb_free=29.6, wall=2056
2024-01-31 05:18:04 | INFO | train_inner | epoch 121:    512 / 1475 loss=2.25, nll_loss=0.601, ppl=1.52, wps=58620.4, ups=15.56, wpb=3766.4, bsz=191.7, num_updates=177500, lr=7.50587e-05, gnorm=0.965, loss_scale=4, train_wall=6, gb_free=29.6, wall=2063
2024-01-31 05:18:10 | INFO | train_inner | epoch 121:    612 / 1475 loss=2.244, nll_loss=0.595, ppl=1.51, wps=58986.3, ups=15.62, wpb=3777.2, bsz=193.5, num_updates=177600, lr=7.50375e-05, gnorm=0.991, loss_scale=4, train_wall=6, gb_free=29.7, wall=2069
2024-01-31 05:18:17 | INFO | train_inner | epoch 121:    712 / 1475 loss=2.244, nll_loss=0.595, ppl=1.51, wps=57339.8, ups=15.64, wpb=3665.5, bsz=199.8, num_updates=177700, lr=7.50164e-05, gnorm=0.956, loss_scale=4, train_wall=6, gb_free=29.8, wall=2076
2024-01-31 05:18:23 | INFO | train_inner | epoch 121:    812 / 1475 loss=2.268, nll_loss=0.621, ppl=1.54, wps=58748.3, ups=15.6, wpb=3766.4, bsz=181.8, num_updates=177800, lr=7.49953e-05, gnorm=1.007, loss_scale=4, train_wall=6, gb_free=29.9, wall=2082
2024-01-31 05:18:30 | INFO | train_inner | epoch 121:    912 / 1475 loss=2.255, nll_loss=0.608, ppl=1.52, wps=58761.1, ups=15.69, wpb=3745, bsz=192.6, num_updates=177900, lr=7.49742e-05, gnorm=0.982, loss_scale=4, train_wall=6, gb_free=29.8, wall=2088
2024-01-31 05:18:36 | INFO | train_inner | epoch 121:   1012 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=58991.9, ups=15.56, wpb=3792.3, bsz=203.8, num_updates=178000, lr=7.49532e-05, gnorm=0.949, loss_scale=4, train_wall=6, gb_free=29.7, wall=2095
2024-01-31 05:18:42 | INFO | train_inner | epoch 121:   1112 / 1475 loss=2.246, nll_loss=0.598, ppl=1.51, wps=57904.5, ups=15.52, wpb=3731.1, bsz=202.7, num_updates=178100, lr=7.49321e-05, gnorm=0.972, loss_scale=4, train_wall=6, gb_free=29.7, wall=2101
2024-01-31 05:18:49 | INFO | train_inner | epoch 121:   1212 / 1475 loss=2.25, nll_loss=0.601, ppl=1.52, wps=58178.4, ups=15.57, wpb=3736.5, bsz=200.2, num_updates=178200, lr=7.49111e-05, gnorm=0.991, loss_scale=4, train_wall=6, gb_free=29.6, wall=2108
2024-01-31 05:18:55 | INFO | train_inner | epoch 121:   1312 / 1475 loss=2.263, nll_loss=0.617, ppl=1.53, wps=59574.7, ups=15.63, wpb=3812.7, bsz=193.3, num_updates=178300, lr=7.48901e-05, gnorm=0.974, loss_scale=4, train_wall=6, gb_free=29.7, wall=2114
2024-01-31 05:19:02 | INFO | train_inner | epoch 121:   1412 / 1475 loss=2.275, nll_loss=0.631, ppl=1.55, wps=59304.3, ups=15.53, wpb=3817.6, bsz=187.9, num_updates=178400, lr=7.48691e-05, gnorm=0.97, loss_scale=4, train_wall=6, gb_free=29.7, wall=2121
2024-01-31 05:19:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:19:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:19:08 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 3.507 | nll_loss 1.902 | ppl 3.74 | wps 135205 | wpb 3189.1 | bsz 163.4 | num_updates 178463 | best_loss 3.385
2024-01-31 05:19:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 178463 updates
2024-01-31 05:19:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint121.pt
2024-01-31 05:19:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint121.pt
2024-01-31 05:19:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint121.pt (epoch 121 @ 178463 updates, score 3.507) (writing took 2.5023336650338024 seconds)
2024-01-31 05:19:11 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2024-01-31 05:19:11 | INFO | train | epoch 121 | loss 2.25 | nll_loss 0.601 | ppl 1.52 | wps 55812.6 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 178463 | lr 7.48559e-05 | gnorm 0.969 | loss_scale 4 | train_wall 91 | gb_free 29.6 | wall 2129
2024-01-31 05:19:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:19:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:19:11 | INFO | fairseq.trainer | begin training epoch 122
2024-01-31 05:19:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:19:13 | INFO | train_inner | epoch 122:     37 / 1475 loss=2.252, nll_loss=0.604, ppl=1.52, wps=33477.9, ups=8.76, wpb=3821.8, bsz=200.2, num_updates=178500, lr=7.48481e-05, gnorm=0.958, loss_scale=4, train_wall=6, gb_free=29.7, wall=2132
2024-01-31 05:19:19 | INFO | train_inner | epoch 122:    137 / 1475 loss=2.247, nll_loss=0.597, ppl=1.51, wps=58055.9, ups=15.67, wpb=3704, bsz=179.4, num_updates=178600, lr=7.48272e-05, gnorm=0.979, loss_scale=4, train_wall=6, gb_free=29.7, wall=2138
2024-01-31 05:19:26 | INFO | train_inner | epoch 122:    237 / 1475 loss=2.23, nll_loss=0.58, ppl=1.49, wps=59796, ups=15.52, wpb=3853.2, bsz=198.4, num_updates=178700, lr=7.48062e-05, gnorm=0.927, loss_scale=4, train_wall=6, gb_free=29.7, wall=2145
2024-01-31 05:19:32 | INFO | train_inner | epoch 122:    337 / 1475 loss=2.244, nll_loss=0.593, ppl=1.51, wps=58867.4, ups=15.69, wpb=3751.3, bsz=189.9, num_updates=178800, lr=7.47853e-05, gnorm=0.97, loss_scale=4, train_wall=6, gb_free=29.7, wall=2151
2024-01-31 05:19:39 | INFO | train_inner | epoch 122:    437 / 1475 loss=2.229, nll_loss=0.579, ppl=1.49, wps=58294.9, ups=15.46, wpb=3770.6, bsz=207.4, num_updates=178900, lr=7.47644e-05, gnorm=0.932, loss_scale=4, train_wall=6, gb_free=29.6, wall=2158
2024-01-31 05:19:45 | INFO | train_inner | epoch 122:    537 / 1475 loss=2.235, nll_loss=0.585, ppl=1.5, wps=58912.4, ups=15.5, wpb=3801.9, bsz=205.8, num_updates=179000, lr=7.47435e-05, gnorm=0.936, loss_scale=4, train_wall=6, gb_free=29.6, wall=2164
2024-01-31 05:19:52 | INFO | train_inner | epoch 122:    637 / 1475 loss=2.234, nll_loss=0.584, ppl=1.5, wps=58244.6, ups=15.5, wpb=3757.6, bsz=205.9, num_updates=179100, lr=7.47226e-05, gnorm=0.946, loss_scale=4, train_wall=6, gb_free=29.7, wall=2171
2024-01-31 05:19:58 | INFO | train_inner | epoch 122:    737 / 1475 loss=2.24, nll_loss=0.591, ppl=1.51, wps=58094.4, ups=15.58, wpb=3729, bsz=211.1, num_updates=179200, lr=7.47018e-05, gnorm=0.945, loss_scale=4, train_wall=6, gb_free=29.8, wall=2177
2024-01-31 05:20:04 | INFO | train_inner | epoch 122:    837 / 1475 loss=2.252, nll_loss=0.604, ppl=1.52, wps=58515.4, ups=15.63, wpb=3743.5, bsz=187.6, num_updates=179300, lr=7.4681e-05, gnorm=0.969, loss_scale=4, train_wall=6, gb_free=29.7, wall=2183
2024-01-31 05:20:11 | INFO | train_inner | epoch 122:    937 / 1475 loss=2.267, nll_loss=0.62, ppl=1.54, wps=59510.3, ups=15.63, wpb=3807.1, bsz=183.3, num_updates=179400, lr=7.46601e-05, gnorm=0.997, loss_scale=4, train_wall=6, gb_free=29.7, wall=2190
2024-01-31 05:20:17 | INFO | train_inner | epoch 122:   1037 / 1475 loss=2.254, nll_loss=0.606, ppl=1.52, wps=59697.9, ups=15.59, wpb=3829.4, bsz=196.8, num_updates=179500, lr=7.46393e-05, gnorm=0.949, loss_scale=4, train_wall=6, gb_free=29.7, wall=2196
2024-01-31 05:20:24 | INFO | train_inner | epoch 122:   1137 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=58258.1, ups=15.52, wpb=3754.1, bsz=200.6, num_updates=179600, lr=7.46186e-05, gnorm=0.974, loss_scale=4, train_wall=6, gb_free=29.7, wall=2203
2024-01-31 05:20:30 | INFO | train_inner | epoch 122:   1237 / 1475 loss=2.275, nll_loss=0.63, ppl=1.55, wps=59211.3, ups=15.57, wpb=3802.6, bsz=189.8, num_updates=179700, lr=7.45978e-05, gnorm=0.999, loss_scale=4, train_wall=6, gb_free=29.7, wall=2209
2024-01-31 05:20:37 | INFO | train_inner | epoch 122:   1337 / 1475 loss=2.272, nll_loss=0.626, ppl=1.54, wps=58649.4, ups=15.59, wpb=3762, bsz=185.3, num_updates=179800, lr=7.4577e-05, gnorm=0.983, loss_scale=4, train_wall=6, gb_free=29.8, wall=2215
2024-01-31 05:20:43 | INFO | train_inner | epoch 122:   1437 / 1475 loss=2.255, nll_loss=0.609, ppl=1.52, wps=58119.4, ups=15.55, wpb=3737.2, bsz=196.7, num_updates=179900, lr=7.45563e-05, gnorm=1.015, loss_scale=4, train_wall=6, gb_free=29.7, wall=2222
2024-01-31 05:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:20:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:20:48 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 3.514 | nll_loss 1.909 | ppl 3.75 | wps 135894 | wpb 3189.1 | bsz 163.4 | num_updates 179938 | best_loss 3.385
2024-01-31 05:20:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 179938 updates
2024-01-31 05:20:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint122.pt
2024-01-31 05:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint122.pt
2024-01-31 05:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint122.pt (epoch 122 @ 179938 updates, score 3.514) (writing took 2.4504980789497495 seconds)
2024-01-31 05:20:50 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2024-01-31 05:20:50 | INFO | train | epoch 122 | loss 2.248 | nll_loss 0.599 | ppl 1.52 | wps 55829.5 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 179938 | lr 7.45484e-05 | gnorm 0.967 | loss_scale 4 | train_wall 91 | gb_free 29.6 | wall 2229
2024-01-31 05:20:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:20:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:20:50 | INFO | fairseq.trainer | begin training epoch 123
2024-01-31 05:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:20:54 | INFO | train_inner | epoch 123:     62 / 1475 loss=2.248, nll_loss=0.598, ppl=1.51, wps=33153.6, ups=8.87, wpb=3736.4, bsz=187, num_updates=180000, lr=7.45356e-05, gnorm=1.002, loss_scale=4, train_wall=6, gb_free=29.6, wall=2233
2024-01-31 05:21:01 | INFO | train_inner | epoch 123:    162 / 1475 loss=2.22, nll_loss=0.569, ppl=1.48, wps=59471.9, ups=15.45, wpb=3849.9, bsz=204.5, num_updates=180100, lr=7.45149e-05, gnorm=0.918, loss_scale=4, train_wall=6, gb_free=29.6, wall=2240
2024-01-31 05:21:07 | INFO | train_inner | epoch 123:    262 / 1475 loss=2.257, nll_loss=0.607, ppl=1.52, wps=58388.3, ups=15.71, wpb=3716.7, bsz=179, num_updates=180200, lr=7.44942e-05, gnorm=1.003, loss_scale=4, train_wall=6, gb_free=29.7, wall=2246
2024-01-31 05:21:13 | INFO | train_inner | epoch 123:    362 / 1475 loss=2.244, nll_loss=0.594, ppl=1.51, wps=58308.7, ups=15.67, wpb=3721, bsz=187.2, num_updates=180300, lr=7.44736e-05, gnorm=0.966, loss_scale=4, train_wall=6, gb_free=29.8, wall=2252
2024-01-31 05:21:20 | INFO | train_inner | epoch 123:    462 / 1475 loss=2.237, nll_loss=0.586, ppl=1.5, wps=58735.7, ups=15.5, wpb=3790.5, bsz=200.7, num_updates=180400, lr=7.44529e-05, gnorm=0.941, loss_scale=4, train_wall=6, gb_free=29.7, wall=2259
2024-01-31 05:21:26 | INFO | train_inner | epoch 123:    562 / 1475 loss=2.229, nll_loss=0.578, ppl=1.49, wps=58897.2, ups=15.48, wpb=3804.4, bsz=210.9, num_updates=180500, lr=7.44323e-05, gnorm=0.925, loss_scale=4, train_wall=6, gb_free=29.7, wall=2265
2024-01-31 05:21:33 | INFO | train_inner | epoch 123:    662 / 1475 loss=2.239, nll_loss=0.589, ppl=1.5, wps=57952.2, ups=15.59, wpb=3716.3, bsz=205.8, num_updates=180600, lr=7.44117e-05, gnorm=0.962, loss_scale=4, train_wall=6, gb_free=29.8, wall=2272
2024-01-31 05:21:39 | INFO | train_inner | epoch 123:    762 / 1475 loss=2.254, nll_loss=0.606, ppl=1.52, wps=58955, ups=15.64, wpb=3768.9, bsz=192, num_updates=180700, lr=7.43911e-05, gnorm=0.966, loss_scale=4, train_wall=6, gb_free=29.8, wall=2278
2024-01-31 05:21:46 | INFO | train_inner | epoch 123:    862 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=58678.2, ups=15.6, wpb=3762, bsz=199.5, num_updates=180800, lr=7.43705e-05, gnorm=1.025, loss_scale=4, train_wall=6, gb_free=29.7, wall=2284
2024-01-31 05:21:52 | INFO | train_inner | epoch 123:    962 / 1475 loss=2.243, nll_loss=0.595, ppl=1.51, wps=58437.1, ups=15.5, wpb=3770.7, bsz=194.2, num_updates=180900, lr=7.435e-05, gnorm=0.976, loss_scale=4, train_wall=6, gb_free=29.6, wall=2291
2024-01-31 05:21:58 | INFO | train_inner | epoch 123:   1062 / 1475 loss=2.249, nll_loss=0.601, ppl=1.52, wps=59180.9, ups=15.54, wpb=3809.1, bsz=203.6, num_updates=181000, lr=7.43294e-05, gnorm=0.967, loss_scale=4, train_wall=6, gb_free=29.7, wall=2297
2024-01-31 05:22:05 | INFO | train_inner | epoch 123:   1162 / 1475 loss=2.252, nll_loss=0.605, ppl=1.52, wps=59120.8, ups=15.58, wpb=3794, bsz=196.5, num_updates=181100, lr=7.43089e-05, gnorm=0.977, loss_scale=4, train_wall=6, gb_free=29.7, wall=2304
2024-01-31 05:22:11 | INFO | train_inner | epoch 123:   1262 / 1475 loss=2.257, nll_loss=0.61, ppl=1.53, wps=59235.3, ups=15.58, wpb=3801.7, bsz=188.8, num_updates=181200, lr=7.42884e-05, gnorm=0.964, loss_scale=4, train_wall=6, gb_free=29.8, wall=2310
2024-01-31 05:22:18 | INFO | train_inner | epoch 123:   1362 / 1475 loss=2.259, nll_loss=0.613, ppl=1.53, wps=58132.2, ups=15.51, wpb=3747.1, bsz=197.5, num_updates=181300, lr=7.42679e-05, gnorm=0.967, loss_scale=4, train_wall=6, gb_free=29.7, wall=2317
2024-01-31 05:22:24 | INFO | train_inner | epoch 123:   1462 / 1475 loss=2.26, nll_loss=0.614, ppl=1.53, wps=58634.2, ups=15.56, wpb=3767.4, bsz=190.2, num_updates=181400, lr=7.42474e-05, gnorm=0.98, loss_scale=4, train_wall=6, gb_free=29.7, wall=2323
2024-01-31 05:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:22:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:22:27 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 3.513 | nll_loss 1.907 | ppl 3.75 | wps 135421 | wpb 3189.1 | bsz 163.4 | num_updates 181413 | best_loss 3.385
2024-01-31 05:22:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 181413 updates
2024-01-31 05:22:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint123.pt
2024-01-31 05:22:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint123.pt
2024-01-31 05:22:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint123.pt (epoch 123 @ 181413 updates, score 3.513) (writing took 2.5005951480707154 seconds)
2024-01-31 05:22:30 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2024-01-31 05:22:30 | INFO | train | epoch 123 | loss 2.246 | nll_loss 0.597 | ppl 1.51 | wps 55799.9 | ups 14.79 | wpb 3771.6 | bsz 195.9 | num_updates 181413 | lr 7.42448e-05 | gnorm 0.967 | loss_scale 4 | train_wall 91 | gb_free 29.6 | wall 2329
2024-01-31 05:22:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:22:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:22:30 | INFO | fairseq.trainer | begin training epoch 124
2024-01-31 05:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:22:36 | INFO | train_inner | epoch 124:     87 / 1475 loss=2.233, nll_loss=0.583, ppl=1.5, wps=33036.9, ups=8.8, wpb=3756.3, bsz=188.2, num_updates=181500, lr=7.4227e-05, gnorm=0.972, loss_scale=4, train_wall=6, gb_free=29.6, wall=2334
2024-01-31 05:22:42 | INFO | train_inner | epoch 124:    187 / 1475 loss=2.241, nll_loss=0.591, ppl=1.51, wps=58180.5, ups=15.65, wpb=3718.8, bsz=181.9, num_updates=181600, lr=7.42065e-05, gnorm=0.977, loss_scale=4, train_wall=6, gb_free=29.6, wall=2341
2024-01-31 05:22:48 | INFO | train_inner | epoch 124:    287 / 1475 loss=2.23, nll_loss=0.579, ppl=1.49, wps=58810.3, ups=15.61, wpb=3767.1, bsz=197.1, num_updates=181700, lr=7.41861e-05, gnorm=0.958, loss_scale=4, train_wall=6, gb_free=29.7, wall=2347
2024-01-31 05:22:55 | INFO | train_inner | epoch 124:    387 / 1475 loss=2.22, nll_loss=0.568, ppl=1.48, wps=58160.4, ups=15.47, wpb=3759.8, bsz=205.8, num_updates=181800, lr=7.41657e-05, gnorm=0.926, loss_scale=4, train_wall=6, gb_free=29.7, wall=2354
2024-01-31 05:23:01 | INFO | train_inner | epoch 124:    487 / 1475 loss=2.252, nll_loss=0.603, ppl=1.52, wps=59330.3, ups=15.65, wpb=3790.1, bsz=184.2, num_updates=181900, lr=7.41453e-05, gnorm=0.985, loss_scale=4, train_wall=6, gb_free=29.7, wall=2360
2024-01-31 05:23:08 | INFO | train_inner | epoch 124:    587 / 1475 loss=2.217, nll_loss=0.566, ppl=1.48, wps=58360, ups=15.45, wpb=3776.9, bsz=210.4, num_updates=182000, lr=7.41249e-05, gnorm=0.927, loss_scale=4, train_wall=6, gb_free=29.7, wall=2367
2024-01-31 05:23:14 | INFO | train_inner | epoch 124:    687 / 1475 loss=2.249, nll_loss=0.6, ppl=1.52, wps=57926.9, ups=15.61, wpb=3710.4, bsz=191.8, num_updates=182100, lr=7.41046e-05, gnorm=0.981, loss_scale=4, train_wall=6, gb_free=29.7, wall=2373
2024-01-31 05:23:21 | INFO | train_inner | epoch 124:    787 / 1475 loss=2.244, nll_loss=0.595, ppl=1.51, wps=56356.4, ups=15.37, wpb=3667.2, bsz=191.9, num_updates=182200, lr=7.40842e-05, gnorm=0.998, loss_scale=4, train_wall=6, gb_free=29.9, wall=2380
2024-01-31 05:23:27 | INFO | train_inner | epoch 124:    887 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=58687.2, ups=15.55, wpb=3773.9, bsz=205.2, num_updates=182300, lr=7.40639e-05, gnorm=0.962, loss_scale=4, train_wall=6, gb_free=29.7, wall=2386
2024-01-31 05:23:33 | INFO | train_inner | epoch 124:    987 / 1475 loss=2.25, nll_loss=0.602, ppl=1.52, wps=59122.6, ups=15.58, wpb=3794.2, bsz=197.6, num_updates=182400, lr=7.40436e-05, gnorm=0.964, loss_scale=4, train_wall=6, gb_free=29.7, wall=2392
2024-01-31 05:23:40 | INFO | train_inner | epoch 124:   1087 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=59949, ups=15.49, wpb=3869.2, bsz=203.5, num_updates=182500, lr=7.40233e-05, gnorm=0.94, loss_scale=4, train_wall=6, gb_free=29.6, wall=2399
2024-01-31 05:23:46 | INFO | train_inner | epoch 124:   1187 / 1475 loss=2.25, nll_loss=0.603, ppl=1.52, wps=59377.8, ups=15.56, wpb=3816.5, bsz=202.3, num_updates=182600, lr=7.4003e-05, gnorm=0.958, loss_scale=4, train_wall=6, gb_free=29.7, wall=2405
2024-01-31 05:23:53 | INFO | train_inner | epoch 124:   1287 / 1475 loss=2.264, nll_loss=0.618, ppl=1.53, wps=59104.1, ups=15.59, wpb=3790.2, bsz=193.8, num_updates=182700, lr=7.39828e-05, gnorm=0.973, loss_scale=4, train_wall=6, gb_free=29.7, wall=2412
2024-01-31 05:23:59 | INFO | train_inner | epoch 124:   1387 / 1475 loss=2.258, nll_loss=0.612, ppl=1.53, wps=59120, ups=15.53, wpb=3807.6, bsz=201.8, num_updates=182800, lr=7.39626e-05, gnorm=1.011, loss_scale=4, train_wall=6, gb_free=29.7, wall=2418
2024-01-31 05:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:24:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:24:07 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 3.517 | nll_loss 1.912 | ppl 3.76 | wps 135234 | wpb 3189.1 | bsz 163.4 | num_updates 182888 | best_loss 3.385
2024-01-31 05:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 182888 updates
2024-01-31 05:24:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint124.pt
2024-01-31 05:24:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint124.pt
2024-01-31 05:24:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint124.pt (epoch 124 @ 182888 updates, score 3.517) (writing took 2.4516542649362236 seconds)
2024-01-31 05:24:10 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2024-01-31 05:24:10 | INFO | train | epoch 124 | loss 2.244 | nll_loss 0.595 | ppl 1.51 | wps 55705.3 | ups 14.77 | wpb 3771.6 | bsz 195.9 | num_updates 182888 | lr 7.39448e-05 | gnorm 0.969 | loss_scale 4 | train_wall 92 | gb_free 29.7 | wall 2429
2024-01-31 05:24:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:24:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:24:10 | INFO | fairseq.trainer | begin training epoch 125
2024-01-31 05:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:24:11 | INFO | train_inner | epoch 125:     12 / 1475 loss=2.265, nll_loss=0.618, ppl=1.54, wps=32906.9, ups=8.74, wpb=3766.8, bsz=183.7, num_updates=182900, lr=7.39423e-05, gnorm=1.008, loss_scale=4, train_wall=6, gb_free=29.7, wall=2430
2024-01-31 05:24:17 | INFO | train_inner | epoch 125:    112 / 1475 loss=2.219, nll_loss=0.567, ppl=1.48, wps=59542.2, ups=15.61, wpb=3813.3, bsz=203.4, num_updates=183000, lr=7.39221e-05, gnorm=0.925, loss_scale=4, train_wall=6, gb_free=29.7, wall=2436
2024-01-31 05:24:23 | INFO | train_inner | epoch 125:    212 / 1475 loss=2.234, nll_loss=0.583, ppl=1.5, wps=58078.4, ups=15.56, wpb=3731.8, bsz=190.9, num_updates=183100, lr=7.39019e-05, gnorm=0.966, loss_scale=4, train_wall=6, gb_free=29.7, wall=2442
2024-01-31 05:24:30 | INFO | train_inner | epoch 125:    312 / 1475 loss=2.237, nll_loss=0.588, ppl=1.5, wps=59481.8, ups=15.53, wpb=3829.1, bsz=190.3, num_updates=183200, lr=7.38818e-05, gnorm=0.962, loss_scale=4, train_wall=6, gb_free=29.7, wall=2449
2024-01-31 05:24:36 | INFO | train_inner | epoch 125:    412 / 1475 loss=2.222, nll_loss=0.571, ppl=1.49, wps=58355.1, ups=15.58, wpb=3745, bsz=208.6, num_updates=183300, lr=7.38616e-05, gnorm=0.95, loss_scale=4, train_wall=6, gb_free=29.9, wall=2455
2024-01-31 05:24:43 | INFO | train_inner | epoch 125:    512 / 1475 loss=2.244, nll_loss=0.594, ppl=1.51, wps=59182.7, ups=15.71, wpb=3767.8, bsz=191.2, num_updates=183400, lr=7.38415e-05, gnorm=0.975, loss_scale=4, train_wall=6, gb_free=29.7, wall=2462
2024-01-31 05:24:49 | INFO | train_inner | epoch 125:    612 / 1475 loss=2.237, nll_loss=0.588, ppl=1.5, wps=57835.2, ups=15.51, wpb=3728.3, bsz=197.4, num_updates=183500, lr=7.38213e-05, gnorm=0.971, loss_scale=4, train_wall=6, gb_free=29.7, wall=2468
2024-01-31 05:24:56 | INFO | train_inner | epoch 125:    712 / 1475 loss=2.249, nll_loss=0.601, ppl=1.52, wps=59442.1, ups=15.67, wpb=3793.4, bsz=185.9, num_updates=183600, lr=7.38012e-05, gnorm=0.97, loss_scale=4, train_wall=6, gb_free=29.7, wall=2474
2024-01-31 05:25:02 | INFO | train_inner | epoch 125:    812 / 1475 loss=2.25, nll_loss=0.602, ppl=1.52, wps=58641, ups=15.58, wpb=3763.4, bsz=193.3, num_updates=183700, lr=7.37812e-05, gnorm=0.984, loss_scale=4, train_wall=6, gb_free=29.9, wall=2481
2024-01-31 05:25:08 | INFO | train_inner | epoch 125:    912 / 1475 loss=2.252, nll_loss=0.604, ppl=1.52, wps=60163.2, ups=15.57, wpb=3865, bsz=189.4, num_updates=183800, lr=7.37611e-05, gnorm=0.964, loss_scale=4, train_wall=6, gb_free=29.6, wall=2487
2024-01-31 05:25:15 | INFO | train_inner | epoch 125:   1012 / 1475 loss=2.251, nll_loss=0.603, ppl=1.52, wps=58381.9, ups=15.66, wpb=3727.8, bsz=191.4, num_updates=183900, lr=7.3741e-05, gnorm=0.982, loss_scale=4, train_wall=6, gb_free=29.7, wall=2494
2024-01-31 05:25:21 | INFO | train_inner | epoch 125:   1112 / 1475 loss=2.243, nll_loss=0.594, ppl=1.51, wps=57357, ups=15.53, wpb=3692.8, bsz=197.9, num_updates=184000, lr=7.3721e-05, gnorm=0.985, loss_scale=4, train_wall=6, gb_free=29.7, wall=2500
2024-01-31 05:25:28 | INFO | train_inner | epoch 125:   1212 / 1475 loss=2.256, nll_loss=0.61, ppl=1.53, wps=59080.6, ups=15.57, wpb=3794.4, bsz=195.7, num_updates=184100, lr=7.3701e-05, gnorm=0.993, loss_scale=4, train_wall=6, gb_free=29.6, wall=2507
2024-01-31 05:25:34 | INFO | train_inner | epoch 125:   1312 / 1475 loss=2.251, nll_loss=0.604, ppl=1.52, wps=59156.6, ups=15.5, wpb=3815.9, bsz=200.9, num_updates=184200, lr=7.36809e-05, gnorm=0.981, loss_scale=4, train_wall=6, gb_free=29.6, wall=2513
2024-01-31 05:25:40 | INFO | train_inner | epoch 125:   1412 / 1475 loss=2.252, nll_loss=0.605, ppl=1.52, wps=58693.1, ups=15.6, wpb=3762.3, bsz=197.5, num_updates=184300, lr=7.3661e-05, gnorm=0.961, loss_scale=4, train_wall=6, gb_free=29.7, wall=2519
2024-01-31 05:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:25:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:25:47 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 3.521 | nll_loss 1.92 | ppl 3.79 | wps 135289 | wpb 3189.1 | bsz 163.4 | num_updates 184363 | best_loss 3.385
2024-01-31 05:25:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 184363 updates
2024-01-31 05:25:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint125.pt
2024-01-31 05:25:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint125.pt
2024-01-31 05:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint125.pt (epoch 125 @ 184363 updates, score 3.521) (writing took 2.545919714961201 seconds)
2024-01-31 05:25:49 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2024-01-31 05:25:49 | INFO | train | epoch 125 | loss 2.242 | nll_loss 0.593 | ppl 1.51 | wps 55816.8 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 184363 | lr 7.36484e-05 | gnorm 0.968 | loss_scale 4 | train_wall 91 | gb_free 29.8 | wall 2528
2024-01-31 05:25:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:25:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:25:49 | INFO | fairseq.trainer | begin training epoch 126
2024-01-31 05:25:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:25:52 | INFO | train_inner | epoch 126:     37 / 1475 loss=2.229, nll_loss=0.579, ppl=1.49, wps=32445.8, ups=8.72, wpb=3720.1, bsz=206.2, num_updates=184400, lr=7.3641e-05, gnorm=0.946, loss_scale=4, train_wall=6, gb_free=29.8, wall=2531
2024-01-31 05:25:58 | INFO | train_inner | epoch 126:    137 / 1475 loss=2.224, nll_loss=0.572, ppl=1.49, wps=58427.1, ups=15.62, wpb=3740.3, bsz=196.5, num_updates=184500, lr=7.3621e-05, gnorm=0.965, loss_scale=4, train_wall=6, gb_free=29.9, wall=2537
2024-01-31 05:26:05 | INFO | train_inner | epoch 126:    237 / 1475 loss=2.221, nll_loss=0.569, ppl=1.48, wps=58621.3, ups=15.58, wpb=3761.7, bsz=205.8, num_updates=184600, lr=7.36011e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.6, wall=2544
2024-01-31 05:26:11 | INFO | train_inner | epoch 126:    337 / 1475 loss=2.225, nll_loss=0.574, ppl=1.49, wps=59594.9, ups=15.56, wpb=3829.7, bsz=209.5, num_updates=184700, lr=7.35811e-05, gnorm=0.929, loss_scale=8, train_wall=6, gb_free=29.6, wall=2550
2024-01-31 05:26:18 | INFO | train_inner | epoch 126:    437 / 1475 loss=2.236, nll_loss=0.586, ppl=1.5, wps=58089.5, ups=15.63, wpb=3716.7, bsz=191.8, num_updates=184800, lr=7.35612e-05, gnorm=0.982, loss_scale=8, train_wall=6, gb_free=29.6, wall=2556
2024-01-31 05:26:24 | INFO | train_inner | epoch 126:    537 / 1475 loss=2.244, nll_loss=0.595, ppl=1.51, wps=58203.7, ups=15.6, wpb=3730.8, bsz=186.3, num_updates=184900, lr=7.35413e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=2563
2024-01-31 05:26:30 | INFO | train_inner | epoch 126:    637 / 1475 loss=2.247, nll_loss=0.599, ppl=1.51, wps=59899.1, ups=15.57, wpb=3846.8, bsz=192.6, num_updates=185000, lr=7.35215e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=2569
2024-01-31 05:26:37 | INFO | train_inner | epoch 126:    737 / 1475 loss=2.241, nll_loss=0.592, ppl=1.51, wps=58433.4, ups=15.65, wpb=3734.2, bsz=187.2, num_updates=185100, lr=7.35016e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=2576
2024-01-31 05:26:43 | INFO | train_inner | epoch 126:    837 / 1475 loss=2.239, nll_loss=0.589, ppl=1.5, wps=58264.6, ups=15.64, wpb=3724.5, bsz=200.1, num_updates=185200, lr=7.34818e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.6, wall=2582
2024-01-31 05:26:50 | INFO | train_inner | epoch 126:    937 / 1475 loss=2.246, nll_loss=0.598, ppl=1.51, wps=59655.3, ups=15.62, wpb=3818.7, bsz=193, num_updates=185300, lr=7.34619e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=2588
2024-01-31 05:26:56 | INFO | train_inner | epoch 126:   1037 / 1475 loss=2.238, nll_loss=0.589, ppl=1.5, wps=58387.9, ups=15.59, wpb=3746.4, bsz=207, num_updates=185400, lr=7.34421e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=2595
2024-01-31 05:27:02 | INFO | train_inner | epoch 126:   1137 / 1475 loss=2.248, nll_loss=0.601, ppl=1.52, wps=57999.8, ups=15.51, wpb=3739.5, bsz=196.4, num_updates=185500, lr=7.34223e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=2601
2024-01-31 05:27:09 | INFO | train_inner | epoch 126:   1237 / 1475 loss=2.25, nll_loss=0.604, ppl=1.52, wps=58902.1, ups=15.62, wpb=3770, bsz=192.8, num_updates=185600, lr=7.34025e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.7, wall=2608
2024-01-31 05:27:15 | INFO | train_inner | epoch 126:   1337 / 1475 loss=2.252, nll_loss=0.605, ppl=1.52, wps=58971, ups=15.55, wpb=3793.3, bsz=196.8, num_updates=185700, lr=7.33828e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.7, wall=2614
2024-01-31 05:27:22 | INFO | train_inner | epoch 126:   1437 / 1475 loss=2.265, nll_loss=0.619, ppl=1.54, wps=59435.9, ups=15.49, wpb=3837.1, bsz=185.4, num_updates=185800, lr=7.3363e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.6, wall=2621
2024-01-31 05:27:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:27:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:27:26 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 3.514 | nll_loss 1.911 | ppl 3.76 | wps 135145 | wpb 3189.1 | bsz 163.4 | num_updates 185838 | best_loss 3.385
2024-01-31 05:27:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 185838 updates
2024-01-31 05:27:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint126.pt
2024-01-31 05:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint126.pt
2024-01-31 05:27:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint126.pt (epoch 126 @ 185838 updates, score 3.514) (writing took 2.4080074899829924 seconds)
2024-01-31 05:27:29 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2024-01-31 05:27:29 | INFO | train | epoch 126 | loss 2.241 | nll_loss 0.592 | ppl 1.51 | wps 55893.1 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 185838 | lr 7.33555e-05 | gnorm 0.967 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 2628
2024-01-31 05:27:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:27:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:27:36 | INFO | fairseq.trainer | begin training epoch 127
2024-01-31 05:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:27:45 | INFO | train_inner | epoch 127:     62 / 1475 loss=2.24, nll_loss=0.59, ppl=1.51, wps=15933.7, ups=4.23, wpb=3764.8, bsz=183.9, num_updates=185900, lr=7.33433e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=2644
2024-01-31 05:27:52 | INFO | train_inner | epoch 127:    162 / 1475 loss=2.225, nll_loss=0.574, ppl=1.49, wps=57967.9, ups=15.53, wpb=3733.4, bsz=193.8, num_updates=186000, lr=7.33236e-05, gnorm=0.954, loss_scale=8, train_wall=6, gb_free=29.7, wall=2651
2024-01-31 05:27:58 | INFO | train_inner | epoch 127:    262 / 1475 loss=2.224, nll_loss=0.573, ppl=1.49, wps=58586.8, ups=15.5, wpb=3779.9, bsz=192.2, num_updates=186100, lr=7.33039e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.7, wall=2657
2024-01-31 05:28:05 | INFO | train_inner | epoch 127:    362 / 1475 loss=2.233, nll_loss=0.583, ppl=1.5, wps=58440.8, ups=15.67, wpb=3728.5, bsz=186.4, num_updates=186200, lr=7.32842e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.7, wall=2664
2024-01-31 05:28:11 | INFO | train_inner | epoch 127:    462 / 1475 loss=2.241, nll_loss=0.593, ppl=1.51, wps=59827.7, ups=15.52, wpb=3854.1, bsz=193.6, num_updates=186300, lr=7.32645e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.7, wall=2670
2024-01-31 05:28:18 | INFO | train_inner | epoch 127:    562 / 1475 loss=2.226, nll_loss=0.576, ppl=1.49, wps=59055.9, ups=15.55, wpb=3797.4, bsz=201.9, num_updates=186400, lr=7.32448e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.8, wall=2676
2024-01-31 05:28:24 | INFO | train_inner | epoch 127:    662 / 1475 loss=2.245, nll_loss=0.596, ppl=1.51, wps=59154.2, ups=15.66, wpb=3778.2, bsz=189.7, num_updates=186500, lr=7.32252e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.7, wall=2683
2024-01-31 05:28:30 | INFO | train_inner | epoch 127:    762 / 1475 loss=2.231, nll_loss=0.581, ppl=1.5, wps=58334.4, ups=15.53, wpb=3755.6, bsz=205.9, num_updates=186600, lr=7.32056e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.6, wall=2689
2024-01-31 05:28:37 | INFO | train_inner | epoch 127:    862 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=58370.4, ups=15.58, wpb=3745.5, bsz=199.9, num_updates=186700, lr=7.3186e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.8, wall=2696
2024-01-31 05:28:43 | INFO | train_inner | epoch 127:    962 / 1475 loss=2.238, nll_loss=0.589, ppl=1.5, wps=59044.9, ups=15.59, wpb=3786.9, bsz=202.6, num_updates=186800, lr=7.31664e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=2702
2024-01-31 05:28:50 | INFO | train_inner | epoch 127:   1062 / 1475 loss=2.239, nll_loss=0.59, ppl=1.51, wps=59717.6, ups=15.37, wpb=3885.6, bsz=212.7, num_updates=186900, lr=7.31468e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.8, wall=2709
2024-01-31 05:28:56 | INFO | train_inner | epoch 127:   1162 / 1475 loss=2.229, nll_loss=0.58, ppl=1.49, wps=55921.8, ups=15.59, wpb=3588.2, bsz=195.2, num_updates=187000, lr=7.31272e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.7, wall=2715
2024-01-31 05:29:03 | INFO | train_inner | epoch 127:   1262 / 1475 loss=2.264, nll_loss=0.618, ppl=1.54, wps=58844.3, ups=15.61, wpb=3768.9, bsz=187.5, num_updates=187100, lr=7.31077e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.7, wall=2721
2024-01-31 05:29:09 | INFO | train_inner | epoch 127:   1362 / 1475 loss=2.25, nll_loss=0.604, ppl=1.52, wps=59568.1, ups=15.54, wpb=3834.4, bsz=198.6, num_updates=187200, lr=7.30882e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.7, wall=2728
2024-01-31 05:29:15 | INFO | train_inner | epoch 127:   1462 / 1475 loss=2.257, nll_loss=0.61, ppl=1.53, wps=59460.8, ups=15.62, wpb=3807.6, bsz=194.9, num_updates=187300, lr=7.30687e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.7, wall=2734
2024-01-31 05:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:29:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:29:18 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 3.518 | nll_loss 1.913 | ppl 3.76 | wps 135119 | wpb 3189.1 | bsz 163.4 | num_updates 187313 | best_loss 3.385
2024-01-31 05:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 187313 updates
2024-01-31 05:29:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint127.pt
2024-01-31 05:29:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint127.pt
2024-01-31 05:29:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint127.pt (epoch 127 @ 187313 updates, score 3.518) (writing took 2.440440826001577 seconds)
2024-01-31 05:29:21 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2024-01-31 05:29:21 | INFO | train | epoch 127 | loss 2.239 | nll_loss 0.59 | ppl 1.51 | wps 49661.3 | ups 13.17 | wpb 3771.6 | bsz 195.9 | num_updates 187313 | lr 7.30661e-05 | gnorm 0.969 | loss_scale 8 | train_wall 91 | gb_free 29.9 | wall 2740
2024-01-31 05:29:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:29:21 | INFO | fairseq.trainer | begin training epoch 128
2024-01-31 05:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:29:27 | INFO | train_inner | epoch 128:     87 / 1475 loss=2.232, nll_loss=0.582, ppl=1.5, wps=33569.7, ups=8.81, wpb=3810, bsz=199.6, num_updates=187400, lr=7.30492e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.6, wall=2746
2024-01-31 05:29:33 | INFO | train_inner | epoch 128:    187 / 1475 loss=2.218, nll_loss=0.566, ppl=1.48, wps=59121.8, ups=15.55, wpb=3803, bsz=198.9, num_updates=187500, lr=7.30297e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=2752
2024-01-31 05:29:40 | INFO | train_inner | epoch 128:    287 / 1475 loss=2.227, nll_loss=0.576, ppl=1.49, wps=59179, ups=15.6, wpb=3794.2, bsz=195.8, num_updates=187600, lr=7.30102e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.9, wall=2758
2024-01-31 05:29:46 | INFO | train_inner | epoch 128:    387 / 1475 loss=2.218, nll_loss=0.567, ppl=1.48, wps=57725.7, ups=15.54, wpb=3715.1, bsz=203, num_updates=187700, lr=7.29908e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.6, wall=2765
2024-01-31 05:29:52 | INFO | train_inner | epoch 128:    487 / 1475 loss=2.235, nll_loss=0.586, ppl=1.5, wps=58839.4, ups=15.47, wpb=3804.5, bsz=193.8, num_updates=187800, lr=7.29713e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.8, wall=2771
2024-01-31 05:29:59 | INFO | train_inner | epoch 128:    587 / 1475 loss=2.237, nll_loss=0.587, ppl=1.5, wps=58342.1, ups=15.66, wpb=3726.1, bsz=195.8, num_updates=187900, lr=7.29519e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=2778
2024-01-31 05:30:05 | INFO | train_inner | epoch 128:    687 / 1475 loss=2.248, nll_loss=0.599, ppl=1.51, wps=58106.1, ups=15.7, wpb=3702, bsz=188.1, num_updates=188000, lr=7.29325e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.9, wall=2784
2024-01-31 05:30:12 | INFO | train_inner | epoch 128:    787 / 1475 loss=2.229, nll_loss=0.579, ppl=1.49, wps=59044.2, ups=15.56, wpb=3793.5, bsz=201.1, num_updates=188100, lr=7.29131e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=2791
2024-01-31 05:30:18 | INFO | train_inner | epoch 128:    887 / 1475 loss=2.231, nll_loss=0.581, ppl=1.5, wps=58079.8, ups=15.5, wpb=3748.3, bsz=197.8, num_updates=188200, lr=7.28937e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=2797
2024-01-31 05:30:25 | INFO | train_inner | epoch 128:    987 / 1475 loss=2.251, nll_loss=0.604, ppl=1.52, wps=58622, ups=15.54, wpb=3771.8, bsz=190.6, num_updates=188300, lr=7.28744e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.8, wall=2803
2024-01-31 05:30:31 | INFO | train_inner | epoch 128:   1087 / 1475 loss=2.239, nll_loss=0.591, ppl=1.51, wps=58970.4, ups=15.57, wpb=3787.3, bsz=201.4, num_updates=188400, lr=7.2855e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.7, wall=2810
2024-01-31 05:30:37 | INFO | train_inner | epoch 128:   1187 / 1475 loss=2.253, nll_loss=0.605, ppl=1.52, wps=60227.6, ups=15.57, wpb=3869.1, bsz=192.4, num_updates=188500, lr=7.28357e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=2816
2024-01-31 05:30:44 | INFO | train_inner | epoch 128:   1287 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=58336.7, ups=15.67, wpb=3723.9, bsz=193.7, num_updates=188600, lr=7.28164e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.7, wall=2823
2024-01-31 05:30:50 | INFO | train_inner | epoch 128:   1387 / 1475 loss=2.253, nll_loss=0.606, ppl=1.52, wps=57461, ups=15.56, wpb=3693, bsz=186.8, num_updates=188700, lr=7.27971e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=2829
2024-01-31 05:30:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:30:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:30:58 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 3.525 | nll_loss 1.922 | ppl 3.79 | wps 135277 | wpb 3189.1 | bsz 163.4 | num_updates 188788 | best_loss 3.385
2024-01-31 05:30:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 188788 updates
2024-01-31 05:30:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint128.pt
2024-01-31 05:30:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint128.pt
2024-01-31 05:31:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint128.pt (epoch 128 @ 188788 updates, score 3.525) (writing took 2.479047483066097 seconds)
2024-01-31 05:31:01 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2024-01-31 05:31:01 | INFO | train | epoch 128 | loss 2.237 | nll_loss 0.588 | ppl 1.5 | wps 55816.3 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 188788 | lr 7.27801e-05 | gnorm 0.973 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 2840
2024-01-31 05:31:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:31:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:31:01 | INFO | fairseq.trainer | begin training epoch 129
2024-01-31 05:31:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:31:02 | INFO | train_inner | epoch 129:     12 / 1475 loss=2.247, nll_loss=0.599, ppl=1.52, wps=33576.7, ups=8.81, wpb=3811.9, bsz=199.1, num_updates=188800, lr=7.27778e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.7, wall=2840
2024-01-31 05:31:08 | INFO | train_inner | epoch 129:    112 / 1475 loss=2.224, nll_loss=0.572, ppl=1.49, wps=59223.5, ups=15.62, wpb=3791.7, bsz=195.8, num_updates=188900, lr=7.27585e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.8, wall=2847
2024-01-31 05:31:14 | INFO | train_inner | epoch 129:    212 / 1475 loss=2.215, nll_loss=0.562, ppl=1.48, wps=57649.8, ups=15.61, wpb=3692.8, bsz=200.6, num_updates=189000, lr=7.27393e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.7, wall=2853
2024-01-31 05:31:21 | INFO | train_inner | epoch 129:    312 / 1475 loss=2.219, nll_loss=0.568, ppl=1.48, wps=57905.2, ups=15.5, wpb=3736.8, bsz=205.3, num_updates=189100, lr=7.27201e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.9, wall=2860
2024-01-31 05:31:27 | INFO | train_inner | epoch 129:    412 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=59645.8, ups=15.69, wpb=3800.9, bsz=181.7, num_updates=189200, lr=7.27008e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.8, wall=2866
2024-01-31 05:31:34 | INFO | train_inner | epoch 129:    512 / 1475 loss=2.229, nll_loss=0.578, ppl=1.49, wps=58266.1, ups=15.52, wpb=3754.8, bsz=201.3, num_updates=189300, lr=7.26816e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=2873
2024-01-31 05:31:40 | INFO | train_inner | epoch 129:    612 / 1475 loss=2.23, nll_loss=0.58, ppl=1.49, wps=58817.3, ups=15.48, wpb=3798.8, bsz=201, num_updates=189400, lr=7.26624e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=2879
2024-01-31 05:31:46 | INFO | train_inner | epoch 129:    712 / 1475 loss=2.232, nll_loss=0.582, ppl=1.5, wps=58352, ups=15.59, wpb=3743.5, bsz=204.2, num_updates=189500, lr=7.26433e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.6, wall=2885
2024-01-31 05:31:53 | INFO | train_inner | epoch 129:    812 / 1475 loss=2.234, nll_loss=0.585, ppl=1.5, wps=58794.8, ups=15.51, wpb=3789.8, bsz=197.7, num_updates=189600, lr=7.26241e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.7, wall=2892
2024-01-31 05:31:59 | INFO | train_inner | epoch 129:    912 / 1475 loss=2.233, nll_loss=0.584, ppl=1.5, wps=58057.3, ups=15.61, wpb=3718.6, bsz=198.2, num_updates=189700, lr=7.2605e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.9, wall=2898
2024-01-31 05:32:06 | INFO | train_inner | epoch 129:   1012 / 1475 loss=2.239, nll_loss=0.59, ppl=1.51, wps=58890.6, ups=15.62, wpb=3770.8, bsz=197.7, num_updates=189800, lr=7.25858e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=2905
2024-01-31 05:32:12 | INFO | train_inner | epoch 129:   1112 / 1475 loss=2.261, nll_loss=0.615, ppl=1.53, wps=59364.8, ups=15.61, wpb=3804.2, bsz=184, num_updates=189900, lr=7.25667e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.7, wall=2911
2024-01-31 05:32:19 | INFO | train_inner | epoch 129:   1212 / 1475 loss=2.25, nll_loss=0.603, ppl=1.52, wps=58476.9, ups=15.62, wpb=3742.7, bsz=193.3, num_updates=190000, lr=7.25476e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.8, wall=2917
2024-01-31 05:32:25 | INFO | train_inner | epoch 129:   1312 / 1475 loss=2.231, nll_loss=0.581, ppl=1.5, wps=59803.6, ups=15.52, wpb=3853.7, bsz=210.5, num_updates=190100, lr=7.25285e-05, gnorm=0.95, loss_scale=8, train_wall=6, gb_free=29.7, wall=2924
2024-01-31 05:32:31 | INFO | train_inner | epoch 129:   1412 / 1475 loss=2.257, nll_loss=0.611, ppl=1.53, wps=59269.3, ups=15.58, wpb=3804.9, bsz=186.7, num_updates=190200, lr=7.25095e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.6, wall=2930
2024-01-31 05:32:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:32:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:32:38 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 3.518 | nll_loss 1.915 | ppl 3.77 | wps 135493 | wpb 3189.1 | bsz 163.4 | num_updates 190263 | best_loss 3.385
2024-01-31 05:32:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 190263 updates
2024-01-31 05:32:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint129.pt
2024-01-31 05:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint129.pt
2024-01-31 05:32:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint129.pt (epoch 129 @ 190263 updates, score 3.518) (writing took 2.4301855550147593 seconds)
2024-01-31 05:32:40 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2024-01-31 05:32:40 | INFO | train | epoch 129 | loss 2.236 | nll_loss 0.587 | ppl 1.5 | wps 55862.4 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 190263 | lr 7.24975e-05 | gnorm 0.974 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 2939
2024-01-31 05:32:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:32:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:32:40 | INFO | fairseq.trainer | begin training epoch 130
2024-01-31 05:32:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:32:43 | INFO | train_inner | epoch 130:     37 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=33658.6, ups=8.85, wpb=3802.9, bsz=186.1, num_updates=190300, lr=7.24904e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.7, wall=2942
2024-01-31 05:32:49 | INFO | train_inner | epoch 130:    137 / 1475 loss=2.227, nll_loss=0.575, ppl=1.49, wps=57977.5, ups=15.55, wpb=3728.9, bsz=192.3, num_updates=190400, lr=7.24714e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=2948
2024-01-31 05:32:56 | INFO | train_inner | epoch 130:    237 / 1475 loss=2.217, nll_loss=0.565, ppl=1.48, wps=58149.3, ups=15.55, wpb=3739.9, bsz=193.9, num_updates=190500, lr=7.24524e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=2954
2024-01-31 05:33:02 | INFO | train_inner | epoch 130:    337 / 1475 loss=2.238, nll_loss=0.587, ppl=1.5, wps=59619.2, ups=15.68, wpb=3802.2, bsz=186.6, num_updates=190600, lr=7.24333e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=2961
2024-01-31 05:33:08 | INFO | train_inner | epoch 130:    437 / 1475 loss=2.224, nll_loss=0.573, ppl=1.49, wps=58630.4, ups=15.5, wpb=3782.1, bsz=197.6, num_updates=190700, lr=7.24144e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=2967
2024-01-31 05:33:15 | INFO | train_inner | epoch 130:    537 / 1475 loss=2.234, nll_loss=0.585, ppl=1.5, wps=60154.6, ups=15.63, wpb=3849.6, bsz=194.6, num_updates=190800, lr=7.23954e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=2974
2024-01-31 05:33:21 | INFO | train_inner | epoch 130:    637 / 1475 loss=2.223, nll_loss=0.572, ppl=1.49, wps=57817.4, ups=15.51, wpb=3728.2, bsz=199.7, num_updates=190900, lr=7.23764e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.7, wall=2980
2024-01-31 05:33:28 | INFO | train_inner | epoch 130:    737 / 1475 loss=2.223, nll_loss=0.573, ppl=1.49, wps=58692.7, ups=15.56, wpb=3772.2, bsz=203.2, num_updates=191000, lr=7.23575e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.8, wall=2987
2024-01-31 05:33:34 | INFO | train_inner | epoch 130:    837 / 1475 loss=2.25, nll_loss=0.602, ppl=1.52, wps=59604.8, ups=15.61, wpb=3818.3, bsz=190.4, num_updates=191100, lr=7.23385e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.7, wall=2993
2024-01-31 05:33:41 | INFO | train_inner | epoch 130:    937 / 1475 loss=2.233, nll_loss=0.583, ppl=1.5, wps=58453.2, ups=15.5, wpb=3771.7, bsz=197.6, num_updates=191200, lr=7.23196e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.6, wall=2999
2024-01-31 05:33:47 | INFO | train_inner | epoch 130:   1037 / 1475 loss=2.238, nll_loss=0.589, ppl=1.5, wps=57794.8, ups=15.67, wpb=3688.7, bsz=193.4, num_updates=191300, lr=7.23007e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=30.1, wall=3006
2024-01-31 05:33:53 | INFO | train_inner | epoch 130:   1137 / 1475 loss=2.229, nll_loss=0.58, ppl=1.49, wps=58978.7, ups=15.49, wpb=3806.8, bsz=208.5, num_updates=191400, lr=7.22818e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=3012
2024-01-31 05:34:00 | INFO | train_inner | epoch 130:   1237 / 1475 loss=2.245, nll_loss=0.598, ppl=1.51, wps=58304.5, ups=15.59, wpb=3738.8, bsz=194.2, num_updates=191500, lr=7.22629e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=3019
2024-01-31 05:34:06 | INFO | train_inner | epoch 130:   1337 / 1475 loss=2.245, nll_loss=0.597, ppl=1.51, wps=58726.6, ups=15.6, wpb=3765.3, bsz=195.8, num_updates=191600, lr=7.22441e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.7, wall=3025
2024-01-31 05:34:13 | INFO | train_inner | epoch 130:   1437 / 1475 loss=2.255, nll_loss=0.608, ppl=1.52, wps=59634.5, ups=15.63, wpb=3815.8, bsz=191.7, num_updates=191700, lr=7.22252e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.8, wall=3031
2024-01-31 05:34:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:34:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:34:17 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 3.523 | nll_loss 1.919 | ppl 3.78 | wps 135508 | wpb 3189.1 | bsz 163.4 | num_updates 191738 | best_loss 3.385
2024-01-31 05:34:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 191738 updates
2024-01-31 05:34:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint130.pt
2024-01-31 05:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint130.pt
2024-01-31 05:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint130.pt (epoch 130 @ 191738 updates, score 3.523) (writing took 2.5029522728873417 seconds)
2024-01-31 05:34:20 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2024-01-31 05:34:20 | INFO | train | epoch 130 | loss 2.234 | nll_loss 0.584 | ppl 1.5 | wps 55836.9 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 191738 | lr 7.22181e-05 | gnorm 0.971 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 3039
2024-01-31 05:34:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:34:20 | INFO | fairseq.trainer | begin training epoch 131
2024-01-31 05:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:34:24 | INFO | train_inner | epoch 131:     62 / 1475 loss=2.233, nll_loss=0.582, ppl=1.5, wps=32750.7, ups=8.83, wpb=3706.9, bsz=192, num_updates=191800, lr=7.22064e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=3043
2024-01-31 05:34:30 | INFO | train_inner | epoch 131:    162 / 1475 loss=2.209, nll_loss=0.557, ppl=1.47, wps=57913.1, ups=15.52, wpb=3730.9, bsz=201.7, num_updates=191900, lr=7.21876e-05, gnorm=0.94, loss_scale=8, train_wall=6, gb_free=29.7, wall=3049
2024-01-31 05:34:37 | INFO | train_inner | epoch 131:    262 / 1475 loss=2.223, nll_loss=0.572, ppl=1.49, wps=58782.5, ups=15.62, wpb=3762.6, bsz=200.7, num_updates=192000, lr=7.21688e-05, gnorm=0.954, loss_scale=8, train_wall=6, gb_free=29.7, wall=3056
2024-01-31 05:34:43 | INFO | train_inner | epoch 131:    362 / 1475 loss=2.226, nll_loss=0.576, ppl=1.49, wps=59648.6, ups=15.57, wpb=3831.8, bsz=198.6, num_updates=192100, lr=7.215e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.7, wall=3062
2024-01-31 05:34:50 | INFO | train_inner | epoch 131:    462 / 1475 loss=2.217, nll_loss=0.567, ppl=1.48, wps=58960.6, ups=15.51, wpb=3801.1, bsz=202.6, num_updates=192200, lr=7.21312e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=3069
2024-01-31 05:34:56 | INFO | train_inner | epoch 131:    562 / 1475 loss=2.235, nll_loss=0.585, ppl=1.5, wps=59129.8, ups=15.63, wpb=3782.4, bsz=193, num_updates=192300, lr=7.21125e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.8, wall=3075
2024-01-31 05:35:02 | INFO | train_inner | epoch 131:    662 / 1475 loss=2.228, nll_loss=0.578, ppl=1.49, wps=59343.1, ups=15.52, wpb=3823.1, bsz=198.2, num_updates=192400, lr=7.20937e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=3081
2024-01-31 05:35:09 | INFO | train_inner | epoch 131:    762 / 1475 loss=2.238, nll_loss=0.588, ppl=1.5, wps=58528.9, ups=15.67, wpb=3734.5, bsz=187.5, num_updates=192500, lr=7.2075e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=3088
2024-01-31 05:35:15 | INFO | train_inner | epoch 131:    862 / 1475 loss=2.232, nll_loss=0.583, ppl=1.5, wps=58600, ups=15.57, wpb=3763.9, bsz=200.1, num_updates=192600, lr=7.20563e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.7, wall=3094
2024-01-31 05:35:22 | INFO | train_inner | epoch 131:    962 / 1475 loss=2.22, nll_loss=0.57, ppl=1.48, wps=56861.4, ups=15.52, wpb=3663, bsz=201, num_updates=192700, lr=7.20376e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.9, wall=3101
2024-01-31 05:35:28 | INFO | train_inner | epoch 131:   1062 / 1475 loss=2.236, nll_loss=0.587, ppl=1.5, wps=59557.6, ups=15.56, wpb=3828.6, bsz=204.2, num_updates=192800, lr=7.20189e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.9, wall=3107
2024-01-31 05:35:35 | INFO | train_inner | epoch 131:   1162 / 1475 loss=2.242, nll_loss=0.594, ppl=1.51, wps=57911.9, ups=15.59, wpb=3713.6, bsz=190.2, num_updates=192900, lr=7.20002e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.6, wall=3113
2024-01-31 05:35:41 | INFO | train_inner | epoch 131:   1262 / 1475 loss=2.256, nll_loss=0.609, ppl=1.53, wps=59333.8, ups=15.57, wpb=3811, bsz=186.9, num_updates=193000, lr=7.19816e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.7, wall=3120
2024-01-31 05:35:47 | INFO | train_inner | epoch 131:   1362 / 1475 loss=2.256, nll_loss=0.609, ppl=1.53, wps=58899.7, ups=15.58, wpb=3780.3, bsz=177.6, num_updates=193100, lr=7.19629e-05, gnorm=1.036, loss_scale=8, train_wall=6, gb_free=29.8, wall=3126
2024-01-31 05:35:54 | INFO | train_inner | epoch 131:   1462 / 1475 loss=2.241, nll_loss=0.593, ppl=1.51, wps=59106.2, ups=15.48, wpb=3817.1, bsz=203.8, num_updates=193200, lr=7.19443e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=3133
2024-01-31 05:35:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:35:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:35:57 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 3.526 | nll_loss 1.924 | ppl 3.8 | wps 135453 | wpb 3189.1 | bsz 163.4 | num_updates 193213 | best_loss 3.385
2024-01-31 05:35:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 193213 updates
2024-01-31 05:35:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint131.pt
2024-01-31 05:35:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint131.pt
2024-01-31 05:35:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint131.pt (epoch 131 @ 193213 updates, score 3.526) (writing took 2.422467687050812 seconds)
2024-01-31 05:35:59 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2024-01-31 05:35:59 | INFO | train | epoch 131 | loss 2.233 | nll_loss 0.583 | ppl 1.5 | wps 55855.7 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 193213 | lr 7.19419e-05 | gnorm 0.973 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 3138
2024-01-31 05:35:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:35:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:35:59 | INFO | fairseq.trainer | begin training epoch 132
2024-01-31 05:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:36:05 | INFO | train_inner | epoch 132:     87 / 1475 loss=2.207, nll_loss=0.554, ppl=1.47, wps=33570.4, ups=8.81, wpb=3808.6, bsz=209.7, num_updates=193300, lr=7.19257e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=3144
2024-01-31 05:36:12 | INFO | train_inner | epoch 132:    187 / 1475 loss=2.21, nll_loss=0.557, ppl=1.47, wps=58645.1, ups=15.53, wpb=3777, bsz=201.6, num_updates=193400, lr=7.19071e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.7, wall=3151
2024-01-31 05:36:18 | INFO | train_inner | epoch 132:    287 / 1475 loss=2.223, nll_loss=0.573, ppl=1.49, wps=59411.9, ups=15.55, wpb=3820.1, bsz=197, num_updates=193500, lr=7.18885e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=3157
2024-01-31 05:36:24 | INFO | train_inner | epoch 132:    387 / 1475 loss=2.216, nll_loss=0.564, ppl=1.48, wps=58740.9, ups=15.62, wpb=3759.7, bsz=193.4, num_updates=193600, lr=7.18699e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=3163
2024-01-31 05:36:31 | INFO | train_inner | epoch 132:    487 / 1475 loss=2.242, nll_loss=0.593, ppl=1.51, wps=58064.1, ups=15.69, wpb=3701.1, bsz=181.1, num_updates=193700, lr=7.18514e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=3170
2024-01-31 05:36:37 | INFO | train_inner | epoch 132:    587 / 1475 loss=2.236, nll_loss=0.586, ppl=1.5, wps=59357.5, ups=15.62, wpb=3800.7, bsz=194.6, num_updates=193800, lr=7.18329e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=3176
2024-01-31 05:36:44 | INFO | train_inner | epoch 132:    687 / 1475 loss=2.237, nll_loss=0.588, ppl=1.5, wps=59646.5, ups=15.55, wpb=3835.7, bsz=190.2, num_updates=193900, lr=7.18143e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.7, wall=3183
2024-01-31 05:36:50 | INFO | train_inner | epoch 132:    787 / 1475 loss=2.232, nll_loss=0.582, ppl=1.5, wps=58096.9, ups=15.59, wpb=3725.4, bsz=191.1, num_updates=194000, lr=7.17958e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=3189
2024-01-31 05:36:57 | INFO | train_inner | epoch 132:    887 / 1475 loss=2.233, nll_loss=0.583, ppl=1.5, wps=59267.6, ups=15.56, wpb=3809.4, bsz=203.8, num_updates=194100, lr=7.17773e-05, gnorm=0.956, loss_scale=8, train_wall=6, gb_free=29.7, wall=3195
2024-01-31 05:37:03 | INFO | train_inner | epoch 132:    987 / 1475 loss=2.225, nll_loss=0.575, ppl=1.49, wps=58090.2, ups=15.56, wpb=3732.9, bsz=201.8, num_updates=194200, lr=7.17588e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.8, wall=3202
2024-01-31 05:37:09 | INFO | train_inner | epoch 132:   1087 / 1475 loss=2.24, nll_loss=0.592, ppl=1.51, wps=58094.9, ups=15.69, wpb=3701.6, bsz=190.8, num_updates=194300, lr=7.17404e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.7, wall=3208
2024-01-31 05:37:16 | INFO | train_inner | epoch 132:   1187 / 1475 loss=2.23, nll_loss=0.581, ppl=1.5, wps=59240.9, ups=15.46, wpb=3832.5, bsz=215.1, num_updates=194400, lr=7.17219e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.7, wall=3215
2024-01-31 05:37:22 | INFO | train_inner | epoch 132:   1287 / 1475 loss=2.241, nll_loss=0.593, ppl=1.51, wps=57797.5, ups=15.6, wpb=3704, bsz=188.8, num_updates=194500, lr=7.17035e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.7, wall=3221
2024-01-31 05:37:29 | INFO | train_inner | epoch 132:   1387 / 1475 loss=2.25, nll_loss=0.603, ppl=1.52, wps=59958.7, ups=15.55, wpb=3855.8, bsz=195.9, num_updates=194600, lr=7.1685e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.7, wall=3228
2024-01-31 05:37:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:37:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:37:36 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 3.522 | nll_loss 1.918 | ppl 3.78 | wps 134631 | wpb 3189.1 | bsz 163.4 | num_updates 194688 | best_loss 3.385
2024-01-31 05:37:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 194688 updates
2024-01-31 05:37:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint132.pt
2024-01-31 05:37:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint132.pt
2024-01-31 05:37:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint132.pt (epoch 132 @ 194688 updates, score 3.522) (writing took 2.4654990050476044 seconds)
2024-01-31 05:37:39 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2024-01-31 05:37:39 | INFO | train | epoch 132 | loss 2.231 | nll_loss 0.581 | ppl 1.5 | wps 55831.9 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 194688 | lr 7.16688e-05 | gnorm 0.972 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 3238
2024-01-31 05:37:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:37:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:37:39 | INFO | fairseq.trainer | begin training epoch 133
2024-01-31 05:37:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:37:40 | INFO | train_inner | epoch 133:     12 / 1475 loss=2.245, nll_loss=0.598, ppl=1.51, wps=32127.7, ups=8.82, wpb=3643.7, bsz=182.8, num_updates=194700, lr=7.16666e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.7, wall=3239
2024-01-31 05:37:46 | INFO | train_inner | epoch 133:    112 / 1475 loss=2.207, nll_loss=0.553, ppl=1.47, wps=58349.1, ups=15.58, wpb=3745.8, bsz=201, num_updates=194800, lr=7.16482e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.6, wall=3245
2024-01-31 05:37:53 | INFO | train_inner | epoch 133:    212 / 1475 loss=2.218, nll_loss=0.567, ppl=1.48, wps=59006.9, ups=15.51, wpb=3803.5, bsz=196.9, num_updates=194900, lr=7.16299e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.7, wall=3252
2024-01-31 05:37:59 | INFO | train_inner | epoch 133:    312 / 1475 loss=2.208, nll_loss=0.556, ppl=1.47, wps=56147.3, ups=15.21, wpb=3690.7, bsz=203, num_updates=195000, lr=7.16115e-05, gnorm=0.954, loss_scale=8, train_wall=6, gb_free=29.7, wall=3258
2024-01-31 05:38:06 | INFO | train_inner | epoch 133:    412 / 1475 loss=2.231, nll_loss=0.58, ppl=1.49, wps=59052.5, ups=15.66, wpb=3770, bsz=193.8, num_updates=195100, lr=7.15931e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=3265
2024-01-31 05:38:12 | INFO | train_inner | epoch 133:    512 / 1475 loss=2.221, nll_loss=0.57, ppl=1.48, wps=58340.7, ups=15.38, wpb=3793.8, bsz=198.6, num_updates=195200, lr=7.15748e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.8, wall=3271
2024-01-31 05:38:19 | INFO | train_inner | epoch 133:    612 / 1475 loss=2.226, nll_loss=0.576, ppl=1.49, wps=58746.5, ups=15.55, wpb=3777.9, bsz=195, num_updates=195300, lr=7.15565e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.7, wall=3278
2024-01-31 05:38:25 | INFO | train_inner | epoch 133:    712 / 1475 loss=2.223, nll_loss=0.572, ppl=1.49, wps=58451.3, ups=15.47, wpb=3778.2, bsz=199.8, num_updates=195400, lr=7.15382e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=3284
2024-01-31 05:38:32 | INFO | train_inner | epoch 133:    812 / 1475 loss=2.222, nll_loss=0.572, ppl=1.49, wps=59214.6, ups=15.57, wpb=3803, bsz=208.1, num_updates=195500, lr=7.15199e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.7, wall=3291
2024-01-31 05:38:38 | INFO | train_inner | epoch 133:    912 / 1475 loss=2.235, nll_loss=0.586, ppl=1.5, wps=59552.1, ups=15.57, wpb=3824.3, bsz=197.9, num_updates=195600, lr=7.15016e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.8, wall=3297
2024-01-31 05:38:44 | INFO | train_inner | epoch 133:   1012 / 1475 loss=2.229, nll_loss=0.58, ppl=1.49, wps=59516, ups=15.49, wpb=3842.6, bsz=199.6, num_updates=195700, lr=7.14833e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=3303
2024-01-31 05:38:51 | INFO | train_inner | epoch 133:   1112 / 1475 loss=2.239, nll_loss=0.591, ppl=1.51, wps=58170.8, ups=15.62, wpb=3723.6, bsz=190.6, num_updates=195800, lr=7.1465e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=3310
2024-01-31 05:38:57 | INFO | train_inner | epoch 133:   1212 / 1475 loss=2.254, nll_loss=0.607, ppl=1.52, wps=59171.5, ups=15.67, wpb=3776.4, bsz=182.6, num_updates=195900, lr=7.14468e-05, gnorm=1.088, loss_scale=8, train_wall=6, gb_free=29.6, wall=3316
2024-01-31 05:39:04 | INFO | train_inner | epoch 133:   1312 / 1475 loss=2.251, nll_loss=0.605, ppl=1.52, wps=59799.2, ups=15.62, wpb=3827.2, bsz=192.2, num_updates=196000, lr=7.14286e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.9, wall=3323
2024-01-31 05:39:10 | INFO | train_inner | epoch 133:   1412 / 1475 loss=2.239, nll_loss=0.591, ppl=1.51, wps=57920.5, ups=15.56, wpb=3723.3, bsz=188.6, num_updates=196100, lr=7.14104e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.6, wall=3329
2024-01-31 05:39:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:39:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:39:16 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 3.523 | nll_loss 1.92 | ppl 3.78 | wps 135537 | wpb 3189.1 | bsz 163.4 | num_updates 196163 | best_loss 3.385
2024-01-31 05:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 196163 updates
2024-01-31 05:39:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint133.pt
2024-01-31 05:39:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint133.pt
2024-01-31 05:39:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint133.pt (epoch 133 @ 196163 updates, score 3.523) (writing took 2.690951308934018 seconds)
2024-01-31 05:39:19 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2024-01-31 05:39:19 | INFO | train | epoch 133 | loss 2.23 | nll_loss 0.58 | ppl 1.49 | wps 55559.8 | ups 14.73 | wpb 3771.6 | bsz 195.9 | num_updates 196163 | lr 7.13989e-05 | gnorm 0.979 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 3338
2024-01-31 05:39:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:39:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:39:19 | INFO | fairseq.trainer | begin training epoch 134
2024-01-31 05:39:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:39:22 | INFO | train_inner | epoch 134:     37 / 1475 loss=2.229, nll_loss=0.579, ppl=1.49, wps=32574.6, ups=8.6, wpb=3789.1, bsz=198, num_updates=196200, lr=7.13922e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.8, wall=3341
2024-01-31 05:39:28 | INFO | train_inner | epoch 134:    137 / 1475 loss=2.21, nll_loss=0.557, ppl=1.47, wps=58406.7, ups=15.66, wpb=3728.6, bsz=194.9, num_updates=196300, lr=7.1374e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.7, wall=3347
2024-01-31 05:39:35 | INFO | train_inner | epoch 134:    237 / 1475 loss=2.221, nll_loss=0.57, ppl=1.48, wps=59491, ups=15.64, wpb=3804.4, bsz=195.6, num_updates=196400, lr=7.13558e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=3353
2024-01-31 05:39:41 | INFO | train_inner | epoch 134:    337 / 1475 loss=2.216, nll_loss=0.564, ppl=1.48, wps=58273.4, ups=15.61, wpb=3734, bsz=194.4, num_updates=196500, lr=7.13376e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.7, wall=3360
2024-01-31 05:39:47 | INFO | train_inner | epoch 134:    437 / 1475 loss=2.221, nll_loss=0.571, ppl=1.49, wps=57688.3, ups=15.62, wpb=3693, bsz=195.8, num_updates=196600, lr=7.13195e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.8, wall=3366
2024-01-31 05:39:54 | INFO | train_inner | epoch 134:    537 / 1475 loss=2.222, nll_loss=0.571, ppl=1.49, wps=58488.3, ups=15.52, wpb=3769.7, bsz=197.5, num_updates=196700, lr=7.13014e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.6, wall=3373
2024-01-31 05:40:00 | INFO | train_inner | epoch 134:    637 / 1475 loss=2.227, nll_loss=0.577, ppl=1.49, wps=59604.5, ups=15.55, wpb=3832.3, bsz=195.9, num_updates=196800, lr=7.12832e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.6, wall=3379
2024-01-31 05:40:07 | INFO | train_inner | epoch 134:    737 / 1475 loss=2.214, nll_loss=0.563, ppl=1.48, wps=59065.7, ups=15.64, wpb=3776.8, bsz=206.4, num_updates=196900, lr=7.12651e-05, gnorm=0.94, loss_scale=8, train_wall=6, gb_free=29.7, wall=3386
2024-01-31 05:40:13 | INFO | train_inner | epoch 134:    837 / 1475 loss=2.225, nll_loss=0.576, ppl=1.49, wps=56403.7, ups=15.6, wpb=3615.2, bsz=192.9, num_updates=197000, lr=7.1247e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=3392
2024-01-31 05:40:19 | INFO | train_inner | epoch 134:    937 / 1475 loss=2.239, nll_loss=0.59, ppl=1.51, wps=58946.9, ups=15.6, wpb=3778.6, bsz=188.2, num_updates=197100, lr=7.1229e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.6, wall=3398
2024-01-31 05:40:26 | INFO | train_inner | epoch 134:   1037 / 1475 loss=2.24, nll_loss=0.593, ppl=1.51, wps=58786.2, ups=15.51, wpb=3791.4, bsz=188.3, num_updates=197200, lr=7.12109e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=3405
2024-01-31 05:40:32 | INFO | train_inner | epoch 134:   1137 / 1475 loss=2.248, nll_loss=0.6, ppl=1.52, wps=59381.9, ups=15.57, wpb=3813.3, bsz=186.5, num_updates=197300, lr=7.11929e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.8, wall=3411
2024-01-31 05:40:39 | INFO | train_inner | epoch 134:   1237 / 1475 loss=2.25, nll_loss=0.603, ppl=1.52, wps=59565.3, ups=15.63, wpb=3810, bsz=186.6, num_updates=197400, lr=7.11748e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=3418
2024-01-31 05:40:45 | INFO | train_inner | epoch 134:   1337 / 1475 loss=2.226, nll_loss=0.577, ppl=1.49, wps=58698.3, ups=15.53, wpb=3780.8, bsz=208.8, num_updates=197500, lr=7.11568e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.8, wall=3424
2024-01-31 05:40:52 | INFO | train_inner | epoch 134:   1437 / 1475 loss=2.238, nll_loss=0.59, ppl=1.51, wps=59630.3, ups=15.47, wpb=3854.6, bsz=207.4, num_updates=197600, lr=7.11388e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.8, wall=3430
2024-01-31 05:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:40:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:40:56 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 3.53 | nll_loss 1.927 | ppl 3.8 | wps 135568 | wpb 3189.1 | bsz 163.4 | num_updates 197638 | best_loss 3.385
2024-01-31 05:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 197638 updates
2024-01-31 05:40:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint134.pt
2024-01-31 05:40:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint134.pt
2024-01-31 05:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint134.pt (epoch 134 @ 197638 updates, score 3.53) (writing took 2.507625943981111 seconds)
2024-01-31 05:40:59 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2024-01-31 05:40:59 | INFO | train | epoch 134 | loss 2.228 | nll_loss 0.578 | ppl 1.49 | wps 55806.2 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 197638 | lr 7.1132e-05 | gnorm 0.973 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 3438
2024-01-31 05:40:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:40:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:40:59 | INFO | fairseq.trainer | begin training epoch 135
2024-01-31 05:40:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:41:03 | INFO | train_inner | epoch 135:     62 / 1475 loss=2.214, nll_loss=0.562, ppl=1.48, wps=33009.4, ups=8.74, wpb=3775.2, bsz=197.5, num_updates=197700, lr=7.11208e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.6, wall=3442
2024-01-31 05:41:09 | INFO | train_inner | epoch 135:    162 / 1475 loss=2.215, nll_loss=0.563, ppl=1.48, wps=59498.5, ups=15.65, wpb=3802.5, bsz=194.4, num_updates=197800, lr=7.11028e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.7, wall=3448
2024-01-31 05:41:16 | INFO | train_inner | epoch 135:    262 / 1475 loss=2.225, nll_loss=0.574, ppl=1.49, wps=59233.1, ups=15.62, wpb=3793.2, bsz=188.2, num_updates=197900, lr=7.10849e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=3455
2024-01-31 05:41:22 | INFO | train_inner | epoch 135:    362 / 1475 loss=2.224, nll_loss=0.574, ppl=1.49, wps=59266.9, ups=15.58, wpb=3804.9, bsz=192.2, num_updates=198000, lr=7.10669e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=3461
2024-01-31 05:41:29 | INFO | train_inner | epoch 135:    462 / 1475 loss=2.218, nll_loss=0.567, ppl=1.48, wps=57648.8, ups=15.67, wpb=3677.8, bsz=198.9, num_updates=198100, lr=7.1049e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.8, wall=3468
2024-01-31 05:41:35 | INFO | train_inner | epoch 135:    562 / 1475 loss=2.213, nll_loss=0.562, ppl=1.48, wps=58570.5, ups=15.56, wpb=3764.5, bsz=201.3, num_updates=198200, lr=7.1031e-05, gnorm=0.95, loss_scale=8, train_wall=6, gb_free=29.6, wall=3474
2024-01-31 05:41:41 | INFO | train_inner | epoch 135:    662 / 1475 loss=2.236, nll_loss=0.587, ppl=1.5, wps=59409.2, ups=15.6, wpb=3807.6, bsz=187.3, num_updates=198300, lr=7.10131e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.7, wall=3480
2024-01-31 05:41:48 | INFO | train_inner | epoch 135:    762 / 1475 loss=2.226, nll_loss=0.576, ppl=1.49, wps=59754.6, ups=15.56, wpb=3839.6, bsz=200.2, num_updates=198400, lr=7.09952e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.8, wall=3487
2024-01-31 05:41:54 | INFO | train_inner | epoch 135:    862 / 1475 loss=2.234, nll_loss=0.584, ppl=1.5, wps=58957.1, ups=15.59, wpb=3782.8, bsz=191.1, num_updates=198500, lr=7.09773e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.7, wall=3493
2024-01-31 05:42:01 | INFO | train_inner | epoch 135:    962 / 1475 loss=2.217, nll_loss=0.567, ppl=1.48, wps=57119.7, ups=15.51, wpb=3683.7, bsz=200.3, num_updates=198600, lr=7.09595e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=3500
2024-01-31 05:42:07 | INFO | train_inner | epoch 135:   1062 / 1475 loss=2.231, nll_loss=0.581, ppl=1.5, wps=58867.1, ups=15.67, wpb=3757.8, bsz=196.8, num_updates=198700, lr=7.09416e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.7, wall=3506
2024-01-31 05:42:14 | INFO | train_inner | epoch 135:   1162 / 1475 loss=2.231, nll_loss=0.582, ppl=1.5, wps=58891.2, ups=15.53, wpb=3790.9, bsz=202.6, num_updates=198800, lr=7.09238e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=3512
2024-01-31 05:42:20 | INFO | train_inner | epoch 135:   1262 / 1475 loss=2.236, nll_loss=0.587, ppl=1.5, wps=58734.7, ups=15.56, wpb=3774.4, bsz=197.6, num_updates=198900, lr=7.09059e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.7, wall=3519
2024-01-31 05:42:26 | INFO | train_inner | epoch 135:   1362 / 1475 loss=2.238, nll_loss=0.589, ppl=1.5, wps=58418.3, ups=15.51, wpb=3766, bsz=192.8, num_updates=199000, lr=7.08881e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=3525
2024-01-31 05:42:33 | INFO | train_inner | epoch 135:   1462 / 1475 loss=2.236, nll_loss=0.588, ppl=1.5, wps=58222.3, ups=15.54, wpb=3747.1, bsz=193.9, num_updates=199100, lr=7.08703e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=30, wall=3532
2024-01-31 05:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:42:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:42:36 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 3.53 | nll_loss 1.928 | ppl 3.81 | wps 135088 | wpb 3189.1 | bsz 163.4 | num_updates 199113 | best_loss 3.385
2024-01-31 05:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 199113 updates
2024-01-31 05:42:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint135.pt
2024-01-31 05:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint135.pt
2024-01-31 05:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint135.pt (epoch 135 @ 199113 updates, score 3.53) (writing took 2.497961055021733 seconds)
2024-01-31 05:42:39 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2024-01-31 05:42:39 | INFO | train | epoch 135 | loss 2.226 | nll_loss 0.576 | ppl 1.49 | wps 55813 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 199113 | lr 7.0868e-05 | gnorm 0.978 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 3537
2024-01-31 05:42:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:42:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:42:39 | INFO | fairseq.trainer | begin training epoch 136
2024-01-31 05:42:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:42:44 | INFO | train_inner | epoch 136:     87 / 1475 loss=2.211, nll_loss=0.559, ppl=1.47, wps=32737.7, ups=8.77, wpb=3731.1, bsz=199.3, num_updates=199200, lr=7.08525e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.7, wall=3543
2024-01-31 05:42:51 | INFO | train_inner | epoch 136:    187 / 1475 loss=2.204, nll_loss=0.551, ppl=1.47, wps=59172.3, ups=15.52, wpb=3811.4, bsz=199.9, num_updates=199300, lr=7.08347e-05, gnorm=0.938, loss_scale=8, train_wall=6, gb_free=29.8, wall=3550
2024-01-31 05:42:57 | INFO | train_inner | epoch 136:    287 / 1475 loss=2.223, nll_loss=0.573, ppl=1.49, wps=59096.3, ups=15.58, wpb=3792.9, bsz=186.7, num_updates=199400, lr=7.0817e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=3556
2024-01-31 05:43:04 | INFO | train_inner | epoch 136:    387 / 1475 loss=2.22, nll_loss=0.569, ppl=1.48, wps=59095.1, ups=15.6, wpb=3787.9, bsz=194.1, num_updates=199500, lr=7.07992e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=3562
2024-01-31 05:43:10 | INFO | train_inner | epoch 136:    487 / 1475 loss=2.226, nll_loss=0.576, ppl=1.49, wps=58951.4, ups=15.53, wpb=3795.4, bsz=193.7, num_updates=199600, lr=7.07815e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.7, wall=3569
2024-01-31 05:43:16 | INFO | train_inner | epoch 136:    587 / 1475 loss=2.22, nll_loss=0.569, ppl=1.48, wps=58188.4, ups=15.63, wpb=3724, bsz=195.7, num_updates=199700, lr=7.07638e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.9, wall=3575
2024-01-31 05:43:23 | INFO | train_inner | epoch 136:    687 / 1475 loss=2.213, nll_loss=0.561, ppl=1.48, wps=59033.1, ups=15.56, wpb=3792.7, bsz=205, num_updates=199800, lr=7.07461e-05, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.7, wall=3582
2024-01-31 05:43:29 | INFO | train_inner | epoch 136:    787 / 1475 loss=2.228, nll_loss=0.579, ppl=1.49, wps=58932.4, ups=15.57, wpb=3785.5, bsz=193.3, num_updates=199900, lr=7.07284e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.7, wall=3588
2024-01-31 05:43:36 | INFO | train_inner | epoch 136:    887 / 1475 loss=2.219, nll_loss=0.569, ppl=1.48, wps=58904.9, ups=15.55, wpb=3789, bsz=203, num_updates=200000, lr=7.07107e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.8, wall=3595
2024-01-31 05:43:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:43:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:43:38 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 3.533 | nll_loss 1.933 | ppl 3.82 | wps 135528 | wpb 3189.1 | bsz 163.4 | num_updates 200000 | best_loss 3.385
2024-01-31 05:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 200000 updates
2024-01-31 05:43:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_136_200000.pt
2024-01-31 05:43:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_136_200000.pt
2024-01-31 05:43:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_136_200000.pt (epoch 136 @ 200000 updates, score 3.533) (writing took 2.4543014710070565 seconds)
2024-01-31 05:43:47 | INFO | train_inner | epoch 136:    987 / 1475 loss=2.229, nll_loss=0.58, ppl=1.49, wps=33421.5, ups=8.94, wpb=3737.4, bsz=199.1, num_updates=200100, lr=7.0693e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.7, wall=3606
2024-01-31 05:43:53 | INFO | train_inner | epoch 136:   1087 / 1475 loss=2.23, nll_loss=0.582, ppl=1.5, wps=58302.1, ups=15.49, wpb=3763.9, bsz=199, num_updates=200200, lr=7.06753e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.8, wall=3612
2024-01-31 05:44:00 | INFO | train_inner | epoch 136:   1187 / 1475 loss=2.231, nll_loss=0.582, ppl=1.5, wps=57924.3, ups=15.61, wpb=3711.7, bsz=200.8, num_updates=200300, lr=7.06577e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.9, wall=3619
2024-01-31 05:44:06 | INFO | train_inner | epoch 136:   1287 / 1475 loss=2.24, nll_loss=0.592, ppl=1.51, wps=59128.5, ups=15.54, wpb=3805.2, bsz=194.2, num_updates=200400, lr=7.06401e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.9, wall=3625
2024-01-31 05:44:13 | INFO | train_inner | epoch 136:   1387 / 1475 loss=2.245, nll_loss=0.598, ppl=1.51, wps=58364.4, ups=15.62, wpb=3737.7, bsz=188.8, num_updates=200500, lr=7.06225e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.6, wall=3631
2024-01-31 05:44:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:44:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:44:20 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 3.522 | nll_loss 1.923 | ppl 3.79 | wps 132685 | wpb 3189.1 | bsz 163.4 | num_updates 200588 | best_loss 3.385
2024-01-31 05:44:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 200588 updates
2024-01-31 05:44:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint136.pt
2024-01-31 05:44:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint136.pt
2024-01-31 05:44:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint136.pt (epoch 136 @ 200588 updates, score 3.522) (writing took 2.444827357074246 seconds)
2024-01-31 05:44:23 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2024-01-31 05:44:23 | INFO | train | epoch 136 | loss 2.225 | nll_loss 0.575 | ppl 1.49 | wps 53264.9 | ups 14.12 | wpb 3771.6 | bsz 195.9 | num_updates 200588 | lr 7.0607e-05 | gnorm 0.976 | loss_scale 8 | train_wall 91 | gb_free 29.6 | wall 3642
2024-01-31 05:44:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:44:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:44:23 | INFO | fairseq.trainer | begin training epoch 137
2024-01-31 05:44:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:44:24 | INFO | train_inner | epoch 137:     12 / 1475 loss=2.231, nll_loss=0.582, ppl=1.5, wps=33598, ups=8.8, wpb=3817, bsz=197.7, num_updates=200600, lr=7.06049e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.8, wall=3643
2024-01-31 05:44:30 | INFO | train_inner | epoch 137:    112 / 1475 loss=2.201, nll_loss=0.548, ppl=1.46, wps=58200, ups=15.6, wpb=3731.2, bsz=195.7, num_updates=200700, lr=7.05873e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.6, wall=3649
2024-01-31 05:44:37 | INFO | train_inner | epoch 137:    212 / 1475 loss=2.229, nll_loss=0.578, ppl=1.49, wps=58899, ups=15.68, wpb=3757.3, bsz=180.4, num_updates=200800, lr=7.05697e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=3656
2024-01-31 05:44:43 | INFO | train_inner | epoch 137:    312 / 1475 loss=2.217, nll_loss=0.565, ppl=1.48, wps=58142.7, ups=15.51, wpb=3747.8, bsz=190.3, num_updates=200900, lr=7.05521e-05, gnorm=0.998, loss_scale=16, train_wall=6, gb_free=29.8, wall=3662
2024-01-31 05:44:50 | INFO | train_inner | epoch 137:    412 / 1475 loss=2.214, nll_loss=0.563, ppl=1.48, wps=59724.3, ups=15.44, wpb=3868.9, bsz=204.6, num_updates=201000, lr=7.05346e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=3669
2024-01-31 05:44:56 | INFO | train_inner | epoch 137:    512 / 1475 loss=2.215, nll_loss=0.563, ppl=1.48, wps=59385.9, ups=15.55, wpb=3819.3, bsz=202.6, num_updates=201100, lr=7.0517e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.8, wall=3675
2024-01-31 05:45:02 | INFO | train_inner | epoch 137:    612 / 1475 loss=2.217, nll_loss=0.567, ppl=1.48, wps=59179.9, ups=15.61, wpb=3791.9, bsz=203, num_updates=201200, lr=7.04995e-05, gnorm=0.965, loss_scale=16, train_wall=6, gb_free=29.7, wall=3681
2024-01-31 05:45:09 | INFO | train_inner | epoch 137:    712 / 1475 loss=2.222, nll_loss=0.572, ppl=1.49, wps=59414.2, ups=15.62, wpb=3804.2, bsz=198.2, num_updates=201300, lr=7.0482e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.7, wall=3688
2024-01-31 05:45:15 | INFO | train_inner | epoch 137:    812 / 1475 loss=2.227, nll_loss=0.576, ppl=1.49, wps=57918.8, ups=15.64, wpb=3702.3, bsz=193, num_updates=201400, lr=7.04645e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.7, wall=3694
2024-01-31 05:45:22 | INFO | train_inner | epoch 137:    912 / 1475 loss=2.213, nll_loss=0.562, ppl=1.48, wps=58433, ups=15.46, wpb=3779.2, bsz=208.2, num_updates=201500, lr=7.0447e-05, gnorm=0.961, loss_scale=16, train_wall=6, gb_free=29.7, wall=3701
2024-01-31 05:45:28 | INFO | train_inner | epoch 137:   1012 / 1475 loss=2.223, nll_loss=0.573, ppl=1.49, wps=58244.7, ups=15.56, wpb=3744.4, bsz=198.7, num_updates=201600, lr=7.04295e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.7, wall=3707
2024-01-31 05:45:35 | INFO | train_inner | epoch 137:   1112 / 1475 loss=2.232, nll_loss=0.582, ppl=1.5, wps=58758.5, ups=15.65, wpb=3755.1, bsz=200.1, num_updates=201700, lr=7.04121e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.6, wall=3713
2024-01-31 05:45:41 | INFO | train_inner | epoch 137:   1212 / 1475 loss=2.236, nll_loss=0.588, ppl=1.5, wps=58999.6, ups=15.56, wpb=3792.8, bsz=192.8, num_updates=201800, lr=7.03946e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=3720
2024-01-31 05:45:47 | INFO | train_inner | epoch 137:   1312 / 1475 loss=2.243, nll_loss=0.595, ppl=1.51, wps=58337.1, ups=15.6, wpb=3739.2, bsz=182.4, num_updates=201900, lr=7.03772e-05, gnorm=1.008, loss_scale=16, train_wall=6, gb_free=29.7, wall=3726
2024-01-31 05:45:54 | INFO | train_inner | epoch 137:   1412 / 1475 loss=2.236, nll_loss=0.588, ppl=1.5, wps=58306.7, ups=15.49, wpb=3763.7, bsz=196.2, num_updates=202000, lr=7.03598e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.7, wall=3733
2024-01-31 05:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:45:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:46:00 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 3.524 | nll_loss 1.927 | ppl 3.8 | wps 134346 | wpb 3189.1 | bsz 163.4 | num_updates 202063 | best_loss 3.385
2024-01-31 05:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 202063 updates
2024-01-31 05:46:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint137.pt
2024-01-31 05:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint137.pt
2024-01-31 05:46:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint137.pt (epoch 137 @ 202063 updates, score 3.524) (writing took 13.81595189194195 seconds)
2024-01-31 05:46:14 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2024-01-31 05:46:14 | INFO | train | epoch 137 | loss 2.224 | nll_loss 0.573 | ppl 1.49 | wps 50091.9 | ups 13.28 | wpb 3771.6 | bsz 195.9 | num_updates 202063 | lr 7.03488e-05 | gnorm 0.978 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3753
2024-01-31 05:46:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:46:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:46:14 | INFO | fairseq.trainer | begin training epoch 138
2024-01-31 05:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:46:17 | INFO | train_inner | epoch 138:     37 / 1475 loss=2.234, nll_loss=0.585, ppl=1.5, wps=16661.6, ups=4.41, wpb=3781.9, bsz=183.8, num_updates=202100, lr=7.03423e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.8, wall=3755
2024-01-31 05:46:23 | INFO | train_inner | epoch 138:    137 / 1475 loss=2.204, nll_loss=0.55, ppl=1.46, wps=58140, ups=15.49, wpb=3752.7, bsz=195.4, num_updates=202200, lr=7.03249e-05, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.7, wall=3762
2024-01-31 05:46:29 | INFO | train_inner | epoch 138:    237 / 1475 loss=2.206, nll_loss=0.554, ppl=1.47, wps=58318.4, ups=15.55, wpb=3750.1, bsz=195.8, num_updates=202300, lr=7.03076e-05, gnorm=1.003, loss_scale=16, train_wall=6, gb_free=29.8, wall=3768
2024-01-31 05:46:36 | INFO | train_inner | epoch 138:    337 / 1475 loss=2.21, nll_loss=0.558, ppl=1.47, wps=58940.3, ups=15.6, wpb=3779, bsz=193.1, num_updates=202400, lr=7.02902e-05, gnorm=0.961, loss_scale=16, train_wall=6, gb_free=29.7, wall=3775
2024-01-31 05:46:42 | INFO | train_inner | epoch 138:    437 / 1475 loss=2.222, nll_loss=0.572, ppl=1.49, wps=59423.8, ups=15.53, wpb=3825.8, bsz=193, num_updates=202500, lr=7.02728e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.8, wall=3781
2024-01-31 05:46:49 | INFO | train_inner | epoch 138:    537 / 1475 loss=2.215, nll_loss=0.564, ppl=1.48, wps=57987.8, ups=15.6, wpb=3717.6, bsz=196.2, num_updates=202600, lr=7.02555e-05, gnorm=0.968, loss_scale=16, train_wall=6, gb_free=29.8, wall=3788
2024-01-31 05:46:55 | INFO | train_inner | epoch 138:    637 / 1475 loss=2.215, nll_loss=0.564, ppl=1.48, wps=57685.8, ups=15.42, wpb=3739.8, bsz=200.7, num_updates=202700, lr=7.02382e-05, gnorm=0.968, loss_scale=16, train_wall=6, gb_free=29.7, wall=3794
2024-01-31 05:47:02 | INFO | train_inner | epoch 138:    737 / 1475 loss=2.226, nll_loss=0.577, ppl=1.49, wps=58048.3, ups=15.62, wpb=3715.6, bsz=195, num_updates=202800, lr=7.02208e-05, gnorm=0.996, loss_scale=16, train_wall=6, gb_free=29.7, wall=3800
2024-01-31 05:47:08 | INFO | train_inner | epoch 138:    837 / 1475 loss=2.214, nll_loss=0.563, ppl=1.48, wps=58679, ups=15.54, wpb=3776.3, bsz=202.7, num_updates=202900, lr=7.02035e-05, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.7, wall=3807
2024-01-31 05:47:14 | INFO | train_inner | epoch 138:    937 / 1475 loss=2.223, nll_loss=0.572, ppl=1.49, wps=59025.4, ups=15.53, wpb=3801.5, bsz=194.7, num_updates=203000, lr=7.01862e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=29.8, wall=3813
2024-01-31 05:47:21 | INFO | train_inner | epoch 138:   1037 / 1475 loss=2.224, nll_loss=0.574, ppl=1.49, wps=57827.7, ups=15.33, wpb=3773.2, bsz=197.8, num_updates=203100, lr=7.0169e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.8, wall=3820
2024-01-31 05:47:27 | INFO | train_inner | epoch 138:   1137 / 1475 loss=2.239, nll_loss=0.591, ppl=1.51, wps=58888.5, ups=15.63, wpb=3767.6, bsz=193, num_updates=203200, lr=7.01517e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.7, wall=3826
2024-01-31 05:47:34 | INFO | train_inner | epoch 138:   1237 / 1475 loss=2.237, nll_loss=0.588, ppl=1.5, wps=59067.9, ups=15.6, wpb=3785.4, bsz=192.7, num_updates=203300, lr=7.01344e-05, gnorm=0.987, loss_scale=16, train_wall=6, gb_free=29.7, wall=3833
2024-01-31 05:47:40 | INFO | train_inner | epoch 138:   1337 / 1475 loss=2.233, nll_loss=0.585, ppl=1.5, wps=59448.7, ups=15.55, wpb=3823.2, bsz=197, num_updates=203400, lr=7.01172e-05, gnorm=0.977, loss_scale=16, train_wall=6, gb_free=29.9, wall=3839
2024-01-31 05:47:47 | INFO | train_inner | epoch 138:   1437 / 1475 loss=2.231, nll_loss=0.582, ppl=1.5, wps=58875.2, ups=15.63, wpb=3765.8, bsz=199.3, num_updates=203500, lr=7.01e-05, gnorm=1.033, loss_scale=16, train_wall=6, gb_free=29.7, wall=3846
2024-01-31 05:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:47:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:47:51 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 3.528 | nll_loss 1.927 | ppl 3.8 | wps 133214 | wpb 3189.1 | bsz 163.4 | num_updates 203538 | best_loss 3.385
2024-01-31 05:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 203538 updates
2024-01-31 05:47:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint138.pt
2024-01-31 05:47:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint138.pt
2024-01-31 05:47:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint138.pt (epoch 138 @ 203538 updates, score 3.528) (writing took 2.4723952579079196 seconds)
2024-01-31 05:47:54 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2024-01-31 05:47:54 | INFO | train | epoch 138 | loss 2.221 | nll_loss 0.571 | ppl 1.49 | wps 55724.1 | ups 14.77 | wpb 3771.6 | bsz 195.9 | num_updates 203538 | lr 7.00934e-05 | gnorm 0.981 | loss_scale 16 | train_wall 92 | gb_free 29.8 | wall 3853
2024-01-31 05:47:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:47:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:47:54 | INFO | fairseq.trainer | begin training epoch 139
2024-01-31 05:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:47:58 | INFO | train_inner | epoch 139:     62 / 1475 loss=2.214, nll_loss=0.562, ppl=1.48, wps=33417.7, ups=8.76, wpb=3813, bsz=193, num_updates=203600, lr=7.00827e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=3857
2024-01-31 05:48:05 | INFO | train_inner | epoch 139:    162 / 1475 loss=2.201, nll_loss=0.548, ppl=1.46, wps=57046.1, ups=15.22, wpb=3747, bsz=200.2, num_updates=203700, lr=7.00655e-05, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.7, wall=3864
2024-01-31 05:48:11 | INFO | train_inner | epoch 139:    262 / 1475 loss=2.199, nll_loss=0.546, ppl=1.46, wps=58848.2, ups=15.51, wpb=3793.5, bsz=199.8, num_updates=203800, lr=7.00484e-05, gnorm=0.935, loss_scale=16, train_wall=6, gb_free=29.7, wall=3870
2024-01-31 05:48:17 | INFO | train_inner | epoch 139:    362 / 1475 loss=2.215, nll_loss=0.564, ppl=1.48, wps=58190.8, ups=15.57, wpb=3738.2, bsz=195, num_updates=203900, lr=7.00312e-05, gnorm=0.989, loss_scale=16, train_wall=6, gb_free=29.6, wall=3876
2024-01-31 05:48:24 | INFO | train_inner | epoch 139:    462 / 1475 loss=2.213, nll_loss=0.562, ppl=1.48, wps=58197.8, ups=15.42, wpb=3774.4, bsz=201.8, num_updates=204000, lr=7.0014e-05, gnorm=0.971, loss_scale=16, train_wall=6, gb_free=29.7, wall=3883
2024-01-31 05:48:30 | INFO | train_inner | epoch 139:    562 / 1475 loss=2.223, nll_loss=0.572, ppl=1.49, wps=59009.9, ups=15.69, wpb=3760.8, bsz=192.3, num_updates=204100, lr=6.99969e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.7, wall=3889
2024-01-31 05:48:37 | INFO | train_inner | epoch 139:    662 / 1475 loss=2.216, nll_loss=0.565, ppl=1.48, wps=59117.3, ups=15.66, wpb=3775.1, bsz=194.2, num_updates=204200, lr=6.99797e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.7, wall=3896
2024-01-31 05:48:43 | INFO | train_inner | epoch 139:    762 / 1475 loss=2.219, nll_loss=0.568, ppl=1.48, wps=59282.1, ups=15.54, wpb=3815.1, bsz=200.6, num_updates=204300, lr=6.99626e-05, gnorm=0.964, loss_scale=16, train_wall=6, gb_free=29.7, wall=3902
2024-01-31 05:48:50 | INFO | train_inner | epoch 139:    862 / 1475 loss=2.21, nll_loss=0.559, ppl=1.47, wps=58512.6, ups=15.49, wpb=3778.4, bsz=214.3, num_updates=204400, lr=6.99455e-05, gnorm=0.966, loss_scale=16, train_wall=6, gb_free=29.7, wall=3909
2024-01-31 05:49:14 | INFO | train_inner | epoch 139:    962 / 1475 loss=2.226, nll_loss=0.576, ppl=1.49, wps=15378, ups=4.15, wpb=3708.4, bsz=191.4, num_updates=204500, lr=6.99284e-05, gnorm=1.01, loss_scale=16, train_wall=10, gb_free=29.7, wall=3933
2024-01-31 05:49:20 | INFO | train_inner | epoch 139:   1062 / 1475 loss=2.23, nll_loss=0.581, ppl=1.5, wps=57390, ups=15.28, wpb=3756.5, bsz=191.9, num_updates=204600, lr=6.99113e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.7, wall=3939
2024-01-31 05:49:27 | INFO | train_inner | epoch 139:   1162 / 1475 loss=2.222, nll_loss=0.571, ppl=1.49, wps=59371.3, ups=15.59, wpb=3807.3, bsz=201.5, num_updates=204700, lr=6.98942e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.7, wall=3946
2024-01-31 05:49:33 | INFO | train_inner | epoch 139:   1262 / 1475 loss=2.243, nll_loss=0.596, ppl=1.51, wps=58616.3, ups=15.72, wpb=3729.3, bsz=182.2, num_updates=204800, lr=6.98771e-05, gnorm=1.019, loss_scale=16, train_wall=6, gb_free=29.7, wall=3952
2024-01-31 05:49:39 | INFO | train_inner | epoch 139:   1362 / 1475 loss=2.243, nll_loss=0.596, ppl=1.51, wps=59749.3, ups=15.63, wpb=3821.9, bsz=191.4, num_updates=204900, lr=6.98601e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.7, wall=3958
2024-01-31 05:49:46 | INFO | train_inner | epoch 139:   1462 / 1475 loss=2.235, nll_loss=0.586, ppl=1.5, wps=59264.7, ups=15.66, wpb=3784.4, bsz=188.6, num_updates=205000, lr=6.9843e-05, gnorm=0.985, loss_scale=16, train_wall=6, gb_free=29.9, wall=3965
2024-01-31 05:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:49:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:49:49 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 3.533 | nll_loss 1.935 | ppl 3.82 | wps 135179 | wpb 3189.1 | bsz 163.4 | num_updates 205013 | best_loss 3.385
2024-01-31 05:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 205013 updates
2024-01-31 05:49:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint139.pt
2024-01-31 05:49:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint139.pt
2024-01-31 05:49:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint139.pt (epoch 139 @ 205013 updates, score 3.533) (writing took 2.6276788290124387 seconds)
2024-01-31 05:49:52 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2024-01-31 05:49:52 | INFO | train | epoch 139 | loss 2.22 | nll_loss 0.57 | ppl 1.48 | wps 47041.8 | ups 12.47 | wpb 3771.6 | bsz 195.9 | num_updates 205013 | lr 6.98408e-05 | gnorm 0.978 | loss_scale 16 | train_wall 95 | gb_free 29.7 | wall 3971
2024-01-31 05:49:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:49:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:49:52 | INFO | fairseq.trainer | begin training epoch 140
2024-01-31 05:49:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:49:58 | INFO | train_inner | epoch 140:     87 / 1475 loss=2.194, nll_loss=0.54, ppl=1.45, wps=30985.6, ups=8.3, wpb=3731.6, bsz=202, num_updates=205100, lr=6.9826e-05, gnorm=0.967, loss_scale=16, train_wall=6, gb_free=29.8, wall=3977
2024-01-31 05:50:04 | INFO | train_inner | epoch 140:    187 / 1475 loss=2.202, nll_loss=0.549, ppl=1.46, wps=59458.7, ups=15.59, wpb=3814, bsz=197.8, num_updates=205200, lr=6.9809e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.7, wall=3983
2024-01-31 05:50:11 | INFO | train_inner | epoch 140:    287 / 1475 loss=2.211, nll_loss=0.559, ppl=1.47, wps=58756.2, ups=15.61, wpb=3764.2, bsz=199, num_updates=205300, lr=6.9792e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.7, wall=3990
2024-01-31 05:50:17 | INFO | train_inner | epoch 140:    387 / 1475 loss=2.209, nll_loss=0.557, ppl=1.47, wps=56696.7, ups=15.66, wpb=3620.7, bsz=194, num_updates=205400, lr=6.9775e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.7, wall=3996
2024-01-31 05:50:23 | INFO | train_inner | epoch 140:    487 / 1475 loss=2.223, nll_loss=0.574, ppl=1.49, wps=58625, ups=15.65, wpb=3745.3, bsz=181.8, num_updates=205500, lr=6.9758e-05, gnorm=1.021, loss_scale=16, train_wall=6, gb_free=29.8, wall=4002
2024-01-31 05:50:30 | INFO | train_inner | epoch 140:    587 / 1475 loss=2.218, nll_loss=0.567, ppl=1.48, wps=59033.8, ups=15.54, wpb=3798.9, bsz=197.3, num_updates=205600, lr=6.9741e-05, gnorm=0.967, loss_scale=16, train_wall=6, gb_free=29.6, wall=4009
2024-01-31 05:50:36 | INFO | train_inner | epoch 140:    687 / 1475 loss=2.205, nll_loss=0.552, ppl=1.47, wps=58698.6, ups=15.49, wpb=3790.2, bsz=208.9, num_updates=205700, lr=6.97241e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.7, wall=4015
2024-01-31 05:50:43 | INFO | train_inner | epoch 140:    787 / 1475 loss=2.207, nll_loss=0.556, ppl=1.47, wps=58892.6, ups=15.51, wpb=3796.7, bsz=213, num_updates=205800, lr=6.97071e-05, gnorm=0.945, loss_scale=16, train_wall=6, gb_free=29.7, wall=4022
2024-01-31 05:50:49 | INFO | train_inner | epoch 140:    887 / 1475 loss=2.237, nll_loss=0.588, ppl=1.5, wps=60487.9, ups=15.69, wpb=3854.5, bsz=186.2, num_updates=205900, lr=6.96902e-05, gnorm=0.987, loss_scale=16, train_wall=6, gb_free=29.7, wall=4028
2024-01-31 05:50:56 | INFO | train_inner | epoch 140:    987 / 1475 loss=2.236, nll_loss=0.588, ppl=1.5, wps=59324.7, ups=15.59, wpb=3805.4, bsz=188.6, num_updates=206000, lr=6.96733e-05, gnorm=0.985, loss_scale=16, train_wall=6, gb_free=29.7, wall=4034
2024-01-31 05:51:02 | INFO | train_inner | epoch 140:   1087 / 1475 loss=2.229, nll_loss=0.579, ppl=1.49, wps=58671.8, ups=15.68, wpb=3742.4, bsz=187.8, num_updates=206100, lr=6.96564e-05, gnorm=1.005, loss_scale=16, train_wall=6, gb_free=29.7, wall=4041
2024-01-31 05:51:08 | INFO | train_inner | epoch 140:   1187 / 1475 loss=2.23, nll_loss=0.581, ppl=1.5, wps=58676.8, ups=15.62, wpb=3755.6, bsz=193.1, num_updates=206200, lr=6.96395e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.9, wall=4047
2024-01-31 05:51:15 | INFO | train_inner | epoch 140:   1287 / 1475 loss=2.238, nll_loss=0.59, ppl=1.5, wps=59765.5, ups=15.66, wpb=3815.5, bsz=190.6, num_updates=206300, lr=6.96226e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.7, wall=4054
2024-01-31 05:51:21 | INFO | train_inner | epoch 140:   1387 / 1475 loss=2.227, nll_loss=0.577, ppl=1.49, wps=58732.5, ups=15.59, wpb=3766.2, bsz=193.3, num_updates=206400, lr=6.96058e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=4060
2024-01-31 05:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:51:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:51:29 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 3.535 | nll_loss 1.938 | ppl 3.83 | wps 135424 | wpb 3189.1 | bsz 163.4 | num_updates 206488 | best_loss 3.385
2024-01-31 05:51:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 206488 updates
2024-01-31 05:51:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint140.pt
2024-01-31 05:51:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint140.pt
2024-01-31 05:51:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint140.pt (epoch 140 @ 206488 updates, score 3.535) (writing took 2.4190536830574274 seconds)
2024-01-31 05:51:32 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2024-01-31 05:51:32 | INFO | train | epoch 140 | loss 2.219 | nll_loss 0.568 | ppl 1.48 | wps 55979.9 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 206488 | lr 6.95909e-05 | gnorm 0.979 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4070
2024-01-31 05:51:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:51:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:51:32 | INFO | fairseq.trainer | begin training epoch 141
2024-01-31 05:51:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:51:32 | INFO | train_inner | epoch 141:     12 / 1475 loss=2.213, nll_loss=0.563, ppl=1.48, wps=33542.4, ups=8.88, wpb=3775.3, bsz=208.7, num_updates=206500, lr=6.95889e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.7, wall=4071
2024-01-31 05:51:39 | INFO | train_inner | epoch 141:    112 / 1475 loss=2.194, nll_loss=0.54, ppl=1.45, wps=58716.8, ups=15.59, wpb=3766.8, bsz=200.6, num_updates=206600, lr=6.95721e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.7, wall=4078
2024-01-31 05:51:45 | INFO | train_inner | epoch 141:    212 / 1475 loss=2.211, nll_loss=0.559, ppl=1.47, wps=58900.6, ups=15.61, wpb=3774.5, bsz=193, num_updates=206700, lr=6.95552e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.7, wall=4084
2024-01-31 05:51:52 | INFO | train_inner | epoch 141:    312 / 1475 loss=2.209, nll_loss=0.557, ppl=1.47, wps=58851.7, ups=15.61, wpb=3770.2, bsz=199, num_updates=206800, lr=6.95384e-05, gnorm=0.967, loss_scale=16, train_wall=6, gb_free=29.7, wall=4091
2024-01-31 05:51:58 | INFO | train_inner | epoch 141:    412 / 1475 loss=2.211, nll_loss=0.559, ppl=1.47, wps=58336.7, ups=15.64, wpb=3730.5, bsz=192.7, num_updates=206900, lr=6.95216e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=4097
2024-01-31 05:52:04 | INFO | train_inner | epoch 141:    512 / 1475 loss=2.21, nll_loss=0.558, ppl=1.47, wps=59212, ups=15.57, wpb=3803.4, bsz=209.7, num_updates=207000, lr=6.95048e-05, gnorm=0.972, loss_scale=16, train_wall=6, gb_free=29.6, wall=4103
2024-01-31 05:52:11 | INFO | train_inner | epoch 141:    612 / 1475 loss=2.219, nll_loss=0.57, ppl=1.48, wps=58796.2, ups=15.61, wpb=3765.5, bsz=187, num_updates=207100, lr=6.9488e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.6, wall=4110
2024-01-31 05:52:17 | INFO | train_inner | epoch 141:    712 / 1475 loss=2.223, nll_loss=0.573, ppl=1.49, wps=59118.4, ups=15.58, wpb=3793.9, bsz=190.6, num_updates=207200, lr=6.94713e-05, gnorm=0.967, loss_scale=16, train_wall=6, gb_free=29.7, wall=4116
2024-01-31 05:52:28 | INFO | train_inner | epoch 141:    812 / 1475 loss=2.219, nll_loss=0.568, ppl=1.48, wps=34664, ups=9.36, wpb=3702.6, bsz=191.2, num_updates=207300, lr=6.94545e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.7, wall=4127
2024-01-31 05:52:34 | INFO | train_inner | epoch 141:    912 / 1475 loss=2.22, nll_loss=0.569, ppl=1.48, wps=57720.4, ups=15.58, wpb=3705.3, bsz=196.4, num_updates=207400, lr=6.94377e-05, gnorm=1, loss_scale=16, train_wall=6, gb_free=29.7, wall=4133
2024-01-31 05:52:41 | INFO | train_inner | epoch 141:   1012 / 1475 loss=2.228, nll_loss=0.579, ppl=1.49, wps=59112.9, ups=15.51, wpb=3811.4, bsz=189.7, num_updates=207500, lr=6.9421e-05, gnorm=0.982, loss_scale=16, train_wall=6, gb_free=29.7, wall=4140
2024-01-31 05:52:47 | INFO | train_inner | epoch 141:   1112 / 1475 loss=2.216, nll_loss=0.565, ppl=1.48, wps=60507.4, ups=15.55, wpb=3892, bsz=212.2, num_updates=207600, lr=6.94043e-05, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.7, wall=4146
2024-01-31 05:52:54 | INFO | train_inner | epoch 141:   1212 / 1475 loss=2.227, nll_loss=0.579, ppl=1.49, wps=57075.3, ups=15.55, wpb=3669.4, bsz=193.4, num_updates=207700, lr=6.93876e-05, gnorm=1.06, loss_scale=16, train_wall=6, gb_free=29.8, wall=4153
2024-01-31 05:53:00 | INFO | train_inner | epoch 141:   1312 / 1475 loss=2.226, nll_loss=0.577, ppl=1.49, wps=59303.3, ups=15.56, wpb=3811.5, bsz=199.2, num_updates=207800, lr=6.93709e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.8, wall=4159
2024-01-31 05:53:07 | INFO | train_inner | epoch 141:   1412 / 1475 loss=2.228, nll_loss=0.58, ppl=1.49, wps=59166.2, ups=15.58, wpb=3797.4, bsz=194.7, num_updates=207900, lr=6.93542e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.9, wall=4165
2024-01-31 05:53:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:53:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:53:13 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 3.535 | nll_loss 1.936 | ppl 3.83 | wps 135115 | wpb 3189.1 | bsz 163.4 | num_updates 207963 | best_loss 3.385
2024-01-31 05:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 207963 updates
2024-01-31 05:53:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint141.pt
2024-01-31 05:53:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint141.pt
2024-01-31 05:53:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint141.pt (epoch 141 @ 207963 updates, score 3.535) (writing took 2.414568627020344 seconds)
2024-01-31 05:53:15 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2024-01-31 05:53:15 | INFO | train | epoch 141 | loss 2.218 | nll_loss 0.568 | ppl 1.48 | wps 53608.6 | ups 14.21 | wpb 3771.6 | bsz 195.9 | num_updates 207963 | lr 6.93437e-05 | gnorm 0.986 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 4174
2024-01-31 05:53:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:53:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:53:15 | INFO | fairseq.trainer | begin training epoch 142
2024-01-31 05:53:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:53:18 | INFO | train_inner | epoch 142:     37 / 1475 loss=2.231, nll_loss=0.582, ppl=1.5, wps=33460.4, ups=8.9, wpb=3760.4, bsz=185.8, num_updates=208000, lr=6.93375e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.7, wall=4177
2024-01-31 05:53:24 | INFO | train_inner | epoch 142:    137 / 1475 loss=2.195, nll_loss=0.541, ppl=1.46, wps=58701.3, ups=15.58, wpb=3768.3, bsz=202.4, num_updates=208100, lr=6.93209e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=4183
2024-01-31 05:53:31 | INFO | train_inner | epoch 142:    237 / 1475 loss=2.202, nll_loss=0.549, ppl=1.46, wps=58640.4, ups=15.54, wpb=3773.1, bsz=195.6, num_updates=208200, lr=6.93042e-05, gnorm=0.955, loss_scale=16, train_wall=6, gb_free=29.7, wall=4190
2024-01-31 05:53:37 | INFO | train_inner | epoch 142:    337 / 1475 loss=2.21, nll_loss=0.558, ppl=1.47, wps=59130.1, ups=15.58, wpb=3794.4, bsz=204.5, num_updates=208300, lr=6.92876e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.7, wall=4196
2024-01-31 05:53:43 | INFO | train_inner | epoch 142:    437 / 1475 loss=2.205, nll_loss=0.553, ppl=1.47, wps=57141.7, ups=15.55, wpb=3675.3, bsz=195.2, num_updates=208400, lr=6.92709e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.7, wall=4202
2024-01-31 05:53:50 | INFO | train_inner | epoch 142:    537 / 1475 loss=2.199, nll_loss=0.546, ppl=1.46, wps=58496.6, ups=15.58, wpb=3754.9, bsz=207.6, num_updates=208500, lr=6.92543e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.8, wall=4209
2024-01-31 05:53:56 | INFO | train_inner | epoch 142:    637 / 1475 loss=2.219, nll_loss=0.569, ppl=1.48, wps=59657.9, ups=15.63, wpb=3817.9, bsz=194, num_updates=208600, lr=6.92377e-05, gnorm=0.965, loss_scale=16, train_wall=6, gb_free=29.7, wall=4215
2024-01-31 05:54:03 | INFO | train_inner | epoch 142:    737 / 1475 loss=2.214, nll_loss=0.563, ppl=1.48, wps=58584.6, ups=15.6, wpb=3754.6, bsz=194.2, num_updates=208700, lr=6.92211e-05, gnorm=0.986, loss_scale=16, train_wall=6, gb_free=29.7, wall=4222
2024-01-31 05:54:09 | INFO | train_inner | epoch 142:    837 / 1475 loss=2.216, nll_loss=0.565, ppl=1.48, wps=59397.7, ups=15.58, wpb=3812.3, bsz=199.4, num_updates=208800, lr=6.92046e-05, gnorm=0.97, loss_scale=16, train_wall=6, gb_free=29.8, wall=4228
2024-01-31 05:54:16 | INFO | train_inner | epoch 142:    937 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=57453.9, ups=15.64, wpb=3673.3, bsz=195.3, num_updates=208900, lr=6.9188e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.7, wall=4234
2024-01-31 05:54:22 | INFO | train_inner | epoch 142:   1037 / 1475 loss=2.228, nll_loss=0.579, ppl=1.49, wps=60014.1, ups=15.55, wpb=3858.3, bsz=190.9, num_updates=209000, lr=6.91714e-05, gnorm=1.023, loss_scale=16, train_wall=6, gb_free=29.8, wall=4241
2024-01-31 05:54:28 | INFO | train_inner | epoch 142:   1137 / 1475 loss=2.238, nll_loss=0.589, ppl=1.5, wps=59456.2, ups=15.6, wpb=3810.5, bsz=185.3, num_updates=209100, lr=6.91549e-05, gnorm=0.996, loss_scale=16, train_wall=6, gb_free=29.8, wall=4247
2024-01-31 05:54:35 | INFO | train_inner | epoch 142:   1237 / 1475 loss=2.233, nll_loss=0.585, ppl=1.5, wps=58371.6, ups=15.68, wpb=3723.4, bsz=188.6, num_updates=209200, lr=6.91384e-05, gnorm=1.012, loss_scale=16, train_wall=6, gb_free=29.7, wall=4254
2024-01-31 05:54:41 | INFO | train_inner | epoch 142:   1337 / 1475 loss=2.223, nll_loss=0.575, ppl=1.49, wps=57288, ups=15.51, wpb=3694.4, bsz=194.3, num_updates=209300, lr=6.91219e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.7, wall=4260
2024-01-31 05:54:48 | INFO | train_inner | epoch 142:   1437 / 1475 loss=2.228, nll_loss=0.579, ppl=1.49, wps=60232, ups=15.56, wpb=3870.9, bsz=197.4, num_updates=209400, lr=6.91053e-05, gnorm=0.982, loss_scale=16, train_wall=6, gb_free=29.6, wall=4267
2024-01-31 05:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:54:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:54:52 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 3.532 | nll_loss 1.932 | ppl 3.82 | wps 135066 | wpb 3189.1 | bsz 163.4 | num_updates 209438 | best_loss 3.385
2024-01-31 05:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 209438 updates
2024-01-31 05:54:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint142.pt
2024-01-31 05:54:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint142.pt
2024-01-31 05:54:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint142.pt (epoch 142 @ 209438 updates, score 3.532) (writing took 2.5639855089830235 seconds)
2024-01-31 05:54:55 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2024-01-31 05:54:55 | INFO | train | epoch 142 | loss 2.216 | nll_loss 0.565 | ppl 1.48 | wps 55865.3 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 209438 | lr 6.90991e-05 | gnorm 0.983 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4274
2024-01-31 05:54:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:54:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:54:55 | INFO | fairseq.trainer | begin training epoch 143
2024-01-31 05:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:54:59 | INFO | train_inner | epoch 143:     62 / 1475 loss=2.196, nll_loss=0.543, ppl=1.46, wps=33155.5, ups=8.81, wpb=3763.5, bsz=199.8, num_updates=209500, lr=6.90889e-05, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.8, wall=4278
2024-01-31 05:55:05 | INFO | train_inner | epoch 143:    162 / 1475 loss=2.195, nll_loss=0.541, ppl=1.45, wps=58736.5, ups=15.59, wpb=3766.9, bsz=196.2, num_updates=209600, lr=6.90724e-05, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.7, wall=4284
2024-01-31 05:55:12 | INFO | train_inner | epoch 143:    262 / 1475 loss=2.2, nll_loss=0.547, ppl=1.46, wps=58170.3, ups=15.53, wpb=3746.5, bsz=198.9, num_updates=209700, lr=6.90559e-05, gnorm=0.965, loss_scale=16, train_wall=6, gb_free=29.7, wall=4291
2024-01-31 05:55:18 | INFO | train_inner | epoch 143:    362 / 1475 loss=2.209, nll_loss=0.557, ppl=1.47, wps=58845.5, ups=15.62, wpb=3768.2, bsz=194.6, num_updates=209800, lr=6.90394e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.8, wall=4297
2024-01-31 05:55:25 | INFO | train_inner | epoch 143:    462 / 1475 loss=2.223, nll_loss=0.572, ppl=1.49, wps=59656.1, ups=15.63, wpb=3818, bsz=183.8, num_updates=209900, lr=6.9023e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.8, wall=4304
2024-01-31 05:55:31 | INFO | train_inner | epoch 143:    562 / 1475 loss=2.224, nll_loss=0.574, ppl=1.49, wps=59427.3, ups=15.62, wpb=3804.4, bsz=191.8, num_updates=210000, lr=6.90066e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.8, wall=4310
2024-01-31 05:55:37 | INFO | train_inner | epoch 143:    662 / 1475 loss=2.21, nll_loss=0.559, ppl=1.47, wps=58881, ups=15.65, wpb=3763.4, bsz=197.8, num_updates=210100, lr=6.89901e-05, gnorm=1.009, loss_scale=16, train_wall=6, gb_free=29.8, wall=4316
2024-01-31 05:55:44 | INFO | train_inner | epoch 143:    762 / 1475 loss=2.205, nll_loss=0.554, ppl=1.47, wps=57764.1, ups=15.5, wpb=3726.7, bsz=203.8, num_updates=210200, lr=6.89737e-05, gnorm=0.96, loss_scale=16, train_wall=6, gb_free=29.9, wall=4323
2024-01-31 05:55:50 | INFO | train_inner | epoch 143:    862 / 1475 loss=2.22, nll_loss=0.569, ppl=1.48, wps=60093.9, ups=15.58, wpb=3857.5, bsz=191.3, num_updates=210300, lr=6.89573e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.7, wall=4329
2024-01-31 05:55:57 | INFO | train_inner | epoch 143:    962 / 1475 loss=2.21, nll_loss=0.559, ppl=1.47, wps=58872.3, ups=15.54, wpb=3787.9, bsz=200.1, num_updates=210400, lr=6.89409e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=29.7, wall=4336
2024-01-31 05:56:03 | INFO | train_inner | epoch 143:   1062 / 1475 loss=2.215, nll_loss=0.565, ppl=1.48, wps=57727.5, ups=15.6, wpb=3701.4, bsz=197.5, num_updates=210500, lr=6.89246e-05, gnorm=1.013, loss_scale=16, train_wall=6, gb_free=29.7, wall=4342
2024-01-31 05:56:10 | INFO | train_inner | epoch 143:   1162 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=58356.4, ups=15.68, wpb=3722, bsz=196.6, num_updates=210600, lr=6.89082e-05, gnorm=0.994, loss_scale=16, train_wall=6, gb_free=29.6, wall=4348
2024-01-31 05:56:16 | INFO | train_inner | epoch 143:   1262 / 1475 loss=2.236, nll_loss=0.588, ppl=1.5, wps=60107.8, ups=15.57, wpb=3859.5, bsz=194.5, num_updates=210700, lr=6.88918e-05, gnorm=0.988, loss_scale=16, train_wall=6, gb_free=29.7, wall=4355
2024-01-31 05:56:22 | INFO | train_inner | epoch 143:   1362 / 1475 loss=2.225, nll_loss=0.577, ppl=1.49, wps=59014, ups=15.58, wpb=3787.9, bsz=194.6, num_updates=210800, lr=6.88755e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.7, wall=4361
2024-01-31 05:56:29 | INFO | train_inner | epoch 143:   1462 / 1475 loss=2.223, nll_loss=0.574, ppl=1.49, wps=58141, ups=15.52, wpb=3746.9, bsz=202.4, num_updates=210900, lr=6.88592e-05, gnorm=1.006, loss_scale=16, train_wall=6, gb_free=29.7, wall=4368
2024-01-31 05:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:56:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:56:32 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 3.536 | nll_loss 1.94 | ppl 3.84 | wps 133880 | wpb 3189.1 | bsz 163.4 | num_updates 210913 | best_loss 3.385
2024-01-31 05:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 210913 updates
2024-01-31 05:56:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint143.pt
2024-01-31 05:56:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint143.pt
2024-01-31 05:56:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint143.pt (epoch 143 @ 210913 updates, score 3.536) (writing took 2.4619176640408114 seconds)
2024-01-31 05:56:34 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2024-01-31 05:56:34 | INFO | train | epoch 143 | loss 2.214 | nll_loss 0.564 | ppl 1.48 | wps 55923.2 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 210913 | lr 6.8857e-05 | gnorm 0.983 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4373
2024-01-31 05:56:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:56:34 | INFO | fairseq.trainer | begin training epoch 144
2024-01-31 05:56:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:56:40 | INFO | train_inner | epoch 144:     87 / 1475 loss=2.198, nll_loss=0.544, ppl=1.46, wps=33550.8, ups=8.88, wpb=3777.2, bsz=197.6, num_updates=211000, lr=6.88428e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.7, wall=4379
2024-01-31 05:56:47 | INFO | train_inner | epoch 144:    187 / 1475 loss=2.203, nll_loss=0.551, ppl=1.46, wps=57450.7, ups=15.56, wpb=3692.8, bsz=191.7, num_updates=211100, lr=6.88265e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=4385
2024-01-31 05:56:53 | INFO | train_inner | epoch 144:    287 / 1475 loss=2.199, nll_loss=0.545, ppl=1.46, wps=57781.1, ups=15.61, wpb=3702.7, bsz=194.4, num_updates=211200, lr=6.88102e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=4392
2024-01-31 05:56:59 | INFO | train_inner | epoch 144:    387 / 1475 loss=2.208, nll_loss=0.557, ppl=1.47, wps=58552.3, ups=15.61, wpb=3751.9, bsz=192.4, num_updates=211300, lr=6.8794e-05, gnorm=1.013, loss_scale=16, train_wall=6, gb_free=29.7, wall=4398
2024-01-31 05:57:06 | INFO | train_inner | epoch 144:    487 / 1475 loss=2.211, nll_loss=0.56, ppl=1.47, wps=59866.1, ups=15.63, wpb=3831.2, bsz=198.6, num_updates=211400, lr=6.87777e-05, gnorm=0.964, loss_scale=16, train_wall=6, gb_free=29.7, wall=4405
2024-01-31 05:57:12 | INFO | train_inner | epoch 144:    587 / 1475 loss=2.212, nll_loss=0.561, ppl=1.47, wps=58779.4, ups=15.61, wpb=3765.1, bsz=195.2, num_updates=211500, lr=6.87614e-05, gnorm=0.977, loss_scale=16, train_wall=6, gb_free=29.8, wall=4411
2024-01-31 05:57:19 | INFO | train_inner | epoch 144:    687 / 1475 loss=2.211, nll_loss=0.56, ppl=1.47, wps=59179.5, ups=15.57, wpb=3801.6, bsz=194.4, num_updates=211600, lr=6.87452e-05, gnorm=0.994, loss_scale=16, train_wall=6, gb_free=29.7, wall=4417
2024-01-31 05:57:25 | INFO | train_inner | epoch 144:    787 / 1475 loss=2.212, nll_loss=0.561, ppl=1.47, wps=58466.4, ups=15.57, wpb=3754.1, bsz=196.8, num_updates=211700, lr=6.87289e-05, gnorm=0.972, loss_scale=16, train_wall=6, gb_free=29.7, wall=4424
2024-01-31 05:57:31 | INFO | train_inner | epoch 144:    887 / 1475 loss=2.202, nll_loss=0.551, ppl=1.46, wps=58207.3, ups=15.52, wpb=3750.9, bsz=210.4, num_updates=211800, lr=6.87127e-05, gnorm=0.963, loss_scale=16, train_wall=6, gb_free=29.7, wall=4430
2024-01-31 05:57:38 | INFO | train_inner | epoch 144:    987 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=59741.9, ups=15.58, wpb=3834, bsz=198.2, num_updates=211900, lr=6.86965e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.6, wall=4437
2024-01-31 05:57:44 | INFO | train_inner | epoch 144:   1087 / 1475 loss=2.216, nll_loss=0.565, ppl=1.48, wps=58314.2, ups=15.63, wpb=3731.2, bsz=196.8, num_updates=212000, lr=6.86803e-05, gnorm=0.986, loss_scale=16, train_wall=6, gb_free=29.7, wall=4443
2024-01-31 05:57:51 | INFO | train_inner | epoch 144:   1187 / 1475 loss=2.225, nll_loss=0.576, ppl=1.49, wps=58787.8, ups=15.55, wpb=3780.2, bsz=188.7, num_updates=212100, lr=6.86641e-05, gnorm=0.986, loss_scale=16, train_wall=6, gb_free=29.7, wall=4450
2024-01-31 05:57:57 | INFO | train_inner | epoch 144:   1287 / 1475 loss=2.233, nll_loss=0.585, ppl=1.5, wps=59345.3, ups=15.68, wpb=3784.9, bsz=189.5, num_updates=212200, lr=6.86479e-05, gnorm=1, loss_scale=16, train_wall=6, gb_free=29.8, wall=4456
2024-01-31 05:58:03 | INFO | train_inner | epoch 144:   1387 / 1475 loss=2.231, nll_loss=0.583, ppl=1.5, wps=58896.2, ups=15.53, wpb=3793.2, bsz=191.8, num_updates=212300, lr=6.86317e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=4462
2024-01-31 05:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:58:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:58:11 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 3.537 | nll_loss 1.94 | ppl 3.84 | wps 134611 | wpb 3189.1 | bsz 163.4 | num_updates 212388 | best_loss 3.385
2024-01-31 05:58:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 212388 updates
2024-01-31 05:58:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint144.pt
2024-01-31 05:58:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint144.pt
2024-01-31 05:58:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint144.pt (epoch 144 @ 212388 updates, score 3.537) (writing took 2.3651978969573975 seconds)
2024-01-31 05:58:14 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2024-01-31 05:58:14 | INFO | train | epoch 144 | loss 2.213 | nll_loss 0.562 | ppl 1.48 | wps 55957.4 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 212388 | lr 6.86175e-05 | gnorm 0.984 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4473
2024-01-31 05:58:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:58:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:58:14 | INFO | fairseq.trainer | begin training epoch 145
2024-01-31 05:58:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:58:15 | INFO | train_inner | epoch 145:     12 / 1475 loss=2.217, nll_loss=0.567, ppl=1.48, wps=34092.5, ups=8.93, wpb=3818.2, bsz=204, num_updates=212400, lr=6.86156e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.7, wall=4474
2024-01-31 05:58:21 | INFO | train_inner | epoch 145:    112 / 1475 loss=2.201, nll_loss=0.547, ppl=1.46, wps=59858.2, ups=15.55, wpb=3848.2, bsz=197.5, num_updates=212500, lr=6.85994e-05, gnorm=0.945, loss_scale=16, train_wall=6, gb_free=29.8, wall=4480
2024-01-31 05:58:28 | INFO | train_inner | epoch 145:    212 / 1475 loss=2.208, nll_loss=0.556, ppl=1.47, wps=59445.9, ups=15.62, wpb=3806.2, bsz=192.6, num_updates=212600, lr=6.85833e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.8, wall=4486
2024-01-31 05:58:34 | INFO | train_inner | epoch 145:    312 / 1475 loss=2.201, nll_loss=0.548, ppl=1.46, wps=59078.8, ups=15.65, wpb=3775.5, bsz=194.8, num_updates=212700, lr=6.85672e-05, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.7, wall=4493
2024-01-31 05:58:40 | INFO | train_inner | epoch 145:    412 / 1475 loss=2.197, nll_loss=0.544, ppl=1.46, wps=59075.6, ups=15.49, wpb=3814, bsz=203.9, num_updates=212800, lr=6.85511e-05, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.7, wall=4499
2024-01-31 05:58:47 | INFO | train_inner | epoch 145:    512 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=59112, ups=15.58, wpb=3793.8, bsz=206.2, num_updates=212900, lr=6.8535e-05, gnorm=0.962, loss_scale=16, train_wall=6, gb_free=29.7, wall=4506
2024-01-31 05:58:53 | INFO | train_inner | epoch 145:    612 / 1475 loss=2.205, nll_loss=0.554, ppl=1.47, wps=58032.7, ups=15.55, wpb=3731.4, bsz=194.1, num_updates=213000, lr=6.85189e-05, gnorm=0.987, loss_scale=16, train_wall=6, gb_free=29.7, wall=4512
2024-01-31 05:59:00 | INFO | train_inner | epoch 145:    712 / 1475 loss=2.214, nll_loss=0.564, ppl=1.48, wps=58285.9, ups=15.57, wpb=3744, bsz=194.5, num_updates=213100, lr=6.85028e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.8, wall=4519
2024-01-31 05:59:06 | INFO | train_inner | epoch 145:    812 / 1475 loss=2.221, nll_loss=0.57, ppl=1.48, wps=58992.9, ups=15.6, wpb=3781.9, bsz=186.5, num_updates=213200, lr=6.84867e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=4525
2024-01-31 05:59:12 | INFO | train_inner | epoch 145:    912 / 1475 loss=2.218, nll_loss=0.568, ppl=1.48, wps=58531.2, ups=15.56, wpb=3760.6, bsz=192.4, num_updates=213300, lr=6.84707e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.7, wall=4531
2024-01-31 05:59:19 | INFO | train_inner | epoch 145:   1012 / 1475 loss=2.221, nll_loss=0.572, ppl=1.49, wps=59281.9, ups=15.64, wpb=3789.6, bsz=193.5, num_updates=213400, lr=6.84546e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.6, wall=4538
2024-01-31 05:59:25 | INFO | train_inner | epoch 145:   1112 / 1475 loss=2.221, nll_loss=0.571, ppl=1.49, wps=59109.2, ups=15.57, wpb=3797, bsz=198.1, num_updates=213500, lr=6.84386e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.7, wall=4544
2024-01-31 05:59:32 | INFO | train_inner | epoch 145:   1212 / 1475 loss=2.221, nll_loss=0.571, ppl=1.49, wps=58203.7, ups=15.66, wpb=3716.6, bsz=186.6, num_updates=213600, lr=6.84226e-05, gnorm=1.011, loss_scale=16, train_wall=6, gb_free=29.7, wall=4551
2024-01-31 05:59:38 | INFO | train_inner | epoch 145:   1312 / 1475 loss=2.218, nll_loss=0.568, ppl=1.48, wps=57579, ups=15.55, wpb=3702.6, bsz=198.5, num_updates=213700, lr=6.84066e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.7, wall=4557
2024-01-31 05:59:45 | INFO | train_inner | epoch 145:   1412 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=57706.8, ups=15.57, wpb=3707, bsz=198.9, num_updates=213800, lr=6.83906e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.8, wall=4563
2024-01-31 05:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 05:59:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:59:51 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 3.541 | nll_loss 1.94 | ppl 3.84 | wps 128317 | wpb 3189.1 | bsz 163.4 | num_updates 213863 | best_loss 3.385
2024-01-31 05:59:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 213863 updates
2024-01-31 05:59:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint145.pt
2024-01-31 05:59:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint145.pt
2024-01-31 05:59:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint145.pt (epoch 145 @ 213863 updates, score 3.541) (writing took 2.471629495965317 seconds)
2024-01-31 05:59:53 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2024-01-31 05:59:53 | INFO | train | epoch 145 | loss 2.211 | nll_loss 0.56 | ppl 1.47 | wps 55818.3 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 213863 | lr 6.83805e-05 | gnorm 0.983 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4572
2024-01-31 05:59:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 05:59:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 05:59:53 | INFO | fairseq.trainer | begin training epoch 146
2024-01-31 05:59:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 05:59:56 | INFO | train_inner | epoch 146:     37 / 1475 loss=2.204, nll_loss=0.553, ppl=1.47, wps=32827.4, ups=8.77, wpb=3742, bsz=196, num_updates=213900, lr=6.83746e-05, gnorm=0.955, loss_scale=16, train_wall=6, gb_free=29.8, wall=4575
2024-01-31 06:00:02 | INFO | train_inner | epoch 146:    137 / 1475 loss=2.188, nll_loss=0.533, ppl=1.45, wps=57884.7, ups=15.6, wpb=3711.2, bsz=193.1, num_updates=214000, lr=6.83586e-05, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.7, wall=4581
2024-01-31 06:00:09 | INFO | train_inner | epoch 146:    237 / 1475 loss=2.201, nll_loss=0.549, ppl=1.46, wps=58796.4, ups=15.59, wpb=3772.2, bsz=198.3, num_updates=214100, lr=6.83426e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.7, wall=4588
2024-01-31 06:00:15 | INFO | train_inner | epoch 146:    337 / 1475 loss=2.196, nll_loss=0.543, ppl=1.46, wps=59339.8, ups=15.52, wpb=3824.5, bsz=194.6, num_updates=214200, lr=6.83267e-05, gnorm=0.987, loss_scale=16, train_wall=6, gb_free=29.7, wall=4594
2024-01-31 06:00:22 | INFO | train_inner | epoch 146:    437 / 1475 loss=2.193, nll_loss=0.54, ppl=1.45, wps=57065.2, ups=15.48, wpb=3686.4, bsz=206.2, num_updates=214300, lr=6.83107e-05, gnorm=0.96, loss_scale=16, train_wall=6, gb_free=29.6, wall=4601
2024-01-31 06:00:28 | INFO | train_inner | epoch 146:    537 / 1475 loss=2.201, nll_loss=0.55, ppl=1.46, wps=57951.4, ups=15.62, wpb=3709.1, bsz=196.4, num_updates=214400, lr=6.82948e-05, gnorm=0.974, loss_scale=16, train_wall=6, gb_free=29.7, wall=4607
2024-01-31 06:00:34 | INFO | train_inner | epoch 146:    637 / 1475 loss=2.22, nll_loss=0.569, ppl=1.48, wps=59198.2, ups=15.64, wpb=3785.1, bsz=189.4, num_updates=214500, lr=6.82789e-05, gnorm=0.996, loss_scale=16, train_wall=6, gb_free=29.7, wall=4613
2024-01-31 06:00:41 | INFO | train_inner | epoch 146:    737 / 1475 loss=2.221, nll_loss=0.571, ppl=1.49, wps=59786.1, ups=15.61, wpb=3829.9, bsz=187.6, num_updates=214600, lr=6.8263e-05, gnorm=0.978, loss_scale=16, train_wall=6, gb_free=29.9, wall=4620
2024-01-31 06:00:47 | INFO | train_inner | epoch 146:    837 / 1475 loss=2.224, nll_loss=0.574, ppl=1.49, wps=58772.5, ups=15.63, wpb=3760.4, bsz=192.4, num_updates=214700, lr=6.82471e-05, gnorm=1.026, loss_scale=16, train_wall=6, gb_free=29.7, wall=4626
2024-01-31 06:00:54 | INFO | train_inner | epoch 146:    937 / 1475 loss=2.214, nll_loss=0.563, ppl=1.48, wps=58881, ups=15.54, wpb=3789.8, bsz=202, num_updates=214800, lr=6.82312e-05, gnorm=0.977, loss_scale=16, train_wall=6, gb_free=29.6, wall=4633
2024-01-31 06:01:00 | INFO | train_inner | epoch 146:   1037 / 1475 loss=2.216, nll_loss=0.567, ppl=1.48, wps=58772.3, ups=15.59, wpb=3771, bsz=192.7, num_updates=214900, lr=6.82153e-05, gnorm=1.012, loss_scale=16, train_wall=6, gb_free=29.6, wall=4639
2024-01-31 06:01:07 | INFO | train_inner | epoch 146:   1137 / 1475 loss=2.223, nll_loss=0.574, ppl=1.49, wps=59236.2, ups=15.53, wpb=3814.8, bsz=198.1, num_updates=215000, lr=6.81994e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.6, wall=4645
2024-01-31 06:01:13 | INFO | train_inner | epoch 146:   1237 / 1475 loss=2.207, nll_loss=0.556, ppl=1.47, wps=59799.8, ups=15.55, wpb=3844.7, bsz=203, num_updates=215100, lr=6.81836e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.7, wall=4652
2024-01-31 06:01:19 | INFO | train_inner | epoch 146:   1337 / 1475 loss=2.228, nll_loss=0.579, ppl=1.49, wps=59507.2, ups=15.62, wpb=3808.7, bsz=191.5, num_updates=215200, lr=6.81677e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.8, wall=4658
2024-01-31 06:01:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 06:01:26 | INFO | train_inner | epoch 146:   1438 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=57602, ups=15.37, wpb=3747.5, bsz=200.2, num_updates=215300, lr=6.81519e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.8, wall=4665
2024-01-31 06:01:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:01:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:01:30 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 3.543 | nll_loss 1.942 | ppl 3.84 | wps 134409 | wpb 3189.1 | bsz 163.4 | num_updates 215337 | best_loss 3.385
2024-01-31 06:01:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 215337 updates
2024-01-31 06:01:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint146.pt
2024-01-31 06:01:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint146.pt
2024-01-31 06:01:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint146.pt (epoch 146 @ 215337 updates, score 3.543) (writing took 2.431036915979348 seconds)
2024-01-31 06:01:33 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2024-01-31 06:01:33 | INFO | train | epoch 146 | loss 2.21 | nll_loss 0.559 | ppl 1.47 | wps 55833.4 | ups 14.8 | wpb 3772 | bsz 196 | num_updates 215337 | lr 6.8146e-05 | gnorm 0.984 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 4672
2024-01-31 06:01:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:01:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:01:33 | INFO | fairseq.trainer | begin training epoch 147
2024-01-31 06:01:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:01:37 | INFO | train_inner | epoch 147:     63 / 1475 loss=2.205, nll_loss=0.553, ppl=1.47, wps=33448.8, ups=8.82, wpb=3793.5, bsz=201.9, num_updates=215400, lr=6.81361e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.7, wall=4676
2024-01-31 06:01:44 | INFO | train_inner | epoch 147:    163 / 1475 loss=2.192, nll_loss=0.538, ppl=1.45, wps=58542.4, ups=15.53, wpb=3769.1, bsz=199.4, num_updates=215500, lr=6.81203e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.8, wall=4683
2024-01-31 06:01:50 | INFO | train_inner | epoch 147:    263 / 1475 loss=2.204, nll_loss=0.551, ppl=1.46, wps=58972.4, ups=15.64, wpb=3771.1, bsz=193.1, num_updates=215600, lr=6.81045e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=4689
2024-01-31 06:01:56 | INFO | train_inner | epoch 147:    363 / 1475 loss=2.217, nll_loss=0.566, ppl=1.48, wps=58422.4, ups=15.63, wpb=3738.3, bsz=179.6, num_updates=215700, lr=6.80887e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.7, wall=4695
2024-01-31 06:02:03 | INFO | train_inner | epoch 147:    463 / 1475 loss=2.206, nll_loss=0.554, ppl=1.47, wps=58941.5, ups=15.63, wpb=3770, bsz=196.1, num_updates=215800, lr=6.80729e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.8, wall=4702
2024-01-31 06:02:09 | INFO | train_inner | epoch 147:    563 / 1475 loss=2.197, nll_loss=0.545, ppl=1.46, wps=58572.6, ups=15.59, wpb=3757.2, bsz=202.3, num_updates=215900, lr=6.80571e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.7, wall=4708
2024-01-31 06:02:16 | INFO | train_inner | epoch 147:    663 / 1475 loss=2.208, nll_loss=0.557, ppl=1.47, wps=58974.2, ups=15.65, wpb=3769.2, bsz=191, num_updates=216000, lr=6.80414e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.7, wall=4715
2024-01-31 06:02:22 | INFO | train_inner | epoch 147:    763 / 1475 loss=2.209, nll_loss=0.557, ppl=1.47, wps=56825.2, ups=15.66, wpb=3629.7, bsz=191, num_updates=216100, lr=6.80256e-05, gnorm=1.076, loss_scale=8, train_wall=6, gb_free=29.6, wall=4721
2024-01-31 06:02:28 | INFO | train_inner | epoch 147:    863 / 1475 loss=2.221, nll_loss=0.571, ppl=1.49, wps=59439.9, ups=15.61, wpb=3807, bsz=190.2, num_updates=216200, lr=6.80099e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.8, wall=4727
2024-01-31 06:02:35 | INFO | train_inner | epoch 147:    963 / 1475 loss=2.213, nll_loss=0.563, ppl=1.48, wps=58936.1, ups=15.52, wpb=3796.9, bsz=193.7, num_updates=216300, lr=6.79942e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.7, wall=4734
2024-01-31 06:02:41 | INFO | train_inner | epoch 147:   1063 / 1475 loss=2.217, nll_loss=0.567, ppl=1.48, wps=58804.7, ups=15.59, wpb=3771.6, bsz=193, num_updates=216400, lr=6.79785e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.6, wall=4740
2024-01-31 06:02:48 | INFO | train_inner | epoch 147:   1163 / 1475 loss=2.215, nll_loss=0.565, ppl=1.48, wps=59537.1, ups=15.54, wpb=3832, bsz=199.9, num_updates=216500, lr=6.79628e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=4747
2024-01-31 06:02:54 | INFO | train_inner | epoch 147:   1263 / 1475 loss=2.22, nll_loss=0.571, ppl=1.49, wps=59229.6, ups=15.52, wpb=3816.5, bsz=194.1, num_updates=216600, lr=6.79471e-05, gnorm=0.986, loss_scale=8, train_wall=6, gb_free=29.7, wall=4753
2024-01-31 06:03:01 | INFO | train_inner | epoch 147:   1363 / 1475 loss=2.215, nll_loss=0.566, ppl=1.48, wps=58239, ups=15.49, wpb=3759, bsz=201.6, num_updates=216700, lr=6.79314e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=4760
2024-01-31 06:03:07 | INFO | train_inner | epoch 147:   1463 / 1475 loss=2.21, nll_loss=0.559, ppl=1.47, wps=59376.6, ups=15.61, wpb=3804.7, bsz=208.7, num_updates=216800, lr=6.79157e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.8, wall=4766
2024-01-31 06:03:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:03:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:03:10 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 3.547 | nll_loss 1.95 | ppl 3.86 | wps 135137 | wpb 3189.1 | bsz 163.4 | num_updates 216812 | best_loss 3.385
2024-01-31 06:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 216812 updates
2024-01-31 06:03:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint147.pt
2024-01-31 06:03:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint147.pt
2024-01-31 06:03:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint147.pt (epoch 147 @ 216812 updates, score 3.547) (writing took 2.4163620449835435 seconds)
2024-01-31 06:03:13 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2024-01-31 06:03:13 | INFO | train | epoch 147 | loss 2.209 | nll_loss 0.558 | ppl 1.47 | wps 55875.4 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 216812 | lr 6.79138e-05 | gnorm 0.99 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 4771
2024-01-31 06:03:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:03:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:03:13 | INFO | fairseq.trainer | begin training epoch 148
2024-01-31 06:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:03:18 | INFO | train_inner | epoch 148:     88 / 1475 loss=2.182, nll_loss=0.528, ppl=1.44, wps=33156.3, ups=8.81, wpb=3761.4, bsz=207.5, num_updates=216900, lr=6.79001e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.7, wall=4777
2024-01-31 06:03:25 | INFO | train_inner | epoch 148:    188 / 1475 loss=2.191, nll_loss=0.537, ppl=1.45, wps=58049.8, ups=15.4, wpb=3768.9, bsz=208.3, num_updates=217000, lr=6.78844e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.7, wall=4784
2024-01-31 06:03:31 | INFO | train_inner | epoch 148:    288 / 1475 loss=2.202, nll_loss=0.549, ppl=1.46, wps=60104, ups=15.64, wpb=3843.9, bsz=200.6, num_updates=217100, lr=6.78688e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.8, wall=4790
2024-01-31 06:03:38 | INFO | train_inner | epoch 148:    388 / 1475 loss=2.189, nll_loss=0.535, ppl=1.45, wps=58903.7, ups=15.56, wpb=3785.5, bsz=205.5, num_updates=217200, lr=6.78532e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=4797
2024-01-31 06:03:44 | INFO | train_inner | epoch 148:    488 / 1475 loss=2.199, nll_loss=0.546, ppl=1.46, wps=58708.1, ups=15.62, wpb=3759.1, bsz=193.7, num_updates=217300, lr=6.78375e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.8, wall=4803
2024-01-31 06:03:51 | INFO | train_inner | epoch 148:    588 / 1475 loss=2.208, nll_loss=0.557, ppl=1.47, wps=58056.4, ups=15.57, wpb=3729.9, bsz=191.1, num_updates=217400, lr=6.78219e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=4809
2024-01-31 06:03:57 | INFO | train_inner | epoch 148:    688 / 1475 loss=2.21, nll_loss=0.56, ppl=1.47, wps=58880.1, ups=15.58, wpb=3780.3, bsz=194.7, num_updates=217500, lr=6.78064e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=4816
2024-01-31 06:04:03 | INFO | train_inner | epoch 148:    788 / 1475 loss=2.203, nll_loss=0.551, ppl=1.47, wps=58539.4, ups=15.65, wpb=3740.3, bsz=192.2, num_updates=217600, lr=6.77908e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=4822
2024-01-31 06:04:10 | INFO | train_inner | epoch 148:    888 / 1475 loss=2.22, nll_loss=0.571, ppl=1.49, wps=59797.2, ups=15.65, wpb=3819.9, bsz=187.4, num_updates=217700, lr=6.77752e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.8, wall=4829
2024-01-31 06:04:16 | INFO | train_inner | epoch 148:    988 / 1475 loss=2.212, nll_loss=0.562, ppl=1.48, wps=58189.5, ups=15.57, wpb=3737, bsz=194.3, num_updates=217800, lr=6.77596e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.8, wall=4835
2024-01-31 06:04:23 | INFO | train_inner | epoch 148:   1088 / 1475 loss=2.202, nll_loss=0.551, ppl=1.46, wps=57822.7, ups=15.48, wpb=3734.9, bsz=206.4, num_updates=217900, lr=6.77441e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.8, wall=4842
2024-01-31 06:04:29 | INFO | train_inner | epoch 148:   1188 / 1475 loss=2.229, nll_loss=0.58, ppl=1.5, wps=59507, ups=15.68, wpb=3796.2, bsz=189.2, num_updates=218000, lr=6.77285e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.7, wall=4848
2024-01-31 06:04:35 | INFO | train_inner | epoch 148:   1288 / 1475 loss=2.212, nll_loss=0.562, ppl=1.48, wps=57533.9, ups=15.57, wpb=3694.8, bsz=189.5, num_updates=218100, lr=6.7713e-05, gnorm=1.016, loss_scale=8, train_wall=6, gb_free=29.6, wall=4854
2024-01-31 06:04:42 | INFO | train_inner | epoch 148:   1388 / 1475 loss=2.224, nll_loss=0.575, ppl=1.49, wps=59696.9, ups=15.6, wpb=3826.5, bsz=194.7, num_updates=218200, lr=6.76975e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=4861
2024-01-31 06:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:04:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:04:50 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 3.539 | nll_loss 1.941 | ppl 3.84 | wps 135158 | wpb 3189.1 | bsz 163.4 | num_updates 218287 | best_loss 3.385
2024-01-31 06:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 218287 updates
2024-01-31 06:04:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint148.pt
2024-01-31 06:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint148.pt
2024-01-31 06:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint148.pt (epoch 148 @ 218287 updates, score 3.539) (writing took 2.4887279419926926 seconds)
2024-01-31 06:04:52 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2024-01-31 06:04:52 | INFO | train | epoch 148 | loss 2.207 | nll_loss 0.556 | ppl 1.47 | wps 55858.5 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 218287 | lr 6.7684e-05 | gnorm 0.987 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 4871
2024-01-31 06:04:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:04:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:04:52 | INFO | fairseq.trainer | begin training epoch 149
2024-01-31 06:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:04:53 | INFO | train_inner | epoch 149:     13 / 1475 loss=2.223, nll_loss=0.573, ppl=1.49, wps=33448.1, ups=8.84, wpb=3784.5, bsz=188.7, num_updates=218300, lr=6.7682e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=4872
2024-01-31 06:05:00 | INFO | train_inner | epoch 149:    113 / 1475 loss=2.186, nll_loss=0.531, ppl=1.45, wps=58954.2, ups=15.65, wpb=3768.1, bsz=193.9, num_updates=218400, lr=6.76665e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.8, wall=4878
2024-01-31 06:05:06 | INFO | train_inner | epoch 149:    213 / 1475 loss=2.205, nll_loss=0.553, ppl=1.47, wps=59528.2, ups=15.53, wpb=3833, bsz=188, num_updates=218500, lr=6.7651e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.7, wall=4885
2024-01-31 06:05:12 | INFO | train_inner | epoch 149:    313 / 1475 loss=2.204, nll_loss=0.552, ppl=1.47, wps=58479.4, ups=15.59, wpb=3752.2, bsz=195.1, num_updates=218600, lr=6.76355e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.9, wall=4891
2024-01-31 06:05:19 | INFO | train_inner | epoch 149:    413 / 1475 loss=2.198, nll_loss=0.546, ppl=1.46, wps=59596.2, ups=15.51, wpb=3842.7, bsz=206.5, num_updates=218700, lr=6.76201e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=4898
2024-01-31 06:05:25 | INFO | train_inner | epoch 149:    513 / 1475 loss=2.207, nll_loss=0.555, ppl=1.47, wps=58306.3, ups=15.61, wpb=3736.1, bsz=186.6, num_updates=218800, lr=6.76046e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=4904
2024-01-31 06:05:32 | INFO | train_inner | epoch 149:    613 / 1475 loss=2.209, nll_loss=0.558, ppl=1.47, wps=58421.2, ups=15.54, wpb=3760.1, bsz=194.1, num_updates=218900, lr=6.75892e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.8, wall=4911
2024-01-31 06:05:38 | INFO | train_inner | epoch 149:    713 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=59124.7, ups=15.4, wpb=3838.1, bsz=216.4, num_updates=219000, lr=6.75737e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.7, wall=4917
2024-01-31 06:05:45 | INFO | train_inner | epoch 149:    813 / 1475 loss=2.209, nll_loss=0.558, ppl=1.47, wps=57903.6, ups=15.52, wpb=3731.1, bsz=192.1, num_updates=219100, lr=6.75583e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=4924
2024-01-31 06:05:51 | INFO | train_inner | epoch 149:    913 / 1475 loss=2.21, nll_loss=0.559, ppl=1.47, wps=58132.5, ups=15.57, wpb=3734.4, bsz=191.4, num_updates=219200, lr=6.75429e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.7, wall=4930
2024-01-31 06:05:57 | INFO | train_inner | epoch 149:   1013 / 1475 loss=2.211, nll_loss=0.56, ppl=1.47, wps=59542.3, ups=15.54, wpb=3832, bsz=196.5, num_updates=219300, lr=6.75275e-05, gnorm=0.986, loss_scale=8, train_wall=6, gb_free=29.7, wall=4936
2024-01-31 06:06:04 | INFO | train_inner | epoch 149:   1113 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=58942.7, ups=15.6, wpb=3777.7, bsz=196.8, num_updates=219400, lr=6.75121e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.6, wall=4943
2024-01-31 06:06:10 | INFO | train_inner | epoch 149:   1213 / 1475 loss=2.219, nll_loss=0.569, ppl=1.48, wps=57702, ups=15.59, wpb=3702.3, bsz=181.8, num_updates=219500, lr=6.74967e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.9, wall=4949
2024-01-31 06:06:17 | INFO | train_inner | epoch 149:   1313 / 1475 loss=2.22, nll_loss=0.571, ppl=1.49, wps=58199.9, ups=15.68, wpb=3712, bsz=193.7, num_updates=219600, lr=6.74814e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.7, wall=4956
2024-01-31 06:06:23 | INFO | train_inner | epoch 149:   1413 / 1475 loss=2.215, nll_loss=0.565, ppl=1.48, wps=58507, ups=15.53, wpb=3767.4, bsz=200.6, num_updates=219700, lr=6.7466e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.8, wall=4962
2024-01-31 06:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:06:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:06:29 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 3.535 | nll_loss 1.938 | ppl 3.83 | wps 134925 | wpb 3189.1 | bsz 163.4 | num_updates 219762 | best_loss 3.385
2024-01-31 06:06:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 219762 updates
2024-01-31 06:06:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint149.pt
2024-01-31 06:06:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint149.pt
2024-01-31 06:06:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint149.pt (epoch 149 @ 219762 updates, score 3.535) (writing took 2.3891855920664966 seconds)
2024-01-31 06:06:32 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2024-01-31 06:06:32 | INFO | train | epoch 149 | loss 2.207 | nll_loss 0.555 | ppl 1.47 | wps 55826.8 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 219762 | lr 6.74565e-05 | gnorm 0.989 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 4971
2024-01-31 06:06:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:06:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:06:32 | INFO | fairseq.trainer | begin training epoch 150
2024-01-31 06:06:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:06:34 | INFO | train_inner | epoch 150:     38 / 1475 loss=2.2, nll_loss=0.547, ppl=1.46, wps=33514.1, ups=8.9, wpb=3766.1, bsz=202.1, num_updates=219800, lr=6.74507e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.7, wall=4973
2024-01-31 06:06:41 | INFO | train_inner | epoch 150:    138 / 1475 loss=2.197, nll_loss=0.543, ppl=1.46, wps=58005.2, ups=15.55, wpb=3730, bsz=194.6, num_updates=219900, lr=6.74353e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.6, wall=4980
2024-01-31 06:06:47 | INFO | train_inner | epoch 150:    238 / 1475 loss=2.196, nll_loss=0.543, ppl=1.46, wps=58760.6, ups=15.61, wpb=3765.1, bsz=196.2, num_updates=220000, lr=6.742e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=4986
2024-01-31 06:06:54 | INFO | train_inner | epoch 150:    338 / 1475 loss=2.198, nll_loss=0.545, ppl=1.46, wps=59420.2, ups=15.48, wpb=3837.6, bsz=195.1, num_updates=220100, lr=6.74047e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.7, wall=4993
2024-01-31 06:07:00 | INFO | train_inner | epoch 150:    438 / 1475 loss=2.193, nll_loss=0.541, ppl=1.45, wps=58897.3, ups=15.56, wpb=3785.3, bsz=203.8, num_updates=220200, lr=6.73894e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.8, wall=4999
2024-01-31 06:07:07 | INFO | train_inner | epoch 150:    538 / 1475 loss=2.196, nll_loss=0.544, ppl=1.46, wps=58578.9, ups=15.57, wpb=3761.6, bsz=194.1, num_updates=220300, lr=6.73741e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.8, wall=5005
2024-01-31 06:07:13 | INFO | train_inner | epoch 150:    638 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=59343.6, ups=15.49, wpb=3832.2, bsz=200.9, num_updates=220400, lr=6.73588e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=5012
2024-01-31 06:07:19 | INFO | train_inner | epoch 150:    738 / 1475 loss=2.198, nll_loss=0.546, ppl=1.46, wps=58552.5, ups=15.54, wpb=3768.4, bsz=205.8, num_updates=220500, lr=6.73435e-05, gnorm=0.954, loss_scale=8, train_wall=6, gb_free=29.6, wall=5018
2024-01-31 06:07:26 | INFO | train_inner | epoch 150:    838 / 1475 loss=2.212, nll_loss=0.563, ppl=1.48, wps=58708.9, ups=15.59, wpb=3766.8, bsz=190.6, num_updates=220600, lr=6.73282e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.7, wall=5025
2024-01-31 06:07:32 | INFO | train_inner | epoch 150:    938 / 1475 loss=2.218, nll_loss=0.568, ppl=1.48, wps=58781.7, ups=15.57, wpb=3774.9, bsz=189.8, num_updates=220700, lr=6.7313e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=5031
2024-01-31 06:07:39 | INFO | train_inner | epoch 150:   1038 / 1475 loss=2.204, nll_loss=0.554, ppl=1.47, wps=57402.8, ups=15.55, wpb=3690.3, bsz=203.4, num_updates=220800, lr=6.72977e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.9, wall=5038
2024-01-31 06:07:45 | INFO | train_inner | epoch 150:   1138 / 1475 loss=2.209, nll_loss=0.558, ppl=1.47, wps=58866.1, ups=15.56, wpb=3783.8, bsz=199.9, num_updates=220900, lr=6.72825e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=30, wall=5044
2024-01-31 06:07:52 | INFO | train_inner | epoch 150:   1238 / 1475 loss=2.217, nll_loss=0.567, ppl=1.48, wps=58226.3, ups=15.59, wpb=3734.7, bsz=192.8, num_updates=221000, lr=6.72673e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.7, wall=5050
2024-01-31 06:07:58 | INFO | train_inner | epoch 150:   1338 / 1475 loss=2.216, nll_loss=0.567, ppl=1.48, wps=58428.2, ups=15.55, wpb=3758, bsz=191, num_updates=221100, lr=6.72521e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=5057
2024-01-31 06:08:04 | INFO | train_inner | epoch 150:   1438 / 1475 loss=2.221, nll_loss=0.573, ppl=1.49, wps=59668, ups=15.56, wpb=3833.5, bsz=196.6, num_updates=221200, lr=6.72369e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=5063
2024-01-31 06:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:08:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:08:09 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 3.547 | nll_loss 1.949 | ppl 3.86 | wps 134971 | wpb 3189.1 | bsz 163.4 | num_updates 221237 | best_loss 3.385
2024-01-31 06:08:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 221237 updates
2024-01-31 06:08:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint150.pt
2024-01-31 06:08:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint150.pt
2024-01-31 06:08:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint150.pt (epoch 150 @ 221237 updates, score 3.547) (writing took 2.4002753159729764 seconds)
2024-01-31 06:08:11 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2024-01-31 06:08:11 | INFO | train | epoch 150 | loss 2.206 | nll_loss 0.555 | ppl 1.47 | wps 55829.1 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 221237 | lr 6.72312e-05 | gnorm 0.988 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 5070
2024-01-31 06:08:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:08:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:08:12 | INFO | fairseq.trainer | begin training epoch 151
2024-01-31 06:08:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:08:16 | INFO | train_inner | epoch 151:     63 / 1475 loss=2.209, nll_loss=0.557, ppl=1.47, wps=33047.9, ups=8.89, wpb=3718.9, bsz=184.2, num_updates=221300, lr=6.72217e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.6, wall=5075
2024-01-31 06:08:22 | INFO | train_inner | epoch 151:    163 / 1475 loss=2.19, nll_loss=0.536, ppl=1.45, wps=59303.7, ups=15.55, wpb=3812.8, bsz=200.8, num_updates=221400, lr=6.72065e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=5081
2024-01-31 06:08:28 | INFO | train_inner | epoch 151:    263 / 1475 loss=2.19, nll_loss=0.537, ppl=1.45, wps=57693.9, ups=15.54, wpb=3713.3, bsz=195.4, num_updates=221500, lr=6.71913e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=5087
2024-01-31 06:08:35 | INFO | train_inner | epoch 151:    363 / 1475 loss=2.192, nll_loss=0.54, ppl=1.45, wps=58905.4, ups=15.52, wpb=3795.1, bsz=207.4, num_updates=221600, lr=6.71762e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5094
2024-01-31 06:08:41 | INFO | train_inner | epoch 151:    463 / 1475 loss=2.187, nll_loss=0.534, ppl=1.45, wps=57967.1, ups=15.48, wpb=3744.8, bsz=200.1, num_updates=221700, lr=6.7161e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.7, wall=5100
2024-01-31 06:08:48 | INFO | train_inner | epoch 151:    563 / 1475 loss=2.211, nll_loss=0.56, ppl=1.47, wps=59287.9, ups=15.6, wpb=3799.3, bsz=184.2, num_updates=221800, lr=6.71459e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.8, wall=5107
2024-01-31 06:08:54 | INFO | train_inner | epoch 151:    663 / 1475 loss=2.203, nll_loss=0.552, ppl=1.47, wps=58014.6, ups=15.56, wpb=3728.8, bsz=194.1, num_updates=221900, lr=6.71307e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.7, wall=5113
2024-01-31 06:09:01 | INFO | train_inner | epoch 151:    763 / 1475 loss=2.203, nll_loss=0.551, ppl=1.46, wps=57697.4, ups=15.58, wpb=3704.1, bsz=190.4, num_updates=222000, lr=6.71156e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.8, wall=5120
2024-01-31 06:09:07 | INFO | train_inner | epoch 151:    863 / 1475 loss=2.209, nll_loss=0.559, ppl=1.47, wps=59948.6, ups=15.51, wpb=3864.4, bsz=196.2, num_updates=222100, lr=6.71005e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.6, wall=5126
2024-01-31 06:09:14 | INFO | train_inner | epoch 151:    963 / 1475 loss=2.207, nll_loss=0.557, ppl=1.47, wps=58023.4, ups=15.55, wpb=3730.9, bsz=190, num_updates=222200, lr=6.70854e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.7, wall=5132
2024-01-31 06:09:20 | INFO | train_inner | epoch 151:   1063 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=59429.1, ups=15.57, wpb=3817.7, bsz=193.5, num_updates=222300, lr=6.70703e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.7, wall=5139
2024-01-31 06:09:26 | INFO | train_inner | epoch 151:   1163 / 1475 loss=2.211, nll_loss=0.561, ppl=1.48, wps=57378.5, ups=15.55, wpb=3690, bsz=193.4, num_updates=222400, lr=6.70552e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.7, wall=5145
2024-01-31 06:09:33 | INFO | train_inner | epoch 151:   1263 / 1475 loss=2.213, nll_loss=0.563, ppl=1.48, wps=59762.1, ups=15.62, wpb=3827, bsz=200.6, num_updates=222500, lr=6.70402e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5152
2024-01-31 06:09:39 | INFO | train_inner | epoch 151:   1363 / 1475 loss=2.224, nll_loss=0.575, ppl=1.49, wps=59647.2, ups=15.54, wpb=3838.2, bsz=192.6, num_updates=222600, lr=6.70251e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.7, wall=5158
2024-01-31 06:09:46 | INFO | train_inner | epoch 151:   1463 / 1475 loss=2.211, nll_loss=0.562, ppl=1.48, wps=59000.7, ups=15.61, wpb=3780.2, bsz=204.6, num_updates=222700, lr=6.701e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5165
2024-01-31 06:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:09:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:09:49 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 3.544 | nll_loss 1.946 | ppl 3.85 | wps 134497 | wpb 3189.1 | bsz 163.4 | num_updates 222712 | best_loss 3.385
2024-01-31 06:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 222712 updates
2024-01-31 06:09:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint151.pt
2024-01-31 06:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint151.pt
2024-01-31 06:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint151.pt (epoch 151 @ 222712 updates, score 3.544) (writing took 2.4214001309592277 seconds)
2024-01-31 06:09:51 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2024-01-31 06:09:51 | INFO | train | epoch 151 | loss 2.204 | nll_loss 0.553 | ppl 1.47 | wps 55797.5 | ups 14.79 | wpb 3771.6 | bsz 195.9 | num_updates 222712 | lr 6.70082e-05 | gnorm 0.99 | loss_scale 8 | train_wall 92 | gb_free 29.6 | wall 5170
2024-01-31 06:09:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:09:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:09:51 | INFO | fairseq.trainer | begin training epoch 152
2024-01-31 06:09:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:09:57 | INFO | train_inner | epoch 152:     88 / 1475 loss=2.19, nll_loss=0.536, ppl=1.45, wps=33791.5, ups=8.79, wpb=3843.2, bsz=207.4, num_updates=222800, lr=6.6995e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=5176
2024-01-31 06:10:03 | INFO | train_inner | epoch 152:    188 / 1475 loss=2.205, nll_loss=0.552, ppl=1.47, wps=58585.5, ups=15.62, wpb=3751, bsz=181, num_updates=222900, lr=6.698e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.7, wall=5182
2024-01-31 06:10:10 | INFO | train_inner | epoch 152:    288 / 1475 loss=2.193, nll_loss=0.54, ppl=1.45, wps=59061.4, ups=15.59, wpb=3788.1, bsz=196.6, num_updates=223000, lr=6.6965e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.8, wall=5189
2024-01-31 06:10:16 | INFO | train_inner | epoch 152:    388 / 1475 loss=2.19, nll_loss=0.537, ppl=1.45, wps=57912.1, ups=15.62, wpb=3708.1, bsz=192, num_updates=223100, lr=6.69499e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.7, wall=5195
2024-01-31 06:10:23 | INFO | train_inner | epoch 152:    488 / 1475 loss=2.195, nll_loss=0.543, ppl=1.46, wps=57985.2, ups=15.49, wpb=3742.6, bsz=193.1, num_updates=223200, lr=6.69349e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.6, wall=5202
2024-01-31 06:10:29 | INFO | train_inner | epoch 152:    588 / 1475 loss=2.198, nll_loss=0.545, ppl=1.46, wps=57523.4, ups=15.67, wpb=3671.5, bsz=193.9, num_updates=223300, lr=6.692e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.8, wall=5208
2024-01-31 06:10:35 | INFO | train_inner | epoch 152:    688 / 1475 loss=2.199, nll_loss=0.546, ppl=1.46, wps=59383.6, ups=15.59, wpb=3808.1, bsz=198, num_updates=223400, lr=6.6905e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.6, wall=5214
2024-01-31 06:10:42 | INFO | train_inner | epoch 152:    788 / 1475 loss=2.209, nll_loss=0.559, ppl=1.47, wps=59632.7, ups=15.59, wpb=3826, bsz=192.1, num_updates=223500, lr=6.689e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=5221
2024-01-31 06:10:48 | INFO | train_inner | epoch 152:    888 / 1475 loss=2.207, nll_loss=0.556, ppl=1.47, wps=58473.5, ups=15.57, wpb=3756.2, bsz=197.8, num_updates=223600, lr=6.6875e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=5227
2024-01-31 06:10:55 | INFO | train_inner | epoch 152:    988 / 1475 loss=2.208, nll_loss=0.558, ppl=1.47, wps=59334.8, ups=15.48, wpb=3833.7, bsz=200.6, num_updates=223700, lr=6.68601e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5234
2024-01-31 06:11:01 | INFO | train_inner | epoch 152:   1088 / 1475 loss=2.201, nll_loss=0.55, ppl=1.46, wps=56167.4, ups=15.6, wpb=3601, bsz=190.6, num_updates=223800, lr=6.68452e-05, gnorm=1.116, loss_scale=8, train_wall=6, gb_free=29.7, wall=5240
2024-01-31 06:11:08 | INFO | train_inner | epoch 152:   1188 / 1475 loss=2.199, nll_loss=0.549, ppl=1.46, wps=59856, ups=15.37, wpb=3895.6, bsz=218.1, num_updates=223900, lr=6.68302e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=5247
2024-01-31 06:11:14 | INFO | train_inner | epoch 152:   1288 / 1475 loss=2.216, nll_loss=0.566, ppl=1.48, wps=58417.9, ups=15.58, wpb=3750.7, bsz=192.6, num_updates=224000, lr=6.68153e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=5253
2024-01-31 06:11:21 | INFO | train_inner | epoch 152:   1388 / 1475 loss=2.21, nll_loss=0.56, ppl=1.47, wps=59078.5, ups=15.51, wpb=3809, bsz=202.9, num_updates=224100, lr=6.68004e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=5259
2024-01-31 06:11:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:11:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:11:28 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 3.544 | nll_loss 1.95 | ppl 3.86 | wps 135630 | wpb 3189.1 | bsz 163.4 | num_updates 224187 | best_loss 3.385
2024-01-31 06:11:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 224187 updates
2024-01-31 06:11:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint152.pt
2024-01-31 06:11:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint152.pt
2024-01-31 06:11:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint152.pt (epoch 152 @ 224187 updates, score 3.544) (writing took 2.5257901549339294 seconds)
2024-01-31 06:11:31 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2024-01-31 06:11:31 | INFO | train | epoch 152 | loss 2.203 | nll_loss 0.552 | ppl 1.47 | wps 55726.9 | ups 14.78 | wpb 3771.6 | bsz 195.9 | num_updates 224187 | lr 6.67874e-05 | gnorm 0.996 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 5270
2024-01-31 06:11:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:11:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:11:31 | INFO | fairseq.trainer | begin training epoch 153
2024-01-31 06:11:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:11:32 | INFO | train_inner | epoch 153:     13 / 1475 loss=2.219, nll_loss=0.57, ppl=1.48, wps=33315.1, ups=8.76, wpb=3802.3, bsz=188.2, num_updates=224200, lr=6.67855e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.8, wall=5271
2024-01-31 06:11:38 | INFO | train_inner | epoch 153:    113 / 1475 loss=2.193, nll_loss=0.539, ppl=1.45, wps=59360, ups=15.5, wpb=3829.4, bsz=200.5, num_updates=224300, lr=6.67706e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=5277
2024-01-31 06:11:45 | INFO | train_inner | epoch 153:    213 / 1475 loss=2.179, nll_loss=0.525, ppl=1.44, wps=58979.7, ups=15.51, wpb=3802.7, bsz=209.8, num_updates=224400, lr=6.67557e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=5284
2024-01-31 06:11:51 | INFO | train_inner | epoch 153:    313 / 1475 loss=2.194, nll_loss=0.541, ppl=1.46, wps=56930, ups=15.64, wpb=3640.9, bsz=188.4, num_updates=224500, lr=6.67409e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=5290
2024-01-31 06:11:58 | INFO | train_inner | epoch 153:    413 / 1475 loss=2.186, nll_loss=0.534, ppl=1.45, wps=58248.3, ups=15.55, wpb=3746.3, bsz=204.9, num_updates=224600, lr=6.6726e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.6, wall=5297
2024-01-31 06:12:04 | INFO | train_inner | epoch 153:    513 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=59410.6, ups=15.48, wpb=3836.9, bsz=200.7, num_updates=224700, lr=6.67112e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=5303
2024-01-31 06:12:11 | INFO | train_inner | epoch 153:    613 / 1475 loss=2.21, nll_loss=0.559, ppl=1.47, wps=58424.7, ups=15.61, wpb=3743.3, bsz=180.5, num_updates=224800, lr=6.66963e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.8, wall=5309
2024-01-31 06:12:17 | INFO | train_inner | epoch 153:    713 / 1475 loss=2.196, nll_loss=0.543, ppl=1.46, wps=58666.8, ups=15.53, wpb=3777.9, bsz=205.1, num_updates=224900, lr=6.66815e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5316
2024-01-31 06:12:23 | INFO | train_inner | epoch 153:    813 / 1475 loss=2.213, nll_loss=0.563, ppl=1.48, wps=59085.5, ups=15.53, wpb=3805.1, bsz=193.4, num_updates=225000, lr=6.66667e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.8, wall=5322
2024-01-31 06:12:30 | INFO | train_inner | epoch 153:    913 / 1475 loss=2.207, nll_loss=0.556, ppl=1.47, wps=59453.6, ups=15.63, wpb=3804.5, bsz=188.9, num_updates=225100, lr=6.66519e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.6, wall=5329
2024-01-31 06:12:36 | INFO | train_inner | epoch 153:   1013 / 1475 loss=2.206, nll_loss=0.555, ppl=1.47, wps=59735.7, ups=15.68, wpb=3809.4, bsz=193.3, num_updates=225200, lr=6.66371e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.7, wall=5335
2024-01-31 06:12:43 | INFO | train_inner | epoch 153:   1113 / 1475 loss=2.212, nll_loss=0.562, ppl=1.48, wps=57836.5, ups=15.63, wpb=3699.5, bsz=183.4, num_updates=225300, lr=6.66223e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.8, wall=5342
2024-01-31 06:12:49 | INFO | train_inner | epoch 153:   1213 / 1475 loss=2.208, nll_loss=0.558, ppl=1.47, wps=58832.7, ups=15.54, wpb=3786.4, bsz=202.8, num_updates=225400, lr=6.66075e-05, gnorm=1.016, loss_scale=8, train_wall=6, gb_free=29.7, wall=5348
2024-01-31 06:12:56 | INFO | train_inner | epoch 153:   1313 / 1475 loss=2.205, nll_loss=0.554, ppl=1.47, wps=58376.1, ups=15.49, wpb=3768.5, bsz=201.3, num_updates=225500, lr=6.65927e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.8, wall=5354
2024-01-31 06:13:02 | INFO | train_inner | epoch 153:   1413 / 1475 loss=2.217, nll_loss=0.568, ppl=1.48, wps=58042.5, ups=15.57, wpb=3728.7, bsz=187.4, num_updates=225600, lr=6.6578e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.7, wall=5361
2024-01-31 06:13:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:13:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:13:08 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 3.552 | nll_loss 1.956 | ppl 3.88 | wps 133973 | wpb 3189.1 | bsz 163.4 | num_updates 225662 | best_loss 3.385
2024-01-31 06:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 225662 updates
2024-01-31 06:13:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint153.pt
2024-01-31 06:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint153.pt
2024-01-31 06:13:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint153.pt (epoch 153 @ 225662 updates, score 3.552) (writing took 2.385963118984364 seconds)
2024-01-31 06:13:11 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2024-01-31 06:13:11 | INFO | train | epoch 153 | loss 2.202 | nll_loss 0.55 | ppl 1.46 | wps 55822 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 225662 | lr 6.65688e-05 | gnorm 0.989 | loss_scale 8 | train_wall 92 | gb_free 29.8 | wall 5370
2024-01-31 06:13:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:13:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:13:11 | INFO | fairseq.trainer | begin training epoch 154
2024-01-31 06:13:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:13:13 | INFO | train_inner | epoch 154:     38 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=33017.1, ups=8.87, wpb=3724.3, bsz=201, num_updates=225700, lr=6.65632e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.6, wall=5372
2024-01-31 06:13:20 | INFO | train_inner | epoch 154:    138 / 1475 loss=2.181, nll_loss=0.526, ppl=1.44, wps=59168.3, ups=15.6, wpb=3792.1, bsz=196.3, num_updates=225800, lr=6.65485e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.7, wall=5379
2024-01-31 06:13:26 | INFO | train_inner | epoch 154:    238 / 1475 loss=2.181, nll_loss=0.527, ppl=1.44, wps=58649.2, ups=15.48, wpb=3789.2, bsz=204.2, num_updates=225900, lr=6.65337e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=5385
2024-01-31 06:13:33 | INFO | train_inner | epoch 154:    338 / 1475 loss=2.204, nll_loss=0.552, ppl=1.47, wps=59286.6, ups=15.57, wpb=3807.8, bsz=180.8, num_updates=226000, lr=6.6519e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=5391
2024-01-31 06:13:39 | INFO | train_inner | epoch 154:    438 / 1475 loss=2.191, nll_loss=0.538, ppl=1.45, wps=59456.2, ups=15.54, wpb=3825.6, bsz=198.5, num_updates=226100, lr=6.65043e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=5398
2024-01-31 06:13:45 | INFO | train_inner | epoch 154:    538 / 1475 loss=2.191, nll_loss=0.538, ppl=1.45, wps=57420.1, ups=15.51, wpb=3702.4, bsz=201.4, num_updates=226200, lr=6.64896e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.9, wall=5404
2024-01-31 06:13:52 | INFO | train_inner | epoch 154:    638 / 1475 loss=2.2, nll_loss=0.549, ppl=1.46, wps=59475.8, ups=15.5, wpb=3836.9, bsz=198.5, num_updates=226300, lr=6.64749e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=5411
2024-01-31 06:13:58 | INFO | train_inner | epoch 154:    738 / 1475 loss=2.198, nll_loss=0.546, ppl=1.46, wps=59779.2, ups=15.54, wpb=3846, bsz=205.4, num_updates=226400, lr=6.64602e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.6, wall=5417
2024-01-31 06:14:05 | INFO | train_inner | epoch 154:    838 / 1475 loss=2.205, nll_loss=0.554, ppl=1.47, wps=57663.4, ups=15.66, wpb=3683.3, bsz=188, num_updates=226500, lr=6.64455e-05, gnorm=1.033, loss_scale=8, train_wall=6, gb_free=29.8, wall=5424
2024-01-31 06:14:11 | INFO | train_inner | epoch 154:    938 / 1475 loss=2.212, nll_loss=0.562, ppl=1.48, wps=58492.4, ups=15.63, wpb=3743, bsz=181.5, num_updates=226600, lr=6.64309e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.8, wall=5430
2024-01-31 06:14:17 | INFO | train_inner | epoch 154:   1038 / 1475 loss=2.208, nll_loss=0.558, ppl=1.47, wps=58897.1, ups=15.59, wpb=3776.8, bsz=188.6, num_updates=226700, lr=6.64162e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.8, wall=5436
2024-01-31 06:14:24 | INFO | train_inner | epoch 154:   1138 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=57609.3, ups=15.55, wpb=3705, bsz=205.6, num_updates=226800, lr=6.64016e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5443
2024-01-31 06:14:30 | INFO | train_inner | epoch 154:   1238 / 1475 loss=2.213, nll_loss=0.563, ppl=1.48, wps=58503.2, ups=15.57, wpb=3757.2, bsz=194.2, num_updates=226900, lr=6.6387e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.7, wall=5449
2024-01-31 06:14:37 | INFO | train_inner | epoch 154:   1338 / 1475 loss=2.22, nll_loss=0.571, ppl=1.49, wps=59117.1, ups=15.63, wpb=3783.2, bsz=190.6, num_updates=227000, lr=6.63723e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=5456
2024-01-31 06:14:43 | INFO | train_inner | epoch 154:   1438 / 1475 loss=2.21, nll_loss=0.561, ppl=1.48, wps=58870, ups=15.45, wpb=3810.4, bsz=203.4, num_updates=227100, lr=6.63577e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.6, wall=5462
2024-01-31 06:14:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:14:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:14:48 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 3.54 | nll_loss 1.947 | ppl 3.86 | wps 134521 | wpb 3189.1 | bsz 163.4 | num_updates 227137 | best_loss 3.385
2024-01-31 06:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 227137 updates
2024-01-31 06:14:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint154.pt
2024-01-31 06:14:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint154.pt
2024-01-31 06:14:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint154.pt (epoch 154 @ 227137 updates, score 3.54) (writing took 2.432553874910809 seconds)
2024-01-31 06:14:50 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2024-01-31 06:14:50 | INFO | train | epoch 154 | loss 2.2 | nll_loss 0.549 | ppl 1.46 | wps 55792.5 | ups 14.79 | wpb 3771.6 | bsz 195.9 | num_updates 227137 | lr 6.63523e-05 | gnorm 0.99 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 5469
2024-01-31 06:14:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:14:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:14:50 | INFO | fairseq.trainer | begin training epoch 155
2024-01-31 06:14:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:14:55 | INFO | train_inner | epoch 155:     63 / 1475 loss=2.181, nll_loss=0.527, ppl=1.44, wps=33260.3, ups=8.78, wpb=3788.6, bsz=206.9, num_updates=227200, lr=6.63431e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.7, wall=5474
2024-01-31 06:15:01 | INFO | train_inner | epoch 155:    163 / 1475 loss=2.203, nll_loss=0.551, ppl=1.47, wps=59261.2, ups=15.6, wpb=3798.4, bsz=178.6, num_updates=227300, lr=6.63285e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5480
2024-01-31 06:15:07 | INFO | train_inner | epoch 155:    263 / 1475 loss=2.174, nll_loss=0.519, ppl=1.43, wps=58960.5, ups=15.47, wpb=3810.8, bsz=215.8, num_updates=227400, lr=6.63139e-05, gnorm=0.932, loss_scale=8, train_wall=6, gb_free=29.7, wall=5486
2024-01-31 06:15:14 | INFO | train_inner | epoch 155:    363 / 1475 loss=2.191, nll_loss=0.539, ppl=1.45, wps=59092.2, ups=15.53, wpb=3805.6, bsz=201.8, num_updates=227500, lr=6.62994e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.8, wall=5493
2024-01-31 06:15:20 | INFO | train_inner | epoch 155:    463 / 1475 loss=2.184, nll_loss=0.53, ppl=1.44, wps=57221.5, ups=15.51, wpb=3688.4, bsz=201.4, num_updates=227600, lr=6.62848e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.6, wall=5499
2024-01-31 06:15:27 | INFO | train_inner | epoch 155:    563 / 1475 loss=2.198, nll_loss=0.546, ppl=1.46, wps=59339.2, ups=15.58, wpb=3809, bsz=188.6, num_updates=227700, lr=6.62702e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.7, wall=5506
2024-01-31 06:15:33 | INFO | train_inner | epoch 155:    663 / 1475 loss=2.201, nll_loss=0.55, ppl=1.46, wps=58317.6, ups=15.56, wpb=3748.2, bsz=194.5, num_updates=227800, lr=6.62557e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=5512
2024-01-31 06:15:40 | INFO | train_inner | epoch 155:    763 / 1475 loss=2.201, nll_loss=0.549, ppl=1.46, wps=59228.3, ups=15.65, wpb=3785.7, bsz=194.6, num_updates=227900, lr=6.62411e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=5519
2024-01-31 06:15:46 | INFO | train_inner | epoch 155:    863 / 1475 loss=2.199, nll_loss=0.547, ppl=1.46, wps=58907.8, ups=15.54, wpb=3791.9, bsz=199.3, num_updates=228000, lr=6.62266e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.7, wall=5525
2024-01-31 06:15:52 | INFO | train_inner | epoch 155:    963 / 1475 loss=2.197, nll_loss=0.546, ppl=1.46, wps=58901, ups=15.55, wpb=3788.5, bsz=195.4, num_updates=228100, lr=6.62121e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=5531
2024-01-31 06:15:59 | INFO | train_inner | epoch 155:   1063 / 1475 loss=2.205, nll_loss=0.556, ppl=1.47, wps=58777.1, ups=15.6, wpb=3768.8, bsz=196.1, num_updates=228200, lr=6.61976e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=5538
2024-01-31 06:16:05 | INFO | train_inner | epoch 155:   1163 / 1475 loss=2.21, nll_loss=0.56, ppl=1.47, wps=59018.6, ups=15.57, wpb=3791.1, bsz=195.7, num_updates=228300, lr=6.61831e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.6, wall=5544
2024-01-31 06:16:12 | INFO | train_inner | epoch 155:   1263 / 1475 loss=2.212, nll_loss=0.563, ppl=1.48, wps=58415.4, ups=15.56, wpb=3754.6, bsz=191, num_updates=228400, lr=6.61686e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.7, wall=5551
2024-01-31 06:16:18 | INFO | train_inner | epoch 155:   1363 / 1475 loss=2.214, nll_loss=0.564, ppl=1.48, wps=57752.1, ups=15.62, wpb=3696.7, bsz=192.3, num_updates=228500, lr=6.61541e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=30, wall=5557
2024-01-31 06:16:25 | INFO | train_inner | epoch 155:   1463 / 1475 loss=2.21, nll_loss=0.56, ppl=1.47, wps=58793.9, ups=15.67, wpb=3752.2, bsz=190.9, num_updates=228600, lr=6.61396e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.6, wall=5563
2024-01-31 06:16:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:16:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:16:28 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 3.55 | nll_loss 1.955 | ppl 3.88 | wps 135131 | wpb 3189.1 | bsz 163.4 | num_updates 228612 | best_loss 3.385
2024-01-31 06:16:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 228612 updates
2024-01-31 06:16:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint155.pt
2024-01-31 06:16:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint155.pt
2024-01-31 06:16:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint155.pt (epoch 155 @ 228612 updates, score 3.55) (writing took 2.4112915409496054 seconds)
2024-01-31 06:16:30 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2024-01-31 06:16:30 | INFO | train | epoch 155 | loss 2.199 | nll_loss 0.547 | ppl 1.46 | wps 55821 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 228612 | lr 6.61379e-05 | gnorm 0.985 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 5569
2024-01-31 06:16:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:16:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:16:30 | INFO | fairseq.trainer | begin training epoch 156
2024-01-31 06:16:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:16:36 | INFO | train_inner | epoch 156:     88 / 1475 loss=2.185, nll_loss=0.532, ppl=1.45, wps=32761.6, ups=8.84, wpb=3707.4, bsz=194.6, num_updates=228700, lr=6.61252e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.8, wall=5575
2024-01-31 06:16:42 | INFO | train_inner | epoch 156:    188 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=58434.4, ups=15.52, wpb=3764.9, bsz=197.4, num_updates=228800, lr=6.61107e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.7, wall=5581
2024-01-31 06:16:49 | INFO | train_inner | epoch 156:    288 / 1475 loss=2.184, nll_loss=0.53, ppl=1.44, wps=58700.2, ups=15.55, wpb=3774.8, bsz=202, num_updates=228900, lr=6.60963e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=5588
2024-01-31 06:16:55 | INFO | train_inner | epoch 156:    388 / 1475 loss=2.196, nll_loss=0.544, ppl=1.46, wps=58873, ups=15.52, wpb=3794.5, bsz=192, num_updates=229000, lr=6.60819e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=5594
2024-01-31 06:17:02 | INFO | train_inner | epoch 156:    488 / 1475 loss=2.204, nll_loss=0.553, ppl=1.47, wps=59725.6, ups=15.61, wpb=3826.3, bsz=190.3, num_updates=229100, lr=6.60674e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=5600
2024-01-31 06:17:08 | INFO | train_inner | epoch 156:    588 / 1475 loss=2.193, nll_loss=0.54, ppl=1.45, wps=59413.6, ups=15.64, wpb=3798.6, bsz=198.4, num_updates=229200, lr=6.6053e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=5607
2024-01-31 06:17:14 | INFO | train_inner | epoch 156:    688 / 1475 loss=2.192, nll_loss=0.54, ppl=1.45, wps=58119.9, ups=15.59, wpb=3728.9, bsz=200.2, num_updates=229300, lr=6.60386e-05, gnorm=0.985, loss_scale=8, train_wall=6, gb_free=29.7, wall=5613
2024-01-31 06:17:21 | INFO | train_inner | epoch 156:    788 / 1475 loss=2.196, nll_loss=0.544, ppl=1.46, wps=57619.2, ups=15.56, wpb=3703.3, bsz=196.1, num_updates=229400, lr=6.60242e-05, gnorm=1.048, loss_scale=8, train_wall=6, gb_free=29.7, wall=5620
2024-01-31 06:17:27 | INFO | train_inner | epoch 156:    888 / 1475 loss=2.199, nll_loss=0.548, ppl=1.46, wps=59080.8, ups=15.53, wpb=3804.1, bsz=198.6, num_updates=229500, lr=6.60098e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.7, wall=5626
2024-01-31 06:17:34 | INFO | train_inner | epoch 156:    988 / 1475 loss=2.201, nll_loss=0.55, ppl=1.46, wps=58486.2, ups=15.52, wpb=3768.9, bsz=190.3, num_updates=229600, lr=6.59955e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=5633
2024-01-31 06:17:40 | INFO | train_inner | epoch 156:   1088 / 1475 loss=2.208, nll_loss=0.557, ppl=1.47, wps=59089, ups=15.52, wpb=3806.9, bsz=195.9, num_updates=229700, lr=6.59811e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.7, wall=5639
2024-01-31 06:17:47 | INFO | train_inner | epoch 156:   1188 / 1475 loss=2.211, nll_loss=0.56, ppl=1.47, wps=58796.3, ups=15.63, wpb=3761.2, bsz=191.1, num_updates=229800, lr=6.59667e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=5645
2024-01-31 06:17:53 | INFO | train_inner | epoch 156:   1288 / 1475 loss=2.214, nll_loss=0.564, ppl=1.48, wps=59190, ups=15.54, wpb=3808.3, bsz=187.8, num_updates=229900, lr=6.59524e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.8, wall=5652
2024-01-31 06:17:59 | INFO | train_inner | epoch 156:   1388 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=57915.1, ups=15.63, wpb=3704.9, bsz=207.4, num_updates=230000, lr=6.5938e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.8, wall=5658
2024-01-31 06:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:18:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:18:07 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 3.544 | nll_loss 1.949 | ppl 3.86 | wps 134962 | wpb 3189.1 | bsz 163.4 | num_updates 230087 | best_loss 3.385
2024-01-31 06:18:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 230087 updates
2024-01-31 06:18:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint156.pt
2024-01-31 06:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint156.pt
2024-01-31 06:18:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint156.pt (epoch 156 @ 230087 updates, score 3.544) (writing took 2.3684387620305642 seconds)
2024-01-31 06:18:10 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2024-01-31 06:18:10 | INFO | train | epoch 156 | loss 2.198 | nll_loss 0.546 | ppl 1.46 | wps 55840.5 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 230087 | lr 6.59256e-05 | gnorm 0.997 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 5669
2024-01-31 06:18:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:18:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:18:10 | INFO | fairseq.trainer | begin training epoch 157
2024-01-31 06:18:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:18:11 | INFO | train_inner | epoch 157:     13 / 1475 loss=2.2, nll_loss=0.55, ppl=1.46, wps=33749.7, ups=8.85, wpb=3814.8, bsz=199.8, num_updates=230100, lr=6.59237e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.7, wall=5670
2024-01-31 06:18:17 | INFO | train_inner | epoch 157:    113 / 1475 loss=2.185, nll_loss=0.532, ppl=1.45, wps=59120, ups=15.59, wpb=3791.1, bsz=189.5, num_updates=230200, lr=6.59094e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=5676
2024-01-31 06:18:24 | INFO | train_inner | epoch 157:    213 / 1475 loss=2.18, nll_loss=0.526, ppl=1.44, wps=59057.6, ups=15.52, wpb=3805.1, bsz=201.6, num_updates=230300, lr=6.58951e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=5682
2024-01-31 06:18:30 | INFO | train_inner | epoch 157:    313 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=57837, ups=15.59, wpb=3710.6, bsz=204.1, num_updates=230400, lr=6.58808e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=5689
2024-01-31 06:18:36 | INFO | train_inner | epoch 157:    413 / 1475 loss=2.195, nll_loss=0.543, ppl=1.46, wps=57780.3, ups=15.58, wpb=3709.5, bsz=188.5, num_updates=230500, lr=6.58665e-05, gnorm=1.062, loss_scale=8, train_wall=6, gb_free=29.7, wall=5695
2024-01-31 06:18:43 | INFO | train_inner | epoch 157:    513 / 1475 loss=2.203, nll_loss=0.551, ppl=1.47, wps=59471.3, ups=15.64, wpb=3801.8, bsz=186.9, num_updates=230600, lr=6.58522e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.7, wall=5702
2024-01-31 06:18:49 | INFO | train_inner | epoch 157:    613 / 1475 loss=2.185, nll_loss=0.532, ppl=1.45, wps=59561.8, ups=15.58, wpb=3824.1, bsz=203.5, num_updates=230700, lr=6.58379e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.6, wall=5708
2024-01-31 06:18:56 | INFO | train_inner | epoch 157:    713 / 1475 loss=2.196, nll_loss=0.545, ppl=1.46, wps=58803.4, ups=15.52, wpb=3788.1, bsz=194.6, num_updates=230800, lr=6.58237e-05, gnorm=0.983, loss_scale=8, train_wall=6, gb_free=29.7, wall=5715
2024-01-31 06:19:02 | INFO | train_inner | epoch 157:    813 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=57355.4, ups=15.56, wpb=3685.3, bsz=190.4, num_updates=230900, lr=6.58094e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=5721
2024-01-31 06:19:08 | INFO | train_inner | epoch 157:    913 / 1475 loss=2.191, nll_loss=0.539, ppl=1.45, wps=59725.6, ups=15.52, wpb=3847.8, bsz=205.8, num_updates=231000, lr=6.57952e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=5727
2024-01-31 06:19:15 | INFO | train_inner | epoch 157:   1013 / 1475 loss=2.189, nll_loss=0.537, ppl=1.45, wps=58532, ups=15.58, wpb=3757, bsz=201.5, num_updates=231100, lr=6.57809e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=5734
2024-01-31 06:19:21 | INFO | train_inner | epoch 157:   1113 / 1475 loss=2.211, nll_loss=0.561, ppl=1.48, wps=59391.5, ups=15.5, wpb=3831.2, bsz=187.4, num_updates=231200, lr=6.57667e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.8, wall=5740
2024-01-31 06:19:28 | INFO | train_inner | epoch 157:   1213 / 1475 loss=2.209, nll_loss=0.559, ppl=1.47, wps=59380.8, ups=15.5, wpb=3831.4, bsz=201.8, num_updates=231300, lr=6.57525e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=5747
2024-01-31 06:19:34 | INFO | train_inner | epoch 157:   1313 / 1475 loss=2.208, nll_loss=0.558, ppl=1.47, wps=58067.2, ups=15.51, wpb=3743.7, bsz=194.4, num_updates=231400, lr=6.57383e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.6, wall=5753
2024-01-31 06:19:41 | INFO | train_inner | epoch 157:   1413 / 1475 loss=2.216, nll_loss=0.567, ppl=1.48, wps=57933.2, ups=15.68, wpb=3693.9, bsz=191, num_updates=231500, lr=6.57241e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.8, wall=5760
2024-01-31 06:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:19:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:19:47 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 3.552 | nll_loss 1.955 | ppl 3.88 | wps 134147 | wpb 3189.1 | bsz 163.4 | num_updates 231562 | best_loss 3.385
2024-01-31 06:19:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 231562 updates
2024-01-31 06:19:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint157.pt
2024-01-31 06:19:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint157.pt
2024-01-31 06:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint157.pt (epoch 157 @ 231562 updates, score 3.552) (writing took 2.5598486439557746 seconds)
2024-01-31 06:19:49 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2024-01-31 06:19:49 | INFO | train | epoch 157 | loss 2.196 | nll_loss 0.545 | ppl 1.46 | wps 55730.5 | ups 14.78 | wpb 3771.6 | bsz 195.9 | num_updates 231562 | lr 6.57153e-05 | gnorm 0.989 | loss_scale 8 | train_wall 91 | gb_free 29.6 | wall 5768
2024-01-31 06:19:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:19:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:19:50 | INFO | fairseq.trainer | begin training epoch 158
2024-01-31 06:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:19:52 | INFO | train_inner | epoch 158:     38 / 1475 loss=2.194, nll_loss=0.542, ppl=1.46, wps=32716.5, ups=8.71, wpb=3757.7, bsz=199.8, num_updates=231600, lr=6.57099e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=5771
2024-01-31 06:19:59 | INFO | train_inner | epoch 158:    138 / 1475 loss=2.182, nll_loss=0.527, ppl=1.44, wps=59624.1, ups=15.56, wpb=3832.3, bsz=193.4, num_updates=231700, lr=6.56957e-05, gnorm=0.963, loss_scale=16, train_wall=6, gb_free=29.7, wall=5777
2024-01-31 06:20:05 | INFO | train_inner | epoch 158:    238 / 1475 loss=2.173, nll_loss=0.519, ppl=1.43, wps=58246.9, ups=15.42, wpb=3778.1, bsz=209.9, num_updates=231800, lr=6.56815e-05, gnorm=0.956, loss_scale=16, train_wall=6, gb_free=29.8, wall=5784
2024-01-31 06:20:11 | INFO | train_inner | epoch 158:    338 / 1475 loss=2.184, nll_loss=0.531, ppl=1.44, wps=57469.3, ups=15.62, wpb=3678.7, bsz=189.3, num_updates=231900, lr=6.56674e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.7, wall=5790
2024-01-31 06:20:18 | INFO | train_inner | epoch 158:    438 / 1475 loss=2.182, nll_loss=0.529, ppl=1.44, wps=57793.4, ups=15.53, wpb=3721.7, bsz=202.6, num_updates=232000, lr=6.56532e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.6, wall=5797
2024-01-31 06:20:24 | INFO | train_inner | epoch 158:    538 / 1475 loss=2.194, nll_loss=0.542, ppl=1.46, wps=58213.4, ups=15.56, wpb=3741.5, bsz=202.7, num_updates=232100, lr=6.56391e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.9, wall=5803
2024-01-31 06:20:31 | INFO | train_inner | epoch 158:    638 / 1475 loss=2.196, nll_loss=0.544, ppl=1.46, wps=59106.3, ups=15.62, wpb=3785.1, bsz=193.9, num_updates=232200, lr=6.56249e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.8, wall=5810
2024-01-31 06:20:37 | INFO | train_inner | epoch 158:    738 / 1475 loss=2.204, nll_loss=0.553, ppl=1.47, wps=58478.9, ups=15.71, wpb=3723.2, bsz=186.6, num_updates=232300, lr=6.56108e-05, gnorm=1.018, loss_scale=16, train_wall=6, gb_free=29.7, wall=5816
2024-01-31 06:20:43 | INFO | train_inner | epoch 158:    838 / 1475 loss=2.203, nll_loss=0.552, ppl=1.47, wps=58356, ups=15.63, wpb=3734.8, bsz=185.6, num_updates=232400, lr=6.55967e-05, gnorm=1.031, loss_scale=16, train_wall=6, gb_free=29.7, wall=5822
2024-01-31 06:20:50 | INFO | train_inner | epoch 158:    938 / 1475 loss=2.208, nll_loss=0.558, ppl=1.47, wps=59246.7, ups=15.61, wpb=3795.6, bsz=186.9, num_updates=232500, lr=6.55826e-05, gnorm=1.004, loss_scale=16, train_wall=6, gb_free=29.7, wall=5829
2024-01-31 06:20:56 | INFO | train_inner | epoch 158:   1038 / 1475 loss=2.207, nll_loss=0.557, ppl=1.47, wps=59096.3, ups=15.5, wpb=3811.9, bsz=189.8, num_updates=232600, lr=6.55685e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.7, wall=5835
2024-01-31 06:21:03 | INFO | train_inner | epoch 158:   1138 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=58585.2, ups=15.57, wpb=3762.2, bsz=204.7, num_updates=232700, lr=6.55544e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=5842
2024-01-31 06:21:09 | INFO | train_inner | epoch 158:   1238 / 1475 loss=2.205, nll_loss=0.555, ppl=1.47, wps=60025.5, ups=15.57, wpb=3855.8, bsz=190.6, num_updates=232800, lr=6.55403e-05, gnorm=1.009, loss_scale=16, train_wall=6, gb_free=29.7, wall=5848
2024-01-31 06:21:16 | INFO | train_inner | epoch 158:   1338 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=58248.1, ups=15.43, wpb=3774.6, bsz=207.6, num_updates=232900, lr=6.55262e-05, gnorm=0.974, loss_scale=16, train_wall=6, gb_free=29.8, wall=5855
2024-01-31 06:21:22 | INFO | train_inner | epoch 158:   1438 / 1475 loss=2.21, nll_loss=0.56, ppl=1.47, wps=58805.6, ups=15.57, wpb=3776, bsz=190.6, num_updates=233000, lr=6.55122e-05, gnorm=1.006, loss_scale=16, train_wall=6, gb_free=29.7, wall=5861
2024-01-31 06:21:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:21:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:21:27 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 3.549 | nll_loss 1.954 | ppl 3.88 | wps 134454 | wpb 3189.1 | bsz 163.4 | num_updates 233037 | best_loss 3.385
2024-01-31 06:21:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 233037 updates
2024-01-31 06:21:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint158.pt
2024-01-31 06:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint158.pt
2024-01-31 06:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint158.pt (epoch 158 @ 233037 updates, score 3.549) (writing took 2.4423463010462 seconds)
2024-01-31 06:21:29 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2024-01-31 06:21:29 | INFO | train | epoch 158 | loss 2.195 | nll_loss 0.543 | ppl 1.46 | wps 55772.5 | ups 14.79 | wpb 3771.6 | bsz 195.9 | num_updates 233037 | lr 6.5507e-05 | gnorm 0.993 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 5868
2024-01-31 06:21:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:21:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:21:29 | INFO | fairseq.trainer | begin training epoch 159
2024-01-31 06:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:21:33 | INFO | train_inner | epoch 159:     63 / 1475 loss=2.191, nll_loss=0.538, ppl=1.45, wps=33801.7, ups=8.82, wpb=3831.8, bsz=194.9, num_updates=233100, lr=6.54981e-05, gnorm=0.987, loss_scale=16, train_wall=6, gb_free=29.7, wall=5872
2024-01-31 06:21:40 | INFO | train_inner | epoch 159:    163 / 1475 loss=2.186, nll_loss=0.532, ppl=1.45, wps=58190.1, ups=15.54, wpb=3745.6, bsz=188.6, num_updates=233200, lr=6.54841e-05, gnorm=1.029, loss_scale=16, train_wall=6, gb_free=29.7, wall=5879
2024-01-31 06:21:46 | INFO | train_inner | epoch 159:    263 / 1475 loss=2.191, nll_loss=0.538, ppl=1.45, wps=58840.3, ups=15.61, wpb=3769.5, bsz=187, num_updates=233300, lr=6.547e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.6, wall=5885
2024-01-31 06:21:53 | INFO | train_inner | epoch 159:    363 / 1475 loss=2.186, nll_loss=0.533, ppl=1.45, wps=58308.5, ups=15.57, wpb=3746, bsz=193.3, num_updates=233400, lr=6.5456e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.8, wall=5892
2024-01-31 06:21:59 | INFO | train_inner | epoch 159:    463 / 1475 loss=2.194, nll_loss=0.541, ppl=1.45, wps=59950.2, ups=15.59, wpb=3845.8, bsz=196.3, num_updates=233500, lr=6.5442e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.6, wall=5898
2024-01-31 06:22:06 | INFO | train_inner | epoch 159:    563 / 1475 loss=2.182, nll_loss=0.529, ppl=1.44, wps=58795.6, ups=15.43, wpb=3810.3, bsz=202.8, num_updates=233600, lr=6.5428e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=5904
2024-01-31 06:22:12 | INFO | train_inner | epoch 159:    663 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=56833.6, ups=15.55, wpb=3654.4, bsz=198.6, num_updates=233700, lr=6.5414e-05, gnorm=0.994, loss_scale=16, train_wall=6, gb_free=29.8, wall=5911
2024-01-31 06:22:18 | INFO | train_inner | epoch 159:    763 / 1475 loss=2.203, nll_loss=0.552, ppl=1.47, wps=59555.3, ups=15.63, wpb=3809.6, bsz=194, num_updates=233800, lr=6.54e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.9, wall=5917
2024-01-31 06:22:25 | INFO | train_inner | epoch 159:    863 / 1475 loss=2.192, nll_loss=0.54, ppl=1.45, wps=58722.6, ups=15.45, wpb=3801.9, bsz=203.8, num_updates=233900, lr=6.5386e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.8, wall=5924
2024-01-31 06:22:31 | INFO | train_inner | epoch 159:    963 / 1475 loss=2.2, nll_loss=0.549, ppl=1.46, wps=59273.4, ups=15.54, wpb=3814.4, bsz=201.1, num_updates=234000, lr=6.5372e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.7, wall=5930
2024-01-31 06:22:38 | INFO | train_inner | epoch 159:   1063 / 1475 loss=2.197, nll_loss=0.546, ppl=1.46, wps=59442, ups=15.52, wpb=3829.5, bsz=202.2, num_updates=234100, lr=6.53581e-05, gnorm=1.018, loss_scale=16, train_wall=6, gb_free=29.7, wall=5937
2024-01-31 06:22:44 | INFO | train_inner | epoch 159:   1163 / 1475 loss=2.208, nll_loss=0.557, ppl=1.47, wps=58886.8, ups=15.67, wpb=3757.8, bsz=193, num_updates=234200, lr=6.53441e-05, gnorm=1.02, loss_scale=16, train_wall=6, gb_free=29.7, wall=5943
2024-01-31 06:22:51 | INFO | train_inner | epoch 159:   1263 / 1475 loss=2.193, nll_loss=0.541, ppl=1.46, wps=57814.7, ups=15.5, wpb=3730.4, bsz=202.4, num_updates=234300, lr=6.53302e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.7, wall=5949
2024-01-31 06:22:57 | INFO | train_inner | epoch 159:   1363 / 1475 loss=2.203, nll_loss=0.552, ppl=1.47, wps=58184.8, ups=15.57, wpb=3737.8, bsz=192.6, num_updates=234400, lr=6.53162e-05, gnorm=1.025, loss_scale=16, train_wall=6, gb_free=29.7, wall=5956
2024-01-31 06:23:03 | INFO | train_inner | epoch 159:   1463 / 1475 loss=2.206, nll_loss=0.557, ppl=1.47, wps=58580.8, ups=15.67, wpb=3738.7, bsz=194.6, num_updates=234500, lr=6.53023e-05, gnorm=1.036, loss_scale=16, train_wall=6, gb_free=29.8, wall=5962
2024-01-31 06:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:23:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:23:06 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 3.549 | nll_loss 1.957 | ppl 3.88 | wps 134758 | wpb 3189.1 | bsz 163.4 | num_updates 234512 | best_loss 3.385
2024-01-31 06:23:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 234512 updates
2024-01-31 06:23:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint159.pt
2024-01-31 06:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint159.pt
2024-01-31 06:23:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint159.pt (epoch 159 @ 234512 updates, score 3.549) (writing took 2.4073400950292125 seconds)
2024-01-31 06:23:09 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2024-01-31 06:23:09 | INFO | train | epoch 159 | loss 2.194 | nll_loss 0.542 | ppl 1.46 | wps 55816.9 | ups 14.8 | wpb 3771.6 | bsz 195.9 | num_updates 234512 | lr 6.53006e-05 | gnorm 1 | loss_scale 16 | train_wall 92 | gb_free 29.8 | wall 5968
2024-01-31 06:23:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:23:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:23:09 | INFO | fairseq.trainer | begin training epoch 160
2024-01-31 06:23:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:23:15 | INFO | train_inner | epoch 160:     88 / 1475 loss=2.176, nll_loss=0.523, ppl=1.44, wps=33057.6, ups=8.83, wpb=3743.3, bsz=203.2, num_updates=234600, lr=6.52884e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.7, wall=5974
2024-01-31 06:23:21 | INFO | train_inner | epoch 160:    188 / 1475 loss=2.184, nll_loss=0.53, ppl=1.44, wps=58850.6, ups=15.59, wpb=3775.7, bsz=188.3, num_updates=234700, lr=6.52745e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.8, wall=5980
2024-01-31 06:23:28 | INFO | train_inner | epoch 160:    288 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=57839, ups=15.49, wpb=3733.8, bsz=190.5, num_updates=234800, lr=6.52606e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.6, wall=5986
2024-01-31 06:23:34 | INFO | train_inner | epoch 160:    388 / 1475 loss=2.179, nll_loss=0.525, ppl=1.44, wps=57783.3, ups=15.46, wpb=3737.8, bsz=205.7, num_updates=234900, lr=6.52467e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.7, wall=5993
2024-01-31 06:23:40 | INFO | train_inner | epoch 160:    488 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=59279.5, ups=15.56, wpb=3809.6, bsz=193.4, num_updates=235000, lr=6.52328e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.8, wall=5999
2024-01-31 06:23:47 | INFO | train_inner | epoch 160:    588 / 1475 loss=2.194, nll_loss=0.541, ppl=1.46, wps=58399.2, ups=15.57, wpb=3751.9, bsz=190.6, num_updates=235100, lr=6.52189e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.8, wall=6006
2024-01-31 06:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 06:23:53 | INFO | train_inner | epoch 160:    689 / 1475 loss=2.194, nll_loss=0.543, ppl=1.46, wps=58605.8, ups=15.37, wpb=3812.9, bsz=200.4, num_updates=235200, lr=6.52051e-05, gnorm=0.982, loss_scale=8, train_wall=6, gb_free=29.7, wall=6012
2024-01-31 06:24:00 | INFO | train_inner | epoch 160:    789 / 1475 loss=2.2, nll_loss=0.549, ppl=1.46, wps=59331.6, ups=15.59, wpb=3805.8, bsz=191.4, num_updates=235300, lr=6.51912e-05, gnorm=0.985, loss_scale=8, train_wall=6, gb_free=29.6, wall=6019
2024-01-31 06:24:06 | INFO | train_inner | epoch 160:    889 / 1475 loss=2.194, nll_loss=0.542, ppl=1.46, wps=57828.8, ups=15.58, wpb=3710.6, bsz=191.3, num_updates=235400, lr=6.51774e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.8, wall=6025
2024-01-31 06:24:13 | INFO | train_inner | epoch 160:    989 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=58846, ups=15.55, wpb=3783.4, bsz=206.2, num_updates=235500, lr=6.51635e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=6032
2024-01-31 06:24:19 | INFO | train_inner | epoch 160:   1089 / 1475 loss=2.205, nll_loss=0.555, ppl=1.47, wps=59297.6, ups=15.59, wpb=3803.3, bsz=189.6, num_updates=235600, lr=6.51497e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.8, wall=6038
2024-01-31 06:24:26 | INFO | train_inner | epoch 160:   1189 / 1475 loss=2.197, nll_loss=0.546, ppl=1.46, wps=59569.2, ups=15.57, wpb=3826.7, bsz=197.2, num_updates=235700, lr=6.51359e-05, gnorm=0.986, loss_scale=8, train_wall=6, gb_free=29.7, wall=6044
2024-01-31 06:24:32 | INFO | train_inner | epoch 160:   1289 / 1475 loss=2.202, nll_loss=0.552, ppl=1.47, wps=58890.2, ups=15.58, wpb=3778.9, bsz=190.6, num_updates=235800, lr=6.51221e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.8, wall=6051
2024-01-31 06:24:38 | INFO | train_inner | epoch 160:   1389 / 1475 loss=2.201, nll_loss=0.549, ppl=1.46, wps=58504, ups=15.66, wpb=3735, bsz=193.7, num_updates=235900, lr=6.51083e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.7, wall=6057
2024-01-31 06:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:24:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:24:46 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 3.548 | nll_loss 1.957 | ppl 3.88 | wps 135938 | wpb 3189.1 | bsz 163.4 | num_updates 235986 | best_loss 3.385
2024-01-31 06:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 235986 updates
2024-01-31 06:24:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint160.pt
2024-01-31 06:24:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint160.pt
2024-01-31 06:24:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint160.pt (epoch 160 @ 235986 updates, score 3.548) (writing took 2.4133498129667714 seconds)
2024-01-31 06:24:49 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2024-01-31 06:24:49 | INFO | train | epoch 160 | loss 2.193 | nll_loss 0.541 | ppl 1.45 | wps 55767.9 | ups 14.78 | wpb 3772.2 | bsz 195.9 | num_updates 235986 | lr 6.50964e-05 | gnorm 0.991 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 6067
2024-01-31 06:24:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:24:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:24:49 | INFO | fairseq.trainer | begin training epoch 161
2024-01-31 06:24:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:24:50 | INFO | train_inner | epoch 161:     14 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=33183.1, ups=8.85, wpb=3751.5, bsz=203.2, num_updates=236000, lr=6.50945e-05, gnorm=0.982, loss_scale=8, train_wall=6, gb_free=29.7, wall=6069
2024-01-31 06:24:56 | INFO | train_inner | epoch 161:    114 / 1475 loss=2.179, nll_loss=0.525, ppl=1.44, wps=58679.2, ups=15.46, wpb=3796.6, bsz=198.4, num_updates=236100, lr=6.50807e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.7, wall=6075
2024-01-31 06:25:03 | INFO | train_inner | epoch 161:    214 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=58905.3, ups=15.52, wpb=3795.1, bsz=194.5, num_updates=236200, lr=6.50669e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=6081
2024-01-31 06:25:09 | INFO | train_inner | epoch 161:    314 / 1475 loss=2.178, nll_loss=0.523, ppl=1.44, wps=58038.5, ups=15.62, wpb=3716, bsz=192.5, num_updates=236300, lr=6.50531e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.6, wall=6088
2024-01-31 06:25:15 | INFO | train_inner | epoch 161:    414 / 1475 loss=2.176, nll_loss=0.522, ppl=1.44, wps=58754.6, ups=15.53, wpb=3784.3, bsz=206.3, num_updates=236400, lr=6.50394e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.7, wall=6094
2024-01-31 06:25:22 | INFO | train_inner | epoch 161:    514 / 1475 loss=2.186, nll_loss=0.532, ppl=1.45, wps=58674, ups=15.52, wpb=3779.6, bsz=198.1, num_updates=236500, lr=6.50256e-05, gnorm=0.985, loss_scale=8, train_wall=6, gb_free=29.8, wall=6101
2024-01-31 06:25:28 | INFO | train_inner | epoch 161:    614 / 1475 loss=2.181, nll_loss=0.529, ppl=1.44, wps=59116.7, ups=15.45, wpb=3826.4, bsz=213.9, num_updates=236600, lr=6.50119e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.7, wall=6107
2024-01-31 06:25:35 | INFO | train_inner | epoch 161:    714 / 1475 loss=2.204, nll_loss=0.554, ppl=1.47, wps=59127.1, ups=15.64, wpb=3780.8, bsz=184.5, num_updates=236700, lr=6.49981e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.7, wall=6114
2024-01-31 06:25:41 | INFO | train_inner | epoch 161:    814 / 1475 loss=2.194, nll_loss=0.542, ppl=1.46, wps=58221.1, ups=15.62, wpb=3727.3, bsz=186.2, num_updates=236800, lr=6.49844e-05, gnorm=1.008, loss_scale=8, train_wall=6, gb_free=29.7, wall=6120
2024-01-31 06:25:48 | INFO | train_inner | epoch 161:    914 / 1475 loss=2.196, nll_loss=0.545, ppl=1.46, wps=58833.9, ups=15.58, wpb=3776.2, bsz=197.7, num_updates=236900, lr=6.49707e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=6126
2024-01-31 06:25:54 | INFO | train_inner | epoch 161:   1014 / 1475 loss=2.202, nll_loss=0.551, ppl=1.47, wps=58861.5, ups=15.55, wpb=3784.6, bsz=196.5, num_updates=237000, lr=6.4957e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=6133
2024-01-31 06:26:00 | INFO | train_inner | epoch 161:   1114 / 1475 loss=2.2, nll_loss=0.55, ppl=1.46, wps=58515.8, ups=15.52, wpb=3770.3, bsz=197, num_updates=237100, lr=6.49433e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=6139
2024-01-31 06:26:07 | INFO | train_inner | epoch 161:   1214 / 1475 loss=2.203, nll_loss=0.552, ppl=1.47, wps=58832.8, ups=15.6, wpb=3770.9, bsz=188.3, num_updates=237200, lr=6.49296e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.6, wall=6146
2024-01-31 06:26:13 | INFO | train_inner | epoch 161:   1314 / 1475 loss=2.196, nll_loss=0.545, ppl=1.46, wps=58848.4, ups=15.47, wpb=3805.1, bsz=199.3, num_updates=237300, lr=6.49159e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.7, wall=6152
2024-01-31 06:26:20 | INFO | train_inner | epoch 161:   1414 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=57522.8, ups=15.64, wpb=3679, bsz=193.8, num_updates=237400, lr=6.49022e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.7, wall=6159
2024-01-31 06:26:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:26:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:26:26 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 3.551 | nll_loss 1.958 | ppl 3.88 | wps 134627 | wpb 3189.1 | bsz 163.4 | num_updates 237461 | best_loss 3.385
2024-01-31 06:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 237461 updates
2024-01-31 06:26:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint161.pt
2024-01-31 06:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint161.pt
2024-01-31 06:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint161.pt (epoch 161 @ 237461 updates, score 3.551) (writing took 2.377672411967069 seconds)
2024-01-31 06:26:28 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2024-01-31 06:26:28 | INFO | train | epoch 161 | loss 2.192 | nll_loss 0.54 | ppl 1.45 | wps 55797.2 | ups 14.79 | wpb 3771.6 | bsz 195.9 | num_updates 237461 | lr 6.48939e-05 | gnorm 0.994 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 6167
2024-01-31 06:26:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:26:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:26:28 | INFO | fairseq.trainer | begin training epoch 162
2024-01-31 06:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:26:31 | INFO | train_inner | epoch 162:     39 / 1475 loss=2.188, nll_loss=0.536, ppl=1.45, wps=33104, ups=8.86, wpb=3737, bsz=192.3, num_updates=237500, lr=6.48886e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.8, wall=6170
2024-01-31 06:26:37 | INFO | train_inner | epoch 162:    139 / 1475 loss=2.179, nll_loss=0.524, ppl=1.44, wps=59101.5, ups=15.55, wpb=3800.4, bsz=197.7, num_updates=237600, lr=6.48749e-05, gnorm=0.986, loss_scale=8, train_wall=6, gb_free=29.7, wall=6176
2024-01-31 06:26:44 | INFO | train_inner | epoch 162:    239 / 1475 loss=2.167, nll_loss=0.512, ppl=1.43, wps=59230.5, ups=15.49, wpb=3824.8, bsz=213, num_updates=237700, lr=6.48613e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.7, wall=6183
2024-01-31 06:26:50 | INFO | train_inner | epoch 162:    339 / 1475 loss=2.185, nll_loss=0.533, ppl=1.45, wps=57961.8, ups=15.65, wpb=3703.3, bsz=189.6, num_updates=237800, lr=6.48476e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.6, wall=6189
2024-01-31 06:26:57 | INFO | train_inner | epoch 162:    439 / 1475 loss=2.199, nll_loss=0.548, ppl=1.46, wps=57829.7, ups=15.61, wpb=3705.5, bsz=184.1, num_updates=237900, lr=6.4834e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=6196
2024-01-31 06:27:03 | INFO | train_inner | epoch 162:    539 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=58495.4, ups=15.59, wpb=3751.5, bsz=203.6, num_updates=238000, lr=6.48204e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.8, wall=6202
2024-01-31 06:27:09 | INFO | train_inner | epoch 162:    639 / 1475 loss=2.192, nll_loss=0.54, ppl=1.45, wps=58647.4, ups=15.65, wpb=3747.7, bsz=187.6, num_updates=238100, lr=6.48068e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.7, wall=6208
2024-01-31 06:27:16 | INFO | train_inner | epoch 162:    739 / 1475 loss=2.195, nll_loss=0.543, ppl=1.46, wps=59333.9, ups=15.64, wpb=3794.8, bsz=194.8, num_updates=238200, lr=6.47932e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.6, wall=6215
2024-01-31 06:27:22 | INFO | train_inner | epoch 162:    839 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=58299.1, ups=15.43, wpb=3777.6, bsz=213.1, num_updates=238300, lr=6.47796e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=6221
2024-01-31 06:27:29 | INFO | train_inner | epoch 162:    939 / 1475 loss=2.194, nll_loss=0.543, ppl=1.46, wps=58872.7, ups=15.54, wpb=3788.1, bsz=198.8, num_updates=238400, lr=6.4766e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.7, wall=6228
2024-01-31 06:27:35 | INFO | train_inner | epoch 162:   1039 / 1475 loss=2.192, nll_loss=0.54, ppl=1.45, wps=59648.6, ups=15.64, wpb=3813.7, bsz=208.5, num_updates=238500, lr=6.47524e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.8, wall=6234
2024-01-31 06:27:42 | INFO | train_inner | epoch 162:   1139 / 1475 loss=2.201, nll_loss=0.549, ppl=1.46, wps=59295.1, ups=15.6, wpb=3800.2, bsz=187.5, num_updates=238600, lr=6.47388e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.7, wall=6240
2024-01-31 06:27:48 | INFO | train_inner | epoch 162:   1239 / 1475 loss=2.206, nll_loss=0.556, ppl=1.47, wps=59458.3, ups=15.66, wpb=3797.5, bsz=185.9, num_updates=238700, lr=6.47253e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=6247
2024-01-31 06:27:54 | INFO | train_inner | epoch 162:   1339 / 1475 loss=2.202, nll_loss=0.551, ppl=1.47, wps=59349.3, ups=15.59, wpb=3806, bsz=199, num_updates=238800, lr=6.47117e-05, gnorm=0.986, loss_scale=8, train_wall=6, gb_free=29.7, wall=6253
2024-01-31 06:28:01 | INFO | train_inner | epoch 162:   1439 / 1475 loss=2.209, nll_loss=0.56, ppl=1.47, wps=58515.3, ups=15.59, wpb=3752.4, bsz=181.4, num_updates=238900, lr=6.46982e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.7, wall=6260
2024-01-31 06:28:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:28:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:28:05 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 3.56 | nll_loss 1.968 | ppl 3.91 | wps 135297 | wpb 3189.1 | bsz 163.4 | num_updates 238936 | best_loss 3.385
2024-01-31 06:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 238936 updates
2024-01-31 06:28:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint162.pt
2024-01-31 06:28:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint162.pt
2024-01-31 06:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint162.pt (epoch 162 @ 238936 updates, score 3.56) (writing took 2.429766667075455 seconds)
2024-01-31 06:28:08 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2024-01-31 06:28:08 | INFO | train | epoch 162 | loss 2.191 | nll_loss 0.539 | ppl 1.45 | wps 55891.1 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 238936 | lr 6.46933e-05 | gnorm 0.995 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 6267
2024-01-31 06:28:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:28:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:28:08 | INFO | fairseq.trainer | begin training epoch 163
2024-01-31 06:28:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:28:12 | INFO | train_inner | epoch 163:     64 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=33436.7, ups=8.85, wpb=3777, bsz=193.8, num_updates=239000, lr=6.46846e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.8, wall=6271
2024-01-31 06:28:18 | INFO | train_inner | epoch 163:    164 / 1475 loss=2.173, nll_loss=0.52, ppl=1.43, wps=59484.9, ups=15.52, wpb=3832.2, bsz=203.4, num_updates=239100, lr=6.46711e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.7, wall=6277
2024-01-31 06:28:25 | INFO | train_inner | epoch 163:    264 / 1475 loss=2.181, nll_loss=0.526, ppl=1.44, wps=57992.1, ups=15.61, wpb=3714.5, bsz=190.6, num_updates=239200, lr=6.46576e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.6, wall=6284
2024-01-31 06:28:31 | INFO | train_inner | epoch 163:    364 / 1475 loss=2.184, nll_loss=0.531, ppl=1.44, wps=58171.7, ups=15.59, wpb=3731.3, bsz=193, num_updates=239300, lr=6.46441e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=6290
2024-01-31 06:28:38 | INFO | train_inner | epoch 163:    464 / 1475 loss=2.185, nll_loss=0.532, ppl=1.45, wps=59156.3, ups=15.53, wpb=3808.6, bsz=193.2, num_updates=239400, lr=6.46306e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.6, wall=6297
2024-01-31 06:28:44 | INFO | train_inner | epoch 163:    564 / 1475 loss=2.192, nll_loss=0.539, ppl=1.45, wps=58662.1, ups=15.55, wpb=3772.6, bsz=193, num_updates=239500, lr=6.46171e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.7, wall=6303
2024-01-31 06:28:51 | INFO | train_inner | epoch 163:    664 / 1475 loss=2.186, nll_loss=0.534, ppl=1.45, wps=59268.2, ups=15.59, wpb=3801.2, bsz=195.1, num_updates=239600, lr=6.46036e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.8, wall=6309
2024-01-31 06:28:57 | INFO | train_inner | epoch 163:    764 / 1475 loss=2.185, nll_loss=0.533, ppl=1.45, wps=57697.9, ups=15.5, wpb=3721.9, bsz=195.7, num_updates=239700, lr=6.45901e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=6316
2024-01-31 06:29:03 | INFO | train_inner | epoch 163:    864 / 1475 loss=2.186, nll_loss=0.534, ppl=1.45, wps=59325.1, ups=15.54, wpb=3817.8, bsz=209.9, num_updates=239800, lr=6.45766e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.7, wall=6322
2024-01-31 06:29:10 | INFO | train_inner | epoch 163:    964 / 1475 loss=2.211, nll_loss=0.562, ppl=1.48, wps=59393.3, ups=15.64, wpb=3798, bsz=185, num_updates=239900, lr=6.45632e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=6329
2024-01-31 06:29:16 | INFO | train_inner | epoch 163:   1064 / 1475 loss=2.187, nll_loss=0.535, ppl=1.45, wps=59372.5, ups=15.58, wpb=3810, bsz=203.4, num_updates=240000, lr=6.45497e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=6335
2024-01-31 06:29:23 | INFO | train_inner | epoch 163:   1164 / 1475 loss=2.192, nll_loss=0.541, ppl=1.45, wps=59224.7, ups=15.61, wpb=3792.9, bsz=197.8, num_updates=240100, lr=6.45363e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.8, wall=6342
2024-01-31 06:29:29 | INFO | train_inner | epoch 163:   1264 / 1475 loss=2.203, nll_loss=0.554, ppl=1.47, wps=58409.4, ups=15.68, wpb=3725.4, bsz=187.9, num_updates=240200, lr=6.45228e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.6, wall=6348
2024-01-31 06:29:35 | INFO | train_inner | epoch 163:   1364 / 1475 loss=2.198, nll_loss=0.547, ppl=1.46, wps=58050.9, ups=15.6, wpb=3722.1, bsz=193.5, num_updates=240300, lr=6.45094e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.9, wall=6354
2024-01-31 06:29:42 | INFO | train_inner | epoch 163:   1464 / 1475 loss=2.192, nll_loss=0.541, ppl=1.45, wps=57979, ups=15.6, wpb=3715.4, bsz=205.2, num_updates=240400, lr=6.4496e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.8, wall=6361
2024-01-31 06:29:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:29:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:29:45 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 3.552 | nll_loss 1.96 | ppl 3.89 | wps 134995 | wpb 3189.1 | bsz 163.4 | num_updates 240411 | best_loss 3.385
2024-01-31 06:29:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 240411 updates
2024-01-31 06:29:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint163.pt
2024-01-31 06:29:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint163.pt
2024-01-31 06:29:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint163.pt (epoch 163 @ 240411 updates, score 3.552) (writing took 2.4311916660517454 seconds)
2024-01-31 06:29:47 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2024-01-31 06:29:47 | INFO | train | epoch 163 | loss 2.189 | nll_loss 0.537 | ppl 1.45 | wps 55886.5 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 240411 | lr 6.44945e-05 | gnorm 0.995 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 6366
2024-01-31 06:29:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:29:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:29:47 | INFO | fairseq.trainer | begin training epoch 164
2024-01-31 06:29:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:29:53 | INFO | train_inner | epoch 164:     89 / 1475 loss=2.184, nll_loss=0.53, ppl=1.44, wps=33018.4, ups=8.84, wpb=3736.1, bsz=190.5, num_updates=240500, lr=6.44826e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.8, wall=6372
2024-01-31 06:30:00 | INFO | train_inner | epoch 164:    189 / 1475 loss=2.183, nll_loss=0.529, ppl=1.44, wps=58828.8, ups=15.61, wpb=3768.9, bsz=186.5, num_updates=240600, lr=6.44692e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.7, wall=6379
2024-01-31 06:30:06 | INFO | train_inner | epoch 164:    289 / 1475 loss=2.174, nll_loss=0.519, ppl=1.43, wps=59457.8, ups=15.59, wpb=3813.2, bsz=197.2, num_updates=240700, lr=6.44558e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.7, wall=6385
2024-01-31 06:30:12 | INFO | train_inner | epoch 164:    389 / 1475 loss=2.188, nll_loss=0.535, ppl=1.45, wps=59716.9, ups=15.59, wpb=3831, bsz=188.5, num_updates=240800, lr=6.44424e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.7, wall=6391
2024-01-31 06:30:19 | INFO | train_inner | epoch 164:    489 / 1475 loss=2.175, nll_loss=0.521, ppl=1.43, wps=58736.5, ups=15.6, wpb=3765.6, bsz=204.9, num_updates=240900, lr=6.4429e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=6398
2024-01-31 06:30:25 | INFO | train_inner | epoch 164:    589 / 1475 loss=2.178, nll_loss=0.525, ppl=1.44, wps=58245.6, ups=15.47, wpb=3764.5, bsz=205.8, num_updates=241000, lr=6.44157e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.8, wall=6404
2024-01-31 06:30:32 | INFO | train_inner | epoch 164:    689 / 1475 loss=2.189, nll_loss=0.537, ppl=1.45, wps=58506.4, ups=15.64, wpb=3741.7, bsz=193.2, num_updates=241100, lr=6.44023e-05, gnorm=1.027, loss_scale=8, train_wall=6, gb_free=29.6, wall=6411
2024-01-31 06:30:38 | INFO | train_inner | epoch 164:    789 / 1475 loss=2.19, nll_loss=0.538, ppl=1.45, wps=58526.6, ups=15.56, wpb=3761, bsz=196, num_updates=241200, lr=6.4389e-05, gnorm=0.985, loss_scale=8, train_wall=6, gb_free=29.7, wall=6417
2024-01-31 06:30:45 | INFO | train_inner | epoch 164:    889 / 1475 loss=2.191, nll_loss=0.54, ppl=1.45, wps=59226, ups=15.55, wpb=3809.9, bsz=199.6, num_updates=241300, lr=6.43756e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=6423
2024-01-31 06:30:51 | INFO | train_inner | epoch 164:    989 / 1475 loss=2.191, nll_loss=0.539, ppl=1.45, wps=59575.3, ups=15.57, wpb=3826.9, bsz=201.8, num_updates=241400, lr=6.43623e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.6, wall=6430
2024-01-31 06:30:57 | INFO | train_inner | epoch 164:   1089 / 1475 loss=2.186, nll_loss=0.535, ppl=1.45, wps=57775.6, ups=15.4, wpb=3751.4, bsz=206.4, num_updates=241500, lr=6.43489e-05, gnorm=1.055, loss_scale=8, train_wall=6, gb_free=29.9, wall=6436
2024-01-31 06:31:04 | INFO | train_inner | epoch 164:   1189 / 1475 loss=2.214, nll_loss=0.565, ppl=1.48, wps=59759.6, ups=15.74, wpb=3797.5, bsz=179.6, num_updates=241600, lr=6.43356e-05, gnorm=1.045, loss_scale=8, train_wall=6, gb_free=29.7, wall=6443
2024-01-31 06:31:10 | INFO | train_inner | epoch 164:   1289 / 1475 loss=2.196, nll_loss=0.546, ppl=1.46, wps=58803.4, ups=15.53, wpb=3786.8, bsz=197, num_updates=241700, lr=6.43223e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.6, wall=6449
2024-01-31 06:31:17 | INFO | train_inner | epoch 164:   1389 / 1475 loss=2.196, nll_loss=0.546, ppl=1.46, wps=58655.4, ups=15.64, wpb=3750.6, bsz=192.4, num_updates=241800, lr=6.4309e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=6456
2024-01-31 06:31:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:31:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:31:24 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 3.556 | nll_loss 1.962 | ppl 3.9 | wps 134847 | wpb 3189.1 | bsz 163.4 | num_updates 241886 | best_loss 3.385
2024-01-31 06:31:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 241886 updates
2024-01-31 06:31:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint164.pt
2024-01-31 06:31:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint164.pt
2024-01-31 06:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint164.pt (epoch 164 @ 241886 updates, score 3.556) (writing took 2.426715859095566 seconds)
2024-01-31 06:31:27 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2024-01-31 06:31:27 | INFO | train | epoch 164 | loss 2.188 | nll_loss 0.536 | ppl 1.45 | wps 55861.3 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 241886 | lr 6.42976e-05 | gnorm 1.002 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 6466
2024-01-31 06:31:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:31:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:31:27 | INFO | fairseq.trainer | begin training epoch 165
2024-01-31 06:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:31:28 | INFO | train_inner | epoch 165:     14 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=32479.1, ups=8.84, wpb=3673.1, bsz=197.8, num_updates=241900, lr=6.42957e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.8, wall=6467
2024-01-31 06:31:34 | INFO | train_inner | epoch 165:    114 / 1475 loss=2.19, nll_loss=0.536, ppl=1.45, wps=59737.6, ups=15.68, wpb=3809.5, bsz=180.2, num_updates=242000, lr=6.42824e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=6473
2024-01-31 06:31:41 | INFO | train_inner | epoch 165:    214 / 1475 loss=2.19, nll_loss=0.537, ppl=1.45, wps=59382.7, ups=15.64, wpb=3797.6, bsz=180.3, num_updates=242100, lr=6.42692e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=6480
2024-01-31 06:31:47 | INFO | train_inner | epoch 165:    314 / 1475 loss=2.181, nll_loss=0.527, ppl=1.44, wps=59089.7, ups=15.59, wpb=3790.8, bsz=192.6, num_updates=242200, lr=6.42559e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.7, wall=6486
2024-01-31 06:31:54 | INFO | train_inner | epoch 165:    414 / 1475 loss=2.168, nll_loss=0.513, ppl=1.43, wps=59200.3, ups=15.48, wpb=3823.5, bsz=219.8, num_updates=242300, lr=6.42426e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.8, wall=6493
2024-01-31 06:32:00 | INFO | train_inner | epoch 165:    514 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=58440.4, ups=15.63, wpb=3740.1, bsz=200.4, num_updates=242400, lr=6.42294e-05, gnorm=0.986, loss_scale=8, train_wall=6, gb_free=29.8, wall=6499
2024-01-31 06:32:06 | INFO | train_inner | epoch 165:    614 / 1475 loss=2.199, nll_loss=0.548, ppl=1.46, wps=59362.2, ups=15.66, wpb=3790.6, bsz=184.2, num_updates=242500, lr=6.42161e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=6505
2024-01-31 06:32:13 | INFO | train_inner | epoch 165:    714 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=57024, ups=15.66, wpb=3642.1, bsz=207.4, num_updates=242600, lr=6.42029e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.7, wall=6512
2024-01-31 06:32:19 | INFO | train_inner | epoch 165:    814 / 1475 loss=2.187, nll_loss=0.535, ppl=1.45, wps=58700.1, ups=15.55, wpb=3775.2, bsz=204.7, num_updates=242700, lr=6.41897e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.7, wall=6518
2024-01-31 06:32:26 | INFO | train_inner | epoch 165:    914 / 1475 loss=2.187, nll_loss=0.536, ppl=1.45, wps=59513.8, ups=15.48, wpb=3845.2, bsz=200.8, num_updates=242800, lr=6.41764e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=6525
2024-01-31 06:32:32 | INFO | train_inner | epoch 165:   1014 / 1475 loss=2.191, nll_loss=0.54, ppl=1.45, wps=57652.3, ups=15.6, wpb=3695.7, bsz=189.8, num_updates=242900, lr=6.41632e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.7, wall=6531
2024-01-31 06:32:39 | INFO | train_inner | epoch 165:   1114 / 1475 loss=2.192, nll_loss=0.541, ppl=1.45, wps=59265.4, ups=15.59, wpb=3801.4, bsz=195.5, num_updates=243000, lr=6.415e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.8, wall=6537
2024-01-31 06:32:45 | INFO | train_inner | epoch 165:   1214 / 1475 loss=2.187, nll_loss=0.535, ppl=1.45, wps=58681.8, ups=15.54, wpb=3776.5, bsz=205.5, num_updates=243100, lr=6.41368e-05, gnorm=1.108, loss_scale=8, train_wall=6, gb_free=29.8, wall=6544
2024-01-31 06:32:51 | INFO | train_inner | epoch 165:   1314 / 1475 loss=2.192, nll_loss=0.541, ppl=1.46, wps=58510.7, ups=15.59, wpb=3752.5, bsz=192.1, num_updates=243200, lr=6.41236e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.7, wall=6550
2024-01-31 06:32:58 | INFO | train_inner | epoch 165:   1414 / 1475 loss=2.201, nll_loss=0.551, ppl=1.46, wps=58783.8, ups=15.59, wpb=3771, bsz=191, num_updates=243300, lr=6.41105e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=6557
2024-01-31 06:33:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:33:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:33:04 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 3.557 | nll_loss 1.965 | ppl 3.91 | wps 135156 | wpb 3189.1 | bsz 163.4 | num_updates 243361 | best_loss 3.385
2024-01-31 06:33:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 243361 updates
2024-01-31 06:33:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint165.pt
2024-01-31 06:33:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint165.pt
2024-01-31 06:33:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint165.pt (epoch 165 @ 243361 updates, score 3.557) (writing took 2.4634977519745007 seconds)
2024-01-31 06:33:07 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2024-01-31 06:33:07 | INFO | train | epoch 165 | loss 2.187 | nll_loss 0.535 | ppl 1.45 | wps 55861.9 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 243361 | lr 6.41024e-05 | gnorm 1.006 | loss_scale 8 | train_wall 91 | gb_free 29.9 | wall 6565
2024-01-31 06:33:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:33:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:33:07 | INFO | fairseq.trainer | begin training epoch 166
2024-01-31 06:33:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:33:09 | INFO | train_inner | epoch 166:     39 / 1475 loss=2.187, nll_loss=0.535, ppl=1.45, wps=32980.9, ups=8.78, wpb=3758.4, bsz=196.3, num_updates=243400, lr=6.40973e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.8, wall=6568
2024-01-31 06:33:16 | INFO | train_inner | epoch 166:    139 / 1475 loss=2.18, nll_loss=0.525, ppl=1.44, wps=58464.7, ups=15.62, wpb=3742.8, bsz=190.9, num_updates=243500, lr=6.40841e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.8, wall=6574
2024-01-31 06:33:22 | INFO | train_inner | epoch 166:    239 / 1475 loss=2.178, nll_loss=0.525, ppl=1.44, wps=56765.9, ups=15.58, wpb=3642.5, bsz=188.9, num_updates=243600, lr=6.4071e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.7, wall=6581
2024-01-31 06:33:28 | INFO | train_inner | epoch 166:    339 / 1475 loss=2.193, nll_loss=0.54, ppl=1.45, wps=59840.6, ups=15.63, wpb=3828.3, bsz=183.7, num_updates=243700, lr=6.40578e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.7, wall=6587
2024-01-31 06:33:35 | INFO | train_inner | epoch 166:    439 / 1475 loss=2.182, nll_loss=0.529, ppl=1.44, wps=58336.7, ups=15.63, wpb=3731.4, bsz=190.9, num_updates=243800, lr=6.40447e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.6, wall=6594
2024-01-31 06:33:41 | INFO | train_inner | epoch 166:    539 / 1475 loss=2.165, nll_loss=0.511, ppl=1.43, wps=58627.4, ups=15.49, wpb=3785.8, bsz=210.8, num_updates=243900, lr=6.40316e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.8, wall=6600
2024-01-31 06:33:48 | INFO | train_inner | epoch 166:    639 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=59188.7, ups=15.58, wpb=3799, bsz=215, num_updates=244000, lr=6.40184e-05, gnorm=0.973, loss_scale=8, train_wall=6, gb_free=29.6, wall=6607
2024-01-31 06:33:54 | INFO | train_inner | epoch 166:    739 / 1475 loss=2.186, nll_loss=0.533, ppl=1.45, wps=58899.8, ups=15.59, wpb=3778.9, bsz=193.2, num_updates=244100, lr=6.40053e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=6613
2024-01-31 06:34:01 | INFO | train_inner | epoch 166:    839 / 1475 loss=2.189, nll_loss=0.537, ppl=1.45, wps=59212, ups=15.55, wpb=3806.7, bsz=197.1, num_updates=244200, lr=6.39922e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=6619
2024-01-31 06:34:07 | INFO | train_inner | epoch 166:    939 / 1475 loss=2.187, nll_loss=0.536, ppl=1.45, wps=58756.6, ups=15.53, wpb=3784.3, bsz=200.1, num_updates=244300, lr=6.39791e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.7, wall=6626
2024-01-31 06:34:13 | INFO | train_inner | epoch 166:   1039 / 1475 loss=2.191, nll_loss=0.54, ppl=1.45, wps=58417.4, ups=15.55, wpb=3756.3, bsz=187.2, num_updates=244400, lr=6.3966e-05, gnorm=1.016, loss_scale=8, train_wall=6, gb_free=29.7, wall=6632
2024-01-31 06:34:20 | INFO | train_inner | epoch 166:   1139 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=58123.4, ups=15.53, wpb=3743.1, bsz=205.2, num_updates=244500, lr=6.39529e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.6, wall=6639
2024-01-31 06:34:26 | INFO | train_inner | epoch 166:   1239 / 1475 loss=2.192, nll_loss=0.542, ppl=1.46, wps=58772.4, ups=15.59, wpb=3769.2, bsz=193.4, num_updates=244600, lr=6.39399e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.8, wall=6645
2024-01-31 06:34:33 | INFO | train_inner | epoch 166:   1339 / 1475 loss=2.206, nll_loss=0.557, ppl=1.47, wps=59747.9, ups=15.65, wpb=3817.7, bsz=187.6, num_updates=244700, lr=6.39268e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.7, wall=6652
2024-01-31 06:34:39 | INFO | train_inner | epoch 166:   1439 / 1475 loss=2.186, nll_loss=0.535, ppl=1.45, wps=59210.1, ups=15.5, wpb=3820.1, bsz=208.1, num_updates=244800, lr=6.39137e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=6658
2024-01-31 06:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:34:44 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 3.558 | nll_loss 1.963 | ppl 3.9 | wps 135019 | wpb 3189.1 | bsz 163.4 | num_updates 244836 | best_loss 3.385
2024-01-31 06:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 244836 updates
2024-01-31 06:34:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint166.pt
2024-01-31 06:34:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint166.pt
2024-01-31 06:34:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint166.pt (epoch 166 @ 244836 updates, score 3.558) (writing took 2.3916776080150157 seconds)
2024-01-31 06:34:46 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2024-01-31 06:34:46 | INFO | train | epoch 166 | loss 2.186 | nll_loss 0.534 | ppl 1.45 | wps 55872.3 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 244836 | lr 6.39091e-05 | gnorm 1.001 | loss_scale 8 | train_wall 91 | gb_free 29.6 | wall 6665
2024-01-31 06:34:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:34:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:34:46 | INFO | fairseq.trainer | begin training epoch 167
2024-01-31 06:34:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:34:50 | INFO | train_inner | epoch 167:     64 / 1475 loss=2.192, nll_loss=0.54, ppl=1.45, wps=34095.1, ups=8.85, wpb=3854.4, bsz=191.3, num_updates=244900, lr=6.39007e-05, gnorm=0.982, loss_scale=8, train_wall=6, gb_free=29.7, wall=6669
2024-01-31 06:34:57 | INFO | train_inner | epoch 167:    164 / 1475 loss=2.168, nll_loss=0.514, ppl=1.43, wps=57873.8, ups=15.33, wpb=3774.4, bsz=198.6, num_updates=245000, lr=6.38877e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.6, wall=6676
2024-01-31 06:35:03 | INFO | train_inner | epoch 167:    264 / 1475 loss=2.177, nll_loss=0.524, ppl=1.44, wps=57875.4, ups=15.56, wpb=3718.4, bsz=190.8, num_updates=245100, lr=6.38746e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=6682
2024-01-31 06:35:10 | INFO | train_inner | epoch 167:    364 / 1475 loss=2.173, nll_loss=0.519, ppl=1.43, wps=58139.5, ups=15.61, wpb=3723.7, bsz=201.4, num_updates=245200, lr=6.38616e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=6689
2024-01-31 06:35:16 | INFO | train_inner | epoch 167:    464 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=59122.3, ups=15.62, wpb=3784, bsz=193.3, num_updates=245300, lr=6.38486e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.8, wall=6695
2024-01-31 06:35:23 | INFO | train_inner | epoch 167:    564 / 1475 loss=2.184, nll_loss=0.532, ppl=1.45, wps=58706.2, ups=15.55, wpb=3774.8, bsz=188.2, num_updates=245400, lr=6.38356e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=6701
2024-01-31 06:35:29 | INFO | train_inner | epoch 167:    664 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=59165.3, ups=15.57, wpb=3798.9, bsz=196.9, num_updates=245500, lr=6.38226e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=6708
2024-01-31 06:35:35 | INFO | train_inner | epoch 167:    764 / 1475 loss=2.186, nll_loss=0.534, ppl=1.45, wps=58928.9, ups=15.66, wpb=3762.9, bsz=197, num_updates=245600, lr=6.38096e-05, gnorm=0.993, loss_scale=8, train_wall=6, gb_free=29.7, wall=6714
2024-01-31 06:35:42 | INFO | train_inner | epoch 167:    864 / 1475 loss=2.188, nll_loss=0.536, ppl=1.45, wps=58339.2, ups=15.6, wpb=3739.4, bsz=191.7, num_updates=245700, lr=6.37966e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.9, wall=6721
2024-01-31 06:35:48 | INFO | train_inner | epoch 167:    964 / 1475 loss=2.182, nll_loss=0.53, ppl=1.44, wps=59261.8, ups=15.56, wpb=3808.8, bsz=207.4, num_updates=245800, lr=6.37836e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=6727
2024-01-31 06:35:55 | INFO | train_inner | epoch 167:   1064 / 1475 loss=2.187, nll_loss=0.535, ppl=1.45, wps=58499.8, ups=15.62, wpb=3744.8, bsz=196.6, num_updates=245900, lr=6.37706e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.7, wall=6734
2024-01-31 06:36:01 | INFO | train_inner | epoch 167:   1164 / 1475 loss=2.197, nll_loss=0.546, ppl=1.46, wps=58085.6, ups=15.57, wpb=3731.7, bsz=192.6, num_updates=246000, lr=6.37577e-05, gnorm=1.032, loss_scale=8, train_wall=6, gb_free=29.7, wall=6740
2024-01-31 06:36:07 | INFO | train_inner | epoch 167:   1264 / 1475 loss=2.189, nll_loss=0.537, ppl=1.45, wps=59359.2, ups=15.58, wpb=3808.7, bsz=202.6, num_updates=246100, lr=6.37447e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.6, wall=6746
2024-01-31 06:36:14 | INFO | train_inner | epoch 167:   1364 / 1475 loss=2.203, nll_loss=0.554, ppl=1.47, wps=60304.8, ups=15.65, wpb=3853.8, bsz=187.7, num_updates=246200, lr=6.37318e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.7, wall=6753
2024-01-31 06:36:20 | INFO | train_inner | epoch 167:   1464 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=58426.2, ups=15.67, wpb=3727.4, bsz=196.8, num_updates=246300, lr=6.37188e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=6759
2024-01-31 06:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:36:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:36:23 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 3.559 | nll_loss 1.969 | ppl 3.92 | wps 133204 | wpb 3189.1 | bsz 163.4 | num_updates 246311 | best_loss 3.385
2024-01-31 06:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 246311 updates
2024-01-31 06:36:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint167.pt
2024-01-31 06:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint167.pt
2024-01-31 06:36:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint167.pt (epoch 167 @ 246311 updates, score 3.559) (writing took 2.3323661281028762 seconds)
2024-01-31 06:36:26 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2024-01-31 06:36:26 | INFO | train | epoch 167 | loss 2.185 | nll_loss 0.533 | ppl 1.45 | wps 55889.4 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 246311 | lr 6.37174e-05 | gnorm 1 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 6765
2024-01-31 06:36:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:36:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:36:26 | INFO | fairseq.trainer | begin training epoch 168
2024-01-31 06:36:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:36:31 | INFO | train_inner | epoch 168:     89 / 1475 loss=2.175, nll_loss=0.521, ppl=1.43, wps=32679.1, ups=8.89, wpb=3675.4, bsz=187.7, num_updates=246400, lr=6.37059e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=6770
2024-01-31 06:36:38 | INFO | train_inner | epoch 168:    189 / 1475 loss=2.168, nll_loss=0.514, ppl=1.43, wps=58230.3, ups=15.56, wpb=3742.8, bsz=194.1, num_updates=246500, lr=6.3693e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=6777
2024-01-31 06:36:44 | INFO | train_inner | epoch 168:    289 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=58503, ups=15.68, wpb=3731.2, bsz=183, num_updates=246600, lr=6.36801e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.7, wall=6783
2024-01-31 06:36:51 | INFO | train_inner | epoch 168:    389 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=59387.4, ups=15.57, wpb=3814, bsz=198.9, num_updates=246700, lr=6.36672e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.7, wall=6790
2024-01-31 06:36:57 | INFO | train_inner | epoch 168:    489 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=59571.7, ups=15.63, wpb=3811.9, bsz=195.3, num_updates=246800, lr=6.36543e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=6796
2024-01-31 06:37:04 | INFO | train_inner | epoch 168:    589 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=57923.4, ups=15.5, wpb=3736.9, bsz=198.4, num_updates=246900, lr=6.36414e-05, gnorm=1.002, loss_scale=8, train_wall=6, gb_free=29.7, wall=6802
2024-01-31 06:37:10 | INFO | train_inner | epoch 168:    689 / 1475 loss=2.176, nll_loss=0.523, ppl=1.44, wps=57826, ups=15.59, wpb=3710.2, bsz=200.1, num_updates=247000, lr=6.36285e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.8, wall=6809
2024-01-31 06:37:16 | INFO | train_inner | epoch 168:    789 / 1475 loss=2.184, nll_loss=0.532, ppl=1.45, wps=60120.4, ups=15.62, wpb=3848.5, bsz=197, num_updates=247100, lr=6.36156e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.6, wall=6815
2024-01-31 06:37:23 | INFO | train_inner | epoch 168:    889 / 1475 loss=2.18, nll_loss=0.529, ppl=1.44, wps=58401.5, ups=15.52, wpb=3763.9, bsz=199.1, num_updates=247200, lr=6.36027e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=6822
2024-01-31 06:37:29 | INFO | train_inner | epoch 168:    989 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=59532.4, ups=15.55, wpb=3827.5, bsz=193.2, num_updates=247300, lr=6.35899e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.8, wall=6828
2024-01-31 06:37:36 | INFO | train_inner | epoch 168:   1089 / 1475 loss=2.191, nll_loss=0.539, ppl=1.45, wps=58712.8, ups=15.65, wpb=3751.8, bsz=196, num_updates=247400, lr=6.3577e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.8, wall=6835
2024-01-31 06:37:42 | INFO | train_inner | epoch 168:   1189 / 1475 loss=2.195, nll_loss=0.545, ppl=1.46, wps=60105.7, ups=15.63, wpb=3844.5, bsz=198, num_updates=247500, lr=6.35642e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=6841
2024-01-31 06:37:48 | INFO | train_inner | epoch 168:   1289 / 1475 loss=2.185, nll_loss=0.534, ppl=1.45, wps=58529.3, ups=15.57, wpb=3758.9, bsz=204.5, num_updates=247600, lr=6.35513e-05, gnorm=1.052, loss_scale=8, train_wall=6, gb_free=29.7, wall=6847
2024-01-31 06:37:55 | INFO | train_inner | epoch 168:   1389 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=58836.4, ups=15.56, wpb=3782, bsz=201.3, num_updates=247700, lr=6.35385e-05, gnorm=1.008, loss_scale=8, train_wall=6, gb_free=29.7, wall=6854
2024-01-31 06:38:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:38:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:38:03 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 3.559 | nll_loss 1.968 | ppl 3.91 | wps 135310 | wpb 3189.1 | bsz 163.4 | num_updates 247786 | best_loss 3.385
2024-01-31 06:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 247786 updates
2024-01-31 06:38:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint168.pt
2024-01-31 06:38:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint168.pt
2024-01-31 06:38:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint168.pt (epoch 168 @ 247786 updates, score 3.559) (writing took 2.339943247032352 seconds)
2024-01-31 06:38:05 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2024-01-31 06:38:05 | INFO | train | epoch 168 | loss 2.184 | nll_loss 0.531 | ppl 1.45 | wps 55970.5 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 247786 | lr 6.35275e-05 | gnorm 1.009 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 6864
2024-01-31 06:38:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:38:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:38:05 | INFO | fairseq.trainer | begin training epoch 169
2024-01-31 06:38:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:38:06 | INFO | train_inner | epoch 169:     14 / 1475 loss=2.196, nll_loss=0.545, ppl=1.46, wps=33463.8, ups=8.95, wpb=3740, bsz=190.7, num_updates=247800, lr=6.35257e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.7, wall=6865
2024-01-31 06:38:12 | INFO | train_inner | epoch 169:    114 / 1475 loss=2.163, nll_loss=0.508, ppl=1.42, wps=59050.8, ups=15.57, wpb=3793.6, bsz=198.1, num_updates=247900, lr=6.35129e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.7, wall=6871
2024-01-31 06:38:19 | INFO | train_inner | epoch 169:    214 / 1475 loss=2.173, nll_loss=0.519, ppl=1.43, wps=60433.7, ups=15.58, wpb=3877.9, bsz=200.6, num_updates=248000, lr=6.35001e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=6878
2024-01-31 06:38:25 | INFO | train_inner | epoch 169:    314 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=58356.6, ups=15.64, wpb=3732.2, bsz=192.6, num_updates=248100, lr=6.34873e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=6884
2024-01-31 06:38:32 | INFO | train_inner | epoch 169:    414 / 1475 loss=2.17, nll_loss=0.516, ppl=1.43, wps=58272.4, ups=15.58, wpb=3739.1, bsz=196.5, num_updates=248200, lr=6.34745e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.8, wall=6891
2024-01-31 06:38:38 | INFO | train_inner | epoch 169:    514 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=58469.1, ups=15.6, wpb=3747.9, bsz=191.2, num_updates=248300, lr=6.34617e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.6, wall=6897
2024-01-31 06:38:45 | INFO | train_inner | epoch 169:    614 / 1475 loss=2.193, nll_loss=0.541, ppl=1.45, wps=59458, ups=15.68, wpb=3790.8, bsz=181.1, num_updates=248400, lr=6.34489e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=6903
2024-01-31 06:38:51 | INFO | train_inner | epoch 169:    714 / 1475 loss=2.188, nll_loss=0.535, ppl=1.45, wps=59822.3, ups=15.51, wpb=3856.1, bsz=195.2, num_updates=248500, lr=6.34361e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.6, wall=6910
2024-01-31 06:38:57 | INFO | train_inner | epoch 169:    814 / 1475 loss=2.178, nll_loss=0.526, ppl=1.44, wps=59166.2, ups=15.57, wpb=3799, bsz=203.1, num_updates=248600, lr=6.34234e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.6, wall=6916
2024-01-31 06:39:04 | INFO | train_inner | epoch 169:    914 / 1475 loss=2.182, nll_loss=0.53, ppl=1.44, wps=57904, ups=15.58, wpb=3715.9, bsz=193.4, num_updates=248700, lr=6.34106e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.6, wall=6923
2024-01-31 06:39:10 | INFO | train_inner | epoch 169:   1014 / 1475 loss=2.193, nll_loss=0.542, ppl=1.46, wps=58701.4, ups=15.55, wpb=3775.4, bsz=191.5, num_updates=248800, lr=6.33979e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=6929
2024-01-31 06:39:17 | INFO | train_inner | epoch 169:   1114 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=57877.6, ups=15.51, wpb=3731, bsz=209.4, num_updates=248900, lr=6.33852e-05, gnorm=1.016, loss_scale=8, train_wall=6, gb_free=29.7, wall=6936
2024-01-31 06:39:23 | INFO | train_inner | epoch 169:   1214 / 1475 loss=2.185, nll_loss=0.533, ppl=1.45, wps=56953, ups=15.7, wpb=3627.8, bsz=193.7, num_updates=249000, lr=6.33724e-05, gnorm=1.043, loss_scale=8, train_wall=6, gb_free=29.8, wall=6942
2024-01-31 06:39:29 | INFO | train_inner | epoch 169:   1314 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=59247.1, ups=15.54, wpb=3812.5, bsz=205.3, num_updates=249100, lr=6.33597e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.7, wall=6948
2024-01-31 06:39:36 | INFO | train_inner | epoch 169:   1414 / 1475 loss=2.195, nll_loss=0.545, ppl=1.46, wps=58941.7, ups=15.59, wpb=3781.6, bsz=194.9, num_updates=249200, lr=6.3347e-05, gnorm=1.032, loss_scale=8, train_wall=6, gb_free=29.8, wall=6955
2024-01-31 06:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:39:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:39:42 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 3.552 | nll_loss 1.96 | ppl 3.89 | wps 134665 | wpb 3189.1 | bsz 163.4 | num_updates 249261 | best_loss 3.385
2024-01-31 06:39:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 249261 updates
2024-01-31 06:39:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint169.pt
2024-01-31 06:39:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint169.pt
2024-01-31 06:39:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint169.pt (epoch 169 @ 249261 updates, score 3.552) (writing took 2.3662940160138533 seconds)
2024-01-31 06:39:44 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2024-01-31 06:39:44 | INFO | train | epoch 169 | loss 2.182 | nll_loss 0.53 | ppl 1.44 | wps 55935.5 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 249261 | lr 6.33392e-05 | gnorm 1.003 | loss_scale 8 | train_wall 91 | gb_free 29.6 | wall 6963
2024-01-31 06:39:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:39:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:39:45 | INFO | fairseq.trainer | begin training epoch 170
2024-01-31 06:39:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:39:47 | INFO | train_inner | epoch 170:     39 / 1475 loss=2.185, nll_loss=0.533, ppl=1.45, wps=33886.4, ups=8.88, wpb=3816.9, bsz=196.2, num_updates=249300, lr=6.33343e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=6966
2024-01-31 06:39:54 | INFO | train_inner | epoch 170:    139 / 1475 loss=2.169, nll_loss=0.514, ppl=1.43, wps=57923.6, ups=15.62, wpb=3707.8, bsz=189.1, num_updates=249400, lr=6.33216e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.6, wall=6972
2024-01-31 06:40:00 | INFO | train_inner | epoch 170:    239 / 1475 loss=2.162, nll_loss=0.507, ppl=1.42, wps=59198.5, ups=15.45, wpb=3832.6, bsz=202.2, num_updates=249500, lr=6.33089e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=6979
2024-01-31 06:40:06 | INFO | train_inner | epoch 170:    339 / 1475 loss=2.177, nll_loss=0.524, ppl=1.44, wps=59916.1, ups=15.51, wpb=3864.2, bsz=201.5, num_updates=249600, lr=6.32962e-05, gnorm=0.982, loss_scale=8, train_wall=6, gb_free=29.6, wall=6985
2024-01-31 06:40:13 | INFO | train_inner | epoch 170:    439 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=59162.8, ups=15.57, wpb=3798.7, bsz=195, num_updates=249700, lr=6.32835e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.8, wall=6992
2024-01-31 06:40:19 | INFO | train_inner | epoch 170:    539 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=58398.4, ups=15.54, wpb=3757.4, bsz=203, num_updates=249800, lr=6.32709e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.6, wall=6998
2024-01-31 06:40:26 | INFO | train_inner | epoch 170:    639 / 1475 loss=2.188, nll_loss=0.537, ppl=1.45, wps=58879.3, ups=15.54, wpb=3788.4, bsz=191.6, num_updates=249900, lr=6.32582e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.7, wall=7005
2024-01-31 06:40:32 | INFO | train_inner | epoch 170:    739 / 1475 loss=2.18, nll_loss=0.527, ppl=1.44, wps=58206.8, ups=15.63, wpb=3723.7, bsz=191.9, num_updates=250000, lr=6.32456e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=7011
2024-01-31 06:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:40:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:40:34 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 3.56 | nll_loss 1.971 | ppl 3.92 | wps 135708 | wpb 3189.1 | bsz 163.4 | num_updates 250000 | best_loss 3.385
2024-01-31 06:40:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 250000 updates
2024-01-31 06:40:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_170_250000.pt
2024-01-31 06:40:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_170_250000.pt
2024-01-31 06:40:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_170_250000.pt (epoch 170 @ 250000 updates, score 3.56) (writing took 2.3611235240241513 seconds)
2024-01-31 06:40:43 | INFO | train_inner | epoch 170:    839 / 1475 loss=2.186, nll_loss=0.534, ppl=1.45, wps=33762.3, ups=9.03, wpb=3738.4, bsz=197.4, num_updates=250100, lr=6.32329e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.7, wall=7022
2024-01-31 06:40:50 | INFO | train_inner | epoch 170:    939 / 1475 loss=2.188, nll_loss=0.536, ppl=1.45, wps=58707.2, ups=15.72, wpb=3733.8, bsz=195.6, num_updates=250200, lr=6.32203e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=7029
2024-01-31 06:40:56 | INFO | train_inner | epoch 170:   1039 / 1475 loss=2.181, nll_loss=0.528, ppl=1.44, wps=58882.7, ups=15.51, wpb=3795.9, bsz=202.2, num_updates=250300, lr=6.32076e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=7035
2024-01-31 06:41:02 | INFO | train_inner | epoch 170:   1139 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=59237.4, ups=15.65, wpb=3785.1, bsz=194.7, num_updates=250400, lr=6.3195e-05, gnorm=1.049, loss_scale=8, train_wall=6, gb_free=29.7, wall=7041
2024-01-31 06:41:09 | INFO | train_inner | epoch 170:   1239 / 1475 loss=2.192, nll_loss=0.541, ppl=1.45, wps=59164.4, ups=15.62, wpb=3788.3, bsz=191.4, num_updates=250500, lr=6.31824e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.7, wall=7048
2024-01-31 06:41:15 | INFO | train_inner | epoch 170:   1339 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=58696.1, ups=15.69, wpb=3739.8, bsz=190.7, num_updates=250600, lr=6.31698e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.7, wall=7054
2024-01-31 06:41:22 | INFO | train_inner | epoch 170:   1439 / 1475 loss=2.196, nll_loss=0.546, ppl=1.46, wps=58554.5, ups=15.57, wpb=3761.6, bsz=193.8, num_updates=250700, lr=6.31572e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=7061
2024-01-31 06:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:41:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:41:26 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 3.56 | nll_loss 1.972 | ppl 3.92 | wps 136462 | wpb 3189.1 | bsz 163.4 | num_updates 250736 | best_loss 3.385
2024-01-31 06:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 250736 updates
2024-01-31 06:41:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint170.pt
2024-01-31 06:41:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint170.pt
2024-01-31 06:41:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint170.pt (epoch 170 @ 250736 updates, score 3.56) (writing took 2.385634107980877 seconds)
2024-01-31 06:41:29 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2024-01-31 06:41:29 | INFO | train | epoch 170 | loss 2.181 | nll_loss 0.529 | ppl 1.44 | wps 53415.5 | ups 14.16 | wpb 3771.6 | bsz 195.9 | num_updates 250736 | lr 6.31527e-05 | gnorm 1.005 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 7068
2024-01-31 06:41:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:41:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:41:29 | INFO | fairseq.trainer | begin training epoch 171
2024-01-31 06:41:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:41:33 | INFO | train_inner | epoch 171:     64 / 1475 loss=2.172, nll_loss=0.519, ppl=1.43, wps=33268, ups=8.89, wpb=3740.1, bsz=198.6, num_updates=250800, lr=6.31446e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.6, wall=7072
2024-01-31 06:41:39 | INFO | train_inner | epoch 171:    164 / 1475 loss=2.168, nll_loss=0.514, ppl=1.43, wps=60664.6, ups=15.58, wpb=3894.1, bsz=194, num_updates=250900, lr=6.3132e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=7078
2024-01-31 06:41:46 | INFO | train_inner | epoch 171:    264 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=59153.9, ups=15.46, wpb=3825.1, bsz=199.9, num_updates=251000, lr=6.31194e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=7085
2024-01-31 06:41:52 | INFO | train_inner | epoch 171:    364 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=59244.5, ups=15.68, wpb=3777.8, bsz=187.3, num_updates=251100, lr=6.31069e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=7091
2024-01-31 06:41:59 | INFO | train_inner | epoch 171:    464 / 1475 loss=2.171, nll_loss=0.517, ppl=1.43, wps=58473.4, ups=15.55, wpb=3761.4, bsz=198.3, num_updates=251200, lr=6.30943e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=7097
2024-01-31 06:42:05 | INFO | train_inner | epoch 171:    564 / 1475 loss=2.177, nll_loss=0.524, ppl=1.44, wps=58629.8, ups=15.53, wpb=3774.1, bsz=195.5, num_updates=251300, lr=6.30818e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.9, wall=7104
2024-01-31 06:42:11 | INFO | train_inner | epoch 171:    664 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=58680.5, ups=15.59, wpb=3762.8, bsz=192, num_updates=251400, lr=6.30692e-05, gnorm=1.056, loss_scale=8, train_wall=6, gb_free=29.8, wall=7110
2024-01-31 06:42:18 | INFO | train_inner | epoch 171:    764 / 1475 loss=2.181, nll_loss=0.529, ppl=1.44, wps=57921.5, ups=15.72, wpb=3684.6, bsz=192.5, num_updates=251500, lr=6.30567e-05, gnorm=1.044, loss_scale=8, train_wall=6, gb_free=29.7, wall=7117
2024-01-31 06:42:24 | INFO | train_inner | epoch 171:    864 / 1475 loss=2.184, nll_loss=0.532, ppl=1.45, wps=57156, ups=15.48, wpb=3692.2, bsz=192.2, num_updates=251600, lr=6.30441e-05, gnorm=1.038, loss_scale=16, train_wall=6, gb_free=29.8, wall=7123
2024-01-31 06:42:31 | INFO | train_inner | epoch 171:    964 / 1475 loss=2.179, nll_loss=0.527, ppl=1.44, wps=59215.2, ups=15.58, wpb=3799.9, bsz=207.7, num_updates=251700, lr=6.30316e-05, gnorm=0.996, loss_scale=16, train_wall=6, gb_free=29.8, wall=7130
2024-01-31 06:42:37 | INFO | train_inner | epoch 171:   1064 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=58596.3, ups=15.69, wpb=3735.1, bsz=196.2, num_updates=251800, lr=6.30191e-05, gnorm=1.032, loss_scale=16, train_wall=6, gb_free=29.7, wall=7136
2024-01-31 06:42:43 | INFO | train_inner | epoch 171:   1164 / 1475 loss=2.187, nll_loss=0.536, ppl=1.45, wps=59431.9, ups=15.55, wpb=3823.1, bsz=197.9, num_updates=251900, lr=6.30066e-05, gnorm=0.998, loss_scale=16, train_wall=6, gb_free=29.8, wall=7142
2024-01-31 06:42:50 | INFO | train_inner | epoch 171:   1264 / 1475 loss=2.183, nll_loss=0.53, ppl=1.44, wps=57617.2, ups=15.61, wpb=3691.8, bsz=196, num_updates=252000, lr=6.29941e-05, gnorm=1.005, loss_scale=16, train_wall=6, gb_free=29.7, wall=7149
2024-01-31 06:42:56 | INFO | train_inner | epoch 171:   1364 / 1475 loss=2.202, nll_loss=0.553, ppl=1.47, wps=59031.7, ups=15.58, wpb=3789.2, bsz=186.9, num_updates=252100, lr=6.29816e-05, gnorm=1.037, loss_scale=16, train_wall=6, gb_free=29.7, wall=7155
2024-01-31 06:43:04 | INFO | train_inner | epoch 171:   1464 / 1475 loss=2.191, nll_loss=0.539, ppl=1.45, wps=46721.6, ups=12.33, wpb=3790.6, bsz=200.2, num_updates=252200, lr=6.29691e-05, gnorm=1.007, loss_scale=16, train_wall=8, gb_free=29.8, wall=7163
2024-01-31 06:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:43:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:43:07 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 3.557 | nll_loss 1.965 | ppl 3.9 | wps 137292 | wpb 3189.1 | bsz 163.4 | num_updates 252211 | best_loss 3.385
2024-01-31 06:43:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 252211 updates
2024-01-31 06:43:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint171.pt
2024-01-31 06:43:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint171.pt
2024-01-31 06:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint171.pt (epoch 171 @ 252211 updates, score 3.557) (writing took 2.5048091979697347 seconds)
2024-01-31 06:43:10 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2024-01-31 06:43:10 | INFO | train | epoch 171 | loss 2.181 | nll_loss 0.528 | ppl 1.44 | wps 54911.2 | ups 14.56 | wpb 3771.6 | bsz 195.9 | num_updates 252211 | lr 6.29677e-05 | gnorm 1.01 | loss_scale 16 | train_wall 93 | gb_free 29.9 | wall 7169
2024-01-31 06:43:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:43:10 | INFO | fairseq.trainer | begin training epoch 172
2024-01-31 06:43:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:43:16 | INFO | train_inner | epoch 172:     89 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=32818.1, ups=8.76, wpb=3745.3, bsz=200, num_updates=252300, lr=6.29566e-05, gnorm=0.975, loss_scale=16, train_wall=6, gb_free=29.7, wall=7175
2024-01-31 06:43:23 | INFO | train_inner | epoch 172:    189 / 1475 loss=2.161, nll_loss=0.505, ppl=1.42, wps=49202.7, ups=13.18, wpb=3731.9, bsz=199.5, num_updates=252400, lr=6.29441e-05, gnorm=0.993, loss_scale=16, train_wall=7, gb_free=30, wall=7182
2024-01-31 06:43:30 | INFO | train_inner | epoch 172:    289 / 1475 loss=2.167, nll_loss=0.513, ppl=1.43, wps=59336.1, ups=15.59, wpb=3804.8, bsz=195.7, num_updates=252500, lr=6.29317e-05, gnorm=0.962, loss_scale=16, train_wall=6, gb_free=29.8, wall=7189
2024-01-31 06:43:36 | INFO | train_inner | epoch 172:    389 / 1475 loss=2.163, nll_loss=0.508, ppl=1.42, wps=59329.6, ups=15.55, wpb=3814.8, bsz=206.2, num_updates=252600, lr=6.29192e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=29.7, wall=7195
2024-01-31 06:43:43 | INFO | train_inner | epoch 172:    489 / 1475 loss=2.185, nll_loss=0.532, ppl=1.45, wps=59640.9, ups=15.6, wpb=3822, bsz=186.7, num_updates=252700, lr=6.29068e-05, gnorm=1.024, loss_scale=16, train_wall=6, gb_free=29.7, wall=7202
2024-01-31 06:43:49 | INFO | train_inner | epoch 172:    589 / 1475 loss=2.177, nll_loss=0.524, ppl=1.44, wps=59694.5, ups=15.65, wpb=3814, bsz=199.4, num_updates=252800, lr=6.28943e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.7, wall=7208
2024-01-31 06:43:55 | INFO | train_inner | epoch 172:    689 / 1475 loss=2.179, nll_loss=0.527, ppl=1.44, wps=59103.4, ups=15.54, wpb=3802.8, bsz=193.9, num_updates=252900, lr=6.28819e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.6, wall=7214
2024-01-31 06:44:02 | INFO | train_inner | epoch 172:    789 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=56979.4, ups=15.63, wpb=3645.2, bsz=197, num_updates=253000, lr=6.28695e-05, gnorm=1.03, loss_scale=16, train_wall=6, gb_free=29.7, wall=7221
2024-01-31 06:44:08 | INFO | train_inner | epoch 172:    889 / 1475 loss=2.182, nll_loss=0.53, ppl=1.44, wps=58913.4, ups=15.56, wpb=3786.9, bsz=195.1, num_updates=253100, lr=6.2857e-05, gnorm=1.003, loss_scale=16, train_wall=6, gb_free=29.7, wall=7227
2024-01-31 06:44:15 | INFO | train_inner | epoch 172:    989 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=59037.5, ups=15.66, wpb=3769.1, bsz=185.6, num_updates=253200, lr=6.28446e-05, gnorm=1.028, loss_scale=16, train_wall=6, gb_free=29.7, wall=7234
2024-01-31 06:44:21 | INFO | train_inner | epoch 172:   1089 / 1475 loss=2.187, nll_loss=0.536, ppl=1.45, wps=57756.8, ups=15.57, wpb=3709.5, bsz=195.9, num_updates=253300, lr=6.28322e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.6, wall=7240
2024-01-31 06:44:28 | INFO | train_inner | epoch 172:   1189 / 1475 loss=2.193, nll_loss=0.543, ppl=1.46, wps=59446.4, ups=15.52, wpb=3830.1, bsz=200.9, num_updates=253400, lr=6.28198e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.7, wall=7246
2024-01-31 06:44:34 | INFO | train_inner | epoch 172:   1289 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=59168, ups=15.48, wpb=3822.3, bsz=196.7, num_updates=253500, lr=6.28074e-05, gnorm=0.999, loss_scale=16, train_wall=6, gb_free=29.7, wall=7253
2024-01-31 06:44:40 | INFO | train_inner | epoch 172:   1389 / 1475 loss=2.186, nll_loss=0.535, ppl=1.45, wps=58689.1, ups=15.64, wpb=3752.1, bsz=192.8, num_updates=253600, lr=6.2795e-05, gnorm=1.005, loss_scale=16, train_wall=6, gb_free=29.8, wall=7259
2024-01-31 06:44:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:44:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:44:48 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 3.557 | nll_loss 1.969 | ppl 3.91 | wps 119430 | wpb 3189.1 | bsz 163.4 | num_updates 253686 | best_loss 3.385
2024-01-31 06:44:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 253686 updates
2024-01-31 06:44:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint172.pt
2024-01-31 06:44:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint172.pt
2024-01-31 06:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint172.pt (epoch 172 @ 253686 updates, score 3.557) (writing took 2.4707451530266553 seconds)
2024-01-31 06:44:51 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2024-01-31 06:44:51 | INFO | train | epoch 172 | loss 2.18 | nll_loss 0.527 | ppl 1.44 | wps 55051 | ups 14.6 | wpb 3771.6 | bsz 195.9 | num_updates 253686 | lr 6.27844e-05 | gnorm 1.001 | loss_scale 16 | train_wall 93 | gb_free 29.8 | wall 7270
2024-01-31 06:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:44:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:44:51 | INFO | fairseq.trainer | begin training epoch 173
2024-01-31 06:44:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:44:52 | INFO | train_inner | epoch 173:     14 / 1475 loss=2.186, nll_loss=0.535, ppl=1.45, wps=31534.2, ups=8.62, wpb=3658.3, bsz=193.2, num_updates=253700, lr=6.27827e-05, gnorm=1.034, loss_scale=16, train_wall=6, gb_free=29.7, wall=7271
2024-01-31 06:44:58 | INFO | train_inner | epoch 173:    114 / 1475 loss=2.169, nll_loss=0.514, ppl=1.43, wps=59764.5, ups=15.59, wpb=3833, bsz=192.1, num_updates=253800, lr=6.27703e-05, gnorm=0.977, loss_scale=16, train_wall=6, gb_free=29.7, wall=7277
2024-01-31 06:45:05 | INFO | train_inner | epoch 173:    214 / 1475 loss=2.171, nll_loss=0.517, ppl=1.43, wps=57572.2, ups=15.07, wpb=3819.8, bsz=194.7, num_updates=253900, lr=6.27579e-05, gnorm=0.986, loss_scale=16, train_wall=6, gb_free=29.8, wall=7284
2024-01-31 06:45:11 | INFO | train_inner | epoch 173:    314 / 1475 loss=2.174, nll_loss=0.52, ppl=1.43, wps=58751, ups=15.61, wpb=3764.8, bsz=183.4, num_updates=254000, lr=6.27456e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.7, wall=7290
2024-01-31 06:45:18 | INFO | train_inner | epoch 173:    414 / 1475 loss=2.165, nll_loss=0.511, ppl=1.43, wps=58332.4, ups=15.52, wpb=3758.1, bsz=200.6, num_updates=254100, lr=6.27332e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.8, wall=7297
2024-01-31 06:45:24 | INFO | train_inner | epoch 173:    514 / 1475 loss=2.188, nll_loss=0.535, ppl=1.45, wps=58712.8, ups=15.66, wpb=3749.5, bsz=183.2, num_updates=254200, lr=6.27209e-05, gnorm=1.029, loss_scale=16, train_wall=6, gb_free=29.7, wall=7303
2024-01-31 06:45:31 | INFO | train_inner | epoch 173:    614 / 1475 loss=2.165, nll_loss=0.511, ppl=1.42, wps=59143.2, ups=15.57, wpb=3797.9, bsz=208.2, num_updates=254300, lr=6.27086e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.7, wall=7310
2024-01-31 06:45:37 | INFO | train_inner | epoch 173:    714 / 1475 loss=2.161, nll_loss=0.507, ppl=1.42, wps=58601.7, ups=15.52, wpb=3775, bsz=213.6, num_updates=254400, lr=6.26962e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=7316
2024-01-31 06:45:44 | INFO | train_inner | epoch 173:    814 / 1475 loss=2.179, nll_loss=0.528, ppl=1.44, wps=58789, ups=15.55, wpb=3781.1, bsz=196.6, num_updates=254500, lr=6.26839e-05, gnorm=1.012, loss_scale=16, train_wall=6, gb_free=29.8, wall=7323
2024-01-31 06:45:50 | INFO | train_inner | epoch 173:    914 / 1475 loss=2.171, nll_loss=0.518, ppl=1.43, wps=58235.1, ups=15.59, wpb=3735.2, bsz=207.8, num_updates=254600, lr=6.26716e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.6, wall=7329
2024-01-31 06:45:56 | INFO | train_inner | epoch 173:   1014 / 1475 loss=2.194, nll_loss=0.543, ppl=1.46, wps=58612.3, ups=15.6, wpb=3756.1, bsz=186.2, num_updates=254700, lr=6.26593e-05, gnorm=1.032, loss_scale=16, train_wall=6, gb_free=29.7, wall=7335
2024-01-31 06:46:03 | INFO | train_inner | epoch 173:   1114 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=59172.8, ups=15.56, wpb=3802.7, bsz=194.6, num_updates=254800, lr=6.2647e-05, gnorm=1.03, loss_scale=16, train_wall=6, gb_free=29.7, wall=7342
2024-01-31 06:46:09 | INFO | train_inner | epoch 173:   1214 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=58457.8, ups=15.59, wpb=3748.9, bsz=195.2, num_updates=254900, lr=6.26347e-05, gnorm=1.027, loss_scale=16, train_wall=6, gb_free=29.8, wall=7348
2024-01-31 06:46:16 | INFO | train_inner | epoch 173:   1314 / 1475 loss=2.185, nll_loss=0.533, ppl=1.45, wps=58949, ups=15.6, wpb=3779.2, bsz=200, num_updates=255000, lr=6.26224e-05, gnorm=1.003, loss_scale=16, train_wall=6, gb_free=29.7, wall=7355
2024-01-31 06:46:22 | INFO | train_inner | epoch 173:   1414 / 1475 loss=2.195, nll_loss=0.545, ppl=1.46, wps=58409.7, ups=15.63, wpb=3735.8, bsz=188.3, num_updates=255100, lr=6.26102e-05, gnorm=1.042, loss_scale=16, train_wall=6, gb_free=29.7, wall=7361
2024-01-31 06:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 06:46:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:46:28 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 3.558 | nll_loss 1.966 | ppl 3.91 | wps 134781 | wpb 3189.1 | bsz 163.4 | num_updates 255161 | best_loss 3.385
2024-01-31 06:46:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 255161 updates
2024-01-31 06:46:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint173.pt
2024-01-31 06:46:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint173.pt
2024-01-31 06:46:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint173.pt (epoch 173 @ 255161 updates, score 3.558) (writing took 2.4225525989895687 seconds)
2024-01-31 06:46:31 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2024-01-31 06:46:31 | INFO | train | epoch 173 | loss 2.178 | nll_loss 0.526 | ppl 1.44 | wps 55738.6 | ups 14.78 | wpb 3771.6 | bsz 195.9 | num_updates 255161 | lr 6.26027e-05 | gnorm 1.007 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 7370
2024-01-31 06:46:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 06:46:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 06:46:31 | INFO | fairseq.trainer | begin training epoch 174
2024-01-31 06:46:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 06:46:33 | INFO | train_inner | epoch 174:     39 / 1475 loss=2.176, nll_loss=0.523, ppl=1.44, wps=33267.5, ups=8.8, wpb=3778.7, bsz=198.9, num_updates=255200, lr=6.25979e-05, gnorm=0.985, loss_scale=16, train_wall=6, gb_free=29.8, wall=7372
2024-01-31 06:46:40 | INFO | train_inner | epoch 174:    139 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=59122.1, ups=15.6, wpb=3790.3, bsz=195.4, num_updates=255300, lr=6.25856e-05, gnorm=0.975, loss_scale=16, train_wall=6, gb_free=29.7, wall=7379
2024-01-31 06:46:46 | INFO | train_inner | epoch 174:    239 / 1475 loss=2.162, nll_loss=0.507, ppl=1.42, wps=58708.7, ups=15.63, wpb=3757.2, bsz=205.6, num_updates=255400, lr=6.25734e-05, gnorm=0.969, loss_scale=16, train_wall=6, gb_free=29.7, wall=7385
2024-01-31 06:46:53 | INFO | train_inner | epoch 174:    339 / 1475 loss=2.165, nll_loss=0.511, ppl=1.42, wps=58595.1, ups=15.58, wpb=3761.1, bsz=200.6, num_updates=255500, lr=6.25611e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.7, wall=7392
2024-01-31 06:46:59 | INFO | train_inner | epoch 174:    439 / 1475 loss=2.173, nll_loss=0.52, ppl=1.43, wps=57764.9, ups=15.54, wpb=3716.2, bsz=196.9, num_updates=255600, lr=6.25489e-05, gnorm=1.013, loss_scale=16, train_wall=6, gb_free=29.9, wall=7398
2024-01-31 06:47:06 | INFO | train_inner | epoch 174:    539 / 1475 loss=2.161, nll_loss=0.506, ppl=1.42, wps=57936.3, ups=15.53, wpb=3730, bsz=209, num_updates=255700, lr=6.25367e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.7, wall=7404
2024-01-31 06:47:12 | INFO | train_inner | epoch 174:    639 / 1475 loss=2.175, nll_loss=0.521, ppl=1.44, wps=59801.9, ups=15.52, wpb=3852.6, bsz=199.3, num_updates=255800, lr=6.25244e-05, gnorm=0.978, loss_scale=16, train_wall=6, gb_free=29.7, wall=7411
2024-01-31 06:47:18 | INFO | train_inner | epoch 174:    739 / 1475 loss=2.184, nll_loss=0.532, ppl=1.45, wps=58466.2, ups=15.69, wpb=3727.2, bsz=186.9, num_updates=255900, lr=6.25122e-05, gnorm=1.038, loss_scale=16, train_wall=6, gb_free=29.7, wall=7417
2024-01-31 06:47:25 | INFO | train_inner | epoch 174:    839 / 1475 loss=2.18, nll_loss=0.529, ppl=1.44, wps=58625.6, ups=15.59, wpb=3761.2, bsz=190.1, num_updates=256000, lr=6.25e-05, gnorm=1.038, loss_scale=16, train_wall=6, gb_free=29.8, wall=7424
slurmstepd: error: *** JOB 23095538 ON uc2n483 CANCELLED AT 2024-01-31T06:47:25 DUE TO TIME LIMIT ***