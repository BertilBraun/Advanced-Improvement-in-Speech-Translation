(base) [uxude@uc2n995 train]$ ls
finetune_asr_covost_23092672.txt  finetune_mt_covost_23095538.txt              finetune_mt_paraphrased_covost.sh
finetune_asr_covost_23095549.txt  finetune_mt_covost_23098029.txt              old
finetune_asr_covost_23098030.txt  finetune_mt_covost_23098278.txt              prepare_all_datasets_23101711.txt
finetune_asr_covost_23098277.txt  finetune_mt_covost_23112578.txt              prepare_all_datasets.py
finetune_asr_covost_23101713.txt  finetune_mt_covost_23129864.txt              prepare_all_datasets.sh
finetune_asr_covost_23108306.txt  finetune_mt_covost_23129905.txt              __pycache__
finetune_asr_covost_23112579.txt  finetune_mt_covost_cosine_23133165.txt       train_punctuation_covost_23092674.txt
finetune_asr_covost_23115498.txt  finetune_mt_covost.sh                        train_punctuation_covost_23095537.txt
finetune_asr_covost_23127064.txt  finetune_mt_paraphrased_covost_23137012.txt  train_punctuation_covost_23096658.txt
finetune_asr_covost.sh            finetune_mt_paraphrased_covost_23137015.txt  train_punctuation_covost_23113088.txt
finetune_mt_covost_23092673.txt   finetune_mt_paraphrased_covost_23137589.txt  train_punctuation_covost.sh
(base) [uxude@uc2n995 train]$ cat finetune_mt_paraphrased_covost_23137589.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...

[notice] A new release of pip is available: 23.3.2 -> 24.0
[notice] To update, run: pip install --upgrade pip
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[INFO] 09:08:09 [Dataset::Prepare Datasets]: Skipping dataset preparation, all config data already exists
Finetuning the paraphrased MT model...
rm: cannot remove '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/preprocess.log': No such file or directory
Binarizing the data...
Binarized data will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Training data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train
Validation data: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train', validpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev', testpref=None, align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train.en: 512755 sents, 9934160 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev.en: 15520 sents, 288077 tokens, 0.00868% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/train.de: 512755 sents, 9949297 tokens, 0.0% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/dev.de: 15520 sents, 302966 tokens, 0.00792% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Binarization complete.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models
Training time: 6 hours
2024-02-10 09:11:52 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 25, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[4], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=25, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 6000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-02-10 09:11:52 | INFO | fairseq.tasks.translation | [en] dictionary: 6616 types
2024-02-10 09:11:52 | INFO | fairseq.tasks.translation | [de] dictionary: 8648 types
2024-02-10 09:11:55 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6616, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8648, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=8648, bias=False)
  )
)
2024-02-10 09:11:55 | INFO | fairseq_cli.train | task: TranslationTask
2024-02-10 09:11:55 | INFO | fairseq_cli.train | model: TransformerModel
2024-02-10 09:11:55 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-02-10 09:11:55 | INFO | fairseq_cli.train | num. shared model params: 51,953,664 (num. trained: 51,953,664)
2024-02-10 09:11:55 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-02-10 09:11:55 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/valid.en-de.en
2024-02-10 09:11:55 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/valid.en-de.de
2024-02-10 09:11:55 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset valid en-de 15520 examples
2024-02-10 09:11:55 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-02-10 09:11:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-10 09:11:55 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-02-10 09:11:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-02-10 09:11:55 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-02-10 09:11:55 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-02-10 09:11:55 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint_last.pt
2024-02-10 09:11:55 | INFO | fairseq.trainer | No existing checkpoint found /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint_last.pt
2024-02-10 09:11:55 | INFO | fairseq.trainer | loading train data for epoch 1
2024-02-10 09:11:55 | INFO | fairseq.data.data_utils | loaded 512,755 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/train.en-de.en
2024-02-10 09:11:55 | INFO | fairseq.data.data_utils | loaded 512,755 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/train.en-de.de
2024-02-10 09:11:55 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset train en-de 512755 examples
2024-02-10 09:11:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:11:55 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-10 09:11:55 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-10 09:11:55 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-10 09:11:58 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-02-10 09:11:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:11:58 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-02-10 09:11:58 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-02-10 09:11:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-02-10 09:12:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:12:00 | INFO | fairseq.trainer | begin training epoch 1
2024-02-10 09:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:12:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-02-10 09:12:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-02-10 09:12:29 | INFO | train_inner | epoch 001:    102 / 685 loss=12.321, nll_loss=12.149, ppl=4540.92, wps=66637.1, ups=4.63, wpb=14379.5, bsz=745.9, num_updates=100, lr=1.66667e-05, gnorm=2.463, loss_scale=32, train_wall=28, gb_free=29.5, wall=34
2024-02-10 09:12:50 | INFO | train_inner | epoch 001:    202 / 685 loss=10.86, nll_loss=10.512, ppl=1460.14, wps=69159.3, ups=4.69, wpb=14743.9, bsz=734, num_updates=200, lr=3.33333e-05, gnorm=1.27, loss_scale=32, train_wall=21, gb_free=29.5, wall=55
2024-02-10 09:12:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-02-10 09:13:12 | INFO | train_inner | epoch 001:    303 / 685 loss=10.097, nll_loss=9.611, ppl=782.09, wps=67307.9, ups=4.62, wpb=14576.9, bsz=753, num_updates=300, lr=5e-05, gnorm=1.298, loss_scale=16, train_wall=21, gb_free=29.5, wall=77
2024-02-10 09:13:33 | INFO | train_inner | epoch 001:    403 / 685 loss=9.856, nll_loss=9.307, ppl=633.59, wps=67959.2, ups=4.67, wpb=14561.9, bsz=760.8, num_updates=400, lr=6.66667e-05, gnorm=1.155, loss_scale=16, train_wall=21, gb_free=29.5, wall=98
2024-02-10 09:13:55 | INFO | train_inner | epoch 001:    503 / 685 loss=9.695, nll_loss=9.117, ppl=555.18, wps=66766.9, ups=4.67, wpb=14284.3, bsz=743.4, num_updates=500, lr=8.33333e-05, gnorm=1.189, loss_scale=16, train_wall=21, gb_free=29.5, wall=120
2024-02-10 09:14:16 | INFO | train_inner | epoch 001:    603 / 685 loss=9.377, nll_loss=8.753, ppl=431.4, wps=67643.5, ups=4.66, wpb=14521.3, bsz=763, num_updates=600, lr=0.0001, gnorm=1.211, loss_scale=16, train_wall=21, gb_free=29.6, wall=141
2024-02-10 09:14:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:14:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:14:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.793 | nll_loss 8.044 | ppl 263.96 | wps 127881 | wpb 3189.1 | bsz 163.4 | num_updates 682
2024-02-10 09:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 682 updates
2024-02-10 09:14:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint1.pt
2024-02-10 09:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint1.pt
2024-02-10 09:14:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint1.pt (epoch 1 @ 682 updates, score 8.793) (writing took 3.9439843269065022 seconds)
2024-02-10 09:14:40 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-02-10 09:14:40 | INFO | train | epoch 001 | loss 10.214 | nll_loss 9.729 | ppl 848.41 | wps 64910.5 | ups 4.47 | wpb 14520.9 | bsz 748.8 | num_updates 682 | lr 0.000113667 | gnorm 1.415 | loss_scale 16 | train_wall 152 | gb_free 29.7 | wall 165
2024-02-10 09:14:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:14:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:14:40 | INFO | fairseq.trainer | begin training epoch 2
2024-02-10 09:14:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:14:44 | INFO | train_inner | epoch 002:     18 / 685 loss=9.066, nll_loss=8.391, ppl=335.78, wps=52641.5, ups=3.6, wpb=14607.3, bsz=741.7, num_updates=700, lr=0.000116667, gnorm=1.324, loss_scale=16, train_wall=21, gb_free=29.5, wall=169
2024-02-10 09:15:05 | INFO | train_inner | epoch 002:    118 / 685 loss=8.77, nll_loss=8.046, ppl=264.28, wps=68282.5, ups=4.69, wpb=14567.7, bsz=739.2, num_updates=800, lr=0.000133333, gnorm=1.234, loss_scale=16, train_wall=21, gb_free=29.6, wall=190
2024-02-10 09:15:27 | INFO | train_inner | epoch 002:    218 / 685 loss=8.51, nll_loss=7.742, ppl=214.09, wps=67989.7, ups=4.68, wpb=14541.1, bsz=737.4, num_updates=900, lr=0.00015, gnorm=1.266, loss_scale=16, train_wall=21, gb_free=29.5, wall=211
2024-02-10 09:15:48 | INFO | train_inner | epoch 002:    318 / 685 loss=8.285, nll_loss=7.479, ppl=178.39, wps=67797.7, ups=4.67, wpb=14508.2, bsz=758.6, num_updates=1000, lr=0.000166667, gnorm=1.295, loss_scale=16, train_wall=21, gb_free=29.5, wall=233
2024-02-10 09:16:10 | INFO | train_inner | epoch 002:    418 / 685 loss=8.071, nll_loss=7.229, ppl=150.04, wps=67736.7, ups=4.68, wpb=14475.4, bsz=743.1, num_updates=1100, lr=0.000183333, gnorm=1.187, loss_scale=16, train_wall=21, gb_free=29.5, wall=254
2024-02-10 09:16:31 | INFO | train_inner | epoch 002:    518 / 685 loss=7.849, nll_loss=6.971, ppl=125.45, wps=67957.7, ups=4.66, wpb=14581.8, bsz=765.5, num_updates=1200, lr=0.0002, gnorm=1.238, loss_scale=16, train_wall=21, gb_free=29.6, wall=276
2024-02-10 09:16:52 | INFO | train_inner | epoch 002:    618 / 685 loss=7.641, nll_loss=6.729, ppl=106.12, wps=68397.2, ups=4.66, wpb=14665.3, bsz=759.7, num_updates=1300, lr=0.000216667, gnorm=1.202, loss_scale=16, train_wall=21, gb_free=29.5, wall=297
2024-02-10 09:17:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:17:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:17:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.162 | nll_loss 6.136 | ppl 70.34 | wps 127458 | wpb 3189.1 | bsz 163.4 | num_updates 1367 | best_loss 7.162
2024-02-10 09:17:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1367 updates
2024-02-10 09:17:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint2.pt
2024-02-10 09:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint2.pt
2024-02-10 09:17:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint2.pt (epoch 2 @ 1367 updates, score 7.162) (writing took 3.847595676081255 seconds)
2024-02-10 09:17:13 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-02-10 09:17:13 | INFO | train | epoch 002 | loss 8.139 | nll_loss 7.31 | ppl 158.67 | wps 65110.9 | ups 4.48 | wpb 14524.4 | bsz 748.5 | num_updates 1367 | lr 0.000227833 | gnorm 1.242 | loss_scale 16 | train_wall 145 | gb_free 29.8 | wall 318
2024-02-10 09:17:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:17:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:17:13 | INFO | fairseq.trainer | begin training epoch 3
2024-02-10 09:17:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:17:20 | INFO | train_inner | epoch 003:     33 / 685 loss=7.425, nll_loss=6.48, ppl=89.23, wps=51718.3, ups=3.61, wpb=14309.6, bsz=741.8, num_updates=1400, lr=0.000233333, gnorm=1.194, loss_scale=16, train_wall=21, gb_free=29.5, wall=325
2024-02-10 09:17:41 | INFO | train_inner | epoch 003:    133 / 685 loss=7.214, nll_loss=6.234, ppl=75.29, wps=66761.5, ups=4.69, wpb=14240, bsz=732.6, num_updates=1500, lr=0.00025, gnorm=1.158, loss_scale=16, train_wall=21, gb_free=29.6, wall=346
2024-02-10 09:18:03 | INFO | train_inner | epoch 003:    233 / 685 loss=6.999, nll_loss=5.985, ppl=63.33, wps=68195.2, ups=4.67, wpb=14604.2, bsz=747.6, num_updates=1600, lr=0.000266667, gnorm=1.089, loss_scale=16, train_wall=21, gb_free=29.6, wall=368
2024-02-10 09:18:24 | INFO | train_inner | epoch 003:    333 / 685 loss=6.821, nll_loss=5.775, ppl=54.77, wps=68186, ups=4.7, wpb=14519.3, bsz=730.7, num_updates=1700, lr=0.000283333, gnorm=1.094, loss_scale=16, train_wall=21, gb_free=29.5, wall=389
2024-02-10 09:18:45 | INFO | train_inner | epoch 003:    433 / 685 loss=6.641, nll_loss=5.565, ppl=47.36, wps=68746.6, ups=4.7, wpb=14612.4, bsz=739.9, num_updates=1800, lr=0.0003, gnorm=1.082, loss_scale=16, train_wall=21, gb_free=29.5, wall=410
2024-02-10 09:19:07 | INFO | train_inner | epoch 003:    533 / 685 loss=6.444, nll_loss=5.336, ppl=40.4, wps=68459.3, ups=4.68, wpb=14620.3, bsz=761.5, num_updates=1900, lr=0.000316667, gnorm=1.085, loss_scale=16, train_wall=21, gb_free=29.5, wall=431
2024-02-10 09:19:28 | INFO | train_inner | epoch 003:    633 / 685 loss=6.253, nll_loss=5.113, ppl=34.61, wps=67812.1, ups=4.66, wpb=14559.4, bsz=773.8, num_updates=2000, lr=0.000333333, gnorm=1.079, loss_scale=16, train_wall=21, gb_free=29.5, wall=453
2024-02-10 09:19:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:19:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:19:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.899 | nll_loss 4.592 | ppl 24.11 | wps 128167 | wpb 3189.1 | bsz 163.4 | num_updates 2052 | best_loss 5.899
2024-02-10 09:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2052 updates
2024-02-10 09:19:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint3.pt
2024-02-10 09:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint3.pt
2024-02-10 09:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint3.pt (epoch 3 @ 2052 updates, score 5.899) (writing took 3.9363799600396305 seconds)
2024-02-10 09:19:46 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-02-10 09:19:46 | INFO | train | epoch 003 | loss 6.711 | nll_loss 5.648 | ppl 50.14 | wps 65158.1 | ups 4.49 | wpb 14524.4 | bsz 748.5 | num_updates 2052 | lr 0.000342 | gnorm 1.097 | loss_scale 16 | train_wall 144 | gb_free 29.7 | wall 470
2024-02-10 09:19:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:19:46 | INFO | fairseq.trainer | begin training epoch 4
2024-02-10 09:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:19:56 | INFO | train_inner | epoch 004:     48 / 685 loss=6.083, nll_loss=4.914, ppl=30.16, wps=52259.7, ups=3.62, wpb=14450, bsz=751.8, num_updates=2100, lr=0.00035, gnorm=1.054, loss_scale=16, train_wall=21, gb_free=29.5, wall=481
2024-02-10 09:20:17 | INFO | train_inner | epoch 004:    148 / 685 loss=5.895, nll_loss=4.695, ppl=25.89, wps=68394.7, ups=4.71, wpb=14516.7, bsz=739, num_updates=2200, lr=0.000366667, gnorm=1.016, loss_scale=16, train_wall=21, gb_free=29.5, wall=502
2024-02-10 09:20:38 | INFO | train_inner | epoch 004:    248 / 685 loss=5.788, nll_loss=4.566, ppl=23.69, wps=67407.3, ups=4.7, wpb=14356.3, bsz=740.3, num_updates=2300, lr=0.000383333, gnorm=1.062, loss_scale=16, train_wall=21, gb_free=29.6, wall=523
2024-02-10 09:21:00 | INFO | train_inner | epoch 004:    348 / 685 loss=5.593, nll_loss=4.34, ppl=20.25, wps=68466.2, ups=4.7, wpb=14562, bsz=742.7, num_updates=2400, lr=0.0004, gnorm=0.985, loss_scale=16, train_wall=21, gb_free=29.6, wall=544
2024-02-10 09:21:21 | INFO | train_inner | epoch 004:    448 / 685 loss=5.422, nll_loss=4.139, ppl=17.61, wps=68335.7, ups=4.68, wpb=14600.8, bsz=774.4, num_updates=2500, lr=0.000416667, gnorm=0.931, loss_scale=16, train_wall=21, gb_free=29.6, wall=566
2024-02-10 09:21:42 | INFO | train_inner | epoch 004:    548 / 685 loss=5.316, nll_loss=4.015, ppl=16.17, wps=68831, ups=4.69, wpb=14660.8, bsz=733.9, num_updates=2600, lr=0.000433333, gnorm=0.944, loss_scale=16, train_wall=21, gb_free=29.5, wall=587
2024-02-10 09:22:04 | INFO | train_inner | epoch 004:    648 / 685 loss=5.181, nll_loss=3.858, ppl=14.5, wps=68957.7, ups=4.69, wpb=14689.1, bsz=747, num_updates=2700, lr=0.00045, gnorm=0.915, loss_scale=16, train_wall=21, gb_free=29.6, wall=608
2024-02-10 09:22:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:22:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:22:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.87 | nll_loss 3.326 | ppl 10.03 | wps 129794 | wpb 3189.1 | bsz 163.4 | num_updates 2737 | best_loss 4.87
2024-02-10 09:22:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2737 updates
2024-02-10 09:22:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint4.pt
2024-02-10 09:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint4.pt
2024-02-10 09:22:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint4.pt (epoch 4 @ 2737 updates, score 4.87) (writing took 3.829992634942755 seconds)
2024-02-10 09:22:18 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-02-10 09:22:18 | INFO | train | epoch 004 | loss 5.542 | nll_loss 4.28 | ppl 19.43 | wps 65418.3 | ups 4.5 | wpb 14524.4 | bsz 748.5 | num_updates 2737 | lr 0.000456167 | gnorm 0.979 | loss_scale 16 | train_wall 144 | gb_free 29.6 | wall 622
2024-02-10 09:22:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:22:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:22:18 | INFO | fairseq.trainer | begin training epoch 5
2024-02-10 09:22:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:22:31 | INFO | train_inner | epoch 005:     63 / 685 loss=5.028, nll_loss=3.683, ppl=12.84, wps=52058.7, ups=3.63, wpb=14347.9, bsz=756, num_updates=2800, lr=0.000466667, gnorm=0.918, loss_scale=16, train_wall=21, gb_free=29.6, wall=636
2024-02-10 09:22:53 | INFO | train_inner | epoch 005:    163 / 685 loss=4.899, nll_loss=3.532, ppl=11.57, wps=68302.9, ups=4.68, wpb=14607.1, bsz=766.7, num_updates=2900, lr=0.000483333, gnorm=0.89, loss_scale=16, train_wall=21, gb_free=29.6, wall=657
2024-02-10 09:23:14 | INFO | train_inner | epoch 005:    263 / 685 loss=4.865, nll_loss=3.491, ppl=11.25, wps=68223.9, ups=4.69, wpb=14541, bsz=749.4, num_updates=3000, lr=0.0005, gnorm=0.881, loss_scale=16, train_wall=21, gb_free=29.6, wall=679
2024-02-10 09:23:35 | INFO | train_inner | epoch 005:    363 / 685 loss=4.801, nll_loss=3.417, ppl=10.68, wps=68792.1, ups=4.72, wpb=14581.4, bsz=748.8, num_updates=3100, lr=0.000516667, gnorm=0.856, loss_scale=16, train_wall=21, gb_free=29.5, wall=700
2024-02-10 09:23:56 | INFO | train_inner | epoch 005:    463 / 685 loss=4.707, nll_loss=3.31, ppl=9.92, wps=68944, ups=4.69, wpb=14691.6, bsz=753.5, num_updates=3200, lr=0.000533333, gnorm=0.817, loss_scale=16, train_wall=21, gb_free=29.6, wall=721
2024-02-10 09:24:18 | INFO | train_inner | epoch 005:    563 / 685 loss=4.689, nll_loss=3.29, ppl=9.78, wps=67468.6, ups=4.71, wpb=14327.5, bsz=738.2, num_updates=3300, lr=0.00055, gnorm=0.833, loss_scale=16, train_wall=21, gb_free=29.5, wall=742
2024-02-10 09:24:39 | INFO | train_inner | epoch 005:    663 / 685 loss=4.609, nll_loss=3.198, ppl=9.18, wps=68733.1, ups=4.71, wpb=14578.2, bsz=744, num_updates=3400, lr=0.000566667, gnorm=0.797, loss_scale=16, train_wall=21, gb_free=29.5, wall=764
2024-02-10 09:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:24:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:24:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.423 | nll_loss 2.822 | ppl 7.07 | wps 131516 | wpb 3189.1 | bsz 163.4 | num_updates 3422 | best_loss 4.423
2024-02-10 09:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3422 updates
2024-02-10 09:24:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint5.pt
2024-02-10 09:24:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint5.pt
2024-02-10 09:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint5.pt (epoch 5 @ 3422 updates, score 4.423) (writing took 3.8447734580840915 seconds)
2024-02-10 09:24:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-02-10 09:24:50 | INFO | train | epoch 005 | loss 4.778 | nll_loss 3.392 | ppl 10.5 | wps 65479.2 | ups 4.51 | wpb 14524.4 | bsz 748.5 | num_updates 3422 | lr 0.000570333 | gnorm 0.85 | loss_scale 16 | train_wall 144 | gb_free 29.7 | wall 774
2024-02-10 09:24:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:24:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:24:50 | INFO | fairseq.trainer | begin training epoch 6
2024-02-10 09:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:25:06 | INFO | train_inner | epoch 006:     78 / 685 loss=4.506, nll_loss=3.081, ppl=8.46, wps=52321.5, ups=3.66, wpb=14309.5, bsz=714.2, num_updates=3500, lr=0.000583333, gnorm=0.802, loss_scale=16, train_wall=21, gb_free=29.5, wall=791
2024-02-10 09:25:34 | INFO | train_inner | epoch 006:    178 / 685 loss=4.395, nll_loss=2.952, ppl=7.74, wps=52492, ups=3.64, wpb=14430.4, bsz=771.4, num_updates=3600, lr=0.0006, gnorm=0.778, loss_scale=16, train_wall=21, gb_free=29.5, wall=818
2024-02-10 09:25:55 | INFO | train_inner | epoch 006:    278 / 685 loss=4.395, nll_loss=2.952, ppl=7.74, wps=69002.6, ups=4.7, wpb=14676.5, bsz=759.6, num_updates=3700, lr=0.000616667, gnorm=0.773, loss_scale=16, train_wall=21, gb_free=29.5, wall=840
2024-02-10 09:26:16 | INFO | train_inner | epoch 006:    378 / 685 loss=4.365, nll_loss=2.92, ppl=7.57, wps=67935.3, ups=4.7, wpb=14439.3, bsz=761.9, num_updates=3800, lr=0.000633333, gnorm=0.773, loss_scale=16, train_wall=21, gb_free=29.6, wall=861
2024-02-10 09:26:38 | INFO | train_inner | epoch 006:    478 / 685 loss=4.315, nll_loss=2.863, ppl=7.28, wps=68777.8, ups=4.71, wpb=14607.2, bsz=752.7, num_updates=3900, lr=0.00065, gnorm=0.757, loss_scale=16, train_wall=21, gb_free=29.5, wall=882
2024-02-10 09:26:59 | INFO | train_inner | epoch 006:    578 / 685 loss=4.304, nll_loss=2.85, ppl=7.21, wps=68825.2, ups=4.72, wpb=14581.2, bsz=742.7, num_updates=4000, lr=0.000666667, gnorm=0.761, loss_scale=16, train_wall=21, gb_free=29.5, wall=903
2024-02-10 09:27:20 | INFO | train_inner | epoch 006:    678 / 685 loss=4.273, nll_loss=2.816, ppl=7.04, wps=68929.9, ups=4.73, wpb=14559.5, bsz=734.8, num_updates=4100, lr=0.000683333, gnorm=0.763, loss_scale=16, train_wall=21, gb_free=29.5, wall=924
2024-02-10 09:27:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:27:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:27:23 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.263 | nll_loss 2.635 | ppl 6.21 | wps 130116 | wpb 3189.1 | bsz 163.4 | num_updates 4107 | best_loss 4.263
2024-02-10 09:27:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4107 updates
2024-02-10 09:27:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint6.pt
2024-02-10 09:27:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint6.pt
2024-02-10 09:27:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint6.pt (epoch 6 @ 4107 updates, score 4.263) (writing took 28.8861335280817 seconds)
2024-02-10 09:27:52 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-02-10 09:27:52 | INFO | train | epoch 006 | loss 4.355 | nll_loss 2.909 | ppl 7.51 | wps 54432.4 | ups 3.75 | wpb 14524.4 | bsz 748.5 | num_updates 4107 | lr 0.0006845 | gnorm 0.772 | loss_scale 16 | train_wall 144 | gb_free 30.3 | wall 957
2024-02-10 09:27:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:27:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:27:52 | INFO | fairseq.trainer | begin training epoch 7
2024-02-10 09:27:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:28:12 | INFO | train_inner | epoch 007:     93 / 685 loss=4.162, nll_loss=2.688, ppl=6.45, wps=27521.6, ups=1.91, wpb=14418.4, bsz=734.2, num_updates=4200, lr=0.0007, gnorm=0.766, loss_scale=16, train_wall=21, gb_free=29.5, wall=977
2024-02-10 09:28:33 | INFO | train_inner | epoch 007:    193 / 685 loss=4.133, nll_loss=2.656, ppl=6.3, wps=69108.2, ups=4.73, wpb=14611.3, bsz=753, num_updates=4300, lr=0.000716667, gnorm=0.736, loss_scale=16, train_wall=21, gb_free=29.6, wall=998
2024-02-10 09:28:54 | INFO | train_inner | epoch 007:    293 / 685 loss=4.13, nll_loss=2.652, ppl=6.29, wps=68117.6, ups=4.75, wpb=14340.9, bsz=736.7, num_updates=4400, lr=0.000733333, gnorm=0.735, loss_scale=32, train_wall=21, gb_free=29.5, wall=1019
2024-02-10 09:29:16 | INFO | train_inner | epoch 007:    393 / 685 loss=4.086, nll_loss=2.601, ppl=6.07, wps=69611.3, ups=4.71, wpb=14769.8, bsz=757.1, num_updates=4500, lr=0.00075, gnorm=0.74, loss_scale=32, train_wall=21, gb_free=29.5, wall=1040
2024-02-10 09:29:40 | INFO | train_inner | epoch 007:    493 / 685 loss=4.092, nll_loss=2.61, ppl=6.11, wps=59396, ups=4.1, wpb=14500.6, bsz=766.2, num_updates=4600, lr=0.000766667, gnorm=0.744, loss_scale=32, train_wall=21, gb_free=29.5, wall=1065
2024-02-10 09:30:01 | INFO | train_inner | epoch 007:    593 / 685 loss=4.069, nll_loss=2.585, ppl=6, wps=68240.8, ups=4.73, wpb=14440, bsz=753.3, num_updates=4700, lr=0.000783333, gnorm=0.753, loss_scale=32, train_wall=21, gb_free=29.5, wall=1086
2024-02-10 09:30:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:30:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:30:23 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.023 | nll_loss 2.406 | ppl 5.3 | wps 130372 | wpb 3189.1 | bsz 163.4 | num_updates 4792 | best_loss 4.023
2024-02-10 09:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4792 updates
2024-02-10 09:30:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint7.pt
2024-02-10 09:30:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint7.pt
2024-02-10 09:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint7.pt (epoch 7 @ 4792 updates, score 4.023) (writing took 3.7975476069841534 seconds)
2024-02-10 09:30:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-02-10 09:30:27 | INFO | train | epoch 007 | loss 4.103 | nll_loss 2.622 | ppl 6.16 | wps 64507.3 | ups 4.44 | wpb 14524.4 | bsz 748.5 | num_updates 4792 | lr 0.000798667 | gnorm 0.744 | loss_scale 32 | train_wall 143 | gb_free 29.9 | wall 1111
2024-02-10 09:30:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:30:27 | INFO | fairseq.trainer | begin training epoch 8
2024-02-10 09:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:30:28 | INFO | train_inner | epoch 008:      8 / 685 loss=4.054, nll_loss=2.568, ppl=5.93, wps=53159.3, ups=3.67, wpb=14500, bsz=737, num_updates=4800, lr=0.0008, gnorm=0.76, loss_scale=32, train_wall=21, gb_free=29.5, wall=1113
2024-02-10 09:30:50 | INFO | train_inner | epoch 008:    108 / 685 loss=3.937, nll_loss=2.433, ppl=5.4, wps=68443.1, ups=4.72, wpb=14492.7, bsz=758, num_updates=4900, lr=0.000816667, gnorm=0.739, loss_scale=32, train_wall=21, gb_free=29.5, wall=1134
2024-02-10 09:31:11 | INFO | train_inner | epoch 008:    208 / 685 loss=3.952, nll_loss=2.45, ppl=5.47, wps=68465.7, ups=4.69, wpb=14612.4, bsz=751.1, num_updates=5000, lr=0.000833333, gnorm=0.726, loss_scale=32, train_wall=21, gb_free=29.5, wall=1156
2024-02-10 09:31:32 | INFO | train_inner | epoch 008:    308 / 685 loss=3.947, nll_loss=2.446, ppl=5.45, wps=68861.9, ups=4.72, wpb=14585.3, bsz=755.8, num_updates=5100, lr=0.00085, gnorm=0.722, loss_scale=32, train_wall=21, gb_free=29.5, wall=1177
2024-02-10 09:31:53 | INFO | train_inner | epoch 008:    408 / 685 loss=3.93, nll_loss=2.427, ppl=5.38, wps=68016.2, ups=4.71, wpb=14429.2, bsz=755.5, num_updates=5200, lr=0.000866667, gnorm=0.738, loss_scale=32, train_wall=21, gb_free=29.5, wall=1198
2024-02-10 09:32:14 | INFO | train_inner | epoch 008:    508 / 685 loss=3.964, nll_loss=2.465, ppl=5.52, wps=67969.6, ups=4.74, wpb=14338.8, bsz=730.2, num_updates=5300, lr=0.000883333, gnorm=0.761, loss_scale=32, train_wall=21, gb_free=29.7, wall=1219
2024-02-10 09:32:37 | INFO | train_inner | epoch 008:    608 / 685 loss=3.933, nll_loss=2.431, ppl=5.39, wps=64107.4, ups=4.36, wpb=14709.7, bsz=747.5, num_updates=5400, lr=0.0009, gnorm=0.731, loss_scale=32, train_wall=23, gb_free=29.5, wall=1242
2024-02-10 09:32:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:32:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:32:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.931 | nll_loss 2.298 | ppl 4.92 | wps 130559 | wpb 3189.1 | bsz 163.4 | num_updates 5477 | best_loss 3.931
2024-02-10 09:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5477 updates
2024-02-10 09:32:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint8.pt
2024-02-10 09:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint8.pt
2024-02-10 09:33:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint8.pt (epoch 8 @ 5477 updates, score 3.931) (writing took 3.858788543846458 seconds)
2024-02-10 09:33:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-02-10 09:33:00 | INFO | train | epoch 008 | loss 3.943 | nll_loss 2.441 | ppl 5.43 | wps 64935.2 | ups 4.47 | wpb 14524.4 | bsz 748.5 | num_updates 5477 | lr 0.000912833 | gnorm 0.74 | loss_scale 32 | train_wall 145 | gb_free 29.6 | wall 1264
2024-02-10 09:33:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:33:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:33:00 | INFO | fairseq.trainer | begin training epoch 9
2024-02-10 09:33:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:33:05 | INFO | train_inner | epoch 009:     23 / 685 loss=3.893, nll_loss=2.386, ppl=5.23, wps=53018, ups=3.65, wpb=14540, bsz=735.1, num_updates=5500, lr=0.000916667, gnorm=0.755, loss_scale=32, train_wall=21, gb_free=29.6, wall=1270
2024-02-10 09:33:26 | INFO | train_inner | epoch 009:    123 / 685 loss=3.829, nll_loss=2.311, ppl=4.96, wps=68265.6, ups=4.72, wpb=14450, bsz=762.5, num_updates=5600, lr=0.000933333, gnorm=0.726, loss_scale=32, train_wall=21, gb_free=29.5, wall=1291
2024-02-10 09:33:47 | INFO | train_inner | epoch 009:    223 / 685 loss=3.828, nll_loss=2.31, ppl=4.96, wps=68284.9, ups=4.71, wpb=14501.7, bsz=768.7, num_updates=5700, lr=0.00095, gnorm=0.752, loss_scale=32, train_wall=21, gb_free=29.5, wall=1312
2024-02-10 09:34:14 | INFO | train_inner | epoch 009:    323 / 685 loss=3.857, nll_loss=2.344, ppl=5.08, wps=54968.5, ups=3.77, wpb=14581.4, bsz=733.2, num_updates=5800, lr=0.000966667, gnorm=0.733, loss_scale=32, train_wall=21, gb_free=29.6, wall=1338
2024-02-10 09:34:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-02-10 09:34:35 | INFO | train_inner | epoch 009:    424 / 685 loss=3.856, nll_loss=2.343, ppl=5.07, wps=67638.8, ups=4.68, wpb=14450, bsz=748, num_updates=5900, lr=0.000983333, gnorm=0.768, loss_scale=16, train_wall=21, gb_free=29.5, wall=1360
2024-02-10 09:34:56 | INFO | train_inner | epoch 009:    524 / 685 loss=3.836, nll_loss=2.321, ppl=5, wps=69624.3, ups=4.74, wpb=14698.1, bsz=739.4, num_updates=6000, lr=0.001, gnorm=0.742, loss_scale=16, train_wall=21, gb_free=29.5, wall=1381
2024-02-10 09:35:17 | INFO | train_inner | epoch 009:    624 / 685 loss=3.852, nll_loss=2.341, ppl=5.07, wps=68413.2, ups=4.74, wpb=14429.1, bsz=744.4, num_updates=6100, lr=0.000991769, gnorm=0.75, loss_scale=16, train_wall=21, gb_free=29.5, wall=1402
2024-02-10 09:35:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:35:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:35:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.878 | nll_loss 2.247 | ppl 4.75 | wps 130922 | wpb 3189.1 | bsz 163.4 | num_updates 6161 | best_loss 3.878
2024-02-10 09:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 6161 updates
2024-02-10 09:35:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint9.pt
2024-02-10 09:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint9.pt
2024-02-10 09:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint9.pt (epoch 9 @ 6161 updates, score 3.878) (writing took 3.8215186370071024 seconds)
2024-02-10 09:35:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-02-10 09:35:36 | INFO | train | epoch 009 | loss 3.838 | nll_loss 2.323 | ppl 5 | wps 63483.7 | ups 4.37 | wpb 14522.3 | bsz 748.7 | num_updates 6161 | lr 0.000986847 | gnorm 0.745 | loss_scale 16 | train_wall 143 | gb_free 29.7 | wall 1421
2024-02-10 09:35:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:35:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:35:36 | INFO | fairseq.trainer | begin training epoch 10
2024-02-10 09:35:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:35:45 | INFO | train_inner | epoch 010:     39 / 685 loss=3.767, nll_loss=2.244, ppl=4.74, wps=52948.1, ups=3.65, wpb=14497.6, bsz=752.3, num_updates=6200, lr=0.000983739, gnorm=0.74, loss_scale=16, train_wall=21, gb_free=29.6, wall=1429
2024-02-10 09:36:06 | INFO | train_inner | epoch 010:    139 / 685 loss=3.723, nll_loss=2.192, ppl=4.57, wps=69362.1, ups=4.74, wpb=14628.6, bsz=744.2, num_updates=6300, lr=0.0009759, gnorm=0.727, loss_scale=16, train_wall=21, gb_free=29.6, wall=1450
2024-02-10 09:36:27 | INFO | train_inner | epoch 010:    239 / 685 loss=3.726, nll_loss=2.195, ppl=4.58, wps=67820.9, ups=4.69, wpb=14461.4, bsz=752.5, num_updates=6400, lr=0.000968246, gnorm=0.738, loss_scale=16, train_wall=21, gb_free=29.5, wall=1472
2024-02-10 09:36:48 | INFO | train_inner | epoch 010:    339 / 685 loss=3.725, nll_loss=2.197, ppl=4.59, wps=69128.1, ups=4.73, wpb=14604.1, bsz=745.8, num_updates=6500, lr=0.000960769, gnorm=0.704, loss_scale=16, train_wall=21, gb_free=29.6, wall=1493
2024-02-10 09:37:09 | INFO | train_inner | epoch 010:    439 / 685 loss=3.712, nll_loss=2.182, ppl=4.54, wps=68693.3, ups=4.75, wpb=14471.3, bsz=748.3, num_updates=6600, lr=0.000953463, gnorm=0.735, loss_scale=16, train_wall=21, gb_free=29.5, wall=1514
2024-02-10 09:37:30 | INFO | train_inner | epoch 010:    539 / 685 loss=3.699, nll_loss=2.168, ppl=4.5, wps=69257, ups=4.73, wpb=14638.7, bsz=750, num_updates=6700, lr=0.00094632, gnorm=0.701, loss_scale=16, train_wall=21, gb_free=29.5, wall=1535
2024-02-10 09:37:52 | INFO | train_inner | epoch 010:    639 / 685 loss=3.698, nll_loss=2.167, ppl=4.49, wps=68392.6, ups=4.73, wpb=14448.2, bsz=747.5, num_updates=6800, lr=0.000939336, gnorm=0.728, loss_scale=16, train_wall=21, gb_free=29.5, wall=1556
2024-02-10 09:38:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:38:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:38:04 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.775 | nll_loss 2.147 | ppl 4.43 | wps 131539 | wpb 3189.1 | bsz 163.4 | num_updates 6846 | best_loss 3.775
2024-02-10 09:38:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6846 updates
2024-02-10 09:38:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt
2024-02-10 09:38:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt
2024-02-10 09:38:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint10.pt (epoch 10 @ 6846 updates, score 3.775) (writing took 3.8022041919175535 seconds)
2024-02-10 09:38:07 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-02-10 09:38:07 | INFO | train | epoch 010 | loss 3.713 | nll_loss 2.183 | ppl 4.54 | wps 65838.7 | ups 4.53 | wpb 14524.4 | bsz 748.5 | num_updates 6846 | lr 0.000936175 | gnorm 0.725 | loss_scale 16 | train_wall 143 | gb_free 29.6 | wall 1572
2024-02-10 09:38:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:38:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:38:07 | INFO | fairseq.trainer | begin training epoch 11
2024-02-10 09:38:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:38:19 | INFO | train_inner | epoch 011:     54 / 685 loss=3.618, nll_loss=2.075, ppl=4.21, wps=52302.4, ups=3.65, wpb=14311.9, bsz=738.4, num_updates=6900, lr=0.000932505, gnorm=0.732, loss_scale=16, train_wall=21, gb_free=29.5, wall=1584
2024-02-10 09:38:40 | INFO | train_inner | epoch 011:    154 / 685 loss=3.587, nll_loss=2.039, ppl=4.11, wps=68542.9, ups=4.74, wpb=14445.8, bsz=738.6, num_updates=7000, lr=0.00092582, gnorm=0.72, loss_scale=16, train_wall=21, gb_free=29.6, wall=1605
2024-02-10 09:39:01 | INFO | train_inner | epoch 011:    254 / 685 loss=3.592, nll_loss=2.046, ppl=4.13, wps=68894.8, ups=4.72, wpb=14585.5, bsz=747.8, num_updates=7100, lr=0.000919277, gnorm=0.71, loss_scale=16, train_wall=21, gb_free=29.5, wall=1626
2024-02-10 09:39:22 | INFO | train_inner | epoch 011:    354 / 685 loss=3.597, nll_loss=2.053, ppl=4.15, wps=69224.3, ups=4.73, wpb=14627.3, bsz=775.2, num_updates=7200, lr=0.000912871, gnorm=0.721, loss_scale=16, train_wall=21, gb_free=29.5, wall=1647
2024-02-10 09:39:44 | INFO | train_inner | epoch 011:    454 / 685 loss=3.585, nll_loss=2.039, ppl=4.11, wps=68171.7, ups=4.72, wpb=14434.9, bsz=760.1, num_updates=7300, lr=0.000906597, gnorm=0.707, loss_scale=16, train_wall=21, gb_free=29.5, wall=1668
2024-02-10 09:40:05 | INFO | train_inner | epoch 011:    554 / 685 loss=3.575, nll_loss=2.028, ppl=4.08, wps=69542.5, ups=4.74, wpb=14659.1, bsz=731, num_updates=7400, lr=0.00090045, gnorm=0.71, loss_scale=16, train_wall=21, gb_free=29.5, wall=1689
2024-02-10 09:40:26 | INFO | train_inner | epoch 011:    654 / 685 loss=3.579, nll_loss=2.034, ppl=4.09, wps=68955.5, ups=4.74, wpb=14548.3, bsz=751.2, num_updates=7500, lr=0.000894427, gnorm=0.708, loss_scale=16, train_wall=21, gb_free=29.5, wall=1710
2024-02-10 09:40:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:40:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:40:36 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.702 | nll_loss 2.069 | ppl 4.2 | wps 107558 | wpb 3189.1 | bsz 163.4 | num_updates 7531 | best_loss 3.702
2024-02-10 09:40:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 7531 updates
2024-02-10 09:40:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt
2024-02-10 09:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt
2024-02-10 09:40:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint11.pt (epoch 11 @ 7531 updates, score 3.702) (writing took 4.234681902918965 seconds)
2024-02-10 09:40:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-02-10 09:40:40 | INFO | train | epoch 011 | loss 3.582 | nll_loss 2.035 | ppl 4.1 | wps 65072.9 | ups 4.48 | wpb 14524.4 | bsz 748.5 | num_updates 7531 | lr 0.000892584 | gnorm 0.713 | loss_scale 16 | train_wall 144 | gb_free 29.6 | wall 1725
2024-02-10 09:40:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:40:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:40:40 | INFO | fairseq.trainer | begin training epoch 12
2024-02-10 09:40:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:40:57 | INFO | train_inner | epoch 012:     69 / 685 loss=3.49, nll_loss=1.929, ppl=3.81, wps=46684.4, ups=3.24, wpb=14414.9, bsz=734, num_updates=7600, lr=0.000888523, gnorm=0.693, loss_scale=16, train_wall=23, gb_free=29.6, wall=1741
2024-02-10 09:41:18 | INFO | train_inner | epoch 012:    169 / 685 loss=3.463, nll_loss=1.899, ppl=3.73, wps=69724.2, ups=4.72, wpb=14758.4, bsz=769.9, num_updates=7700, lr=0.000882735, gnorm=0.667, loss_scale=16, train_wall=21, gb_free=29.6, wall=1762
2024-02-10 09:41:39 | INFO | train_inner | epoch 012:    269 / 685 loss=3.473, nll_loss=1.912, ppl=3.76, wps=69966.6, ups=4.73, wpb=14799.4, bsz=752.5, num_updates=7800, lr=0.000877058, gnorm=0.689, loss_scale=16, train_wall=21, gb_free=29.5, wall=1784
2024-02-10 09:42:00 | INFO | train_inner | epoch 012:    369 / 685 loss=3.477, nll_loss=1.917, ppl=3.78, wps=68676.6, ups=4.75, wpb=14461.5, bsz=732.8, num_updates=7900, lr=0.000871489, gnorm=0.714, loss_scale=16, train_wall=21, gb_free=29.6, wall=1805
2024-02-10 09:42:21 | INFO | train_inner | epoch 012:    469 / 685 loss=3.467, nll_loss=1.907, ppl=3.75, wps=68960.9, ups=4.74, wpb=14563.8, bsz=758.2, num_updates=8000, lr=0.000866025, gnorm=0.67, loss_scale=16, train_wall=21, gb_free=29.5, wall=1826
2024-02-10 09:42:42 | INFO | train_inner | epoch 012:    569 / 685 loss=3.47, nll_loss=1.91, ppl=3.76, wps=67587.4, ups=4.72, wpb=14310.2, bsz=748.6, num_updates=8100, lr=0.000860663, gnorm=0.67, loss_scale=16, train_wall=21, gb_free=29.5, wall=1847
2024-02-10 09:43:03 | INFO | train_inner | epoch 012:    669 / 685 loss=3.468, nll_loss=1.908, ppl=3.75, wps=68254.3, ups=4.74, wpb=14412, bsz=742.6, num_updates=8200, lr=0.000855399, gnorm=0.689, loss_scale=16, train_wall=21, gb_free=29.5, wall=1868
2024-02-10 09:43:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:43:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:43:09 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.706 | nll_loss 2.087 | ppl 4.25 | wps 131512 | wpb 3189.1 | bsz 163.4 | num_updates 8216 | best_loss 3.702
2024-02-10 09:43:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 8216 updates
2024-02-10 09:43:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt
2024-02-10 09:43:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt
2024-02-10 09:43:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint12.pt (epoch 12 @ 8216 updates, score 3.706) (writing took 2.4442212481517345 seconds)
2024-02-10 09:43:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-02-10 09:43:11 | INFO | train | epoch 012 | loss 3.469 | nll_loss 1.907 | ppl 3.75 | wps 65869.8 | ups 4.54 | wpb 14524.4 | bsz 748.5 | num_updates 8216 | lr 0.000854566 | gnorm 0.686 | loss_scale 16 | train_wall 144 | gb_free 29.8 | wall 1876
2024-02-10 09:43:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:43:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:43:11 | INFO | fairseq.trainer | begin training epoch 13
2024-02-10 09:43:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:43:29 | INFO | train_inner | epoch 013:     84 / 685 loss=3.359, nll_loss=1.781, ppl=3.44, wps=55511.9, ups=3.85, wpb=14436.1, bsz=755.5, num_updates=8300, lr=0.00085023, gnorm=0.699, loss_scale=16, train_wall=21, gb_free=29.6, wall=1894
2024-02-10 09:43:51 | INFO | train_inner | epoch 013:    184 / 685 loss=3.364, nll_loss=1.787, ppl=3.45, wps=68307.7, ups=4.72, wpb=14478, bsz=771.1, num_updates=8400, lr=0.000845154, gnorm=0.666, loss_scale=16, train_wall=21, gb_free=29.5, wall=1915
2024-02-10 09:44:12 | INFO | train_inner | epoch 013:    284 / 685 loss=3.387, nll_loss=1.813, ppl=3.51, wps=68638.2, ups=4.74, wpb=14475.8, bsz=733, num_updates=8500, lr=0.000840168, gnorm=0.696, loss_scale=16, train_wall=21, gb_free=29.6, wall=1936
2024-02-10 09:44:33 | INFO | train_inner | epoch 013:    384 / 685 loss=3.399, nll_loss=1.828, ppl=3.55, wps=68168.2, ups=4.75, wpb=14357.9, bsz=725, num_updates=8600, lr=0.000835269, gnorm=0.665, loss_scale=16, train_wall=21, gb_free=29.5, wall=1957
2024-02-10 09:44:54 | INFO | train_inner | epoch 013:    484 / 685 loss=3.385, nll_loss=1.814, ppl=3.52, wps=69787.9, ups=4.73, wpb=14742.4, bsz=753.3, num_updates=8700, lr=0.000830455, gnorm=0.655, loss_scale=16, train_wall=21, gb_free=29.6, wall=1979
2024-02-10 09:45:15 | INFO | train_inner | epoch 013:    584 / 685 loss=3.375, nll_loss=1.803, ppl=3.49, wps=68879.6, ups=4.73, wpb=14567.7, bsz=769, num_updates=8800, lr=0.000825723, gnorm=0.68, loss_scale=16, train_wall=21, gb_free=29.5, wall=2000
2024-02-10 09:45:36 | INFO | train_inner | epoch 013:    684 / 685 loss=3.378, nll_loss=1.808, ppl=3.5, wps=69596.8, ups=4.77, wpb=14603.9, bsz=737.7, num_updates=8900, lr=0.000821071, gnorm=0.665, loss_scale=16, train_wall=21, gb_free=29.5, wall=2021
2024-02-10 09:45:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:45:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:45:38 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.66 | nll_loss 2.031 | ppl 4.09 | wps 131596 | wpb 3189.1 | bsz 163.4 | num_updates 8901 | best_loss 3.66
2024-02-10 09:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 8901 updates
2024-02-10 09:45:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt
2024-02-10 09:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt
2024-02-10 09:45:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint13.pt (epoch 13 @ 8901 updates, score 3.66) (writing took 3.903975722147152 seconds)
2024-02-10 09:45:42 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-02-10 09:45:42 | INFO | train | epoch 013 | loss 3.376 | nll_loss 1.802 | ppl 3.49 | wps 65905.1 | ups 4.54 | wpb 14524.4 | bsz 748.5 | num_updates 8901 | lr 0.000821025 | gnorm 0.674 | loss_scale 16 | train_wall 143 | gb_free 29.6 | wall 2027
2024-02-10 09:45:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:45:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:45:42 | INFO | fairseq.trainer | begin training epoch 14
2024-02-10 09:45:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:45:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-10 09:46:04 | INFO | train_inner | epoch 014:    100 / 685 loss=3.275, nll_loss=1.687, ppl=3.22, wps=52674.8, ups=3.62, wpb=14555.2, bsz=746.3, num_updates=9000, lr=0.000816497, gnorm=0.694, loss_scale=8, train_wall=21, gb_free=29.5, wall=2048
2024-02-10 09:46:25 | INFO | train_inner | epoch 014:    200 / 685 loss=3.292, nll_loss=1.707, ppl=3.26, wps=69173, ups=4.71, wpb=14672.8, bsz=756.7, num_updates=9100, lr=0.000811998, gnorm=0.661, loss_scale=8, train_wall=21, gb_free=29.5, wall=2069
2024-02-10 09:46:46 | INFO | train_inner | epoch 014:    300 / 685 loss=3.296, nll_loss=1.712, ppl=3.28, wps=67859.1, ups=4.75, wpb=14293.4, bsz=751.3, num_updates=9200, lr=0.000807573, gnorm=0.704, loss_scale=8, train_wall=21, gb_free=29.5, wall=2091
2024-02-10 09:47:07 | INFO | train_inner | epoch 014:    400 / 685 loss=3.293, nll_loss=1.709, ppl=3.27, wps=68391.6, ups=4.72, wpb=14476, bsz=748.6, num_updates=9300, lr=0.000803219, gnorm=0.658, loss_scale=8, train_wall=21, gb_free=29.5, wall=2112
2024-02-10 09:47:28 | INFO | train_inner | epoch 014:    500 / 685 loss=3.308, nll_loss=1.726, ppl=3.31, wps=68376.7, ups=4.74, wpb=14435.7, bsz=748.9, num_updates=9400, lr=0.000798935, gnorm=0.669, loss_scale=8, train_wall=21, gb_free=29.5, wall=2133
2024-02-10 09:47:49 | INFO | train_inner | epoch 014:    600 / 685 loss=3.294, nll_loss=1.711, ppl=3.27, wps=69111.7, ups=4.74, wpb=14565.8, bsz=746.5, num_updates=9500, lr=0.000794719, gnorm=0.637, loss_scale=8, train_wall=21, gb_free=29.5, wall=2154
2024-02-10 09:48:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:48:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:48:09 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.609 | nll_loss 1.971 | ppl 3.92 | wps 131442 | wpb 3189.1 | bsz 163.4 | num_updates 9585 | best_loss 3.609
2024-02-10 09:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 9585 updates
2024-02-10 09:48:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt
2024-02-10 09:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt
2024-02-10 09:48:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint14.pt (epoch 14 @ 9585 updates, score 3.609) (writing took 3.8226681330706924 seconds)
2024-02-10 09:48:13 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-02-10 09:48:13 | INFO | train | epoch 014 | loss 3.295 | nll_loss 1.711 | ppl 3.27 | wps 65862.5 | ups 4.53 | wpb 14523.6 | bsz 748.7 | num_updates 9585 | lr 0.000791188 | gnorm 0.671 | loss_scale 8 | train_wall 143 | gb_free 30 | wall 2178
2024-02-10 09:48:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:48:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:48:13 | INFO | fairseq.trainer | begin training epoch 15
2024-02-10 09:48:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:48:16 | INFO | train_inner | epoch 015:     15 / 685 loss=3.3, nll_loss=1.718, ppl=3.29, wps=53595.7, ups=3.67, wpb=14593.4, bsz=731.3, num_updates=9600, lr=0.000790569, gnorm=0.697, loss_scale=8, train_wall=21, gb_free=29.5, wall=2181
2024-02-10 09:48:38 | INFO | train_inner | epoch 015:    115 / 685 loss=3.216, nll_loss=1.62, ppl=3.07, wps=69159.1, ups=4.75, wpb=14573, bsz=727.7, num_updates=9700, lr=0.000786484, gnorm=0.629, loss_scale=8, train_wall=21, gb_free=29.5, wall=2202
2024-02-10 09:48:59 | INFO | train_inner | epoch 015:    215 / 685 loss=3.219, nll_loss=1.625, ppl=3.08, wps=68853, ups=4.73, wpb=14565.2, bsz=759.6, num_updates=9800, lr=0.000782461, gnorm=0.655, loss_scale=8, train_wall=21, gb_free=29.6, wall=2223
2024-02-10 09:49:20 | INFO | train_inner | epoch 015:    315 / 685 loss=3.219, nll_loss=1.626, ppl=3.09, wps=68075, ups=4.73, wpb=14391.9, bsz=756.4, num_updates=9900, lr=0.000778499, gnorm=0.655, loss_scale=8, train_wall=21, gb_free=29.6, wall=2244
2024-02-10 09:49:41 | INFO | train_inner | epoch 015:    415 / 685 loss=3.217, nll_loss=1.624, ppl=3.08, wps=68609, ups=4.73, wpb=14514, bsz=763.1, num_updates=10000, lr=0.000774597, gnorm=0.646, loss_scale=8, train_wall=21, gb_free=29.5, wall=2266
2024-02-10 09:50:02 | INFO | train_inner | epoch 015:    515 / 685 loss=3.239, nll_loss=1.649, ppl=3.14, wps=69007.5, ups=4.73, wpb=14579, bsz=733.1, num_updates=10100, lr=0.000770752, gnorm=0.664, loss_scale=8, train_wall=21, gb_free=29.5, wall=2287
2024-02-10 09:50:23 | INFO | train_inner | epoch 015:    615 / 685 loss=3.23, nll_loss=1.64, ppl=3.12, wps=68683.8, ups=4.74, wpb=14497.7, bsz=756.1, num_updates=10200, lr=0.000766965, gnorm=0.64, loss_scale=8, train_wall=21, gb_free=29.6, wall=2308
2024-02-10 09:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:50:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:50:40 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.598 | nll_loss 1.975 | ppl 3.93 | wps 131399 | wpb 3189.1 | bsz 163.4 | num_updates 10270 | best_loss 3.598
2024-02-10 09:50:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 10270 updates
2024-02-10 09:50:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt
2024-02-10 09:50:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt
2024-02-10 09:50:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint15.pt (epoch 15 @ 10270 updates, score 3.598) (writing took 3.80312406597659 seconds)
2024-02-10 09:50:44 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-02-10 09:50:44 | INFO | train | epoch 015 | loss 3.225 | nll_loss 1.633 | ppl 3.1 | wps 65959.7 | ups 4.54 | wpb 14524.4 | bsz 748.5 | num_updates 10270 | lr 0.000764347 | gnorm 0.654 | loss_scale 8 | train_wall 143 | gb_free 29.7 | wall 2329
2024-02-10 09:50:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:50:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:50:44 | INFO | fairseq.trainer | begin training epoch 16
2024-02-10 09:50:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:50:50 | INFO | train_inner | epoch 016:     30 / 685 loss=3.211, nll_loss=1.617, ppl=3.07, wps=53400.6, ups=3.67, wpb=14554.4, bsz=748.7, num_updates=10300, lr=0.000763233, gnorm=0.67, loss_scale=8, train_wall=21, gb_free=29.5, wall=2335
2024-02-10 09:51:12 | INFO | train_inner | epoch 016:    130 / 685 loss=3.15, nll_loss=1.546, ppl=2.92, wps=69105.9, ups=4.74, wpb=14565.8, bsz=738, num_updates=10400, lr=0.000759555, gnorm=0.627, loss_scale=8, train_wall=21, gb_free=29.5, wall=2356
2024-02-10 09:51:33 | INFO | train_inner | epoch 016:    230 / 685 loss=3.172, nll_loss=1.572, ppl=2.97, wps=67928.5, ups=4.74, wpb=14325.4, bsz=738.9, num_updates=10500, lr=0.000755929, gnorm=0.652, loss_scale=8, train_wall=21, gb_free=29.5, wall=2377
2024-02-10 09:51:54 | INFO | train_inner | epoch 016:    330 / 685 loss=3.162, nll_loss=1.561, ppl=2.95, wps=68812.1, ups=4.73, wpb=14550.7, bsz=757.6, num_updates=10600, lr=0.000752355, gnorm=0.664, loss_scale=8, train_wall=21, gb_free=29.5, wall=2398
2024-02-10 09:52:15 | INFO | train_inner | epoch 016:    430 / 685 loss=3.165, nll_loss=1.565, ppl=2.96, wps=69328.1, ups=4.72, wpb=14679.8, bsz=746.7, num_updates=10700, lr=0.000748831, gnorm=0.638, loss_scale=8, train_wall=21, gb_free=29.5, wall=2420
2024-02-10 09:52:36 | INFO | train_inner | epoch 016:    530 / 685 loss=3.178, nll_loss=1.581, ppl=2.99, wps=69180.3, ups=4.73, wpb=14619.6, bsz=756.1, num_updates=10800, lr=0.000745356, gnorm=0.658, loss_scale=8, train_wall=21, gb_free=29.5, wall=2441
2024-02-10 09:52:57 | INFO | train_inner | epoch 016:    630 / 685 loss=3.165, nll_loss=1.567, ppl=2.96, wps=69203.5, ups=4.74, wpb=14609.3, bsz=749.6, num_updates=10900, lr=0.000741929, gnorm=0.612, loss_scale=8, train_wall=21, gb_free=29.5, wall=2462
2024-02-10 09:53:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:53:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:53:11 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.569 | nll_loss 1.935 | ppl 3.82 | wps 131768 | wpb 3189.1 | bsz 163.4 | num_updates 10955 | best_loss 3.569
2024-02-10 09:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 10955 updates
2024-02-10 09:53:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt
2024-02-10 09:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt
2024-02-10 09:53:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint16.pt (epoch 16 @ 10955 updates, score 3.569) (writing took 3.8123949950095266 seconds)
2024-02-10 09:53:15 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-02-10 09:53:15 | INFO | train | epoch 016 | loss 3.165 | nll_loss 1.565 | ppl 2.96 | wps 65953.1 | ups 4.54 | wpb 14524.4 | bsz 748.5 | num_updates 10955 | lr 0.000740064 | gnorm 0.644 | loss_scale 8 | train_wall 143 | gb_free 29.9 | wall 2479
2024-02-10 09:53:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:53:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:53:15 | INFO | fairseq.trainer | begin training epoch 17
2024-02-10 09:53:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:53:24 | INFO | train_inner | epoch 017:     45 / 685 loss=3.135, nll_loss=1.531, ppl=2.89, wps=52179, ups=3.67, wpb=14236.5, bsz=755.4, num_updates=11000, lr=0.000738549, gnorm=0.632, loss_scale=8, train_wall=21, gb_free=29.5, wall=2489
2024-02-10 09:53:46 | INFO | train_inner | epoch 017:    145 / 685 loss=3.091, nll_loss=1.48, ppl=2.79, wps=68116.8, ups=4.74, wpb=14384.9, bsz=742.5, num_updates=11100, lr=0.000735215, gnorm=0.624, loss_scale=8, train_wall=21, gb_free=29.6, wall=2510
2024-02-10 09:54:07 | INFO | train_inner | epoch 017:    245 / 685 loss=3.103, nll_loss=1.494, ppl=2.82, wps=68960.8, ups=4.76, wpb=14499, bsz=738.8, num_updates=11200, lr=0.000731925, gnorm=0.611, loss_scale=8, train_wall=21, gb_free=29.5, wall=2531
2024-02-10 09:54:28 | INFO | train_inner | epoch 017:    345 / 685 loss=3.118, nll_loss=1.514, ppl=2.86, wps=69090.5, ups=4.73, wpb=14597.9, bsz=768.6, num_updates=11300, lr=0.000728679, gnorm=0.644, loss_scale=8, train_wall=21, gb_free=29.5, wall=2552
2024-02-10 09:54:49 | INFO | train_inner | epoch 017:    445 / 685 loss=3.115, nll_loss=1.51, ppl=2.85, wps=68517.5, ups=4.74, wpb=14452.8, bsz=766.5, num_updates=11400, lr=0.000725476, gnorm=0.647, loss_scale=8, train_wall=21, gb_free=29.5, wall=2574
2024-02-10 09:55:10 | INFO | train_inner | epoch 017:    545 / 685 loss=3.118, nll_loss=1.514, ppl=2.86, wps=69390.5, ups=4.75, wpb=14603.6, bsz=746.6, num_updates=11500, lr=0.000722315, gnorm=0.61, loss_scale=8, train_wall=21, gb_free=29.6, wall=2595
2024-02-10 09:55:31 | INFO | train_inner | epoch 017:    645 / 685 loss=3.117, nll_loss=1.513, ppl=2.85, wps=69791.4, ups=4.76, wpb=14673.3, bsz=742.9, num_updates=11600, lr=0.000719195, gnorm=0.609, loss_scale=8, train_wall=21, gb_free=29.5, wall=2616
2024-02-10 09:55:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:55:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:55:41 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.538 | nll_loss 1.909 | ppl 3.75 | wps 131457 | wpb 3189.1 | bsz 163.4 | num_updates 11640 | best_loss 3.538
2024-02-10 09:55:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 11640 updates
2024-02-10 09:55:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt
2024-02-10 09:55:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt
2024-02-10 09:55:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint17.pt (epoch 17 @ 11640 updates, score 3.538) (writing took 3.7980772310402244 seconds)
2024-02-10 09:55:45 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-02-10 09:55:45 | INFO | train | epoch 017 | loss 3.111 | nll_loss 1.505 | ppl 2.84 | wps 66131.5 | ups 4.55 | wpb 14524.4 | bsz 748.5 | num_updates 11640 | lr 0.000717958 | gnorm 0.626 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 2630
2024-02-10 09:55:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:55:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:55:45 | INFO | fairseq.trainer | begin training epoch 18
2024-02-10 09:55:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:55:58 | INFO | train_inner | epoch 018:     60 / 685 loss=3.078, nll_loss=1.466, ppl=2.76, wps=53575.1, ups=3.69, wpb=14524.1, bsz=726.2, num_updates=11700, lr=0.000716115, gnorm=0.624, loss_scale=8, train_wall=21, gb_free=29.5, wall=2643
2024-02-10 09:56:19 | INFO | train_inner | epoch 018:    160 / 685 loss=3.05, nll_loss=1.436, ppl=2.71, wps=69539.5, ups=4.76, wpb=14616.1, bsz=739.3, num_updates=11800, lr=0.000713074, gnorm=0.622, loss_scale=8, train_wall=21, gb_free=29.5, wall=2664
2024-02-10 09:56:40 | INFO | train_inner | epoch 018:    260 / 685 loss=3.061, nll_loss=1.449, ppl=2.73, wps=68655.2, ups=4.74, wpb=14470.4, bsz=762.7, num_updates=11900, lr=0.000710072, gnorm=0.639, loss_scale=8, train_wall=21, gb_free=29.5, wall=2685
2024-02-10 09:57:01 | INFO | train_inner | epoch 018:    360 / 685 loss=3.059, nll_loss=1.447, ppl=2.73, wps=69461.1, ups=4.75, wpb=14631.5, bsz=755.5, num_updates=12000, lr=0.000707107, gnorm=0.596, loss_scale=8, train_wall=21, gb_free=29.5, wall=2706
2024-02-10 09:57:22 | INFO | train_inner | epoch 018:    460 / 685 loss=3.072, nll_loss=1.462, ppl=2.75, wps=68806.6, ups=4.78, wpb=14402.3, bsz=722.5, num_updates=12100, lr=0.000704179, gnorm=0.607, loss_scale=8, train_wall=21, gb_free=29.5, wall=2727
2024-02-10 09:57:43 | INFO | train_inner | epoch 018:    560 / 685 loss=3.069, nll_loss=1.459, ppl=2.75, wps=69069.2, ups=4.72, wpb=14620.8, bsz=761.1, num_updates=12200, lr=0.000701287, gnorm=0.594, loss_scale=8, train_wall=21, gb_free=29.6, wall=2748
2024-02-10 09:58:04 | INFO | train_inner | epoch 018:    660 / 685 loss=3.077, nll_loss=1.47, ppl=2.77, wps=68437.2, ups=4.74, wpb=14433, bsz=762.1, num_updates=12300, lr=0.00069843, gnorm=0.623, loss_scale=8, train_wall=21, gb_free=29.5, wall=2769
2024-02-10 09:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 09:58:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:58:12 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.547 | nll_loss 1.928 | ppl 3.81 | wps 131295 | wpb 3189.1 | bsz 163.4 | num_updates 12325 | best_loss 3.538
2024-02-10 09:58:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 12325 updates
2024-02-10 09:58:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt
2024-02-10 09:58:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt
2024-02-10 09:58:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint18.pt (epoch 18 @ 12325 updates, score 3.547) (writing took 2.3964251270517707 seconds)
2024-02-10 09:58:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-02-10 09:58:14 | INFO | train | epoch 018 | loss 3.063 | nll_loss 1.451 | ppl 2.73 | wps 66803.9 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 12325 | lr 0.000697722 | gnorm 0.612 | loss_scale 8 | train_wall 142 | gb_free 29.8 | wall 2779
2024-02-10 09:58:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 09:58:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 09:58:14 | INFO | fairseq.trainer | begin training epoch 19
2024-02-10 09:58:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 09:58:30 | INFO | train_inner | epoch 019:     75 / 685 loss=3.01, nll_loss=1.39, ppl=2.62, wps=55709.9, ups=3.88, wpb=14345.4, bsz=745.3, num_updates=12400, lr=0.000695608, gnorm=0.605, loss_scale=8, train_wall=21, gb_free=29.6, wall=2795
2024-02-10 09:58:51 | INFO | train_inner | epoch 019:    175 / 685 loss=2.994, nll_loss=1.373, ppl=2.59, wps=68895.1, ups=4.72, wpb=14584.7, bsz=754.4, num_updates=12500, lr=0.00069282, gnorm=0.566, loss_scale=8, train_wall=21, gb_free=29.6, wall=2816
2024-02-10 09:59:12 | INFO | train_inner | epoch 019:    275 / 685 loss=3.027, nll_loss=1.411, ppl=2.66, wps=69060.8, ups=4.75, wpb=14528.5, bsz=736.9, num_updates=12600, lr=0.000690066, gnorm=0.597, loss_scale=8, train_wall=21, gb_free=29.6, wall=2837
2024-02-10 09:59:33 | INFO | train_inner | epoch 019:    375 / 685 loss=3.026, nll_loss=1.411, ppl=2.66, wps=69311.3, ups=4.75, wpb=14594, bsz=746.6, num_updates=12700, lr=0.000687343, gnorm=0.589, loss_scale=8, train_wall=21, gb_free=29.5, wall=2858
2024-02-10 09:59:54 | INFO | train_inner | epoch 019:    475 / 685 loss=3.033, nll_loss=1.419, ppl=2.67, wps=69156.6, ups=4.76, wpb=14517.1, bsz=759.3, num_updates=12800, lr=0.000684653, gnorm=0.579, loss_scale=8, train_wall=21, gb_free=29.6, wall=2879
2024-02-10 10:00:16 | INFO | train_inner | epoch 019:    575 / 685 loss=3.026, nll_loss=1.412, ppl=2.66, wps=69232.8, ups=4.73, wpb=14621.7, bsz=762.1, num_updates=12900, lr=0.000681994, gnorm=0.604, loss_scale=8, train_wall=21, gb_free=29.5, wall=2900
2024-02-10 10:00:37 | INFO | train_inner | epoch 019:    675 / 685 loss=3.032, nll_loss=1.419, ppl=2.67, wps=69109.4, ups=4.76, wpb=14519.3, bsz=733.4, num_updates=13000, lr=0.000679366, gnorm=0.593, loss_scale=8, train_wall=21, gb_free=29.6, wall=2921
2024-02-10 10:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:00:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:00:41 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.546 | nll_loss 1.935 | ppl 3.82 | wps 126364 | wpb 3189.1 | bsz 163.4 | num_updates 13010 | best_loss 3.538
2024-02-10 10:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 13010 updates
2024-02-10 10:00:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt
2024-02-10 10:00:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt
2024-02-10 10:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint19.pt (epoch 19 @ 13010 updates, score 3.546) (writing took 2.370486319065094 seconds)
2024-02-10 10:00:43 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-02-10 10:00:43 | INFO | train | epoch 019 | loss 3.019 | nll_loss 1.402 | ppl 2.64 | wps 66756.9 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 13010 | lr 0.000679105 | gnorm 0.591 | loss_scale 8 | train_wall 142 | gb_free 29.8 | wall 2928
2024-02-10 10:00:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:00:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:00:43 | INFO | fairseq.trainer | begin training epoch 20
2024-02-10 10:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:01:02 | INFO | train_inner | epoch 020:     90 / 685 loss=2.971, nll_loss=1.346, ppl=2.54, wps=55663.7, ups=3.87, wpb=14384.9, bsz=758.1, num_updates=13100, lr=0.000676768, gnorm=0.615, loss_scale=16, train_wall=21, gb_free=29.5, wall=2947
2024-02-10 10:01:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-02-10 10:01:24 | INFO | train_inner | epoch 020:    191 / 685 loss=2.972, nll_loss=1.349, ppl=2.55, wps=68065.4, ups=4.72, wpb=14419.8, bsz=730.7, num_updates=13200, lr=0.0006742, gnorm=0.585, loss_scale=8, train_wall=21, gb_free=29.5, wall=2968
2024-02-10 10:01:45 | INFO | train_inner | epoch 020:    291 / 685 loss=2.98, nll_loss=1.358, ppl=2.56, wps=69344.6, ups=4.77, wpb=14528.3, bsz=730.6, num_updates=13300, lr=0.00067166, gnorm=0.6, loss_scale=8, train_wall=21, gb_free=29.5, wall=2989
2024-02-10 10:02:06 | INFO | train_inner | epoch 020:    391 / 685 loss=2.99, nll_loss=1.372, ppl=2.59, wps=69737, ups=4.76, wpb=14657.2, bsz=771.8, num_updates=13400, lr=0.00066915, gnorm=0.568, loss_scale=8, train_wall=21, gb_free=29.5, wall=3010
2024-02-10 10:02:26 | INFO | train_inner | epoch 020:    491 / 685 loss=2.989, nll_loss=1.37, ppl=2.59, wps=68905.9, ups=4.77, wpb=14448.2, bsz=744.1, num_updates=13500, lr=0.000666667, gnorm=0.592, loss_scale=8, train_wall=21, gb_free=29.5, wall=3031
2024-02-10 10:02:47 | INFO | train_inner | epoch 020:    591 / 685 loss=2.991, nll_loss=1.373, ppl=2.59, wps=70297.9, ups=4.77, wpb=14740.9, bsz=739.9, num_updates=13600, lr=0.000664211, gnorm=0.569, loss_scale=8, train_wall=21, gb_free=29.6, wall=3052
2024-02-10 10:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:03:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:03:09 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.478 | nll_loss 1.852 | ppl 3.61 | wps 132297 | wpb 3189.1 | bsz 163.4 | num_updates 13694 | best_loss 3.478
2024-02-10 10:03:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 13694 updates
2024-02-10 10:03:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt
2024-02-10 10:03:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt
2024-02-10 10:03:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint20.pt (epoch 20 @ 13694 updates, score 3.478) (writing took 3.8280839568469673 seconds)
2024-02-10 10:03:13 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-02-10 10:03:13 | INFO | train | epoch 020 | loss 2.983 | nll_loss 1.363 | ppl 2.57 | wps 66218.9 | ups 4.56 | wpb 14524.3 | bsz 748.6 | num_updates 13694 | lr 0.000661928 | gnorm 0.587 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 3078
2024-02-10 10:03:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:03:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:03:13 | INFO | fairseq.trainer | begin training epoch 21
2024-02-10 10:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:03:15 | INFO | train_inner | epoch 021:      6 / 685 loss=2.986, nll_loss=1.369, ppl=2.58, wps=52925.1, ups=3.67, wpb=14415.9, bsz=760.6, num_updates=13700, lr=0.000661783, gnorm=0.6, loss_scale=8, train_wall=21, gb_free=29.5, wall=3079
2024-02-10 10:03:36 | INFO | train_inner | epoch 021:    106 / 685 loss=2.924, nll_loss=1.294, ppl=2.45, wps=69151.3, ups=4.77, wpb=14501.5, bsz=740.2, num_updates=13800, lr=0.00065938, gnorm=0.547, loss_scale=8, train_wall=21, gb_free=29.5, wall=3100
2024-02-10 10:03:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-10 10:03:57 | INFO | train_inner | epoch 021:    207 / 685 loss=2.935, nll_loss=1.308, ppl=2.48, wps=68485.2, ups=4.7, wpb=14563.2, bsz=762.1, num_updates=13900, lr=0.000657004, gnorm=0.567, loss_scale=4, train_wall=21, gb_free=29.6, wall=3122
2024-02-10 10:04:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-10 10:04:18 | INFO | train_inner | epoch 021:    308 / 685 loss=2.952, nll_loss=1.328, ppl=2.51, wps=68569.2, ups=4.71, wpb=14567.5, bsz=753.9, num_updates=14000, lr=0.000654654, gnorm=0.64, loss_scale=2, train_wall=21, gb_free=29.5, wall=3143
2024-02-10 10:04:39 | INFO | train_inner | epoch 021:    408 / 685 loss=2.95, nll_loss=1.326, ppl=2.51, wps=68696.2, ups=4.76, wpb=14445.7, bsz=761.3, num_updates=14100, lr=0.000652328, gnorm=0.569, loss_scale=2, train_wall=21, gb_free=29.6, wall=3164
2024-02-10 10:05:00 | INFO | train_inner | epoch 021:    508 / 685 loss=2.953, nll_loss=1.33, ppl=2.51, wps=68110.5, ups=4.75, wpb=14338.5, bsz=729.4, num_updates=14200, lr=0.000650027, gnorm=0.57, loss_scale=2, train_wall=21, gb_free=29.5, wall=3185
2024-02-10 10:05:21 | INFO | train_inner | epoch 021:    608 / 685 loss=2.961, nll_loss=1.341, ppl=2.53, wps=69987.7, ups=4.77, wpb=14686.1, bsz=744.5, num_updates=14300, lr=0.00064775, gnorm=0.565, loss_scale=2, train_wall=21, gb_free=29.5, wall=3206
2024-02-10 10:05:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:05:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:05:40 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.474 | nll_loss 1.854 | ppl 3.61 | wps 132611 | wpb 3189.1 | bsz 163.4 | num_updates 14377 | best_loss 3.474
2024-02-10 10:05:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 14377 updates
2024-02-10 10:05:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt
2024-02-10 10:05:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt
2024-02-10 10:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint21.pt (epoch 21 @ 14377 updates, score 3.474) (writing took 3.8152082699816674 seconds)
2024-02-10 10:05:44 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-02-10 10:05:44 | INFO | train | epoch 021 | loss 2.948 | nll_loss 1.324 | ppl 2.5 | wps 66012.3 | ups 4.55 | wpb 14521.6 | bsz 747.8 | num_updates 14377 | lr 0.000646013 | gnorm 0.576 | loss_scale 2 | train_wall 142 | gb_free 29.6 | wall 3228
2024-02-10 10:05:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:05:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:05:44 | INFO | fairseq.trainer | begin training epoch 22
2024-02-10 10:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:05:48 | INFO | train_inner | epoch 022:     23 / 685 loss=2.95, nll_loss=1.328, ppl=2.51, wps=53348.7, ups=3.67, wpb=14527.5, bsz=740.7, num_updates=14400, lr=0.000645497, gnorm=0.575, loss_scale=2, train_wall=21, gb_free=29.6, wall=3233
2024-02-10 10:06:10 | INFO | train_inner | epoch 022:    123 / 685 loss=2.887, nll_loss=1.253, ppl=2.38, wps=69346.5, ups=4.75, wpb=14597.6, bsz=752.6, num_updates=14500, lr=0.000643268, gnorm=0.563, loss_scale=2, train_wall=21, gb_free=29.6, wall=3254
2024-02-10 10:06:31 | INFO | train_inner | epoch 022:    223 / 685 loss=2.895, nll_loss=1.264, ppl=2.4, wps=69723.2, ups=4.76, wpb=14640.9, bsz=741.8, num_updates=14600, lr=0.000641061, gnorm=0.549, loss_scale=2, train_wall=21, gb_free=29.6, wall=3275
2024-02-10 10:06:52 | INFO | train_inner | epoch 022:    323 / 685 loss=2.922, nll_loss=1.297, ppl=2.46, wps=70219.9, ups=4.75, wpb=14796.7, bsz=765.2, num_updates=14700, lr=0.000638877, gnorm=0.599, loss_scale=2, train_wall=21, gb_free=29.5, wall=3296
2024-02-10 10:07:13 | INFO | train_inner | epoch 022:    423 / 685 loss=2.922, nll_loss=1.296, ppl=2.45, wps=67971.3, ups=4.76, wpb=14282.6, bsz=761.4, num_updates=14800, lr=0.000636715, gnorm=0.578, loss_scale=2, train_wall=21, gb_free=29.6, wall=3317
2024-02-10 10:07:34 | INFO | train_inner | epoch 022:    523 / 685 loss=2.934, nll_loss=1.311, ppl=2.48, wps=69564.9, ups=4.76, wpb=14624.3, bsz=745.4, num_updates=14900, lr=0.000634574, gnorm=0.582, loss_scale=2, train_wall=21, gb_free=29.5, wall=3338
2024-02-10 10:07:55 | INFO | train_inner | epoch 022:    623 / 685 loss=2.935, nll_loss=1.312, ppl=2.48, wps=68818.4, ups=4.77, wpb=14426.3, bsz=740, num_updates=15000, lr=0.000632456, gnorm=0.563, loss_scale=2, train_wall=21, gb_free=29.5, wall=3359
2024-02-10 10:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:08:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:08:10 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.469 | nll_loss 1.847 | ppl 3.6 | wps 132520 | wpb 3189.1 | bsz 163.4 | num_updates 15062 | best_loss 3.469
2024-02-10 10:08:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 15062 updates
2024-02-10 10:08:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt
2024-02-10 10:08:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt
2024-02-10 10:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint22.pt (epoch 22 @ 15062 updates, score 3.469) (writing took 3.7971334990579635 seconds)
2024-02-10 10:08:14 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-02-10 10:08:14 | INFO | train | epoch 022 | loss 2.917 | nll_loss 1.29 | ppl 2.44 | wps 66321.1 | ups 4.57 | wpb 14524.4 | bsz 748.5 | num_updates 15062 | lr 0.000631152 | gnorm 0.573 | loss_scale 2 | train_wall 142 | gb_free 29.6 | wall 3378
2024-02-10 10:08:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:08:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:08:14 | INFO | fairseq.trainer | begin training epoch 23
2024-02-10 10:08:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:08:22 | INFO | train_inner | epoch 023:     38 / 685 loss=2.905, nll_loss=1.276, ppl=2.42, wps=52885.7, ups=3.69, wpb=14317.9, bsz=739, num_updates=15100, lr=0.000630358, gnorm=0.564, loss_scale=2, train_wall=21, gb_free=29.5, wall=3386
2024-02-10 10:08:43 | INFO | train_inner | epoch 023:    138 / 685 loss=2.863, nll_loss=1.228, ppl=2.34, wps=68826.6, ups=4.75, wpb=14480.8, bsz=757.9, num_updates=15200, lr=0.000628281, gnorm=0.547, loss_scale=2, train_wall=21, gb_free=29.5, wall=3407
2024-02-10 10:09:04 | INFO | train_inner | epoch 023:    238 / 685 loss=2.877, nll_loss=1.244, ppl=2.37, wps=69124.6, ups=4.78, wpb=14474.3, bsz=743, num_updates=15300, lr=0.000626224, gnorm=0.526, loss_scale=2, train_wall=21, gb_free=29.5, wall=3428
2024-02-10 10:09:25 | INFO | train_inner | epoch 023:    338 / 685 loss=2.896, nll_loss=1.267, ppl=2.41, wps=69217.1, ups=4.77, wpb=14515.7, bsz=738.5, num_updates=15400, lr=0.000624188, gnorm=0.553, loss_scale=2, train_wall=21, gb_free=29.5, wall=3449
2024-02-10 10:09:46 | INFO | train_inner | epoch 023:    438 / 685 loss=2.894, nll_loss=1.266, ppl=2.41, wps=69397.4, ups=4.73, wpb=14662, bsz=743.4, num_updates=15500, lr=0.000622171, gnorm=0.537, loss_scale=2, train_wall=21, gb_free=29.5, wall=3470
2024-02-10 10:10:07 | INFO | train_inner | epoch 023:    538 / 685 loss=2.895, nll_loss=1.268, ppl=2.41, wps=68529.2, ups=4.74, wpb=14454.5, bsz=754.9, num_updates=15600, lr=0.000620174, gnorm=0.555, loss_scale=2, train_wall=21, gb_free=29.6, wall=3492
2024-02-10 10:10:28 | INFO | train_inner | epoch 023:    638 / 685 loss=2.898, nll_loss=1.271, ppl=2.41, wps=68834.9, ups=4.73, wpb=14543.4, bsz=743.9, num_updates=15700, lr=0.000618195, gnorm=0.558, loss_scale=2, train_wall=21, gb_free=29.6, wall=3513
2024-02-10 10:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:10:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:10:40 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.464 | nll_loss 1.843 | ppl 3.59 | wps 132291 | wpb 3189.1 | bsz 163.4 | num_updates 15747 | best_loss 3.464
2024-02-10 10:10:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 15747 updates
2024-02-10 10:10:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt
2024-02-10 10:10:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt
2024-02-10 10:10:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint23.pt (epoch 23 @ 15747 updates, score 3.464) (writing took 3.885794922010973 seconds)
2024-02-10 10:10:44 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-02-10 10:10:44 | INFO | train | epoch 023 | loss 2.887 | nll_loss 1.257 | ppl 2.39 | wps 66153.8 | ups 4.55 | wpb 14524.4 | bsz 748.5 | num_updates 15747 | lr 0.000617272 | gnorm 0.547 | loss_scale 2 | train_wall 142 | gb_free 29.7 | wall 3529
2024-02-10 10:10:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:10:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:10:44 | INFO | fairseq.trainer | begin training epoch 24
2024-02-10 10:10:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:10:55 | INFO | train_inner | epoch 024:     53 / 685 loss=2.876, nll_loss=1.245, ppl=2.37, wps=53602.5, ups=3.67, wpb=14597.6, bsz=746.8, num_updates=15800, lr=0.000616236, gnorm=0.544, loss_scale=2, train_wall=21, gb_free=29.6, wall=3540
2024-02-10 10:11:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2024-02-10 10:11:17 | INFO | train_inner | epoch 024:    154 / 685 loss=2.84, nll_loss=1.203, ppl=2.3, wps=67916.3, ups=4.69, wpb=14487.2, bsz=765.3, num_updates=15900, lr=0.000614295, gnorm=0.546, loss_scale=1, train_wall=21, gb_free=29.5, wall=3561
2024-02-10 10:11:38 | INFO | train_inner | epoch 024:    254 / 685 loss=2.854, nll_loss=1.219, ppl=2.33, wps=69215.5, ups=4.75, wpb=14569.6, bsz=744, num_updates=16000, lr=0.000612372, gnorm=0.537, loss_scale=1, train_wall=21, gb_free=29.5, wall=3582
2024-02-10 10:12:01 | INFO | train_inner | epoch 024:    354 / 685 loss=2.863, nll_loss=1.231, ppl=2.35, wps=62351.3, ups=4.25, wpb=14663.4, bsz=740.5, num_updates=16100, lr=0.000610468, gnorm=0.534, loss_scale=1, train_wall=23, gb_free=29.6, wall=3606
2024-02-10 10:12:23 | INFO | train_inner | epoch 024:    454 / 685 loss=2.87, nll_loss=1.24, ppl=2.36, wps=65969, ups=4.56, wpb=14480.6, bsz=726.8, num_updates=16200, lr=0.000608581, gnorm=0.594, loss_scale=1, train_wall=22, gb_free=29.6, wall=3628
2024-02-10 10:12:44 | INFO | train_inner | epoch 024:    554 / 685 loss=2.874, nll_loss=1.245, ppl=2.37, wps=67999.4, ups=4.66, wpb=14581.5, bsz=750.7, num_updates=16300, lr=0.000606711, gnorm=0.535, loss_scale=1, train_wall=21, gb_free=29.5, wall=3649
2024-02-10 10:13:06 | INFO | train_inner | epoch 024:    654 / 685 loss=2.87, nll_loss=1.24, ppl=2.36, wps=66638.1, ups=4.62, wpb=14434.8, bsz=769.3, num_updates=16400, lr=0.000604858, gnorm=0.547, loss_scale=1, train_wall=21, gb_free=29.5, wall=3671
2024-02-10 10:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:13:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:13:15 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.48 | nll_loss 1.861 | ppl 3.63 | wps 131546 | wpb 3189.1 | bsz 163.4 | num_updates 16431 | best_loss 3.464
2024-02-10 10:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 16431 updates
2024-02-10 10:13:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt
2024-02-10 10:13:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt
2024-02-10 10:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint24.pt (epoch 24 @ 16431 updates, score 3.48) (writing took 2.544674628879875 seconds)
2024-02-10 10:13:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-02-10 10:13:17 | INFO | train | epoch 024 | loss 2.861 | nll_loss 1.229 | ppl 2.34 | wps 64709.8 | ups 4.46 | wpb 14524.6 | bsz 748.2 | num_updates 16431 | lr 0.000604288 | gnorm 0.548 | loss_scale 1 | train_wall 147 | gb_free 29.9 | wall 3682
2024-02-10 10:13:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:13:17 | INFO | fairseq.trainer | begin training epoch 25
2024-02-10 10:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:13:32 | INFO | train_inner | epoch 025:     69 / 685 loss=2.827, nll_loss=1.191, ppl=2.28, wps=55058, ups=3.84, wpb=14341.4, bsz=761.4, num_updates=16500, lr=0.000603023, gnorm=0.543, loss_scale=1, train_wall=21, gb_free=29.5, wall=3697
2024-02-10 10:13:53 | INFO | train_inner | epoch 025:    169 / 685 loss=2.818, nll_loss=1.18, ppl=2.27, wps=68485.9, ups=4.71, wpb=14554.1, bsz=766, num_updates=16600, lr=0.000601204, gnorm=0.549, loss_scale=1, train_wall=21, gb_free=29.5, wall=3718
2024-02-10 10:14:15 | INFO | train_inner | epoch 025:    269 / 685 loss=2.84, nll_loss=1.205, ppl=2.31, wps=67935.7, ups=4.69, wpb=14494, bsz=728.2, num_updates=16700, lr=0.000599401, gnorm=0.554, loss_scale=1, train_wall=21, gb_free=29.5, wall=3739
2024-02-10 10:14:36 | INFO | train_inner | epoch 025:    369 / 685 loss=2.844, nll_loss=1.211, ppl=2.31, wps=68113.3, ups=4.67, wpb=14577.1, bsz=743.4, num_updates=16800, lr=0.000597614, gnorm=0.533, loss_scale=1, train_wall=21, gb_free=29.6, wall=3761
2024-02-10 10:14:58 | INFO | train_inner | epoch 025:    469 / 685 loss=2.851, nll_loss=1.218, ppl=2.33, wps=68081.4, ups=4.65, wpb=14644.3, bsz=733, num_updates=16900, lr=0.000595844, gnorm=0.541, loss_scale=1, train_wall=21, gb_free=29.5, wall=3782
2024-02-10 10:15:19 | INFO | train_inner | epoch 025:    569 / 685 loss=2.852, nll_loss=1.221, ppl=2.33, wps=68038.2, ups=4.68, wpb=14538.5, bsz=758.4, num_updates=17000, lr=0.000594089, gnorm=0.551, loss_scale=1, train_wall=21, gb_free=29.5, wall=3804
2024-02-10 10:15:40 | INFO | train_inner | epoch 025:    669 / 685 loss=2.851, nll_loss=1.22, ppl=2.33, wps=68562.6, ups=4.71, wpb=14548.7, bsz=765, num_updates=17100, lr=0.000592349, gnorm=0.547, loss_scale=1, train_wall=21, gb_free=29.5, wall=3825
2024-02-10 10:15:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:15:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:15:46 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.456 | nll_loss 1.846 | ppl 3.6 | wps 128551 | wpb 3189.1 | bsz 163.4 | num_updates 17116 | best_loss 3.456
2024-02-10 10:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 17116 updates
2024-02-10 10:15:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt
2024-02-10 10:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt
2024-02-10 10:15:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint25.pt (epoch 25 @ 17116 updates, score 3.456) (writing took 3.8749798969365656 seconds)
2024-02-10 10:15:50 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-02-10 10:15:50 | INFO | train | epoch 025 | loss 2.839 | nll_loss 1.205 | ppl 2.31 | wps 65344.9 | ups 4.5 | wpb 14524.4 | bsz 748.5 | num_updates 17116 | lr 0.000592072 | gnorm 0.546 | loss_scale 1 | train_wall 144 | gb_free 29.7 | wall 3834
2024-02-10 10:15:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:15:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:15:50 | INFO | fairseq.trainer | begin training epoch 26
2024-02-10 10:15:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:16:08 | INFO | train_inner | epoch 026:     84 / 685 loss=2.801, nll_loss=1.161, ppl=2.24, wps=52756.7, ups=3.67, wpb=14365.1, bsz=720.9, num_updates=17200, lr=0.000590624, gnorm=0.521, loss_scale=1, train_wall=21, gb_free=29.5, wall=3852
2024-02-10 10:16:29 | INFO | train_inner | epoch 026:    184 / 685 loss=2.797, nll_loss=1.157, ppl=2.23, wps=67934.5, ups=4.74, wpb=14347.1, bsz=780.3, num_updates=17300, lr=0.000588915, gnorm=0.519, loss_scale=1, train_wall=21, gb_free=29.5, wall=3873
2024-02-10 10:16:50 | INFO | train_inner | epoch 026:    284 / 685 loss=2.813, nll_loss=1.176, ppl=2.26, wps=69861.4, ups=4.76, wpb=14683.1, bsz=754.6, num_updates=17400, lr=0.00058722, gnorm=0.514, loss_scale=1, train_wall=21, gb_free=29.5, wall=3894
2024-02-10 10:17:19 | INFO | train_inner | epoch 026:    384 / 685 loss=2.822, nll_loss=1.187, ppl=2.28, wps=49059.8, ups=3.4, wpb=14417.2, bsz=723.5, num_updates=17500, lr=0.00058554, gnorm=0.524, loss_scale=1, train_wall=22, gb_free=29.5, wall=3924
2024-02-10 10:17:40 | INFO | train_inner | epoch 026:    484 / 685 loss=2.811, nll_loss=1.176, ppl=2.26, wps=68817.6, ups=4.73, wpb=14547.9, bsz=759.4, num_updates=17600, lr=0.000583874, gnorm=0.544, loss_scale=1, train_wall=21, gb_free=29.5, wall=3945
2024-02-10 10:18:01 | INFO | train_inner | epoch 026:    584 / 685 loss=2.833, nll_loss=1.202, ppl=2.3, wps=69818.6, ups=4.76, wpb=14662.8, bsz=742.6, num_updates=17700, lr=0.000582223, gnorm=0.546, loss_scale=1, train_wall=21, gb_free=29.6, wall=3966
2024-02-10 10:18:23 | INFO | train_inner | epoch 026:    684 / 685 loss=2.835, nll_loss=1.203, ppl=2.3, wps=67217.5, ups=4.61, wpb=14595.4, bsz=749, num_updates=17800, lr=0.000580585, gnorm=0.528, loss_scale=1, train_wall=21, gb_free=29.5, wall=3988
2024-02-10 10:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:18:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:18:25 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.441 | nll_loss 1.83 | ppl 3.56 | wps 132166 | wpb 3189.1 | bsz 163.4 | num_updates 17801 | best_loss 3.441
2024-02-10 10:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 17801 updates
2024-02-10 10:18:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt
2024-02-10 10:18:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt
2024-02-10 10:18:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint26.pt (epoch 26 @ 17801 updates, score 3.441) (writing took 4.009408739162609 seconds)
2024-02-10 10:18:29 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-02-10 10:18:29 | INFO | train | epoch 026 | loss 2.815 | nll_loss 1.179 | ppl 2.26 | wps 62327.8 | ups 4.29 | wpb 14524.4 | bsz 748.5 | num_updates 17801 | lr 0.000580568 | gnorm 0.527 | loss_scale 1 | train_wall 144 | gb_free 29.7 | wall 3994
2024-02-10 10:18:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:18:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:18:30 | INFO | fairseq.trainer | begin training epoch 27
2024-02-10 10:18:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:18:50 | INFO | train_inner | epoch 027:     99 / 685 loss=2.768, nll_loss=1.125, ppl=2.18, wps=52255.7, ups=3.63, wpb=14391.6, bsz=750.8, num_updates=17900, lr=0.000578961, gnorm=0.51, loss_scale=1, train_wall=21, gb_free=29.6, wall=4015
2024-02-10 10:19:11 | INFO | train_inner | epoch 027:    199 / 685 loss=2.784, nll_loss=1.144, ppl=2.21, wps=69122.8, ups=4.76, wpb=14526.5, bsz=745.8, num_updates=18000, lr=0.00057735, gnorm=0.518, loss_scale=1, train_wall=21, gb_free=29.6, wall=4036
2024-02-10 10:19:33 | INFO | train_inner | epoch 027:    299 / 685 loss=2.794, nll_loss=1.156, ppl=2.23, wps=69248.5, ups=4.74, wpb=14595.2, bsz=758, num_updates=18100, lr=0.000575753, gnorm=0.521, loss_scale=1, train_wall=21, gb_free=29.6, wall=4057
2024-02-10 10:19:54 | INFO | train_inner | epoch 027:    399 / 685 loss=2.788, nll_loss=1.15, ppl=2.22, wps=68783, ups=4.74, wpb=14522.5, bsz=759.6, num_updates=18200, lr=0.000574169, gnorm=0.522, loss_scale=1, train_wall=21, gb_free=29.5, wall=4078
2024-02-10 10:20:15 | INFO | train_inner | epoch 027:    499 / 685 loss=2.797, nll_loss=1.16, ppl=2.23, wps=68377.7, ups=4.72, wpb=14496.4, bsz=765.3, num_updates=18300, lr=0.000572598, gnorm=0.526, loss_scale=1, train_wall=21, gb_free=29.5, wall=4100
2024-02-10 10:20:36 | INFO | train_inner | epoch 027:    599 / 685 loss=2.819, nll_loss=1.186, ppl=2.28, wps=69886.2, ups=4.77, wpb=14656.9, bsz=720.7, num_updates=18400, lr=0.00057104, gnorm=0.513, loss_scale=1, train_wall=21, gb_free=29.5, wall=4120
2024-02-10 10:20:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:20:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:20:56 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.44 | nll_loss 1.828 | ppl 3.55 | wps 130326 | wpb 3189.1 | bsz 163.4 | num_updates 18486 | best_loss 3.44
2024-02-10 10:20:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 18486 updates
2024-02-10 10:20:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt
2024-02-10 10:20:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt
2024-02-10 10:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint27.pt (epoch 27 @ 18486 updates, score 3.44) (writing took 3.8728166150394827 seconds)
2024-02-10 10:21:00 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-02-10 10:21:00 | INFO | train | epoch 027 | loss 2.795 | nll_loss 1.158 | ppl 2.23 | wps 66068.2 | ups 4.55 | wpb 14524.4 | bsz 748.5 | num_updates 18486 | lr 0.00056971 | gnorm 0.523 | loss_scale 1 | train_wall 142 | gb_free 29.7 | wall 4145
2024-02-10 10:21:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:21:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:21:00 | INFO | fairseq.trainer | begin training epoch 28
2024-02-10 10:21:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:21:03 | INFO | train_inner | epoch 028:     14 / 685 loss=2.809, nll_loss=1.174, ppl=2.26, wps=52805.2, ups=3.67, wpb=14375.6, bsz=742.2, num_updates=18500, lr=0.000569495, gnorm=0.549, loss_scale=1, train_wall=21, gb_free=29.5, wall=4148
2024-02-10 10:21:24 | INFO | train_inner | epoch 028:    114 / 685 loss=2.747, nll_loss=1.101, ppl=2.15, wps=68586.5, ups=4.75, wpb=14448.6, bsz=752.6, num_updates=18600, lr=0.000567962, gnorm=0.74, loss_scale=1, train_wall=21, gb_free=29.5, wall=4169
2024-02-10 10:21:45 | INFO | train_inner | epoch 028:    214 / 685 loss=2.765, nll_loss=1.123, ppl=2.18, wps=69597.9, ups=4.74, wpb=14671.2, bsz=763.3, num_updates=18700, lr=0.000566441, gnorm=0.556, loss_scale=1, train_wall=21, gb_free=29.6, wall=4190
2024-02-10 10:22:06 | INFO | train_inner | epoch 028:    314 / 685 loss=2.77, nll_loss=1.129, ppl=2.19, wps=69303.6, ups=4.75, wpb=14603, bsz=746.6, num_updates=18800, lr=0.000564933, gnorm=0.517, loss_scale=1, train_wall=21, gb_free=29.5, wall=4211
2024-02-10 10:22:27 | INFO | train_inner | epoch 028:    414 / 685 loss=2.782, nll_loss=1.145, ppl=2.21, wps=68882.5, ups=4.76, wpb=14484.2, bsz=744.9, num_updates=18900, lr=0.000563436, gnorm=0.532, loss_scale=1, train_wall=21, gb_free=29.5, wall=4232
2024-02-10 10:22:48 | INFO | train_inner | epoch 028:    514 / 685 loss=2.791, nll_loss=1.155, ppl=2.23, wps=69369.6, ups=4.75, wpb=14615.3, bsz=758.6, num_updates=19000, lr=0.000561951, gnorm=0.545, loss_scale=1, train_wall=21, gb_free=29.5, wall=4253
2024-02-10 10:23:09 | INFO | train_inner | epoch 028:    614 / 685 loss=2.795, nll_loss=1.159, ppl=2.23, wps=69316.7, ups=4.75, wpb=14578, bsz=730.2, num_updates=19100, lr=0.000560478, gnorm=0.732, loss_scale=1, train_wall=21, gb_free=29.5, wall=4274
2024-02-10 10:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:23:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:23:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.434 | nll_loss 1.811 | ppl 3.51 | wps 130676 | wpb 3189.1 | bsz 163.4 | num_updates 19171 | best_loss 3.434
2024-02-10 10:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 19171 updates
2024-02-10 10:23:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt
2024-02-10 10:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt
2024-02-10 10:23:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint28.pt (epoch 28 @ 19171 updates, score 3.434) (writing took 3.8452223031781614 seconds)
2024-02-10 10:23:30 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-02-10 10:23:30 | INFO | train | epoch 028 | loss 2.776 | nll_loss 1.137 | ppl 2.2 | wps 66156.6 | ups 4.55 | wpb 14524.4 | bsz 748.5 | num_updates 19171 | lr 0.00055944 | gnorm 0.606 | loss_scale 1 | train_wall 142 | gb_free 29.6 | wall 4295
2024-02-10 10:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:23:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:23:30 | INFO | fairseq.trainer | begin training epoch 29
2024-02-10 10:23:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:23:37 | INFO | train_inner | epoch 029:     29 / 685 loss=2.773, nll_loss=1.134, ppl=2.19, wps=52510.4, ups=3.68, wpb=14270.6, bsz=728.9, num_updates=19200, lr=0.000559017, gnorm=0.613, loss_scale=1, train_wall=21, gb_free=29.5, wall=4301
2024-02-10 10:23:58 | INFO | train_inner | epoch 029:    129 / 685 loss=2.737, nll_loss=1.092, ppl=2.13, wps=69377.2, ups=4.77, wpb=14555.5, bsz=727, num_updates=19300, lr=0.000557567, gnorm=0.703, loss_scale=1, train_wall=21, gb_free=29.5, wall=4322
2024-02-10 10:24:19 | INFO | train_inner | epoch 029:    229 / 685 loss=2.751, nll_loss=1.109, ppl=2.16, wps=68489.8, ups=4.76, wpb=14376.3, bsz=755, num_updates=19400, lr=0.000556128, gnorm=0.514, loss_scale=1, train_wall=21, gb_free=29.5, wall=4343
2024-02-10 10:24:40 | INFO | train_inner | epoch 029:    329 / 685 loss=2.755, nll_loss=1.114, ppl=2.16, wps=69662, ups=4.75, wpb=14654.2, bsz=753.4, num_updates=19500, lr=0.0005547, gnorm=0.517, loss_scale=1, train_wall=21, gb_free=29.5, wall=4364
2024-02-10 10:25:01 | INFO | train_inner | epoch 029:    429 / 685 loss=2.762, nll_loss=1.123, ppl=2.18, wps=69484.4, ups=4.75, wpb=14620.2, bsz=763, num_updates=19600, lr=0.000553283, gnorm=0.501, loss_scale=1, train_wall=21, gb_free=29.5, wall=4385
2024-02-10 10:25:27 | INFO | train_inner | epoch 029:    529 / 685 loss=2.775, nll_loss=1.138, ppl=2.2, wps=54053.9, ups=3.73, wpb=14482.6, bsz=729.1, num_updates=19700, lr=0.000551877, gnorm=0.508, loss_scale=1, train_wall=21, gb_free=29.5, wall=4412
2024-02-10 10:25:49 | INFO | train_inner | epoch 029:    629 / 685 loss=2.768, nll_loss=1.129, ppl=2.19, wps=67311.8, ups=4.64, wpb=14515, bsz=763.5, num_updates=19800, lr=0.000550482, gnorm=0.523, loss_scale=1, train_wall=21, gb_free=29.6, wall=4434
2024-02-10 10:26:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:26:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:26:03 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.441 | nll_loss 1.82 | ppl 3.53 | wps 131463 | wpb 3189.1 | bsz 163.4 | num_updates 19856 | best_loss 3.434
2024-02-10 10:26:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 19856 updates
2024-02-10 10:26:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt
2024-02-10 10:26:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt
2024-02-10 10:26:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint29.pt (epoch 29 @ 19856 updates, score 3.441) (writing took 2.3782954269554466 seconds)
2024-02-10 10:26:05 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-02-10 10:26:05 | INFO | train | epoch 029 | loss 2.759 | nll_loss 1.118 | ppl 2.17 | wps 64189.4 | ups 4.42 | wpb 14524.4 | bsz 748.5 | num_updates 19856 | lr 0.000549705 | gnorm 0.543 | loss_scale 1 | train_wall 142 | gb_free 29.6 | wall 4450
2024-02-10 10:26:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:26:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:26:05 | INFO | fairseq.trainer | begin training epoch 30
2024-02-10 10:26:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:26:15 | INFO | train_inner | epoch 030:     44 / 685 loss=2.751, nll_loss=1.109, ppl=2.16, wps=56167.4, ups=3.88, wpb=14460.5, bsz=754.2, num_updates=19900, lr=0.000549097, gnorm=0.563, loss_scale=1, train_wall=21, gb_free=29.6, wall=4459
2024-02-10 10:26:36 | INFO | train_inner | epoch 030:    144 / 685 loss=2.727, nll_loss=1.082, ppl=2.12, wps=68964.4, ups=4.76, wpb=14496.5, bsz=749.5, num_updates=20000, lr=0.000547723, gnorm=0.595, loss_scale=2, train_wall=21, gb_free=29.5, wall=4480
2024-02-10 10:26:57 | INFO | train_inner | epoch 030:    244 / 685 loss=2.74, nll_loss=1.097, ppl=2.14, wps=69822.9, ups=4.77, wpb=14633, bsz=739.3, num_updates=20100, lr=0.000546358, gnorm=0.572, loss_scale=2, train_wall=21, gb_free=29.5, wall=4501
2024-02-10 10:27:18 | INFO | train_inner | epoch 030:    344 / 685 loss=2.738, nll_loss=1.095, ppl=2.14, wps=68537.2, ups=4.76, wpb=14400.4, bsz=755.8, num_updates=20200, lr=0.000545004, gnorm=0.533, loss_scale=2, train_wall=21, gb_free=29.5, wall=4522
2024-02-10 10:27:39 | INFO | train_inner | epoch 030:    444 / 685 loss=2.748, nll_loss=1.107, ppl=2.15, wps=69649.6, ups=4.77, wpb=14608.9, bsz=742.6, num_updates=20300, lr=0.00054366, gnorm=0.509, loss_scale=2, train_wall=21, gb_free=29.6, wall=4543
2024-02-10 10:28:00 | INFO | train_inner | epoch 030:    544 / 685 loss=2.749, nll_loss=1.109, ppl=2.16, wps=68706.9, ups=4.75, wpb=14468, bsz=750.8, num_updates=20400, lr=0.000542326, gnorm=0.508, loss_scale=2, train_wall=21, gb_free=29.5, wall=4564
2024-02-10 10:28:21 | INFO | train_inner | epoch 030:    644 / 685 loss=2.758, nll_loss=1.12, ppl=2.17, wps=69244.5, ups=4.76, wpb=14551, bsz=753.5, num_updates=20500, lr=0.000541002, gnorm=0.503, loss_scale=2, train_wall=21, gb_free=29.5, wall=4585
2024-02-10 10:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:28:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:28:32 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.45 | nll_loss 1.834 | ppl 3.56 | wps 131572 | wpb 3189.1 | bsz 163.4 | num_updates 20541 | best_loss 3.434
2024-02-10 10:28:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 20541 updates
2024-02-10 10:28:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt
2024-02-10 10:28:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt
2024-02-10 10:28:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint30.pt (epoch 30 @ 20541 updates, score 3.45) (writing took 2.422752598999068 seconds)
2024-02-10 10:28:34 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-02-10 10:28:34 | INFO | train | epoch 030 | loss 2.742 | nll_loss 1.1 | ppl 2.14 | wps 66914.5 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 20541 | lr 0.000540462 | gnorm 0.538 | loss_scale 2 | train_wall 142 | gb_free 29.7 | wall 4599
2024-02-10 10:28:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:28:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:28:34 | INFO | fairseq.trainer | begin training epoch 31
2024-02-10 10:28:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:28:47 | INFO | train_inner | epoch 031:     59 / 685 loss=2.717, nll_loss=1.072, ppl=2.1, wps=56364.7, ups=3.87, wpb=14551.5, bsz=748.4, num_updates=20600, lr=0.000539687, gnorm=0.496, loss_scale=2, train_wall=21, gb_free=29.5, wall=4611
2024-02-10 10:29:08 | INFO | train_inner | epoch 031:    159 / 685 loss=2.708, nll_loss=1.062, ppl=2.09, wps=70360.7, ups=4.77, wpb=14740.6, bsz=744.6, num_updates=20700, lr=0.000538382, gnorm=0.505, loss_scale=2, train_wall=21, gb_free=29.6, wall=4632
2024-02-10 10:29:28 | INFO | train_inner | epoch 031:    259 / 685 loss=2.724, nll_loss=1.081, ppl=2.11, wps=70025.7, ups=4.77, wpb=14680.1, bsz=744.2, num_updates=20800, lr=0.000537086, gnorm=0.498, loss_scale=2, train_wall=21, gb_free=29.5, wall=4653
2024-02-10 10:29:49 | INFO | train_inner | epoch 031:    359 / 685 loss=2.734, nll_loss=1.093, ppl=2.13, wps=68930.2, ups=4.78, wpb=14432.8, bsz=742.1, num_updates=20900, lr=0.0005358, gnorm=0.498, loss_scale=2, train_wall=21, gb_free=29.5, wall=4674
2024-02-10 10:30:10 | INFO | train_inner | epoch 031:    459 / 685 loss=2.728, nll_loss=1.085, ppl=2.12, wps=68553.2, ups=4.75, wpb=14421.8, bsz=758.1, num_updates=21000, lr=0.000534522, gnorm=0.504, loss_scale=2, train_wall=21, gb_free=29.6, wall=4695
2024-02-10 10:30:32 | INFO | train_inner | epoch 031:    559 / 685 loss=2.744, nll_loss=1.104, ppl=2.15, wps=68687.7, ups=4.75, wpb=14459, bsz=746.1, num_updates=21100, lr=0.000533254, gnorm=0.51, loss_scale=2, train_wall=21, gb_free=29.5, wall=4716
2024-02-10 10:30:53 | INFO | train_inner | epoch 031:    659 / 685 loss=2.735, nll_loss=1.095, ppl=2.14, wps=69192.5, ups=4.76, wpb=14536.4, bsz=755.5, num_updates=21200, lr=0.000531995, gnorm=0.492, loss_scale=2, train_wall=21, gb_free=29.6, wall=4737
2024-02-10 10:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:30:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:31:00 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.418 | nll_loss 1.812 | ppl 3.51 | wps 132697 | wpb 3189.1 | bsz 163.4 | num_updates 21226 | best_loss 3.418
2024-02-10 10:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 21226 updates
2024-02-10 10:31:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt
2024-02-10 10:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt
2024-02-10 10:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint31.pt (epoch 31 @ 21226 updates, score 3.418) (writing took 3.9797219689935446 seconds)
2024-02-10 10:31:04 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-02-10 10:31:04 | INFO | train | epoch 031 | loss 2.726 | nll_loss 1.083 | ppl 2.12 | wps 66271.7 | ups 4.56 | wpb 14524.4 | bsz 748.5 | num_updates 21226 | lr 0.000531669 | gnorm 0.501 | loss_scale 2 | train_wall 142 | gb_free 29.7 | wall 4749
2024-02-10 10:31:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:31:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:31:04 | INFO | fairseq.trainer | begin training epoch 32
2024-02-10 10:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:31:20 | INFO | train_inner | epoch 032:     74 / 685 loss=2.701, nll_loss=1.054, ppl=2.08, wps=52793.2, ups=3.65, wpb=14475.8, bsz=762.6, num_updates=21300, lr=0.000530745, gnorm=0.504, loss_scale=2, train_wall=21, gb_free=29.6, wall=4765
2024-02-10 10:31:41 | INFO | train_inner | epoch 032:    174 / 685 loss=2.699, nll_loss=1.051, ppl=2.07, wps=68798.1, ups=4.73, wpb=14543.5, bsz=727.8, num_updates=21400, lr=0.000529503, gnorm=0.494, loss_scale=2, train_wall=21, gb_free=29.6, wall=4786
2024-02-10 10:32:02 | INFO | train_inner | epoch 032:    274 / 685 loss=2.703, nll_loss=1.057, ppl=2.08, wps=68904.1, ups=4.72, wpb=14603.6, bsz=763.1, num_updates=21500, lr=0.000528271, gnorm=0.508, loss_scale=2, train_wall=21, gb_free=29.6, wall=4807
2024-02-10 10:32:23 | INFO | train_inner | epoch 032:    374 / 685 loss=2.709, nll_loss=1.065, ppl=2.09, wps=68726.2, ups=4.73, wpb=14528.9, bsz=756.2, num_updates=21600, lr=0.000527046, gnorm=0.499, loss_scale=2, train_wall=21, gb_free=29.5, wall=4828
2024-02-10 10:32:45 | INFO | train_inner | epoch 032:    474 / 685 loss=2.714, nll_loss=1.07, ppl=2.1, wps=68922, ups=4.74, wpb=14526.3, bsz=731.3, num_updates=21700, lr=0.00052583, gnorm=0.501, loss_scale=2, train_wall=21, gb_free=29.5, wall=4849
2024-02-10 10:33:06 | INFO | train_inner | epoch 032:    574 / 685 loss=2.729, nll_loss=1.088, ppl=2.13, wps=68454.3, ups=4.76, wpb=14375.9, bsz=725.8, num_updates=21800, lr=0.000524623, gnorm=0.52, loss_scale=2, train_wall=21, gb_free=29.5, wall=4870
2024-02-10 10:33:27 | INFO | train_inner | epoch 032:    674 / 685 loss=2.732, nll_loss=1.093, ppl=2.13, wps=68471.2, ups=4.69, wpb=14587.1, bsz=781.4, num_updates=21900, lr=0.000523424, gnorm=0.51, loss_scale=2, train_wall=21, gb_free=29.5, wall=4891
2024-02-10 10:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:33:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:33:31 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.415 | nll_loss 1.804 | ppl 3.49 | wps 129445 | wpb 3189.1 | bsz 163.4 | num_updates 21911 | best_loss 3.415
2024-02-10 10:33:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 21911 updates
2024-02-10 10:33:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt
2024-02-10 10:33:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt
2024-02-10 10:33:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint32.pt (epoch 32 @ 21911 updates, score 3.415) (writing took 3.837491267127916 seconds)
2024-02-10 10:33:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-02-10 10:33:35 | INFO | train | epoch 032 | loss 2.712 | nll_loss 1.067 | ppl 2.1 | wps 65821.8 | ups 4.53 | wpb 14524.4 | bsz 748.5 | num_updates 21911 | lr 0.000523293 | gnorm 0.505 | loss_scale 2 | train_wall 143 | gb_free 29.8 | wall 4900
2024-02-10 10:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:33:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:33:35 | INFO | fairseq.trainer | begin training epoch 33
2024-02-10 10:33:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:33:54 | INFO | train_inner | epoch 033:     89 / 685 loss=2.686, nll_loss=1.038, ppl=2.05, wps=52659, ups=3.65, wpb=14422.8, bsz=734.6, num_updates=22000, lr=0.000522233, gnorm=0.499, loss_scale=2, train_wall=21, gb_free=29.5, wall=4919
2024-02-10 10:34:15 | INFO | train_inner | epoch 033:    189 / 685 loss=2.681, nll_loss=1.031, ppl=2.04, wps=68013.1, ups=4.75, wpb=14321.8, bsz=725, num_updates=22100, lr=0.00052105, gnorm=0.493, loss_scale=2, train_wall=21, gb_free=29.5, wall=4940
2024-02-10 10:34:36 | INFO | train_inner | epoch 033:    289 / 685 loss=2.694, nll_loss=1.049, ppl=2.07, wps=69783.3, ups=4.72, wpb=14781.4, bsz=758.9, num_updates=22200, lr=0.000519875, gnorm=0.493, loss_scale=2, train_wall=21, gb_free=29.5, wall=4961
2024-02-10 10:34:58 | INFO | train_inner | epoch 033:    389 / 685 loss=2.701, nll_loss=1.056, ppl=2.08, wps=68968.7, ups=4.73, wpb=14587.9, bsz=752, num_updates=22300, lr=0.000518708, gnorm=0.509, loss_scale=2, train_wall=21, gb_free=29.5, wall=4982
2024-02-10 10:35:19 | INFO | train_inner | epoch 033:    489 / 685 loss=2.697, nll_loss=1.052, ppl=2.07, wps=67823.3, ups=4.68, wpb=14479.5, bsz=777, num_updates=22400, lr=0.000517549, gnorm=0.51, loss_scale=2, train_wall=21, gb_free=29.6, wall=5004
2024-02-10 10:35:40 | INFO | train_inner | epoch 033:    589 / 685 loss=2.71, nll_loss=1.066, ppl=2.09, wps=67660.4, ups=4.74, wpb=14283.5, bsz=725.7, num_updates=22500, lr=0.000516398, gnorm=0.504, loss_scale=2, train_wall=21, gb_free=29.5, wall=5025
2024-02-10 10:36:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:36:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:36:03 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.425 | nll_loss 1.816 | ppl 3.52 | wps 129459 | wpb 3189.1 | bsz 163.4 | num_updates 22596 | best_loss 3.415
2024-02-10 10:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 22596 updates
2024-02-10 10:36:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt
2024-02-10 10:36:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt
2024-02-10 10:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint33.pt (epoch 33 @ 22596 updates, score 3.425) (writing took 2.3761445619165897 seconds)
2024-02-10 10:36:05 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-02-10 10:36:05 | INFO | train | epoch 033 | loss 2.697 | nll_loss 1.052 | ppl 2.07 | wps 66452.2 | ups 4.58 | wpb 14524.4 | bsz 748.5 | num_updates 22596 | lr 0.0005153 | gnorm 0.503 | loss_scale 2 | train_wall 143 | gb_free 29.7 | wall 5050
2024-02-10 10:36:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:36:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:36:05 | INFO | fairseq.trainer | begin training epoch 34
2024-02-10 10:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:36:06 | INFO | train_inner | epoch 034:      4 / 685 loss=2.713, nll_loss=1.071, ppl=2.1, wps=56389.5, ups=3.85, wpb=14638.4, bsz=757.2, num_updates=22600, lr=0.000515254, gnorm=0.52, loss_scale=2, train_wall=21, gb_free=29.5, wall=5051
2024-02-10 10:36:27 | INFO | train_inner | epoch 034:    104 / 685 loss=2.654, nll_loss=1.001, ppl=2, wps=68266.2, ups=4.73, wpb=14431.4, bsz=745.6, num_updates=22700, lr=0.000514118, gnorm=0.487, loss_scale=2, train_wall=21, gb_free=29.5, wall=5072
2024-02-10 10:36:48 | INFO | train_inner | epoch 034:    204 / 685 loss=2.671, nll_loss=1.023, ppl=2.03, wps=69108.6, ups=4.74, wpb=14578.1, bsz=739.8, num_updates=22800, lr=0.000512989, gnorm=0.483, loss_scale=2, train_wall=21, gb_free=29.6, wall=5093
2024-02-10 10:37:09 | INFO | train_inner | epoch 034:    304 / 685 loss=2.686, nll_loss=1.039, ppl=2.06, wps=68909.5, ups=4.72, wpb=14593.5, bsz=751.3, num_updates=22900, lr=0.000511868, gnorm=0.498, loss_scale=2, train_wall=21, gb_free=29.5, wall=5114
2024-02-10 10:37:31 | INFO | train_inner | epoch 034:    404 / 685 loss=2.695, nll_loss=1.051, ppl=2.07, wps=69480.5, ups=4.74, wpb=14646.4, bsz=734.5, num_updates=23000, lr=0.000510754, gnorm=0.489, loss_scale=2, train_wall=21, gb_free=29.5, wall=5135
2024-02-10 10:37:52 | INFO | train_inner | epoch 034:    504 / 685 loss=2.688, nll_loss=1.042, ppl=2.06, wps=68762.6, ups=4.72, wpb=14573.1, bsz=760, num_updates=23100, lr=0.000509647, gnorm=0.491, loss_scale=2, train_wall=21, gb_free=29.6, wall=5156
2024-02-10 10:38:13 | INFO | train_inner | epoch 034:    604 / 685 loss=2.696, nll_loss=1.053, ppl=2.07, wps=67477.1, ups=4.63, wpb=14571.7, bsz=761.7, num_updates=23200, lr=0.000508548, gnorm=0.491, loss_scale=2, train_wall=21, gb_free=29.5, wall=5178
2024-02-10 10:38:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:38:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:38:33 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.429 | nll_loss 1.816 | ppl 3.52 | wps 129709 | wpb 3189.1 | bsz 163.4 | num_updates 23281 | best_loss 3.415
2024-02-10 10:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 23281 updates
2024-02-10 10:38:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt
2024-02-10 10:38:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt
2024-02-10 10:38:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint34.pt (epoch 34 @ 23281 updates, score 3.429) (writing took 2.47505912091583 seconds)
2024-02-10 10:38:35 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-02-10 10:38:35 | INFO | train | epoch 034 | loss 2.683 | nll_loss 1.037 | ppl 2.05 | wps 66276.4 | ups 4.56 | wpb 14524.4 | bsz 748.5 | num_updates 23281 | lr 0.000507662 | gnorm 0.526 | loss_scale 2 | train_wall 143 | gb_free 29.7 | wall 5200
2024-02-10 10:38:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:38:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:38:35 | INFO | fairseq.trainer | begin training epoch 35
2024-02-10 10:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:38:39 | INFO | train_inner | epoch 035:     19 / 685 loss=2.687, nll_loss=1.041, ppl=2.06, wps=54889.9, ups=3.85, wpb=14267, bsz=744.9, num_updates=23300, lr=0.000507455, gnorm=0.738, loss_scale=2, train_wall=21, gb_free=29.5, wall=5204
2024-02-10 10:39:00 | INFO | train_inner | epoch 035:    119 / 685 loss=2.656, nll_loss=1.005, ppl=2.01, wps=69225.3, ups=4.74, wpb=14595.5, bsz=742.6, num_updates=23400, lr=0.00050637, gnorm=0.475, loss_scale=2, train_wall=21, gb_free=29.5, wall=5225
2024-02-10 10:39:22 | INFO | train_inner | epoch 035:    219 / 685 loss=2.662, nll_loss=1.012, ppl=2.02, wps=68619.8, ups=4.73, wpb=14502.5, bsz=738.6, num_updates=23500, lr=0.000505291, gnorm=0.512, loss_scale=2, train_wall=21, gb_free=29.6, wall=5246
2024-02-10 10:39:43 | INFO | train_inner | epoch 035:    319 / 685 loss=2.675, nll_loss=1.028, ppl=2.04, wps=69216.2, ups=4.74, wpb=14614.8, bsz=750.6, num_updates=23600, lr=0.000504219, gnorm=0.485, loss_scale=2, train_wall=21, gb_free=29.5, wall=5267
2024-02-10 10:40:04 | INFO | train_inner | epoch 035:    419 / 685 loss=2.669, nll_loss=1.021, ppl=2.03, wps=68092.4, ups=4.73, wpb=14402.1, bsz=753.7, num_updates=23700, lr=0.000503155, gnorm=0.496, loss_scale=2, train_wall=21, gb_free=29.5, wall=5288
2024-02-10 10:40:25 | INFO | train_inner | epoch 035:    519 / 685 loss=2.685, nll_loss=1.041, ppl=2.06, wps=68775.2, ups=4.71, wpb=14609.3, bsz=745.6, num_updates=23800, lr=0.000502096, gnorm=0.492, loss_scale=2, train_wall=21, gb_free=29.5, wall=5310
2024-02-10 10:40:46 | INFO | train_inner | epoch 035:    619 / 685 loss=2.683, nll_loss=1.038, ppl=2.05, wps=68802, ups=4.73, wpb=14541.5, bsz=751.8, num_updates=23900, lr=0.000501045, gnorm=0.492, loss_scale=2, train_wall=21, gb_free=29.5, wall=5331
2024-02-10 10:41:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:41:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:41:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.403 | nll_loss 1.789 | ppl 3.46 | wps 129425 | wpb 3189.1 | bsz 163.4 | num_updates 23966 | best_loss 3.403
2024-02-10 10:41:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 23966 updates
2024-02-10 10:41:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt
2024-02-10 10:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt
2024-02-10 10:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint35.pt (epoch 35 @ 23966 updates, score 3.403) (writing took 4.0003509358502924 seconds)
2024-02-10 10:41:06 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-02-10 10:41:06 | INFO | train | epoch 035 | loss 2.672 | nll_loss 1.024 | ppl 2.03 | wps 65768.1 | ups 4.53 | wpb 14524.4 | bsz 748.5 | num_updates 23966 | lr 0.000500355 | gnorm 0.494 | loss_scale 2 | train_wall 143 | gb_free 29.6 | wall 5351
2024-02-10 10:41:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:41:06 | INFO | fairseq.trainer | begin training epoch 36
2024-02-10 10:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:41:14 | INFO | train_inner | epoch 036:     34 / 685 loss=2.663, nll_loss=1.015, ppl=2.02, wps=52036.9, ups=3.62, wpb=14371.8, bsz=761.2, num_updates=24000, lr=0.0005, gnorm=0.492, loss_scale=2, train_wall=21, gb_free=29.6, wall=5358
2024-02-10 10:41:35 | INFO | train_inner | epoch 036:    134 / 685 loss=2.632, nll_loss=0.979, ppl=1.97, wps=68494.3, ups=4.71, wpb=14556.4, bsz=764.7, num_updates=24100, lr=0.000498962, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.5, wall=5380
2024-02-10 10:42:02 | INFO | train_inner | epoch 036:    234 / 685 loss=2.655, nll_loss=1.006, ppl=2.01, wps=53770.4, ups=3.72, wpb=14460.9, bsz=732.7, num_updates=24200, lr=0.00049793, gnorm=0.498, loss_scale=4, train_wall=21, gb_free=29.5, wall=5407
2024-02-10 10:42:23 | INFO | train_inner | epoch 036:    334 / 685 loss=2.654, nll_loss=1.005, ppl=2.01, wps=68963.9, ups=4.74, wpb=14553.9, bsz=749.1, num_updates=24300, lr=0.000496904, gnorm=0.491, loss_scale=4, train_wall=21, gb_free=29.5, wall=5428
2024-02-10 10:42:44 | INFO | train_inner | epoch 036:    434 / 685 loss=2.672, nll_loss=1.025, ppl=2.04, wps=69782.5, ups=4.77, wpb=14628.1, bsz=737.4, num_updates=24400, lr=0.000495885, gnorm=0.57, loss_scale=4, train_wall=21, gb_free=29.5, wall=5449
2024-02-10 10:43:05 | INFO | train_inner | epoch 036:    534 / 685 loss=2.666, nll_loss=1.019, ppl=2.03, wps=69085.3, ups=4.76, wpb=14521, bsz=766.6, num_updates=24500, lr=0.000494872, gnorm=0.549, loss_scale=4, train_wall=21, gb_free=29.5, wall=5470
2024-02-10 10:43:26 | INFO | train_inner | epoch 036:    634 / 685 loss=2.675, nll_loss=1.03, ppl=2.04, wps=69477.8, ups=4.77, wpb=14555.2, bsz=743.9, num_updates=24600, lr=0.000493865, gnorm=0.484, loss_scale=4, train_wall=21, gb_free=29.6, wall=5491
2024-02-10 10:43:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:43:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:43:39 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.402 | nll_loss 1.79 | ppl 3.46 | wps 132160 | wpb 3189.1 | bsz 163.4 | num_updates 24651 | best_loss 3.402
2024-02-10 10:43:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 24651 updates
2024-02-10 10:43:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt
2024-02-10 10:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt
2024-02-10 10:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint36.pt (epoch 36 @ 24651 updates, score 3.402) (writing took 3.8366290838457644 seconds)
2024-02-10 10:43:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-02-10 10:43:43 | INFO | train | epoch 036 | loss 2.659 | nll_loss 1.011 | ppl 2.02 | wps 63686 | ups 4.38 | wpb 14524.4 | bsz 748.5 | num_updates 24651 | lr 0.000493354 | gnorm 0.508 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 5507
2024-02-10 10:43:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:43:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:43:43 | INFO | fairseq.trainer | begin training epoch 37
2024-02-10 10:43:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:43:53 | INFO | train_inner | epoch 037:     49 / 685 loss=2.651, nll_loss=1.001, ppl=2, wps=53458.1, ups=3.69, wpb=14474.1, bsz=733, num_updates=24700, lr=0.000492864, gnorm=0.491, loss_scale=4, train_wall=21, gb_free=29.6, wall=5518
2024-02-10 10:44:14 | INFO | train_inner | epoch 037:    149 / 685 loss=2.63, nll_loss=0.977, ppl=1.97, wps=68872.6, ups=4.77, wpb=14441.8, bsz=751.7, num_updates=24800, lr=0.000491869, gnorm=0.481, loss_scale=4, train_wall=21, gb_free=29.5, wall=5539
2024-02-10 10:44:35 | INFO | train_inner | epoch 037:    249 / 685 loss=2.634, nll_loss=0.983, ppl=1.98, wps=68786.4, ups=4.76, wpb=14464.1, bsz=752.6, num_updates=24900, lr=0.000490881, gnorm=0.481, loss_scale=4, train_wall=21, gb_free=29.5, wall=5560
2024-02-10 10:44:56 | INFO | train_inner | epoch 037:    349 / 685 loss=2.648, nll_loss=0.999, ppl=2, wps=68894.1, ups=4.77, wpb=14456.7, bsz=737.4, num_updates=25000, lr=0.000489898, gnorm=0.481, loss_scale=4, train_wall=21, gb_free=29.5, wall=5581
2024-02-10 10:45:17 | INFO | train_inner | epoch 037:    449 / 685 loss=2.66, nll_loss=1.014, ppl=2.02, wps=69473.7, ups=4.75, wpb=14640.5, bsz=757.8, num_updates=25100, lr=0.000488921, gnorm=0.483, loss_scale=4, train_wall=21, gb_free=29.5, wall=5602
2024-02-10 10:45:38 | INFO | train_inner | epoch 037:    549 / 685 loss=2.656, nll_loss=1.009, ppl=2.01, wps=69539.8, ups=4.76, wpb=14604.3, bsz=750.6, num_updates=25200, lr=0.00048795, gnorm=0.494, loss_scale=4, train_wall=21, gb_free=29.5, wall=5623
2024-02-10 10:45:59 | INFO | train_inner | epoch 037:    649 / 685 loss=2.667, nll_loss=1.022, ppl=2.03, wps=69551.4, ups=4.76, wpb=14626.3, bsz=752.7, num_updates=25300, lr=0.000486985, gnorm=0.498, loss_scale=4, train_wall=21, gb_free=29.5, wall=5644
2024-02-10 10:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:46:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:46:09 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.429 | nll_loss 1.821 | ppl 3.53 | wps 131670 | wpb 3189.1 | bsz 163.4 | num_updates 25336 | best_loss 3.402
2024-02-10 10:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 25336 updates
2024-02-10 10:46:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt
2024-02-10 10:46:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt
2024-02-10 10:46:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint37.pt (epoch 37 @ 25336 updates, score 3.429) (writing took 2.50162651296705 seconds)
2024-02-10 10:46:12 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-02-10 10:46:12 | INFO | train | epoch 037 | loss 2.648 | nll_loss 0.999 | ppl 2 | wps 66773.4 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 25336 | lr 0.000486639 | gnorm 0.486 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 5656
2024-02-10 10:46:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:46:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:46:12 | INFO | fairseq.trainer | begin training epoch 38
2024-02-10 10:46:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:46:25 | INFO | train_inner | epoch 038:     64 / 685 loss=2.625, nll_loss=0.973, ppl=1.96, wps=55438.2, ups=3.83, wpb=14484.9, bsz=749.4, num_updates=25400, lr=0.000486025, gnorm=0.486, loss_scale=4, train_wall=21, gb_free=29.5, wall=5670
2024-02-10 10:46:46 | INFO | train_inner | epoch 038:    164 / 685 loss=2.618, nll_loss=0.965, ppl=1.95, wps=69722.9, ups=4.74, wpb=14700.1, bsz=755.7, num_updates=25500, lr=0.000485071, gnorm=0.483, loss_scale=4, train_wall=21, gb_free=29.5, wall=5691
2024-02-10 10:47:07 | INFO | train_inner | epoch 038:    264 / 685 loss=2.634, nll_loss=0.984, ppl=1.98, wps=69352.4, ups=4.77, wpb=14543.5, bsz=739, num_updates=25600, lr=0.000484123, gnorm=0.536, loss_scale=4, train_wall=21, gb_free=29.6, wall=5712
2024-02-10 10:47:28 | INFO | train_inner | epoch 038:    364 / 685 loss=2.645, nll_loss=0.996, ppl=1.99, wps=68592.4, ups=4.77, wpb=14381.4, bsz=740, num_updates=25700, lr=0.00048318, gnorm=0.484, loss_scale=4, train_wall=21, gb_free=29.5, wall=5733
2024-02-10 10:47:49 | INFO | train_inner | epoch 038:    464 / 685 loss=2.644, nll_loss=0.996, ppl=1.99, wps=69365.2, ups=4.77, wpb=14548.4, bsz=743.1, num_updates=25800, lr=0.000482243, gnorm=0.485, loss_scale=4, train_wall=21, gb_free=29.5, wall=5754
2024-02-10 10:48:10 | INFO | train_inner | epoch 038:    564 / 685 loss=2.646, nll_loss=0.999, ppl=2, wps=68576.4, ups=4.76, wpb=14406.8, bsz=747.3, num_updates=25900, lr=0.000481311, gnorm=0.483, loss_scale=4, train_wall=21, gb_free=29.5, wall=5775
2024-02-10 10:48:31 | INFO | train_inner | epoch 038:    664 / 685 loss=2.651, nll_loss=1.005, ppl=2.01, wps=69140.1, ups=4.74, wpb=14599, bsz=774.2, num_updates=26000, lr=0.000480384, gnorm=0.492, loss_scale=4, train_wall=21, gb_free=29.5, wall=5796
2024-02-10 10:48:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:48:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:48:38 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.419 | nll_loss 1.804 | ppl 3.49 | wps 132208 | wpb 3189.1 | bsz 163.4 | num_updates 26021 | best_loss 3.402
2024-02-10 10:48:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 26021 updates
2024-02-10 10:48:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt
2024-02-10 10:48:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt
2024-02-10 10:48:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint38.pt (epoch 38 @ 26021 updates, score 3.419) (writing took 2.459203017875552 seconds)
2024-02-10 10:48:40 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-02-10 10:48:40 | INFO | train | epoch 038 | loss 2.637 | nll_loss 0.987 | ppl 1.98 | wps 66887.7 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 26021 | lr 0.000480191 | gnorm 0.493 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 5805
2024-02-10 10:48:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:48:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:48:40 | INFO | fairseq.trainer | begin training epoch 39
2024-02-10 10:48:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:48:57 | INFO | train_inner | epoch 039:     79 / 685 loss=2.614, nll_loss=0.961, ppl=1.95, wps=55630.6, ups=3.86, wpb=14397.2, bsz=748.2, num_updates=26100, lr=0.000479463, gnorm=0.504, loss_scale=4, train_wall=21, gb_free=29.5, wall=5822
2024-02-10 10:49:18 | INFO | train_inner | epoch 039:    179 / 685 loss=2.618, nll_loss=0.966, ppl=1.95, wps=69334.3, ups=4.76, wpb=14566.3, bsz=748.6, num_updates=26200, lr=0.000478547, gnorm=0.494, loss_scale=4, train_wall=21, gb_free=29.5, wall=5843
2024-02-10 10:49:39 | INFO | train_inner | epoch 039:    279 / 685 loss=2.618, nll_loss=0.966, ppl=1.95, wps=68681, ups=4.76, wpb=14432.1, bsz=749.9, num_updates=26300, lr=0.000477637, gnorm=0.486, loss_scale=4, train_wall=21, gb_free=29.6, wall=5864
2024-02-10 10:50:00 | INFO | train_inner | epoch 039:    379 / 685 loss=2.633, nll_loss=0.984, ppl=1.98, wps=69356, ups=4.76, wpb=14579, bsz=745.9, num_updates=26400, lr=0.000476731, gnorm=0.497, loss_scale=4, train_wall=21, gb_free=29.6, wall=5885
2024-02-10 10:50:21 | INFO | train_inner | epoch 039:    479 / 685 loss=2.63, nll_loss=0.98, ppl=1.97, wps=68891.6, ups=4.75, wpb=14516.8, bsz=745.5, num_updates=26500, lr=0.000475831, gnorm=0.498, loss_scale=4, train_wall=21, gb_free=29.5, wall=5906
2024-02-10 10:50:43 | INFO | train_inner | epoch 039:    579 / 685 loss=2.639, nll_loss=0.991, ppl=1.99, wps=69549, ups=4.72, wpb=14732.7, bsz=761.7, num_updates=26600, lr=0.000474936, gnorm=0.484, loss_scale=4, train_wall=21, gb_free=29.5, wall=5927
2024-02-10 10:51:04 | INFO | train_inner | epoch 039:    679 / 685 loss=2.642, nll_loss=0.994, ppl=1.99, wps=69008.8, ups=4.76, wpb=14487.6, bsz=742.5, num_updates=26700, lr=0.000474045, gnorm=0.487, loss_scale=4, train_wall=21, gb_free=29.5, wall=5948
2024-02-10 10:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:51:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:51:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.405 | nll_loss 1.794 | ppl 3.47 | wps 132167 | wpb 3189.1 | bsz 163.4 | num_updates 26706 | best_loss 3.402
2024-02-10 10:51:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 26706 updates
2024-02-10 10:51:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt
2024-02-10 10:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt
2024-02-10 10:51:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint39.pt (epoch 39 @ 26706 updates, score 3.405) (writing took 2.4851223560981452 seconds)
2024-02-10 10:51:09 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-02-10 10:51:09 | INFO | train | epoch 039 | loss 2.627 | nll_loss 0.977 | ppl 1.97 | wps 66757.1 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 26706 | lr 0.000473992 | gnorm 0.494 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 5954
2024-02-10 10:51:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:51:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:51:09 | INFO | fairseq.trainer | begin training epoch 40
2024-02-10 10:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:51:29 | INFO | train_inner | epoch 040:     94 / 685 loss=2.592, nll_loss=0.937, ppl=1.91, wps=55568.8, ups=3.88, wpb=14328.2, bsz=739.8, num_updates=26800, lr=0.00047316, gnorm=0.482, loss_scale=4, train_wall=21, gb_free=29.6, wall=5974
2024-02-10 10:51:50 | INFO | train_inner | epoch 040:    194 / 685 loss=2.601, nll_loss=0.947, ppl=1.93, wps=68890.2, ups=4.75, wpb=14505.8, bsz=756, num_updates=26900, lr=0.00047228, gnorm=0.471, loss_scale=4, train_wall=21, gb_free=29.5, wall=5995
2024-02-10 10:52:11 | INFO | train_inner | epoch 040:    294 / 685 loss=2.613, nll_loss=0.961, ppl=1.95, wps=69849.9, ups=4.75, wpb=14691.5, bsz=749.6, num_updates=27000, lr=0.000471405, gnorm=0.469, loss_scale=4, train_wall=21, gb_free=29.5, wall=6016
2024-02-10 10:52:32 | INFO | train_inner | epoch 040:    394 / 685 loss=2.624, nll_loss=0.974, ppl=1.96, wps=69310.9, ups=4.75, wpb=14579.4, bsz=752.2, num_updates=27100, lr=0.000470534, gnorm=0.488, loss_scale=4, train_wall=21, gb_free=29.5, wall=6037
2024-02-10 10:52:53 | INFO | train_inner | epoch 040:    494 / 685 loss=2.632, nll_loss=0.984, ppl=1.98, wps=69935.5, ups=4.76, wpb=14679.8, bsz=736.7, num_updates=27200, lr=0.000469668, gnorm=0.6, loss_scale=4, train_wall=21, gb_free=29.5, wall=6058
2024-02-10 10:53:14 | INFO | train_inner | epoch 040:    594 / 685 loss=2.633, nll_loss=0.984, ppl=1.98, wps=68537.4, ups=4.77, wpb=14364, bsz=743.4, num_updates=27300, lr=0.000468807, gnorm=0.503, loss_scale=4, train_wall=21, gb_free=29.5, wall=6079
2024-02-10 10:53:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:53:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:53:36 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.405 | nll_loss 1.79 | ppl 3.46 | wps 131609 | wpb 3189.1 | bsz 163.4 | num_updates 27391 | best_loss 3.402
2024-02-10 10:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 27391 updates
2024-02-10 10:53:36 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt
2024-02-10 10:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt
2024-02-10 10:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint40.pt (epoch 40 @ 27391 updates, score 3.405) (writing took 2.4480466060340405 seconds)
2024-02-10 10:53:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-02-10 10:53:38 | INFO | train | epoch 040 | loss 2.617 | nll_loss 0.967 | ppl 1.95 | wps 66900 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 27391 | lr 0.000468028 | gnorm 0.507 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 6103
2024-02-10 10:53:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:53:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:53:38 | INFO | fairseq.trainer | begin training epoch 41
2024-02-10 10:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:53:40 | INFO | train_inner | epoch 041:      9 / 685 loss=2.627, nll_loss=0.978, ppl=1.97, wps=55682.1, ups=3.88, wpb=14339.7, bsz=744.9, num_updates=27400, lr=0.000467951, gnorm=0.569, loss_scale=4, train_wall=21, gb_free=29.5, wall=6105
2024-02-10 10:54:01 | INFO | train_inner | epoch 041:    109 / 685 loss=2.582, nll_loss=0.926, ppl=1.9, wps=69285.3, ups=4.74, wpb=14608.2, bsz=768.1, num_updates=27500, lr=0.000467099, gnorm=0.486, loss_scale=4, train_wall=21, gb_free=29.6, wall=6126
2024-02-10 10:54:22 | INFO | train_inner | epoch 041:    209 / 685 loss=2.599, nll_loss=0.945, ppl=1.93, wps=69841.8, ups=4.77, wpb=14652.8, bsz=736.9, num_updates=27600, lr=0.000466252, gnorm=0.573, loss_scale=4, train_wall=21, gb_free=29.5, wall=6147
2024-02-10 10:54:43 | INFO | train_inner | epoch 041:    309 / 685 loss=2.608, nll_loss=0.957, ppl=1.94, wps=69699.5, ups=4.77, wpb=14615.1, bsz=739.6, num_updates=27700, lr=0.00046541, gnorm=0.494, loss_scale=4, train_wall=21, gb_free=29.6, wall=6168
2024-02-10 10:55:04 | INFO | train_inner | epoch 041:    409 / 685 loss=2.617, nll_loss=0.967, ppl=1.95, wps=68927.8, ups=4.76, wpb=14476.7, bsz=741.1, num_updates=27800, lr=0.000464572, gnorm=0.521, loss_scale=4, train_wall=21, gb_free=29.5, wall=6189
2024-02-10 10:55:25 | INFO | train_inner | epoch 041:    509 / 685 loss=2.612, nll_loss=0.961, ppl=1.95, wps=68324.7, ups=4.74, wpb=14409, bsz=770, num_updates=27900, lr=0.000463739, gnorm=0.485, loss_scale=4, train_wall=21, gb_free=29.6, wall=6210
2024-02-10 10:55:46 | INFO | train_inner | epoch 041:    609 / 685 loss=2.617, nll_loss=0.967, ppl=1.95, wps=68880.6, ups=4.75, wpb=14511.5, bsz=765.6, num_updates=28000, lr=0.00046291, gnorm=0.481, loss_scale=4, train_wall=21, gb_free=29.5, wall=6231
2024-02-10 10:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:56:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:56:04 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.414 | nll_loss 1.811 | ppl 3.51 | wps 132359 | wpb 3189.1 | bsz 163.4 | num_updates 28076 | best_loss 3.402
2024-02-10 10:56:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 28076 updates
2024-02-10 10:56:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt
2024-02-10 10:56:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt
2024-02-10 10:56:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint41.pt (epoch 41 @ 28076 updates, score 3.414) (writing took 2.5237725488841534 seconds)
2024-02-10 10:56:07 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-02-10 10:56:07 | INFO | train | epoch 041 | loss 2.608 | nll_loss 0.957 | ppl 1.94 | wps 66842 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 28076 | lr 0.000462283 | gnorm 0.509 | loss_scale 4 | train_wall 142 | gb_free 29.8 | wall 6252
2024-02-10 10:56:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:56:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:56:07 | INFO | fairseq.trainer | begin training epoch 42
2024-02-10 10:56:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:56:12 | INFO | train_inner | epoch 042:     24 / 685 loss=2.618, nll_loss=0.969, ppl=1.96, wps=56187.5, ups=3.87, wpb=14516.3, bsz=716.9, num_updates=28100, lr=0.000462086, gnorm=0.486, loss_scale=4, train_wall=21, gb_free=29.5, wall=6257
2024-02-10 10:56:33 | INFO | train_inner | epoch 042:    124 / 685 loss=2.577, nll_loss=0.921, ppl=1.89, wps=69621.8, ups=4.77, wpb=14591.6, bsz=747.6, num_updates=28200, lr=0.000461266, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.5, wall=6278
2024-02-10 10:56:54 | INFO | train_inner | epoch 042:    224 / 685 loss=2.593, nll_loss=0.94, ppl=1.92, wps=70075.3, ups=4.75, wpb=14766.2, bsz=754.7, num_updates=28300, lr=0.00046045, gnorm=0.47, loss_scale=8, train_wall=21, gb_free=29.5, wall=6299
2024-02-10 10:57:15 | INFO | train_inner | epoch 042:    324 / 685 loss=2.593, nll_loss=0.938, ppl=1.92, wps=67416.2, ups=4.77, wpb=14122.4, bsz=741.6, num_updates=28400, lr=0.000459639, gnorm=0.489, loss_scale=8, train_wall=21, gb_free=29.5, wall=6320
2024-02-10 10:57:36 | INFO | train_inner | epoch 042:    424 / 685 loss=2.611, nll_loss=0.961, ppl=1.95, wps=69856.4, ups=4.78, wpb=14627.7, bsz=745.1, num_updates=28500, lr=0.000458831, gnorm=0.479, loss_scale=8, train_wall=21, gb_free=29.5, wall=6341
2024-02-10 10:57:57 | INFO | train_inner | epoch 042:    524 / 685 loss=2.608, nll_loss=0.957, ppl=1.94, wps=68841.4, ups=4.76, wpb=14462.4, bsz=748.6, num_updates=28600, lr=0.000458029, gnorm=0.481, loss_scale=8, train_wall=21, gb_free=29.5, wall=6362
2024-02-10 10:58:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-10 10:58:18 | INFO | train_inner | epoch 042:    625 / 685 loss=2.609, nll_loss=0.959, ppl=1.94, wps=68293.4, ups=4.72, wpb=14477.9, bsz=743.5, num_updates=28700, lr=0.00045723, gnorm=0.483, loss_scale=4, train_wall=21, gb_free=29.5, wall=6383
2024-02-10 10:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 10:58:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:58:33 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.397 | nll_loss 1.786 | ppl 3.45 | wps 131226 | wpb 3189.1 | bsz 163.4 | num_updates 28760 | best_loss 3.397
2024-02-10 10:58:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 28760 updates
2024-02-10 10:58:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt
2024-02-10 10:58:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt
2024-02-10 10:58:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint42.pt (epoch 42 @ 28760 updates, score 3.397) (writing took 3.7859473121352494 seconds)
2024-02-10 10:58:37 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-02-10 10:58:37 | INFO | train | epoch 042 | loss 2.599 | nll_loss 0.947 | ppl 1.93 | wps 66235.7 | ups 4.56 | wpb 14522.1 | bsz 748.7 | num_updates 28760 | lr 0.000456753 | gnorm 0.477 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 6402
2024-02-10 10:58:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 10:58:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 10:58:37 | INFO | fairseq.trainer | begin training epoch 43
2024-02-10 10:58:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 10:58:45 | INFO | train_inner | epoch 043:     40 / 685 loss=2.595, nll_loss=0.943, ppl=1.92, wps=53483.4, ups=3.68, wpb=14528.5, bsz=766.8, num_updates=28800, lr=0.000456435, gnorm=0.483, loss_scale=4, train_wall=21, gb_free=29.5, wall=6410
2024-02-10 10:59:06 | INFO | train_inner | epoch 043:    140 / 685 loss=2.563, nll_loss=0.906, ppl=1.87, wps=68884.6, ups=4.76, wpb=14457.2, bsz=751.3, num_updates=28900, lr=0.000455645, gnorm=0.57, loss_scale=4, train_wall=21, gb_free=29.5, wall=6431
2024-02-10 10:59:27 | INFO | train_inner | epoch 043:    240 / 685 loss=2.587, nll_loss=0.934, ppl=1.91, wps=69793.5, ups=4.76, wpb=14657.8, bsz=744.5, num_updates=29000, lr=0.000454859, gnorm=0.479, loss_scale=4, train_wall=21, gb_free=29.6, wall=6452
2024-02-10 10:59:48 | INFO | train_inner | epoch 043:    340 / 685 loss=2.593, nll_loss=0.941, ppl=1.92, wps=69021.8, ups=4.77, wpb=14482.8, bsz=737.9, num_updates=29100, lr=0.000454077, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.5, wall=6473
2024-02-10 11:00:09 | INFO | train_inner | epoch 043:    440 / 685 loss=2.598, nll_loss=0.947, ppl=1.93, wps=69423.5, ups=4.76, wpb=14577.6, bsz=745.8, num_updates=29200, lr=0.000453298, gnorm=0.472, loss_scale=4, train_wall=21, gb_free=29.5, wall=6494
2024-02-10 11:00:31 | INFO | train_inner | epoch 043:    540 / 685 loss=2.592, nll_loss=0.94, ppl=1.92, wps=68635.3, ups=4.75, wpb=14459.1, bsz=740.7, num_updates=29300, lr=0.000452524, gnorm=0.473, loss_scale=4, train_wall=21, gb_free=29.5, wall=6515
2024-02-10 11:00:52 | INFO | train_inner | epoch 043:    640 / 685 loss=2.605, nll_loss=0.955, ppl=1.94, wps=68933.5, ups=4.76, wpb=14491.3, bsz=763, num_updates=29400, lr=0.000451754, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.5, wall=6536
2024-02-10 11:01:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:01:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:01:03 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.404 | nll_loss 1.796 | ppl 3.47 | wps 131876 | wpb 3189.1 | bsz 163.4 | num_updates 29445 | best_loss 3.397
2024-02-10 11:01:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 29445 updates
2024-02-10 11:01:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt
2024-02-10 11:01:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt
2024-02-10 11:01:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint43.pt (epoch 43 @ 29445 updates, score 3.404) (writing took 2.460278243990615 seconds)
2024-02-10 11:01:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-02-10 11:01:06 | INFO | train | epoch 043 | loss 2.59 | nll_loss 0.937 | ppl 1.91 | wps 66890.9 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 29445 | lr 0.000451409 | gnorm 0.489 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 6550
2024-02-10 11:01:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:01:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:01:06 | INFO | fairseq.trainer | begin training epoch 44
2024-02-10 11:01:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:01:17 | INFO | train_inner | epoch 044:     55 / 685 loss=2.581, nll_loss=0.928, ppl=1.9, wps=55986.1, ups=3.86, wpb=14500.5, bsz=768.6, num_updates=29500, lr=0.000450988, gnorm=0.478, loss_scale=4, train_wall=21, gb_free=29.5, wall=6562
2024-02-10 11:01:38 | INFO | train_inner | epoch 044:    155 / 685 loss=2.571, nll_loss=0.916, ppl=1.89, wps=69456.1, ups=4.76, wpb=14579.1, bsz=730.9, num_updates=29600, lr=0.000450225, gnorm=0.477, loss_scale=4, train_wall=21, gb_free=29.6, wall=6583
2024-02-10 11:01:59 | INFO | train_inner | epoch 044:    255 / 685 loss=2.571, nll_loss=0.915, ppl=1.89, wps=68510.3, ups=4.76, wpb=14393.9, bsz=758.1, num_updates=29700, lr=0.000449467, gnorm=0.472, loss_scale=4, train_wall=21, gb_free=29.5, wall=6604
2024-02-10 11:02:20 | INFO | train_inner | epoch 044:    355 / 685 loss=2.585, nll_loss=0.933, ppl=1.91, wps=69374.7, ups=4.77, wpb=14533.5, bsz=723, num_updates=29800, lr=0.000448712, gnorm=0.484, loss_scale=4, train_wall=21, gb_free=29.5, wall=6625
2024-02-10 11:02:41 | INFO | train_inner | epoch 044:    455 / 685 loss=2.587, nll_loss=0.935, ppl=1.91, wps=69481, ups=4.75, wpb=14621.9, bsz=759.3, num_updates=29900, lr=0.000447961, gnorm=0.481, loss_scale=4, train_wall=21, gb_free=29.5, wall=6646
2024-02-10 11:03:02 | INFO | train_inner | epoch 044:    555 / 685 loss=2.589, nll_loss=0.938, ppl=1.92, wps=69487.3, ups=4.74, wpb=14646.4, bsz=766.3, num_updates=30000, lr=0.000447214, gnorm=0.469, loss_scale=4, train_wall=21, gb_free=29.5, wall=6667
2024-02-10 11:03:24 | INFO | train_inner | epoch 044:    655 / 685 loss=2.599, nll_loss=0.949, ppl=1.93, wps=69675.7, ups=4.76, wpb=14650.8, bsz=749.8, num_updates=30100, lr=0.00044647, gnorm=0.471, loss_scale=4, train_wall=21, gb_free=29.5, wall=6688
2024-02-10 11:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:03:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:03:32 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.417 | nll_loss 1.804 | ppl 3.49 | wps 131117 | wpb 3189.1 | bsz 163.4 | num_updates 30130 | best_loss 3.397
2024-02-10 11:03:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 30130 updates
2024-02-10 11:03:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt
2024-02-10 11:03:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt
2024-02-10 11:03:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint44.pt (epoch 44 @ 30130 updates, score 3.417) (writing took 2.4201741779688746 seconds)
2024-02-10 11:03:34 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-02-10 11:03:34 | INFO | train | epoch 044 | loss 2.582 | nll_loss 0.929 | ppl 1.9 | wps 66884.1 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 30130 | lr 0.000446248 | gnorm 0.476 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 6699
2024-02-10 11:03:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:03:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:03:34 | INFO | fairseq.trainer | begin training epoch 45
2024-02-10 11:03:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:03:49 | INFO | train_inner | epoch 045:     70 / 685 loss=2.56, nll_loss=0.903, ppl=1.87, wps=55669.7, ups=3.87, wpb=14369, bsz=765.5, num_updates=30200, lr=0.00044573, gnorm=0.483, loss_scale=4, train_wall=21, gb_free=29.5, wall=6714
2024-02-10 11:04:12 | INFO | train_inner | epoch 045:    170 / 685 loss=2.556, nll_loss=0.899, ppl=1.86, wps=63653.7, ups=4.4, wpb=14466.7, bsz=739.4, num_updates=30300, lr=0.000444994, gnorm=0.475, loss_scale=4, train_wall=22, gb_free=29.5, wall=6737
2024-02-10 11:04:33 | INFO | train_inner | epoch 045:    270 / 685 loss=2.565, nll_loss=0.91, ppl=1.88, wps=68519.2, ups=4.75, wpb=14417.4, bsz=754.6, num_updates=30400, lr=0.000444262, gnorm=0.601, loss_scale=4, train_wall=21, gb_free=29.5, wall=6758
2024-02-10 11:04:54 | INFO | train_inner | epoch 045:    370 / 685 loss=2.573, nll_loss=0.919, ppl=1.89, wps=69099.2, ups=4.76, wpb=14503.9, bsz=755, num_updates=30500, lr=0.000443533, gnorm=0.516, loss_scale=4, train_wall=21, gb_free=29.6, wall=6779
2024-02-10 11:05:15 | INFO | train_inner | epoch 045:    470 / 685 loss=2.587, nll_loss=0.936, ppl=1.91, wps=69309.8, ups=4.75, wpb=14600, bsz=742.1, num_updates=30600, lr=0.000442807, gnorm=0.468, loss_scale=4, train_wall=21, gb_free=29.5, wall=6800
2024-02-10 11:05:36 | INFO | train_inner | epoch 045:    570 / 685 loss=2.589, nll_loss=0.938, ppl=1.92, wps=69117.7, ups=4.73, wpb=14601.4, bsz=737.5, num_updates=30700, lr=0.000442086, gnorm=0.471, loss_scale=4, train_wall=21, gb_free=29.5, wall=6821
2024-02-10 11:05:57 | INFO | train_inner | epoch 045:    670 / 685 loss=2.591, nll_loss=0.941, ppl=1.92, wps=69383.7, ups=4.77, wpb=14560.4, bsz=742.6, num_updates=30800, lr=0.000441367, gnorm=0.479, loss_scale=4, train_wall=21, gb_free=29.6, wall=6842
2024-02-10 11:06:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:06:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:06:03 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.397 | nll_loss 1.788 | ppl 3.45 | wps 131054 | wpb 3189.1 | bsz 163.4 | num_updates 30815 | best_loss 3.397
2024-02-10 11:06:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 30815 updates
2024-02-10 11:06:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt
2024-02-10 11:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt
2024-02-10 11:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint45.pt (epoch 45 @ 30815 updates, score 3.397) (writing took 3.830211647087708 seconds)
2024-02-10 11:06:06 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-02-10 11:06:06 | INFO | train | epoch 045 | loss 2.574 | nll_loss 0.92 | ppl 1.89 | wps 65452.4 | ups 4.51 | wpb 14524.4 | bsz 748.5 | num_updates 30815 | lr 0.00044126 | gnorm 0.5 | loss_scale 4 | train_wall 144 | gb_free 29.7 | wall 6851
2024-02-10 11:06:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:06:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:06:06 | INFO | fairseq.trainer | begin training epoch 46
2024-02-10 11:06:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:06:24 | INFO | train_inner | epoch 046:     85 / 685 loss=2.547, nll_loss=0.888, ppl=1.85, wps=53233.1, ups=3.67, wpb=14486.1, bsz=752.2, num_updates=30900, lr=0.000440653, gnorm=0.478, loss_scale=4, train_wall=21, gb_free=29.5, wall=6869
2024-02-10 11:06:46 | INFO | train_inner | epoch 046:    185 / 685 loss=2.554, nll_loss=0.898, ppl=1.86, wps=69159.6, ups=4.75, wpb=14558.5, bsz=757.7, num_updates=31000, lr=0.000439941, gnorm=0.498, loss_scale=4, train_wall=21, gb_free=29.5, wall=6890
2024-02-10 11:07:07 | INFO | train_inner | epoch 046:    285 / 685 loss=2.561, nll_loss=0.906, ppl=1.87, wps=68995.4, ups=4.76, wpb=14488, bsz=750.9, num_updates=31100, lr=0.000439233, gnorm=0.467, loss_scale=4, train_wall=21, gb_free=29.5, wall=6911
2024-02-10 11:07:28 | INFO | train_inner | epoch 046:    385 / 685 loss=2.569, nll_loss=0.915, ppl=1.89, wps=69816.9, ups=4.77, wpb=14648.8, bsz=740.8, num_updates=31200, lr=0.000438529, gnorm=0.468, loss_scale=4, train_wall=21, gb_free=29.5, wall=6932
2024-02-10 11:07:49 | INFO | train_inner | epoch 046:    485 / 685 loss=2.574, nll_loss=0.922, ppl=1.89, wps=69745, ups=4.77, wpb=14634, bsz=738.8, num_updates=31300, lr=0.000437828, gnorm=0.469, loss_scale=4, train_wall=21, gb_free=29.5, wall=6953
2024-02-10 11:08:10 | INFO | train_inner | epoch 046:    585 / 685 loss=2.583, nll_loss=0.932, ppl=1.91, wps=68798, ups=4.76, wpb=14451.7, bsz=742.4, num_updates=31400, lr=0.00043713, gnorm=0.489, loss_scale=4, train_wall=21, gb_free=29.5, wall=6974
2024-02-10 11:08:30 | INFO | train_inner | epoch 046:    685 / 685 loss=2.581, nll_loss=0.929, ppl=1.9, wps=68902, ups=4.81, wpb=14329.3, bsz=753.6, num_updates=31500, lr=0.000436436, gnorm=0.479, loss_scale=4, train_wall=21, gb_free=29.9, wall=6995
2024-02-10 11:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:08:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:08:33 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.401 | nll_loss 1.796 | ppl 3.47 | wps 130558 | wpb 3189.1 | bsz 163.4 | num_updates 31500 | best_loss 3.397
2024-02-10 11:08:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 31500 updates
2024-02-10 11:08:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt
2024-02-10 11:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt
2024-02-10 11:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint46.pt (epoch 46 @ 31500 updates, score 3.401) (writing took 2.393392567988485 seconds)
2024-02-10 11:08:35 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-02-10 11:08:35 | INFO | train | epoch 046 | loss 2.566 | nll_loss 0.912 | ppl 1.88 | wps 66929 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 31500 | lr 0.000436436 | gnorm 0.477 | loss_scale 4 | train_wall 142 | gb_free 29.9 | wall 7000
2024-02-10 11:08:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:08:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:08:35 | INFO | fairseq.trainer | begin training epoch 47
2024-02-10 11:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:08:56 | INFO | train_inner | epoch 047:    100 / 685 loss=2.536, nll_loss=0.877, ppl=1.84, wps=55910.6, ups=3.85, wpb=14510.6, bsz=740.2, num_updates=31600, lr=0.000435745, gnorm=0.465, loss_scale=4, train_wall=21, gb_free=29.5, wall=7021
2024-02-10 11:09:17 | INFO | train_inner | epoch 047:    200 / 685 loss=2.55, nll_loss=0.893, ppl=1.86, wps=69128.5, ups=4.75, wpb=14538.6, bsz=742.7, num_updates=31700, lr=0.000435057, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.5, wall=7042
2024-02-10 11:09:38 | INFO | train_inner | epoch 047:    300 / 685 loss=2.547, nll_loss=0.89, ppl=1.85, wps=68215.2, ups=4.76, wpb=14331.8, bsz=756.4, num_updates=31800, lr=0.000434372, gnorm=0.473, loss_scale=4, train_wall=21, gb_free=29.5, wall=7063
2024-02-10 11:09:59 | INFO | train_inner | epoch 047:    400 / 685 loss=2.559, nll_loss=0.905, ppl=1.87, wps=69914.1, ups=4.76, wpb=14691.4, bsz=753.1, num_updates=31900, lr=0.000433691, gnorm=0.485, loss_scale=4, train_wall=21, gb_free=29.5, wall=7084
2024-02-10 11:10:20 | INFO | train_inner | epoch 047:    500 / 685 loss=2.567, nll_loss=0.914, ppl=1.88, wps=69003.1, ups=4.78, wpb=14434.8, bsz=733.5, num_updates=32000, lr=0.000433013, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.6, wall=7105
2024-02-10 11:10:41 | INFO | train_inner | epoch 047:    600 / 685 loss=2.573, nll_loss=0.921, ppl=1.89, wps=69155.7, ups=4.74, wpb=14593.3, bsz=761.8, num_updates=32100, lr=0.000432338, gnorm=0.476, loss_scale=4, train_wall=21, gb_free=29.5, wall=7126
2024-02-10 11:10:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:10:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:11:02 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.41 | nll_loss 1.799 | ppl 3.48 | wps 126159 | wpb 3189.1 | bsz 163.4 | num_updates 32185 | best_loss 3.397
2024-02-10 11:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 32185 updates
2024-02-10 11:11:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt
2024-02-10 11:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt
2024-02-10 11:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint47.pt (epoch 47 @ 32185 updates, score 3.41) (writing took 2.38752179290168 seconds)
2024-02-10 11:11:04 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-02-10 11:11:04 | INFO | train | epoch 047 | loss 2.559 | nll_loss 0.904 | ppl 1.87 | wps 66859.7 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 32185 | lr 0.000431766 | gnorm 0.474 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 7149
2024-02-10 11:11:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:11:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:11:04 | INFO | fairseq.trainer | begin training epoch 48
2024-02-10 11:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:11:07 | INFO | train_inner | epoch 048:     15 / 685 loss=2.571, nll_loss=0.92, ppl=1.89, wps=56430.3, ups=3.87, wpb=14599.2, bsz=751.9, num_updates=32200, lr=0.000431666, gnorm=0.473, loss_scale=4, train_wall=21, gb_free=29.5, wall=7152
2024-02-10 11:11:28 | INFO | train_inner | epoch 048:    115 / 685 loss=2.531, nll_loss=0.871, ppl=1.83, wps=69433.9, ups=4.78, wpb=14527.1, bsz=728.8, num_updates=32300, lr=0.000430997, gnorm=0.467, loss_scale=4, train_wall=21, gb_free=29.5, wall=7173
2024-02-10 11:11:49 | INFO | train_inner | epoch 048:    215 / 685 loss=2.539, nll_loss=0.882, ppl=1.84, wps=68859.7, ups=4.75, wpb=14495.7, bsz=756.9, num_updates=32400, lr=0.000430331, gnorm=0.475, loss_scale=4, train_wall=21, gb_free=29.5, wall=7194
2024-02-10 11:12:10 | INFO | train_inner | epoch 048:    315 / 685 loss=2.545, nll_loss=0.89, ppl=1.85, wps=69184, ups=4.75, wpb=14566.1, bsz=760.6, num_updates=32500, lr=0.000429669, gnorm=0.462, loss_scale=4, train_wall=21, gb_free=29.6, wall=7215
2024-02-10 11:12:31 | INFO | train_inner | epoch 048:    415 / 685 loss=2.561, nll_loss=0.908, ppl=1.88, wps=69123.2, ups=4.77, wpb=14494.3, bsz=754.8, num_updates=32600, lr=0.000429009, gnorm=0.475, loss_scale=4, train_wall=21, gb_free=29.5, wall=7236
2024-02-10 11:12:52 | INFO | train_inner | epoch 048:    515 / 685 loss=2.565, nll_loss=0.912, ppl=1.88, wps=69175.4, ups=4.79, wpb=14454.4, bsz=738.6, num_updates=32700, lr=0.000428353, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.6, wall=7257
2024-02-10 11:13:13 | INFO | train_inner | epoch 048:    615 / 685 loss=2.563, nll_loss=0.911, ppl=1.88, wps=69485.2, ups=4.75, wpb=14614.2, bsz=755.7, num_updates=32800, lr=0.000427699, gnorm=0.465, loss_scale=8, train_wall=21, gb_free=29.5, wall=7278
2024-02-10 11:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:13:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:13:30 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.401 | nll_loss 1.787 | ppl 3.45 | wps 131255 | wpb 3189.1 | bsz 163.4 | num_updates 32870 | best_loss 3.397
2024-02-10 11:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 32870 updates
2024-02-10 11:13:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt
2024-02-10 11:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt
2024-02-10 11:13:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint48.pt (epoch 48 @ 32870 updates, score 3.401) (writing took 2.426273331977427 seconds)
2024-02-10 11:13:32 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-02-10 11:13:32 | INFO | train | epoch 048 | loss 2.552 | nll_loss 0.896 | ppl 1.86 | wps 67003.3 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 32870 | lr 0.000427244 | gnorm 0.472 | loss_scale 8 | train_wall 142 | gb_free 29.7 | wall 7297
2024-02-10 11:13:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:13:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:13:32 | INFO | fairseq.trainer | begin training epoch 49
2024-02-10 11:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:13:39 | INFO | train_inner | epoch 049:     30 / 685 loss=2.547, nll_loss=0.891, ppl=1.85, wps=56770.4, ups=3.89, wpb=14579.2, bsz=739.8, num_updates=32900, lr=0.000427049, gnorm=0.48, loss_scale=8, train_wall=21, gb_free=29.6, wall=7303
2024-02-10 11:14:00 | INFO | train_inner | epoch 049:    130 / 685 loss=2.521, nll_loss=0.862, ppl=1.82, wps=68431.6, ups=4.75, wpb=14413.8, bsz=760.8, num_updates=33000, lr=0.000426401, gnorm=0.451, loss_scale=8, train_wall=21, gb_free=29.7, wall=7325
2024-02-10 11:14:21 | INFO | train_inner | epoch 049:    230 / 685 loss=2.534, nll_loss=0.877, ppl=1.84, wps=69273.2, ups=4.77, wpb=14515.8, bsz=746.5, num_updates=33100, lr=0.000425757, gnorm=0.463, loss_scale=8, train_wall=21, gb_free=29.5, wall=7345
2024-02-10 11:14:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-10 11:14:42 | INFO | train_inner | epoch 049:    331 / 685 loss=2.546, nll_loss=0.89, ppl=1.85, wps=67871.1, ups=4.66, wpb=14565.8, bsz=751.9, num_updates=33200, lr=0.000425115, gnorm=0.466, loss_scale=4, train_wall=21, gb_free=29.5, wall=7367
2024-02-10 11:15:03 | INFO | train_inner | epoch 049:    431 / 685 loss=2.551, nll_loss=0.897, ppl=1.86, wps=69245.3, ups=4.73, wpb=14627.5, bsz=757.7, num_updates=33300, lr=0.000424476, gnorm=0.465, loss_scale=4, train_wall=21, gb_free=29.5, wall=7388
2024-02-10 11:15:24 | INFO | train_inner | epoch 049:    531 / 685 loss=2.561, nll_loss=0.91, ppl=1.88, wps=70103.1, ups=4.76, wpb=14730.6, bsz=753.8, num_updates=33400, lr=0.00042384, gnorm=0.473, loss_scale=4, train_wall=21, gb_free=29.5, wall=7409
2024-02-10 11:15:45 | INFO | train_inner | epoch 049:    631 / 685 loss=2.557, nll_loss=0.904, ppl=1.87, wps=68076.2, ups=4.76, wpb=14301.2, bsz=731.9, num_updates=33500, lr=0.000423207, gnorm=0.491, loss_scale=4, train_wall=21, gb_free=29.6, wall=7430
2024-02-10 11:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:15:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:15:59 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.42 | nll_loss 1.822 | ppl 3.54 | wps 132534 | wpb 3189.1 | bsz 163.4 | num_updates 33554 | best_loss 3.397
2024-02-10 11:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 33554 updates
2024-02-10 11:15:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt
2024-02-10 11:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt
2024-02-10 11:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint49.pt (epoch 49 @ 33554 updates, score 3.42) (writing took 2.416946711950004 seconds)
2024-02-10 11:16:01 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-02-10 11:16:01 | INFO | train | epoch 049 | loss 2.545 | nll_loss 0.889 | ppl 1.85 | wps 66709 | ups 4.59 | wpb 14524.7 | bsz 748.6 | num_updates 33554 | lr 0.000422867 | gnorm 0.468 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 7446
2024-02-10 11:16:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:16:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:16:01 | INFO | fairseq.trainer | begin training epoch 50
2024-02-10 11:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:16:11 | INFO | train_inner | epoch 050:     46 / 685 loss=2.537, nll_loss=0.881, ppl=1.84, wps=56266.9, ups=3.89, wpb=14459.5, bsz=747.4, num_updates=33600, lr=0.000422577, gnorm=0.471, loss_scale=4, train_wall=21, gb_free=29.5, wall=7456
2024-02-10 11:16:32 | INFO | train_inner | epoch 050:    146 / 685 loss=2.523, nll_loss=0.864, ppl=1.82, wps=69432.3, ups=4.79, wpb=14509.8, bsz=729.7, num_updates=33700, lr=0.00042195, gnorm=0.464, loss_scale=4, train_wall=21, gb_free=29.5, wall=7477
2024-02-10 11:16:53 | INFO | train_inner | epoch 050:    246 / 685 loss=2.525, nll_loss=0.867, ppl=1.82, wps=68931.8, ups=4.75, wpb=14504.2, bsz=761.4, num_updates=33800, lr=0.000421325, gnorm=0.457, loss_scale=4, train_wall=21, gb_free=29.6, wall=7498
2024-02-10 11:17:14 | INFO | train_inner | epoch 050:    346 / 685 loss=2.542, nll_loss=0.887, ppl=1.85, wps=69211.9, ups=4.77, wpb=14511.5, bsz=731.2, num_updates=33900, lr=0.000420703, gnorm=0.468, loss_scale=4, train_wall=21, gb_free=29.6, wall=7519
2024-02-10 11:17:40 | INFO | train_inner | epoch 050:    446 / 685 loss=2.54, nll_loss=0.885, ppl=1.85, wps=56848.9, ups=3.89, wpb=14608.9, bsz=753.3, num_updates=34000, lr=0.000420084, gnorm=0.463, loss_scale=4, train_wall=25, gb_free=29.5, wall=7544
2024-02-10 11:18:01 | INFO | train_inner | epoch 050:    546 / 685 loss=2.552, nll_loss=0.9, ppl=1.87, wps=69477.5, ups=4.74, wpb=14643.7, bsz=753, num_updates=34100, lr=0.000419468, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=7565
2024-02-10 11:18:22 | INFO | train_inner | epoch 050:    646 / 685 loss=2.55, nll_loss=0.897, ppl=1.86, wps=68793.4, ups=4.74, wpb=14498.8, bsz=752.6, num_updates=34200, lr=0.000418854, gnorm=0.478, loss_scale=4, train_wall=21, gb_free=29.5, wall=7587
2024-02-10 11:18:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:18:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:18:32 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.418 | nll_loss 1.812 | ppl 3.51 | wps 131302 | wpb 3189.1 | bsz 163.4 | num_updates 34239 | best_loss 3.397
2024-02-10 11:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 34239 updates
2024-02-10 11:18:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt
2024-02-10 11:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt
2024-02-10 11:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint50.pt (epoch 50 @ 34239 updates, score 3.418) (writing took 2.5382010040339082 seconds)
2024-02-10 11:18:35 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-02-10 11:18:35 | INFO | train | epoch 050 | loss 2.537 | nll_loss 0.881 | ppl 1.84 | wps 64803.6 | ups 4.46 | wpb 14524.4 | bsz 748.5 | num_updates 34239 | lr 0.000418615 | gnorm 0.466 | loss_scale 4 | train_wall 147 | gb_free 29.6 | wall 7600
2024-02-10 11:18:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:18:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:18:35 | INFO | fairseq.trainer | begin training epoch 51
2024-02-10 11:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:18:48 | INFO | train_inner | epoch 051:     61 / 685 loss=2.522, nll_loss=0.864, ppl=1.82, wps=55683.1, ups=3.85, wpb=14456.6, bsz=758.6, num_updates=34300, lr=0.000418243, gnorm=0.463, loss_scale=4, train_wall=21, gb_free=29.5, wall=7613
2024-02-10 11:19:09 | INFO | train_inner | epoch 051:    161 / 685 loss=2.504, nll_loss=0.843, ppl=1.79, wps=69033.5, ups=4.74, wpb=14572, bsz=769.2, num_updates=34400, lr=0.000417635, gnorm=0.45, loss_scale=4, train_wall=21, gb_free=29.5, wall=7634
2024-02-10 11:19:30 | INFO | train_inner | epoch 051:    261 / 685 loss=2.526, nll_loss=0.869, ppl=1.83, wps=69341, ups=4.76, wpb=14578.9, bsz=749.3, num_updates=34500, lr=0.000417029, gnorm=0.46, loss_scale=4, train_wall=21, gb_free=29.5, wall=7655
2024-02-10 11:19:51 | INFO | train_inner | epoch 051:    361 / 685 loss=2.53, nll_loss=0.873, ppl=1.83, wps=69360.9, ups=4.78, wpb=14523.6, bsz=753.2, num_updates=34600, lr=0.000416426, gnorm=0.463, loss_scale=4, train_wall=21, gb_free=29.5, wall=7676
2024-02-10 11:20:12 | INFO | train_inner | epoch 051:    461 / 685 loss=2.545, nll_loss=0.891, ppl=1.85, wps=69391.3, ups=4.77, wpb=14548.2, bsz=732.7, num_updates=34700, lr=0.000415825, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.5, wall=7697
2024-02-10 11:20:33 | INFO | train_inner | epoch 051:    561 / 685 loss=2.549, nll_loss=0.895, ppl=1.86, wps=68530, ups=4.74, wpb=14445, bsz=738.7, num_updates=34800, lr=0.000415227, gnorm=0.475, loss_scale=4, train_wall=21, gb_free=29.6, wall=7718
2024-02-10 11:20:54 | INFO | train_inner | epoch 051:    661 / 685 loss=2.547, nll_loss=0.893, ppl=1.86, wps=68404.4, ups=4.74, wpb=14432.9, bsz=755.2, num_updates=34900, lr=0.000414632, gnorm=0.487, loss_scale=4, train_wall=21, gb_free=29.7, wall=7739
2024-02-10 11:20:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:20:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:21:01 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.408 | nll_loss 1.802 | ppl 3.49 | wps 130233 | wpb 3189.1 | bsz 163.4 | num_updates 34924 | best_loss 3.397
2024-02-10 11:21:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 34924 updates
2024-02-10 11:21:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt
2024-02-10 11:21:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt
2024-02-10 11:21:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint51.pt (epoch 51 @ 34924 updates, score 3.408) (writing took 2.408472070004791 seconds)
2024-02-10 11:21:04 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-02-10 11:21:04 | INFO | train | epoch 051 | loss 2.532 | nll_loss 0.875 | ppl 1.83 | wps 66835.3 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 34924 | lr 0.00041449 | gnorm 0.471 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 7748
2024-02-10 11:21:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:21:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:21:04 | INFO | fairseq.trainer | begin training epoch 52
2024-02-10 11:21:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:21:20 | INFO | train_inner | epoch 052:     76 / 685 loss=2.512, nll_loss=0.853, ppl=1.81, wps=55312.1, ups=3.83, wpb=14454.6, bsz=752.6, num_updates=35000, lr=0.000414039, gnorm=0.49, loss_scale=4, train_wall=21, gb_free=29.6, wall=7765
2024-02-10 11:21:41 | INFO | train_inner | epoch 052:    176 / 685 loss=2.517, nll_loss=0.857, ppl=1.81, wps=68584.8, ups=4.78, wpb=14344, bsz=723.8, num_updates=35100, lr=0.000413449, gnorm=0.458, loss_scale=4, train_wall=21, gb_free=29.5, wall=7786
2024-02-10 11:22:02 | INFO | train_inner | epoch 052:    276 / 685 loss=2.523, nll_loss=0.867, ppl=1.82, wps=70164.5, ups=4.75, wpb=14780.2, bsz=741.9, num_updates=35200, lr=0.000412861, gnorm=0.454, loss_scale=4, train_wall=21, gb_free=29.6, wall=7807
2024-02-10 11:22:32 | INFO | train_inner | epoch 052:    376 / 685 loss=2.525, nll_loss=0.868, ppl=1.83, wps=49019.4, ups=3.34, wpb=14684.9, bsz=758.3, num_updates=35300, lr=0.000412276, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.5, wall=7837
2024-02-10 11:22:53 | INFO | train_inner | epoch 052:    476 / 685 loss=2.532, nll_loss=0.877, ppl=1.84, wps=69000.8, ups=4.77, wpb=14469.6, bsz=745.4, num_updates=35400, lr=0.000411693, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.5, wall=7858
2024-02-10 11:23:14 | INFO | train_inner | epoch 052:    576 / 685 loss=2.535, nll_loss=0.881, ppl=1.84, wps=69663, ups=4.76, wpb=14631, bsz=746.9, num_updates=35500, lr=0.000411113, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=7879
2024-02-10 11:23:36 | INFO | train_inner | epoch 052:    676 / 685 loss=2.535, nll_loss=0.88, ppl=1.84, wps=66539, ups=4.64, wpb=14339.1, bsz=753.4, num_updates=35600, lr=0.000410535, gnorm=0.477, loss_scale=4, train_wall=21, gb_free=29.5, wall=7900
2024-02-10 11:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:23:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:23:40 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.401 | nll_loss 1.797 | ppl 3.47 | wps 132515 | wpb 3189.1 | bsz 163.4 | num_updates 35609 | best_loss 3.397
2024-02-10 11:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 35609 updates
2024-02-10 11:23:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt
2024-02-10 11:23:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt
2024-02-10 11:23:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint52.pt (epoch 52 @ 35609 updates, score 3.401) (writing took 2.4972700248472393 seconds)
2024-02-10 11:23:42 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-02-10 11:23:42 | INFO | train | epoch 052 | loss 2.525 | nll_loss 0.868 | ppl 1.83 | wps 62744 | ups 4.32 | wpb 14524.4 | bsz 748.5 | num_updates 35609 | lr 0.000410484 | gnorm 0.465 | loss_scale 4 | train_wall 143 | gb_free 29.7 | wall 7907
2024-02-10 11:23:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:23:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:23:44 | INFO | fairseq.trainer | begin training epoch 53
2024-02-10 11:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:24:08 | INFO | train_inner | epoch 053:     91 / 685 loss=2.495, nll_loss=0.833, ppl=1.78, wps=44096.6, ups=3.05, wpb=14448.1, bsz=781, num_updates=35700, lr=0.00040996, gnorm=0.454, loss_scale=4, train_wall=21, gb_free=29.5, wall=7933
2024-02-10 11:24:29 | INFO | train_inner | epoch 053:    191 / 685 loss=2.509, nll_loss=0.85, ppl=1.8, wps=69730.6, ups=4.77, wpb=14606.9, bsz=737.7, num_updates=35800, lr=0.000409387, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=7954
2024-02-10 11:24:50 | INFO | train_inner | epoch 053:    291 / 685 loss=2.522, nll_loss=0.866, ppl=1.82, wps=69835, ups=4.76, wpb=14666.6, bsz=745.7, num_updates=35900, lr=0.000408816, gnorm=0.457, loss_scale=4, train_wall=21, gb_free=29.5, wall=7975
2024-02-10 11:25:11 | INFO | train_inner | epoch 053:    391 / 685 loss=2.519, nll_loss=0.863, ppl=1.82, wps=69187.1, ups=4.77, wpb=14509.6, bsz=732.6, num_updates=36000, lr=0.000408248, gnorm=0.462, loss_scale=4, train_wall=21, gb_free=29.5, wall=7996
2024-02-10 11:25:32 | INFO | train_inner | epoch 053:    491 / 685 loss=2.517, nll_loss=0.86, ppl=1.82, wps=68986.8, ups=4.77, wpb=14452.7, bsz=736.9, num_updates=36100, lr=0.000407682, gnorm=0.478, loss_scale=4, train_wall=21, gb_free=29.6, wall=8017
2024-02-10 11:25:53 | INFO | train_inner | epoch 053:    591 / 685 loss=2.533, nll_loss=0.878, ppl=1.84, wps=68284.1, ups=4.73, wpb=14439.6, bsz=761.8, num_updates=36200, lr=0.000407119, gnorm=0.466, loss_scale=4, train_wall=21, gb_free=29.5, wall=8038
2024-02-10 11:26:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:26:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:26:15 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.411 | nll_loss 1.806 | ppl 3.5 | wps 133606 | wpb 3189.1 | bsz 163.4 | num_updates 36294 | best_loss 3.397
2024-02-10 11:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 36294 updates
2024-02-10 11:26:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt
2024-02-10 11:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt
2024-02-10 11:26:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint53.pt (epoch 53 @ 36294 updates, score 3.411) (writing took 2.4039056228939444 seconds)
2024-02-10 11:26:18 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-02-10 11:26:18 | INFO | train | epoch 053 | loss 2.519 | nll_loss 0.862 | ppl 1.82 | wps 63988.7 | ups 4.41 | wpb 14524.4 | bsz 748.5 | num_updates 36294 | lr 0.000406591 | gnorm 0.463 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 8062
2024-02-10 11:26:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:26:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:26:18 | INFO | fairseq.trainer | begin training epoch 54
2024-02-10 11:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:26:19 | INFO | train_inner | epoch 054:      6 / 685 loss=2.533, nll_loss=0.878, ppl=1.84, wps=56053.7, ups=3.89, wpb=14421.5, bsz=752.6, num_updates=36300, lr=0.000406558, gnorm=0.467, loss_scale=4, train_wall=21, gb_free=29.5, wall=8064
2024-02-10 11:26:40 | INFO | train_inner | epoch 054:    106 / 685 loss=2.49, nll_loss=0.83, ppl=1.78, wps=69775.2, ups=4.74, wpb=14717, bsz=759.8, num_updates=36400, lr=0.000405999, gnorm=0.46, loss_scale=4, train_wall=21, gb_free=29.5, wall=8085
2024-02-10 11:27:01 | INFO | train_inner | epoch 054:    206 / 685 loss=2.502, nll_loss=0.843, ppl=1.79, wps=68417.3, ups=4.77, wpb=14353.3, bsz=735.6, num_updates=36500, lr=0.000405442, gnorm=0.456, loss_scale=4, train_wall=21, gb_free=29.5, wall=8106
2024-02-10 11:27:22 | INFO | train_inner | epoch 054:    306 / 685 loss=2.509, nll_loss=0.851, ppl=1.8, wps=69650.3, ups=4.77, wpb=14594.9, bsz=737.8, num_updates=36600, lr=0.000404888, gnorm=0.46, loss_scale=4, train_wall=21, gb_free=29.5, wall=8127
2024-02-10 11:27:43 | INFO | train_inner | epoch 054:    406 / 685 loss=2.517, nll_loss=0.86, ppl=1.81, wps=69654.7, ups=4.74, wpb=14700.3, bsz=782.4, num_updates=36700, lr=0.000404336, gnorm=0.465, loss_scale=4, train_wall=21, gb_free=29.5, wall=8148
2024-02-10 11:28:04 | INFO | train_inner | epoch 054:    506 / 685 loss=2.519, nll_loss=0.862, ppl=1.82, wps=68004.4, ups=4.75, wpb=14312.3, bsz=756.6, num_updates=36800, lr=0.000403786, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.5, wall=8169
2024-02-10 11:28:26 | INFO | train_inner | epoch 054:    606 / 685 loss=2.527, nll_loss=0.873, ppl=1.83, wps=68666.8, ups=4.71, wpb=14583.5, bsz=740.1, num_updates=36900, lr=0.000403239, gnorm=0.488, loss_scale=4, train_wall=21, gb_free=29.5, wall=8190
2024-02-10 11:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:28:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:28:44 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.397 | nll_loss 1.789 | ppl 3.46 | wps 130844 | wpb 3189.1 | bsz 163.4 | num_updates 36979 | best_loss 3.397
2024-02-10 11:28:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 36979 updates
2024-02-10 11:28:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt
2024-02-10 11:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt
2024-02-10 11:28:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint54.pt (epoch 54 @ 36979 updates, score 3.397) (writing took 3.9163238699547946 seconds)
2024-02-10 11:28:48 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-02-10 11:28:48 | INFO | train | epoch 054 | loss 2.513 | nll_loss 0.856 | ppl 1.81 | wps 66095.7 | ups 4.55 | wpb 14524.4 | bsz 748.5 | num_updates 36979 | lr 0.000402808 | gnorm 0.468 | loss_scale 4 | train_wall 142 | gb_free 29.8 | wall 8213
2024-02-10 11:28:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:28:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:28:48 | INFO | fairseq.trainer | begin training epoch 55
2024-02-10 11:28:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:28:53 | INFO | train_inner | epoch 055:     21 / 685 loss=2.523, nll_loss=0.867, ppl=1.82, wps=52797.2, ups=3.67, wpb=14380.1, bsz=716.9, num_updates=37000, lr=0.000402694, gnorm=0.479, loss_scale=4, train_wall=21, gb_free=29.5, wall=8217
2024-02-10 11:29:14 | INFO | train_inner | epoch 055:    121 / 685 loss=2.49, nll_loss=0.828, ppl=1.78, wps=69256.3, ups=4.74, wpb=14619.4, bsz=739, num_updates=37100, lr=0.000402151, gnorm=0.455, loss_scale=4, train_wall=21, gb_free=29.5, wall=8239
2024-02-10 11:29:35 | INFO | train_inner | epoch 055:    221 / 685 loss=2.487, nll_loss=0.827, ppl=1.77, wps=68231, ups=4.72, wpb=14452.5, bsz=776.2, num_updates=37200, lr=0.00040161, gnorm=0.506, loss_scale=4, train_wall=21, gb_free=29.5, wall=8260
2024-02-10 11:29:56 | INFO | train_inner | epoch 055:    321 / 685 loss=2.496, nll_loss=0.837, ppl=1.79, wps=69265.5, ups=4.75, wpb=14593.3, bsz=756.8, num_updates=37300, lr=0.000401071, gnorm=0.457, loss_scale=8, train_wall=21, gb_free=29.6, wall=8281
2024-02-10 11:30:17 | INFO | train_inner | epoch 055:    421 / 685 loss=2.518, nll_loss=0.861, ppl=1.82, wps=69117.3, ups=4.77, wpb=14477.2, bsz=741.5, num_updates=37400, lr=0.000400534, gnorm=0.464, loss_scale=8, train_wall=21, gb_free=29.5, wall=8302
2024-02-10 11:30:38 | INFO | train_inner | epoch 055:    521 / 685 loss=2.526, nll_loss=0.872, ppl=1.83, wps=69428.7, ups=4.74, wpb=14656.4, bsz=737.4, num_updates=37500, lr=0.0004, gnorm=0.484, loss_scale=8, train_wall=21, gb_free=29.5, wall=8323
2024-02-10 11:30:59 | INFO | train_inner | epoch 055:    621 / 685 loss=2.522, nll_loss=0.867, ppl=1.82, wps=69162.8, ups=4.76, wpb=14528.5, bsz=758.8, num_updates=37600, lr=0.000399468, gnorm=0.47, loss_scale=8, train_wall=21, gb_free=29.5, wall=8344
2024-02-10 11:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:31:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:31:15 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.404 | nll_loss 1.8 | ppl 3.48 | wps 132255 | wpb 3189.1 | bsz 163.4 | num_updates 37664 | best_loss 3.397
2024-02-10 11:31:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 37664 updates
2024-02-10 11:31:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt
2024-02-10 11:31:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt
2024-02-10 11:31:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint55.pt (epoch 55 @ 37664 updates, score 3.404) (writing took 2.4258956499397755 seconds)
2024-02-10 11:31:17 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-02-10 11:31:17 | INFO | train | epoch 055 | loss 2.507 | nll_loss 0.849 | ppl 1.8 | wps 66704.6 | ups 4.59 | wpb 14524.4 | bsz 748.5 | num_updates 37664 | lr 0.000399128 | gnorm 0.472 | loss_scale 8 | train_wall 143 | gb_free 29.8 | wall 8362
2024-02-10 11:31:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:31:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:31:17 | INFO | fairseq.trainer | begin training epoch 56
2024-02-10 11:31:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:31:25 | INFO | train_inner | epoch 056:     36 / 685 loss=2.5, nll_loss=0.841, ppl=1.79, wps=55512.4, ups=3.87, wpb=14358.8, bsz=732.2, num_updates=37700, lr=0.000398938, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.5, wall=8370
2024-02-10 11:31:46 | INFO | train_inner | epoch 056:    136 / 685 loss=2.48, nll_loss=0.818, ppl=1.76, wps=69221.1, ups=4.74, wpb=14589.4, bsz=763.6, num_updates=37800, lr=0.00039841, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.5, wall=8391
2024-02-10 11:32:07 | INFO | train_inner | epoch 056:    236 / 685 loss=2.495, nll_loss=0.835, ppl=1.78, wps=69166.1, ups=4.79, wpb=14446.3, bsz=724.2, num_updates=37900, lr=0.000397884, gnorm=0.462, loss_scale=8, train_wall=21, gb_free=29.5, wall=8412
2024-02-10 11:32:28 | INFO | train_inner | epoch 056:    336 / 685 loss=2.505, nll_loss=0.848, ppl=1.8, wps=69701.3, ups=4.76, wpb=14656.5, bsz=769.4, num_updates=38000, lr=0.00039736, gnorm=0.458, loss_scale=8, train_wall=21, gb_free=29.5, wall=8433
2024-02-10 11:32:49 | INFO | train_inner | epoch 056:    436 / 685 loss=2.504, nll_loss=0.846, ppl=1.8, wps=69433.7, ups=4.77, wpb=14570.6, bsz=752.9, num_updates=38100, lr=0.000396838, gnorm=0.462, loss_scale=8, train_wall=21, gb_free=29.5, wall=8454
2024-02-10 11:33:10 | INFO | train_inner | epoch 056:    536 / 685 loss=2.51, nll_loss=0.853, ppl=1.81, wps=68927, ups=4.78, wpb=14433.1, bsz=750.4, num_updates=38200, lr=0.000396318, gnorm=0.46, loss_scale=8, train_wall=21, gb_free=29.6, wall=8475
2024-02-10 11:33:31 | INFO | train_inner | epoch 056:    636 / 685 loss=2.514, nll_loss=0.859, ppl=1.81, wps=69282, ups=4.78, wpb=14491.3, bsz=739.9, num_updates=38300, lr=0.0003958, gnorm=0.46, loss_scale=8, train_wall=21, gb_free=29.5, wall=8496
2024-02-10 11:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:33:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:33:43 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.396 | nll_loss 1.792 | ppl 3.46 | wps 131901 | wpb 3189.1 | bsz 163.4 | num_updates 38349 | best_loss 3.396
2024-02-10 11:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 38349 updates
2024-02-10 11:33:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt
2024-02-10 11:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt
2024-02-10 11:33:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint56.pt (epoch 56 @ 38349 updates, score 3.396) (writing took 3.866113713942468 seconds)
2024-02-10 11:33:47 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-02-10 11:33:47 | INFO | train | epoch 056 | loss 2.501 | nll_loss 0.843 | ppl 1.79 | wps 66407.9 | ups 4.57 | wpb 14524.4 | bsz 748.5 | num_updates 38349 | lr 0.000395547 | gnorm 0.459 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 8512
2024-02-10 11:33:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:33:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:33:47 | INFO | fairseq.trainer | begin training epoch 57
2024-02-10 11:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:33:58 | INFO | train_inner | epoch 057:     51 / 685 loss=2.497, nll_loss=0.838, ppl=1.79, wps=53664, ups=3.68, wpb=14571.4, bsz=741.3, num_updates=38400, lr=0.000395285, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.5, wall=8523
2024-02-10 11:34:19 | INFO | train_inner | epoch 057:    151 / 685 loss=2.48, nll_loss=0.819, ppl=1.76, wps=68919.2, ups=4.76, wpb=14481.4, bsz=756, num_updates=38500, lr=0.000394771, gnorm=0.457, loss_scale=8, train_wall=21, gb_free=29.5, wall=8544
2024-02-10 11:34:40 | INFO | train_inner | epoch 057:    251 / 685 loss=2.487, nll_loss=0.826, ppl=1.77, wps=69173.7, ups=4.78, wpb=14474.9, bsz=753.4, num_updates=38600, lr=0.000394259, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.5, wall=8565
2024-02-10 11:35:01 | INFO | train_inner | epoch 057:    351 / 685 loss=2.5, nll_loss=0.842, ppl=1.79, wps=69555.3, ups=4.78, wpb=14546.2, bsz=737.7, num_updates=38700, lr=0.00039375, gnorm=0.457, loss_scale=8, train_wall=21, gb_free=29.6, wall=8586
2024-02-10 11:35:22 | INFO | train_inner | epoch 057:    451 / 685 loss=2.503, nll_loss=0.846, ppl=1.8, wps=69871, ups=4.78, wpb=14613.1, bsz=721.4, num_updates=38800, lr=0.000393242, gnorm=0.461, loss_scale=8, train_wall=21, gb_free=29.5, wall=8607
2024-02-10 11:35:43 | INFO | train_inner | epoch 057:    551 / 685 loss=2.504, nll_loss=0.847, ppl=1.8, wps=68728.2, ups=4.75, wpb=14460.1, bsz=760.7, num_updates=38900, lr=0.000392736, gnorm=0.473, loss_scale=8, train_wall=21, gb_free=29.6, wall=8628
2024-02-10 11:36:04 | INFO | train_inner | epoch 057:    651 / 685 loss=2.505, nll_loss=0.85, ppl=1.8, wps=69622.5, ups=4.75, wpb=14666.9, bsz=778.7, num_updates=39000, lr=0.000392232, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.5, wall=8649
2024-02-10 11:36:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:36:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:36:13 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.41 | nll_loss 1.808 | ppl 3.5 | wps 131615 | wpb 3189.1 | bsz 163.4 | num_updates 39034 | best_loss 3.396
2024-02-10 11:36:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 39034 updates
2024-02-10 11:36:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt
2024-02-10 11:36:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt
2024-02-10 11:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint57.pt (epoch 57 @ 39034 updates, score 3.41) (writing took 2.4075128769036382 seconds)
2024-02-10 11:36:16 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-02-10 11:36:16 | INFO | train | epoch 057 | loss 2.496 | nll_loss 0.838 | ppl 1.79 | wps 67035.1 | ups 4.62 | wpb 14524.4 | bsz 748.5 | num_updates 39034 | lr 0.000392061 | gnorm 0.461 | loss_scale 8 | train_wall 142 | gb_free 29.9 | wall 8660
2024-02-10 11:36:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:36:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:36:16 | INFO | fairseq.trainer | begin training epoch 58
2024-02-10 11:36:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:36:30 | INFO | train_inner | epoch 058:     66 / 685 loss=2.482, nll_loss=0.821, ppl=1.77, wps=56504.1, ups=3.89, wpb=14512.5, bsz=730.9, num_updates=39100, lr=0.00039173, gnorm=0.463, loss_scale=8, train_wall=21, gb_free=29.5, wall=8674
2024-02-10 11:36:51 | INFO | train_inner | epoch 058:    166 / 685 loss=2.48, nll_loss=0.82, ppl=1.76, wps=69064.4, ups=4.74, wpb=14573.5, bsz=762.1, num_updates=39200, lr=0.00039123, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.6, wall=8695
2024-02-10 11:37:12 | INFO | train_inner | epoch 058:    266 / 685 loss=2.488, nll_loss=0.829, ppl=1.78, wps=68805.8, ups=4.76, wpb=14450.3, bsz=749, num_updates=39300, lr=0.000390732, gnorm=0.472, loss_scale=8, train_wall=21, gb_free=29.5, wall=8716
2024-02-10 11:37:33 | INFO | train_inner | epoch 058:    366 / 685 loss=2.494, nll_loss=0.836, ppl=1.79, wps=69628.3, ups=4.77, wpb=14603.8, bsz=747.8, num_updates=39400, lr=0.000390236, gnorm=0.462, loss_scale=8, train_wall=21, gb_free=29.5, wall=8737
2024-02-10 11:37:54 | INFO | train_inner | epoch 058:    466 / 685 loss=2.492, nll_loss=0.833, ppl=1.78, wps=68679.5, ups=4.79, wpb=14344.4, bsz=746.2, num_updates=39500, lr=0.000389742, gnorm=0.467, loss_scale=8, train_wall=21, gb_free=29.6, wall=8758
2024-02-10 11:38:15 | INFO | train_inner | epoch 058:    566 / 685 loss=2.502, nll_loss=0.845, ppl=1.8, wps=68922.7, ups=4.78, wpb=14420.3, bsz=746.2, num_updates=39600, lr=0.000389249, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.5, wall=8779
2024-02-10 11:38:36 | INFO | train_inner | epoch 058:    666 / 685 loss=2.508, nll_loss=0.853, ppl=1.81, wps=70281, ups=4.76, wpb=14768.6, bsz=750.5, num_updates=39700, lr=0.000388759, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.5, wall=8800
2024-02-10 11:38:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:38:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:38:42 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.437 | nll_loss 1.844 | ppl 3.59 | wps 131104 | wpb 3189.1 | bsz 163.4 | num_updates 39719 | best_loss 3.396
2024-02-10 11:38:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 39719 updates
2024-02-10 11:38:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt
2024-02-10 11:38:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt
2024-02-10 11:38:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint58.pt (epoch 58 @ 39719 updates, score 3.437) (writing took 2.5329786699730903 seconds)
2024-02-10 11:38:44 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-02-10 11:38:44 | INFO | train | epoch 058 | loss 2.491 | nll_loss 0.833 | ppl 1.78 | wps 66973.9 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 39719 | lr 0.000388666 | gnorm 0.464 | loss_scale 8 | train_wall 142 | gb_free 29.9 | wall 8809
2024-02-10 11:38:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:38:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:38:44 | INFO | fairseq.trainer | begin training epoch 59
2024-02-10 11:38:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:39:01 | INFO | train_inner | epoch 059:     81 / 685 loss=2.47, nll_loss=0.807, ppl=1.75, wps=55294.2, ups=3.88, wpb=14257.4, bsz=742, num_updates=39800, lr=0.00038827, gnorm=0.485, loss_scale=8, train_wall=21, gb_free=29.6, wall=8826
2024-02-10 11:39:29 | INFO | train_inner | epoch 059:    181 / 685 loss=2.473, nll_loss=0.811, ppl=1.75, wps=52524.7, ups=3.57, wpb=14705.1, bsz=738.3, num_updates=39900, lr=0.000387783, gnorm=0.452, loss_scale=8, train_wall=21, gb_free=29.5, wall=8854
2024-02-10 11:39:50 | INFO | train_inner | epoch 059:    281 / 685 loss=2.479, nll_loss=0.819, ppl=1.76, wps=68756.4, ups=4.78, wpb=14392.6, bsz=757.4, num_updates=40000, lr=0.000387298, gnorm=0.457, loss_scale=8, train_wall=21, gb_free=29.5, wall=8875
2024-02-10 11:40:11 | INFO | train_inner | epoch 059:    381 / 685 loss=2.484, nll_loss=0.825, ppl=1.77, wps=69021.4, ups=4.74, wpb=14547.8, bsz=763.8, num_updates=40100, lr=0.000386815, gnorm=0.453, loss_scale=8, train_wall=21, gb_free=29.5, wall=8896
2024-02-10 11:40:32 | INFO | train_inner | epoch 059:    481 / 685 loss=2.498, nll_loss=0.842, ppl=1.79, wps=69824.7, ups=4.77, wpb=14634, bsz=734.4, num_updates=40200, lr=0.000386334, gnorm=0.463, loss_scale=8, train_wall=21, gb_free=29.5, wall=8917
2024-02-10 11:40:53 | INFO | train_inner | epoch 059:    581 / 685 loss=2.496, nll_loss=0.839, ppl=1.79, wps=69752, ups=4.77, wpb=14622.9, bsz=744.8, num_updates=40300, lr=0.000385854, gnorm=0.464, loss_scale=8, train_wall=21, gb_free=29.5, wall=8938
2024-02-10 11:41:14 | INFO | train_inner | epoch 059:    681 / 685 loss=2.498, nll_loss=0.841, ppl=1.79, wps=68957.6, ups=4.75, wpb=14531.4, bsz=761.4, num_updates=40400, lr=0.000385376, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.6, wall=8959
2024-02-10 11:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:41:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:41:17 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.397 | nll_loss 1.794 | ppl 3.47 | wps 132167 | wpb 3189.1 | bsz 163.4 | num_updates 40404 | best_loss 3.396
2024-02-10 11:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 40404 updates
2024-02-10 11:41:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt
2024-02-10 11:41:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt
2024-02-10 11:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint59.pt (epoch 59 @ 40404 updates, score 3.397) (writing took 2.4475897080264986 seconds)
2024-02-10 11:41:20 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-02-10 11:41:20 | INFO | train | epoch 059 | loss 2.485 | nll_loss 0.826 | ppl 1.77 | wps 63956 | ups 4.4 | wpb 14524.4 | bsz 748.5 | num_updates 40404 | lr 0.000385357 | gnorm 0.46 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 8964
2024-02-10 11:41:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:41:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:41:20 | INFO | fairseq.trainer | begin training epoch 60
2024-02-10 11:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:41:40 | INFO | train_inner | epoch 060:     96 / 685 loss=2.459, nll_loss=0.795, ppl=1.73, wps=55644.6, ups=3.89, wpb=14303.5, bsz=737, num_updates=40500, lr=0.0003849, gnorm=0.459, loss_scale=8, train_wall=21, gb_free=29.5, wall=8985
2024-02-10 11:42:01 | INFO | train_inner | epoch 060:    196 / 685 loss=2.474, nll_loss=0.813, ppl=1.76, wps=69484.6, ups=4.78, wpb=14543.9, bsz=738.6, num_updates=40600, lr=0.000384426, gnorm=0.466, loss_scale=8, train_wall=21, gb_free=29.5, wall=9006
2024-02-10 11:42:22 | INFO | train_inner | epoch 060:    296 / 685 loss=2.469, nll_loss=0.808, ppl=1.75, wps=70208.6, ups=4.74, wpb=14809.8, bsz=783.6, num_updates=40700, lr=0.000383953, gnorm=0.445, loss_scale=8, train_wall=21, gb_free=29.5, wall=9027
2024-02-10 11:42:43 | INFO | train_inner | epoch 060:    396 / 685 loss=2.483, nll_loss=0.824, ppl=1.77, wps=69202.5, ups=4.78, wpb=14491.9, bsz=741.9, num_updates=40800, lr=0.000383482, gnorm=0.461, loss_scale=8, train_wall=21, gb_free=29.5, wall=9048
2024-02-10 11:43:04 | INFO | train_inner | epoch 060:    496 / 685 loss=2.486, nll_loss=0.827, ppl=1.77, wps=68857.8, ups=4.76, wpb=14457, bsz=746.2, num_updates=40900, lr=0.000383013, gnorm=0.468, loss_scale=8, train_wall=21, gb_free=29.5, wall=9069
2024-02-10 11:43:25 | INFO | train_inner | epoch 060:    596 / 685 loss=2.493, nll_loss=0.835, ppl=1.78, wps=69026.2, ups=4.79, wpb=14413.8, bsz=740.2, num_updates=41000, lr=0.000382546, gnorm=0.46, loss_scale=8, train_wall=21, gb_free=29.5, wall=9090
2024-02-10 11:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:43:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:43:46 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.425 | nll_loss 1.826 | ppl 3.55 | wps 131615 | wpb 3189.1 | bsz 163.4 | num_updates 41089 | best_loss 3.396
2024-02-10 11:43:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 41089 updates
2024-02-10 11:43:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt
2024-02-10 11:43:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt
2024-02-10 11:43:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint60.pt (epoch 60 @ 41089 updates, score 3.425) (writing took 2.4533441939856857 seconds)
2024-02-10 11:43:48 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-02-10 11:43:48 | INFO | train | epoch 060 | loss 2.48 | nll_loss 0.821 | ppl 1.77 | wps 67016.4 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 41089 | lr 0.000382132 | gnorm 0.459 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 9113
2024-02-10 11:43:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:43:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:43:48 | INFO | fairseq.trainer | begin training epoch 61
2024-02-10 11:43:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:43:51 | INFO | train_inner | epoch 061:     11 / 685 loss=2.497, nll_loss=0.841, ppl=1.79, wps=56330.7, ups=3.87, wpb=14538, bsz=740.6, num_updates=41100, lr=0.00038208, gnorm=0.462, loss_scale=8, train_wall=21, gb_free=29.5, wall=9115
2024-02-10 11:44:12 | INFO | train_inner | epoch 061:    111 / 685 loss=2.457, nll_loss=0.795, ppl=1.73, wps=69025.2, ups=4.74, wpb=14567.9, bsz=755.8, num_updates=41200, lr=0.000381616, gnorm=0.524, loss_scale=8, train_wall=21, gb_free=29.5, wall=9136
2024-02-10 11:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-10 11:44:33 | INFO | train_inner | epoch 061:    212 / 685 loss=2.472, nll_loss=0.81, ppl=1.75, wps=69315.3, ups=4.74, wpb=14629.9, bsz=730.8, num_updates=41300, lr=0.000381154, gnorm=0.494, loss_scale=4, train_wall=21, gb_free=29.5, wall=9158
2024-02-10 11:44:54 | INFO | train_inner | epoch 061:    312 / 685 loss=2.479, nll_loss=0.82, ppl=1.77, wps=68919.9, ups=4.75, wpb=14495.9, bsz=757.9, num_updates=41400, lr=0.000380693, gnorm=0.456, loss_scale=4, train_wall=21, gb_free=29.6, wall=9179
2024-02-10 11:45:15 | INFO | train_inner | epoch 061:    412 / 685 loss=2.474, nll_loss=0.815, ppl=1.76, wps=69164.9, ups=4.76, wpb=14520.2, bsz=750.4, num_updates=41500, lr=0.000380235, gnorm=0.456, loss_scale=4, train_wall=21, gb_free=29.5, wall=9200
2024-02-10 11:45:36 | INFO | train_inner | epoch 061:    512 / 685 loss=2.482, nll_loss=0.824, ppl=1.77, wps=68763.1, ups=4.75, wpb=14466.7, bsz=761.8, num_updates=41600, lr=0.000379777, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.6, wall=9221
2024-02-10 11:45:57 | INFO | train_inner | epoch 061:    612 / 685 loss=2.48, nll_loss=0.822, ppl=1.77, wps=69359.3, ups=4.76, wpb=14560.4, bsz=756.2, num_updates=41700, lr=0.000379322, gnorm=0.468, loss_scale=4, train_wall=21, gb_free=29.5, wall=9242
2024-02-10 11:46:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:46:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:46:14 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.398 | nll_loss 1.794 | ppl 3.47 | wps 131289 | wpb 3189.1 | bsz 163.4 | num_updates 41773 | best_loss 3.396
2024-02-10 11:46:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 41773 updates
2024-02-10 11:46:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt
2024-02-10 11:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt
2024-02-10 11:46:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint61.pt (epoch 61 @ 41773 updates, score 3.398) (writing took 2.4391790381632745 seconds)
2024-02-10 11:46:17 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-02-10 11:46:17 | INFO | train | epoch 061 | loss 2.476 | nll_loss 0.816 | ppl 1.76 | wps 66849.2 | ups 4.6 | wpb 14524.8 | bsz 748.3 | num_updates 41773 | lr 0.00037899 | gnorm 0.477 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 9262
2024-02-10 11:46:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:46:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:46:17 | INFO | fairseq.trainer | begin training epoch 62
2024-02-10 11:46:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:46:23 | INFO | train_inner | epoch 062:     27 / 685 loss=2.483, nll_loss=0.825, ppl=1.77, wps=56596.4, ups=3.89, wpb=14553.7, bsz=732.2, num_updates=41800, lr=0.000378868, gnorm=0.46, loss_scale=4, train_wall=21, gb_free=29.5, wall=9267
2024-02-10 11:46:44 | INFO | train_inner | epoch 062:    127 / 685 loss=2.45, nll_loss=0.787, ppl=1.72, wps=69481.3, ups=4.77, wpb=14576, bsz=748.6, num_updates=41900, lr=0.000378415, gnorm=0.449, loss_scale=4, train_wall=21, gb_free=29.5, wall=9288
2024-02-10 11:47:05 | INFO | train_inner | epoch 062:    227 / 685 loss=2.46, nll_loss=0.798, ppl=1.74, wps=68005.8, ups=4.75, wpb=14303, bsz=758.8, num_updates=42000, lr=0.000377964, gnorm=0.465, loss_scale=4, train_wall=21, gb_free=29.5, wall=9309
2024-02-10 11:47:28 | INFO | train_inner | epoch 062:    327 / 685 loss=2.474, nll_loss=0.814, ppl=1.76, wps=61588.2, ups=4.23, wpb=14563.2, bsz=752.6, num_updates=42100, lr=0.000377515, gnorm=0.464, loss_scale=4, train_wall=21, gb_free=29.6, wall=9333
2024-02-10 11:47:49 | INFO | train_inner | epoch 062:    427 / 685 loss=2.475, nll_loss=0.816, ppl=1.76, wps=69755.6, ups=4.76, wpb=14656.6, bsz=768.2, num_updates=42200, lr=0.000377068, gnorm=0.463, loss_scale=4, train_wall=21, gb_free=29.5, wall=9354
2024-02-10 11:48:10 | INFO | train_inner | epoch 062:    527 / 685 loss=2.476, nll_loss=0.817, ppl=1.76, wps=69592.7, ups=4.77, wpb=14592.7, bsz=747.4, num_updates=42300, lr=0.000376622, gnorm=0.452, loss_scale=4, train_wall=21, gb_free=29.5, wall=9375
2024-02-10 11:48:31 | INFO | train_inner | epoch 062:    627 / 685 loss=2.489, nll_loss=0.832, ppl=1.78, wps=69231.1, ups=4.78, wpb=14498, bsz=733, num_updates=42400, lr=0.000376177, gnorm=0.466, loss_scale=4, train_wall=21, gb_free=29.5, wall=9396
2024-02-10 11:48:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:48:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:48:46 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.42 | nll_loss 1.816 | ppl 3.52 | wps 131282 | wpb 3189.1 | bsz 163.4 | num_updates 42458 | best_loss 3.396
2024-02-10 11:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 42458 updates
2024-02-10 11:48:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt
2024-02-10 11:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt
2024-02-10 11:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint62.pt (epoch 62 @ 42458 updates, score 3.42) (writing took 2.43077502399683 seconds)
2024-02-10 11:48:48 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-02-10 11:48:48 | INFO | train | epoch 062 | loss 2.471 | nll_loss 0.811 | ppl 1.75 | wps 65815.3 | ups 4.53 | wpb 14524.4 | bsz 748.5 | num_updates 42458 | lr 0.00037592 | gnorm 0.461 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 9413
2024-02-10 11:48:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:48:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:48:48 | INFO | fairseq.trainer | begin training epoch 63
2024-02-10 11:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:48:57 | INFO | train_inner | epoch 063:     42 / 685 loss=2.467, nll_loss=0.806, ppl=1.75, wps=55694.7, ups=3.89, wpb=14323.9, bsz=725.8, num_updates=42500, lr=0.000375735, gnorm=0.475, loss_scale=4, train_wall=21, gb_free=29.6, wall=9422
2024-02-10 11:49:18 | INFO | train_inner | epoch 063:    142 / 685 loss=2.447, nll_loss=0.784, ppl=1.72, wps=69672.7, ups=4.76, wpb=14630, bsz=757.6, num_updates=42600, lr=0.000375293, gnorm=0.448, loss_scale=4, train_wall=21, gb_free=29.5, wall=9443
2024-02-10 11:49:39 | INFO | train_inner | epoch 063:    242 / 685 loss=2.464, nll_loss=0.804, ppl=1.75, wps=69076.2, ups=4.78, wpb=14464, bsz=730.3, num_updates=42700, lr=0.000374854, gnorm=0.454, loss_scale=4, train_wall=21, gb_free=29.5, wall=9464
2024-02-10 11:50:00 | INFO | train_inner | epoch 063:    342 / 685 loss=2.468, nll_loss=0.808, ppl=1.75, wps=69290.7, ups=4.77, wpb=14517.3, bsz=736.3, num_updates=42800, lr=0.000374415, gnorm=0.467, loss_scale=4, train_wall=21, gb_free=29.5, wall=9485
2024-02-10 11:50:21 | INFO | train_inner | epoch 063:    442 / 685 loss=2.469, nll_loss=0.809, ppl=1.75, wps=68410.4, ups=4.77, wpb=14355.1, bsz=763.4, num_updates=42900, lr=0.000373979, gnorm=0.456, loss_scale=4, train_wall=21, gb_free=29.5, wall=9506
2024-02-10 11:50:42 | INFO | train_inner | epoch 063:    542 / 685 loss=2.473, nll_loss=0.815, ppl=1.76, wps=69173.9, ups=4.74, wpb=14586.7, bsz=766.1, num_updates=43000, lr=0.000373544, gnorm=0.457, loss_scale=4, train_wall=21, gb_free=29.5, wall=9527
2024-02-10 11:51:03 | INFO | train_inner | epoch 063:    642 / 685 loss=2.48, nll_loss=0.822, ppl=1.77, wps=70174.9, ups=4.78, wpb=14693.5, bsz=731.2, num_updates=43100, lr=0.00037311, gnorm=0.454, loss_scale=4, train_wall=21, gb_free=29.5, wall=9548
2024-02-10 11:51:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:51:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:51:14 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.414 | nll_loss 1.818 | ppl 3.53 | wps 132263 | wpb 3189.1 | bsz 163.4 | num_updates 43143 | best_loss 3.396
2024-02-10 11:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 43143 updates
2024-02-10 11:51:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt
2024-02-10 11:51:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt
2024-02-10 11:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint63.pt (epoch 63 @ 43143 updates, score 3.414) (writing took 2.404864795971662 seconds)
2024-02-10 11:51:17 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-02-10 11:51:17 | INFO | train | epoch 063 | loss 2.467 | nll_loss 0.807 | ppl 1.75 | wps 66835.5 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 43143 | lr 0.000372924 | gnorm 0.457 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 9562
2024-02-10 11:51:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:51:17 | INFO | fairseq.trainer | begin training epoch 64
2024-02-10 11:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:51:29 | INFO | train_inner | epoch 064:     57 / 685 loss=2.462, nll_loss=0.802, ppl=1.74, wps=54951.8, ups=3.83, wpb=14332.2, bsz=751.2, num_updates=43200, lr=0.000372678, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.6, wall=9574
2024-02-10 11:51:50 | INFO | train_inner | epoch 064:    157 / 685 loss=2.447, nll_loss=0.784, ppl=1.72, wps=69038.6, ups=4.77, wpb=14464.7, bsz=745.6, num_updates=43300, lr=0.000372247, gnorm=0.448, loss_scale=4, train_wall=21, gb_free=29.5, wall=9595
2024-02-10 11:52:11 | INFO | train_inner | epoch 064:    257 / 685 loss=2.454, nll_loss=0.793, ppl=1.73, wps=69184.5, ups=4.74, wpb=14607.3, bsz=786.7, num_updates=43400, lr=0.000371818, gnorm=0.453, loss_scale=4, train_wall=21, gb_free=29.5, wall=9616
2024-02-10 11:52:32 | INFO | train_inner | epoch 064:    357 / 685 loss=2.457, nll_loss=0.796, ppl=1.74, wps=69811.6, ups=4.76, wpb=14661, bsz=741.9, num_updates=43500, lr=0.000371391, gnorm=0.452, loss_scale=4, train_wall=21, gb_free=29.5, wall=9637
2024-02-10 11:52:53 | INFO | train_inner | epoch 064:    457 / 685 loss=2.463, nll_loss=0.803, ppl=1.75, wps=69010.4, ups=4.77, wpb=14475.7, bsz=745.6, num_updates=43600, lr=0.000370965, gnorm=0.454, loss_scale=4, train_wall=21, gb_free=29.5, wall=9658
2024-02-10 11:53:14 | INFO | train_inner | epoch 064:    557 / 685 loss=2.478, nll_loss=0.819, ppl=1.76, wps=69638.1, ups=4.78, wpb=14568, bsz=740.4, num_updates=43700, lr=0.00037054, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.5, wall=9679
2024-02-10 11:53:35 | INFO | train_inner | epoch 064:    657 / 685 loss=2.475, nll_loss=0.818, ppl=1.76, wps=69842.2, ups=4.78, wpb=14623.4, bsz=743, num_updates=43800, lr=0.000370117, gnorm=0.457, loss_scale=4, train_wall=21, gb_free=29.6, wall=9700
2024-02-10 11:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:53:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:53:43 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.416 | nll_loss 1.817 | ppl 3.52 | wps 131074 | wpb 3189.1 | bsz 163.4 | num_updates 43828 | best_loss 3.396
2024-02-10 11:53:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 43828 updates
2024-02-10 11:53:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt
2024-02-10 11:53:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt
2024-02-10 11:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint64.pt (epoch 64 @ 43828 updates, score 3.416) (writing took 2.4444890890736133 seconds)
2024-02-10 11:53:45 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-02-10 11:53:45 | INFO | train | epoch 064 | loss 2.462 | nll_loss 0.802 | ppl 1.74 | wps 66996.2 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 43828 | lr 0.000369998 | gnorm 0.462 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 9710
2024-02-10 11:53:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:53:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:53:45 | INFO | fairseq.trainer | begin training epoch 65
2024-02-10 11:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:54:01 | INFO | train_inner | epoch 065:     72 / 685 loss=2.448, nll_loss=0.785, ppl=1.72, wps=55584.4, ups=3.89, wpb=14298.9, bsz=746.4, num_updates=43900, lr=0.000369695, gnorm=0.509, loss_scale=4, train_wall=21, gb_free=29.5, wall=9725
2024-02-10 11:54:22 | INFO | train_inner | epoch 065:    172 / 685 loss=2.442, nll_loss=0.778, ppl=1.72, wps=69278.9, ups=4.76, wpb=14568.6, bsz=755.4, num_updates=44000, lr=0.000369274, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=9746
2024-02-10 11:54:43 | INFO | train_inner | epoch 065:    272 / 685 loss=2.457, nll_loss=0.797, ppl=1.74, wps=69331.3, ups=4.75, wpb=14591.4, bsz=757.5, num_updates=44100, lr=0.000368856, gnorm=0.446, loss_scale=4, train_wall=21, gb_free=29.6, wall=9767
2024-02-10 11:55:04 | INFO | train_inner | epoch 065:    372 / 685 loss=2.464, nll_loss=0.804, ppl=1.75, wps=69815.8, ups=4.79, wpb=14570.8, bsz=730.2, num_updates=44200, lr=0.000368438, gnorm=0.469, loss_scale=4, train_wall=21, gb_free=29.5, wall=9788
2024-02-10 11:55:25 | INFO | train_inner | epoch 065:    472 / 685 loss=2.461, nll_loss=0.801, ppl=1.74, wps=69446.2, ups=4.76, wpb=14597.5, bsz=768.6, num_updates=44300, lr=0.000368022, gnorm=0.465, loss_scale=4, train_wall=21, gb_free=29.5, wall=9809
2024-02-10 11:55:46 | INFO | train_inner | epoch 065:    572 / 685 loss=2.464, nll_loss=0.805, ppl=1.75, wps=69152.7, ups=4.76, wpb=14532.5, bsz=750.6, num_updates=44400, lr=0.000367607, gnorm=0.463, loss_scale=4, train_wall=21, gb_free=29.5, wall=9830
2024-02-10 11:56:06 | INFO | train_inner | epoch 065:    672 / 685 loss=2.476, nll_loss=0.818, ppl=1.76, wps=69312.8, ups=4.78, wpb=14489, bsz=731.5, num_updates=44500, lr=0.000367194, gnorm=0.467, loss_scale=4, train_wall=21, gb_free=29.5, wall=9851
2024-02-10 11:56:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:56:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:56:11 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.405 | nll_loss 1.807 | ppl 3.5 | wps 131757 | wpb 3189.1 | bsz 163.4 | num_updates 44513 | best_loss 3.396
2024-02-10 11:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 44513 updates
2024-02-10 11:56:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt
2024-02-10 11:56:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt
2024-02-10 11:56:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint65.pt (epoch 65 @ 44513 updates, score 3.405) (writing took 2.429844915866852 seconds)
2024-02-10 11:56:14 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-02-10 11:56:14 | INFO | train | epoch 065 | loss 2.458 | nll_loss 0.798 | ppl 1.74 | wps 67037 | ups 4.62 | wpb 14524.4 | bsz 748.5 | num_updates 44513 | lr 0.00036714 | gnorm 0.464 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 9858
2024-02-10 11:56:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:56:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:56:14 | INFO | fairseq.trainer | begin training epoch 66
2024-02-10 11:56:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:56:32 | INFO | train_inner | epoch 066:     87 / 685 loss=2.438, nll_loss=0.774, ppl=1.71, wps=55540.4, ups=3.89, wpb=14275.2, bsz=720.8, num_updates=44600, lr=0.000366782, gnorm=0.46, loss_scale=4, train_wall=21, gb_free=29.5, wall=9877
2024-02-10 11:56:53 | INFO | train_inner | epoch 066:    187 / 685 loss=2.441, nll_loss=0.778, ppl=1.71, wps=69074.5, ups=4.73, wpb=14616.7, bsz=785.3, num_updates=44700, lr=0.000366372, gnorm=0.521, loss_scale=4, train_wall=21, gb_free=29.6, wall=9898
2024-02-10 11:57:14 | INFO | train_inner | epoch 066:    287 / 685 loss=2.447, nll_loss=0.785, ppl=1.72, wps=69770.9, ups=4.76, wpb=14651, bsz=748.6, num_updates=44800, lr=0.000365963, gnorm=0.567, loss_scale=4, train_wall=21, gb_free=29.5, wall=9919
2024-02-10 11:57:35 | INFO | train_inner | epoch 066:    387 / 685 loss=2.457, nll_loss=0.796, ppl=1.74, wps=68554.1, ups=4.78, wpb=14334.5, bsz=739.1, num_updates=44900, lr=0.000365555, gnorm=0.475, loss_scale=4, train_wall=21, gb_free=29.6, wall=9940
2024-02-10 11:57:56 | INFO | train_inner | epoch 066:    487 / 685 loss=2.46, nll_loss=0.801, ppl=1.74, wps=69751.8, ups=4.74, wpb=14702.6, bsz=755.8, num_updates=45000, lr=0.000365148, gnorm=0.453, loss_scale=4, train_wall=21, gb_free=29.5, wall=9961
2024-02-10 11:58:17 | INFO | train_inner | epoch 066:    587 / 685 loss=2.463, nll_loss=0.804, ppl=1.75, wps=69063.6, ups=4.76, wpb=14520.6, bsz=734.8, num_updates=45100, lr=0.000364743, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=9982
2024-02-10 11:58:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 11:58:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:58:40 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.412 | nll_loss 1.814 | ppl 3.52 | wps 131514 | wpb 3189.1 | bsz 163.4 | num_updates 45198 | best_loss 3.396
2024-02-10 11:58:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 45198 updates
2024-02-10 11:58:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt
2024-02-10 11:58:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt
2024-02-10 11:58:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint66.pt (epoch 66 @ 45198 updates, score 3.412) (writing took 2.458396650152281 seconds)
2024-02-10 11:58:43 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-02-10 11:58:43 | INFO | train | epoch 066 | loss 2.454 | nll_loss 0.793 | ppl 1.73 | wps 66915.3 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 45198 | lr 0.000364348 | gnorm 0.486 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 10007
2024-02-10 11:58:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 11:58:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 11:58:43 | INFO | fairseq.trainer | begin training epoch 67
2024-02-10 11:58:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 11:58:43 | INFO | train_inner | epoch 067:      2 / 685 loss=2.474, nll_loss=0.816, ppl=1.76, wps=56159.4, ups=3.89, wpb=14441.4, bsz=753, num_updates=45200, lr=0.00036434, gnorm=0.467, loss_scale=4, train_wall=21, gb_free=29.5, wall=10008
2024-02-10 11:59:04 | INFO | train_inner | epoch 067:    102 / 685 loss=2.433, nll_loss=0.768, ppl=1.7, wps=69545.1, ups=4.79, wpb=14525.5, bsz=726.5, num_updates=45300, lr=0.000363937, gnorm=0.457, loss_scale=4, train_wall=21, gb_free=29.5, wall=10029
2024-02-10 11:59:25 | INFO | train_inner | epoch 067:    202 / 685 loss=2.44, nll_loss=0.777, ppl=1.71, wps=69450.1, ups=4.77, wpb=14572.4, bsz=752.2, num_updates=45400, lr=0.000363536, gnorm=0.462, loss_scale=8, train_wall=21, gb_free=29.6, wall=10050
2024-02-10 11:59:46 | INFO | train_inner | epoch 067:    302 / 685 loss=2.447, nll_loss=0.786, ppl=1.72, wps=69071.9, ups=4.75, wpb=14535.1, bsz=766.9, num_updates=45500, lr=0.000363137, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.5, wall=10071
2024-02-10 12:00:07 | INFO | train_inner | epoch 067:    402 / 685 loss=2.45, nll_loss=0.789, ppl=1.73, wps=69354.7, ups=4.75, wpb=14585.7, bsz=760.8, num_updates=45600, lr=0.000362738, gnorm=0.472, loss_scale=8, train_wall=21, gb_free=29.5, wall=10092
2024-02-10 12:00:28 | INFO | train_inner | epoch 067:    502 / 685 loss=2.458, nll_loss=0.799, ppl=1.74, wps=69209.8, ups=4.79, wpb=14449.1, bsz=731.9, num_updates=45700, lr=0.000362341, gnorm=0.461, loss_scale=8, train_wall=21, gb_free=29.5, wall=10113
2024-02-10 12:00:49 | INFO | train_inner | epoch 067:    602 / 685 loss=2.452, nll_loss=0.792, ppl=1.73, wps=69252.5, ups=4.76, wpb=14541.4, bsz=755.5, num_updates=45800, lr=0.000361945, gnorm=0.452, loss_scale=8, train_wall=21, gb_free=29.5, wall=10134
2024-02-10 12:01:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:01:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:01:08 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.411 | nll_loss 1.811 | ppl 3.51 | wps 131954 | wpb 3189.1 | bsz 163.4 | num_updates 45883 | best_loss 3.396
2024-02-10 12:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 45883 updates
2024-02-10 12:01:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt
2024-02-10 12:01:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt
2024-02-10 12:01:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint67.pt (epoch 67 @ 45883 updates, score 3.411) (writing took 2.422307573026046 seconds)
2024-02-10 12:01:11 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-02-10 12:01:11 | INFO | train | epoch 067 | loss 2.449 | nll_loss 0.788 | ppl 1.73 | wps 67070.7 | ups 4.62 | wpb 14524.4 | bsz 748.5 | num_updates 45883 | lr 0.000361618 | gnorm 0.46 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 10156
2024-02-10 12:01:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:01:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:01:11 | INFO | fairseq.trainer | begin training epoch 68
2024-02-10 12:01:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:01:15 | INFO | train_inner | epoch 068:     17 / 685 loss=2.459, nll_loss=0.8, ppl=1.74, wps=56458, ups=3.9, wpb=14473.8, bsz=737.6, num_updates=45900, lr=0.000361551, gnorm=0.456, loss_scale=8, train_wall=21, gb_free=29.5, wall=10159
2024-02-10 12:01:36 | INFO | train_inner | epoch 068:    117 / 685 loss=2.427, nll_loss=0.762, ppl=1.7, wps=68795.1, ups=4.75, wpb=14486.7, bsz=747.8, num_updates=46000, lr=0.000361158, gnorm=0.445, loss_scale=8, train_wall=21, gb_free=29.5, wall=10180
2024-02-10 12:01:57 | INFO | train_inner | epoch 068:    217 / 685 loss=2.439, nll_loss=0.777, ppl=1.71, wps=69832.6, ups=4.77, wpb=14626.9, bsz=750.1, num_updates=46100, lr=0.000360766, gnorm=0.457, loss_scale=8, train_wall=21, gb_free=29.5, wall=10201
2024-02-10 12:02:18 | INFO | train_inner | epoch 068:    317 / 685 loss=2.436, nll_loss=0.774, ppl=1.71, wps=68989.4, ups=4.75, wpb=14518.5, bsz=771.4, num_updates=46200, lr=0.000360375, gnorm=0.451, loss_scale=8, train_wall=21, gb_free=29.5, wall=10222
2024-02-10 12:02:39 | INFO | train_inner | epoch 068:    417 / 685 loss=2.45, nll_loss=0.79, ppl=1.73, wps=69183.8, ups=4.77, wpb=14518, bsz=763.3, num_updates=46300, lr=0.000359986, gnorm=0.461, loss_scale=8, train_wall=21, gb_free=29.5, wall=10243
2024-02-10 12:02:59 | INFO | train_inner | epoch 068:    517 / 685 loss=2.455, nll_loss=0.796, ppl=1.74, wps=69231.8, ups=4.78, wpb=14482.7, bsz=739.6, num_updates=46400, lr=0.000359597, gnorm=0.469, loss_scale=8, train_wall=21, gb_free=29.6, wall=10264
2024-02-10 12:03:20 | INFO | train_inner | epoch 068:    617 / 685 loss=2.459, nll_loss=0.801, ppl=1.74, wps=69864.7, ups=4.77, wpb=14639.5, bsz=739.1, num_updates=46500, lr=0.000359211, gnorm=0.46, loss_scale=8, train_wall=21, gb_free=29.5, wall=10285
2024-02-10 12:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:03:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:03:37 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.44 | nll_loss 1.842 | ppl 3.58 | wps 132439 | wpb 3189.1 | bsz 163.4 | num_updates 46568 | best_loss 3.396
2024-02-10 12:03:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 46568 updates
2024-02-10 12:03:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt
2024-02-10 12:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt
2024-02-10 12:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint68.pt (epoch 68 @ 46568 updates, score 3.44) (writing took 2.4994996241293848 seconds)
2024-02-10 12:03:39 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-02-10 12:03:39 | INFO | train | epoch 068 | loss 2.445 | nll_loss 0.784 | ppl 1.72 | wps 67001 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 46568 | lr 0.000358948 | gnorm 0.46 | loss_scale 8 | train_wall 142 | gb_free 29.6 | wall 10304
2024-02-10 12:03:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:03:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:03:39 | INFO | fairseq.trainer | begin training epoch 69
2024-02-10 12:03:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:03:46 | INFO | train_inner | epoch 069:     32 / 685 loss=2.444, nll_loss=0.782, ppl=1.72, wps=55742.7, ups=3.88, wpb=14376, bsz=735.1, num_updates=46600, lr=0.000358825, gnorm=0.472, loss_scale=8, train_wall=21, gb_free=29.5, wall=10311
2024-02-10 12:04:07 | INFO | train_inner | epoch 069:    132 / 685 loss=2.425, nll_loss=0.76, ppl=1.69, wps=69232, ups=4.77, wpb=14518, bsz=745.2, num_updates=46700, lr=0.000358441, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.5, wall=10332
2024-02-10 12:04:28 | INFO | train_inner | epoch 069:    232 / 685 loss=2.434, nll_loss=0.771, ppl=1.71, wps=69626.4, ups=4.76, wpb=14620.9, bsz=757.8, num_updates=46800, lr=0.000358057, gnorm=0.449, loss_scale=8, train_wall=21, gb_free=29.5, wall=10353
2024-02-10 12:04:49 | INFO | train_inner | epoch 069:    332 / 685 loss=2.436, nll_loss=0.774, ppl=1.71, wps=69098.6, ups=4.77, wpb=14489.5, bsz=755.8, num_updates=46900, lr=0.000357676, gnorm=0.452, loss_scale=8, train_wall=21, gb_free=29.5, wall=10374
2024-02-10 12:05:10 | INFO | train_inner | epoch 069:    432 / 685 loss=2.444, nll_loss=0.784, ppl=1.72, wps=69502, ups=4.77, wpb=14585.3, bsz=755.4, num_updates=47000, lr=0.000357295, gnorm=0.458, loss_scale=8, train_wall=21, gb_free=29.5, wall=10395
2024-02-10 12:05:31 | INFO | train_inner | epoch 069:    532 / 685 loss=2.454, nll_loss=0.794, ppl=1.73, wps=69736.5, ups=4.78, wpb=14602.2, bsz=756.2, num_updates=47100, lr=0.000356915, gnorm=0.455, loss_scale=8, train_wall=21, gb_free=29.5, wall=10416
2024-02-10 12:05:52 | INFO | train_inner | epoch 069:    632 / 685 loss=2.46, nll_loss=0.802, ppl=1.74, wps=69741.3, ups=4.77, wpb=14616.4, bsz=732.1, num_updates=47200, lr=0.000356537, gnorm=0.483, loss_scale=8, train_wall=21, gb_free=29.5, wall=10437
2024-02-10 12:06:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:06:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:06:05 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.416 | nll_loss 1.812 | ppl 3.51 | wps 132255 | wpb 3189.1 | bsz 163.4 | num_updates 47253 | best_loss 3.396
2024-02-10 12:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 47253 updates
2024-02-10 12:06:05 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt
2024-02-10 12:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt
2024-02-10 12:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint69.pt (epoch 69 @ 47253 updates, score 3.416) (writing took 2.465970807010308 seconds)
2024-02-10 12:06:08 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-02-10 12:06:08 | INFO | train | epoch 069 | loss 2.441 | nll_loss 0.78 | ppl 1.72 | wps 67023.4 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 47253 | lr 0.000356337 | gnorm 0.459 | loss_scale 8 | train_wall 142 | gb_free 29.7 | wall 10452
2024-02-10 12:06:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:06:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:06:08 | INFO | fairseq.trainer | begin training epoch 70
2024-02-10 12:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:06:18 | INFO | train_inner | epoch 070:     47 / 685 loss=2.434, nll_loss=0.772, ppl=1.71, wps=55284.3, ups=3.88, wpb=14261.1, bsz=750, num_updates=47300, lr=0.00035616, gnorm=0.458, loss_scale=8, train_wall=21, gb_free=29.5, wall=10463
2024-02-10 12:06:39 | INFO | train_inner | epoch 070:    147 / 685 loss=2.421, nll_loss=0.756, ppl=1.69, wps=68934.5, ups=4.75, wpb=14514.3, bsz=754, num_updates=47400, lr=0.000355784, gnorm=0.444, loss_scale=8, train_wall=21, gb_free=29.5, wall=10484
2024-02-10 12:07:00 | INFO | train_inner | epoch 070:    247 / 685 loss=2.44, nll_loss=0.778, ppl=1.71, wps=69122.8, ups=4.79, wpb=14426.9, bsz=727.7, num_updates=47500, lr=0.000355409, gnorm=0.465, loss_scale=8, train_wall=21, gb_free=29.5, wall=10504
2024-02-10 12:07:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-02-10 12:07:21 | INFO | train_inner | epoch 070:    348 / 685 loss=2.435, nll_loss=0.773, ppl=1.71, wps=68561.3, ups=4.71, wpb=14551.1, bsz=755.8, num_updates=47600, lr=0.000355036, gnorm=0.457, loss_scale=4, train_wall=21, gb_free=29.6, wall=10526
2024-02-10 12:07:42 | INFO | train_inner | epoch 070:    448 / 685 loss=2.437, nll_loss=0.775, ppl=1.71, wps=69639.6, ups=4.78, wpb=14578.1, bsz=759.6, num_updates=47700, lr=0.000354663, gnorm=0.474, loss_scale=4, train_wall=21, gb_free=29.5, wall=10547
2024-02-10 12:08:03 | INFO | train_inner | epoch 070:    548 / 685 loss=2.448, nll_loss=0.789, ppl=1.73, wps=69315.2, ups=4.77, wpb=14527.5, bsz=741.8, num_updates=47800, lr=0.000354292, gnorm=0.465, loss_scale=4, train_wall=21, gb_free=29.5, wall=10568
2024-02-10 12:08:24 | INFO | train_inner | epoch 070:    648 / 685 loss=2.451, nll_loss=0.792, ppl=1.73, wps=70292.8, ups=4.78, wpb=14717.1, bsz=744, num_updates=47900, lr=0.000353922, gnorm=0.462, loss_scale=4, train_wall=21, gb_free=29.5, wall=10589
2024-02-10 12:08:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:08:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:08:34 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.408 | nll_loss 1.806 | ppl 3.5 | wps 130126 | wpb 3189.1 | bsz 163.4 | num_updates 47937 | best_loss 3.396
2024-02-10 12:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 47937 updates
2024-02-10 12:08:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt
2024-02-10 12:08:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt
2024-02-10 12:08:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint70.pt (epoch 70 @ 47937 updates, score 3.408) (writing took 2.4269837378524244 seconds)
2024-02-10 12:08:36 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-02-10 12:08:36 | INFO | train | epoch 070 | loss 2.438 | nll_loss 0.776 | ppl 1.71 | wps 66916.2 | ups 4.61 | wpb 14523.3 | bsz 748.5 | num_updates 47937 | lr 0.000353786 | gnorm 0.461 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 10601
2024-02-10 12:08:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:08:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:08:36 | INFO | fairseq.trainer | begin training epoch 71
2024-02-10 12:08:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:08:50 | INFO | train_inner | epoch 071:     63 / 685 loss=2.425, nll_loss=0.761, ppl=1.69, wps=55640.3, ups=3.88, wpb=14343.9, bsz=744.6, num_updates=48000, lr=0.000353553, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.6, wall=10614
2024-02-10 12:09:11 | INFO | train_inner | epoch 071:    163 / 685 loss=2.42, nll_loss=0.756, ppl=1.69, wps=69272.8, ups=4.75, wpb=14576.5, bsz=748.6, num_updates=48100, lr=0.000353186, gnorm=0.448, loss_scale=4, train_wall=21, gb_free=29.6, wall=10635
2024-02-10 12:09:32 | INFO | train_inner | epoch 071:    263 / 685 loss=2.435, nll_loss=0.773, ppl=1.71, wps=68667.8, ups=4.75, wpb=14461.4, bsz=748.6, num_updates=48200, lr=0.000352819, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=10656
2024-02-10 12:09:53 | INFO | train_inner | epoch 071:    363 / 685 loss=2.431, nll_loss=0.768, ppl=1.7, wps=70084.1, ups=4.77, wpb=14693.6, bsz=737.8, num_updates=48300, lr=0.000352454, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=10677
2024-02-10 12:10:14 | INFO | train_inner | epoch 071:    463 / 685 loss=2.442, nll_loss=0.782, ppl=1.72, wps=69351.7, ups=4.77, wpb=14547.6, bsz=757, num_updates=48400, lr=0.000352089, gnorm=0.472, loss_scale=4, train_wall=21, gb_free=29.5, wall=10698
2024-02-10 12:10:35 | INFO | train_inner | epoch 071:    563 / 685 loss=2.438, nll_loss=0.777, ppl=1.71, wps=68396.3, ups=4.76, wpb=14362.9, bsz=759.8, num_updates=48500, lr=0.000351726, gnorm=0.462, loss_scale=4, train_wall=21, gb_free=29.5, wall=10719
2024-02-10 12:10:56 | INFO | train_inner | epoch 071:    663 / 685 loss=2.451, nll_loss=0.791, ppl=1.73, wps=69691.1, ups=4.79, wpb=14556.3, bsz=739.7, num_updates=48600, lr=0.000351364, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=10740
2024-02-10 12:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:11:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:11:02 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.423 | nll_loss 1.826 | ppl 3.55 | wps 132041 | wpb 3189.1 | bsz 163.4 | num_updates 48622 | best_loss 3.396
2024-02-10 12:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 48622 updates
2024-02-10 12:11:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt
2024-02-10 12:11:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt
2024-02-10 12:11:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint71.pt (epoch 71 @ 48622 updates, score 3.423) (writing took 2.4148015899118036 seconds)
2024-02-10 12:11:05 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-02-10 12:11:05 | INFO | train | epoch 071 | loss 2.434 | nll_loss 0.772 | ppl 1.71 | wps 66990.6 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 48622 | lr 0.000351285 | gnorm 0.46 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 10749
2024-02-10 12:11:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:11:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:11:05 | INFO | fairseq.trainer | begin training epoch 72
2024-02-10 12:11:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:11:21 | INFO | train_inner | epoch 072:     78 / 685 loss=2.414, nll_loss=0.75, ppl=1.68, wps=56683.9, ups=3.88, wpb=14605.2, bsz=760.5, num_updates=48700, lr=0.000351003, gnorm=0.477, loss_scale=4, train_wall=21, gb_free=29.5, wall=10766
2024-02-10 12:11:42 | INFO | train_inner | epoch 072:    178 / 685 loss=2.42, nll_loss=0.755, ppl=1.69, wps=68478.5, ups=4.74, wpb=14433.5, bsz=736.3, num_updates=48800, lr=0.000350643, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=10787
2024-02-10 12:12:03 | INFO | train_inner | epoch 072:    278 / 685 loss=2.422, nll_loss=0.759, ppl=1.69, wps=69074.2, ups=4.74, wpb=14564.9, bsz=771, num_updates=48900, lr=0.000350285, gnorm=0.45, loss_scale=4, train_wall=21, gb_free=29.5, wall=10808
2024-02-10 12:12:24 | INFO | train_inner | epoch 072:    378 / 685 loss=2.432, nll_loss=0.77, ppl=1.71, wps=68985.4, ups=4.77, wpb=14477.5, bsz=745.2, num_updates=49000, lr=0.000349927, gnorm=0.462, loss_scale=4, train_wall=21, gb_free=29.5, wall=10829
2024-02-10 12:12:45 | INFO | train_inner | epoch 072:    478 / 685 loss=2.437, nll_loss=0.776, ppl=1.71, wps=69860.5, ups=4.78, wpb=14606.7, bsz=717.4, num_updates=49100, lr=0.000349571, gnorm=0.453, loss_scale=4, train_wall=21, gb_free=29.6, wall=10850
2024-02-10 12:13:06 | INFO | train_inner | epoch 072:    578 / 685 loss=2.436, nll_loss=0.775, ppl=1.71, wps=68168, ups=4.76, wpb=14322.5, bsz=759.4, num_updates=49200, lr=0.000349215, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=10871
2024-02-10 12:13:27 | INFO | train_inner | epoch 072:    678 / 685 loss=2.451, nll_loss=0.793, ppl=1.73, wps=70034.3, ups=4.78, wpb=14664, bsz=747.1, num_updates=49300, lr=0.000348861, gnorm=0.47, loss_scale=4, train_wall=21, gb_free=29.5, wall=10892
2024-02-10 12:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:13:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:13:31 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.42 | nll_loss 1.82 | ppl 3.53 | wps 131973 | wpb 3189.1 | bsz 163.4 | num_updates 49307 | best_loss 3.396
2024-02-10 12:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 49307 updates
2024-02-10 12:13:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt
2024-02-10 12:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt
2024-02-10 12:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint72.pt (epoch 72 @ 49307 updates, score 3.42) (writing took 2.410072394181043 seconds)
2024-02-10 12:13:33 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-02-10 12:13:33 | INFO | train | epoch 072 | loss 2.43 | nll_loss 0.768 | ppl 1.7 | wps 66941.5 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 49307 | lr 0.000348836 | gnorm 0.462 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 10898
2024-02-10 12:13:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:13:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:13:33 | INFO | fairseq.trainer | begin training epoch 73
2024-02-10 12:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:13:53 | INFO | train_inner | epoch 073:     93 / 685 loss=2.407, nll_loss=0.741, ppl=1.67, wps=55959, ups=3.91, wpb=14319.5, bsz=739.4, num_updates=49400, lr=0.000348508, gnorm=0.447, loss_scale=4, train_wall=20, gb_free=29.6, wall=10918
2024-02-10 12:14:14 | INFO | train_inner | epoch 073:    193 / 685 loss=2.414, nll_loss=0.75, ppl=1.68, wps=69336.4, ups=4.77, wpb=14540.3, bsz=755.4, num_updates=49500, lr=0.000348155, gnorm=0.454, loss_scale=4, train_wall=21, gb_free=29.5, wall=10939
2024-02-10 12:14:35 | INFO | train_inner | epoch 073:    293 / 685 loss=2.428, nll_loss=0.766, ppl=1.7, wps=69271.3, ups=4.76, wpb=14549.5, bsz=741.6, num_updates=49600, lr=0.000347804, gnorm=0.455, loss_scale=4, train_wall=21, gb_free=29.5, wall=10960
2024-02-10 12:14:56 | INFO | train_inner | epoch 073:    393 / 685 loss=2.425, nll_loss=0.763, ppl=1.7, wps=69646.2, ups=4.76, wpb=14619.7, bsz=762.5, num_updates=49700, lr=0.000347454, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=10981
2024-02-10 12:15:17 | INFO | train_inner | epoch 073:    493 / 685 loss=2.432, nll_loss=0.771, ppl=1.71, wps=69971.3, ups=4.78, wpb=14653.4, bsz=760.3, num_updates=49800, lr=0.000347105, gnorm=0.448, loss_scale=4, train_wall=21, gb_free=29.5, wall=11001
2024-02-10 12:15:38 | INFO | train_inner | epoch 073:    593 / 685 loss=2.438, nll_loss=0.779, ppl=1.72, wps=69508.9, ups=4.79, wpb=14498.6, bsz=749.3, num_updates=49900, lr=0.000346757, gnorm=0.487, loss_scale=4, train_wall=21, gb_free=29.6, wall=11022
2024-02-10 12:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:15:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:16:00 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.41 | nll_loss 1.815 | ppl 3.52 | wps 132620 | wpb 3189.1 | bsz 163.4 | num_updates 49992 | best_loss 3.396
2024-02-10 12:16:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 49992 updates
2024-02-10 12:16:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt
2024-02-10 12:16:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt
2024-02-10 12:16:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint73.pt (epoch 73 @ 49992 updates, score 3.41) (writing took 2.3875636409502476 seconds)
2024-02-10 12:16:02 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-02-10 12:16:02 | INFO | train | epoch 073 | loss 2.427 | nll_loss 0.764 | ppl 1.7 | wps 66909 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 49992 | lr 0.000346438 | gnorm 0.458 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 11047
2024-02-10 12:16:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:16:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:16:02 | INFO | fairseq.trainer | begin training epoch 74
2024-02-10 12:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:16:04 | INFO | train_inner | epoch 074:      8 / 685 loss=2.439, nll_loss=0.779, ppl=1.72, wps=54994.6, ups=3.81, wpb=14425, bsz=729, num_updates=50000, lr=0.00034641, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=11049
2024-02-10 12:16:25 | INFO | train_inner | epoch 074:    108 / 685 loss=2.405, nll_loss=0.739, ppl=1.67, wps=69813.2, ups=4.79, wpb=14571.9, bsz=754.8, num_updates=50100, lr=0.000346064, gnorm=0.445, loss_scale=4, train_wall=21, gb_free=29.5, wall=11069
2024-02-10 12:16:46 | INFO | train_inner | epoch 074:    208 / 685 loss=2.413, nll_loss=0.748, ppl=1.68, wps=69246.5, ups=4.77, wpb=14519.5, bsz=747.8, num_updates=50200, lr=0.000345719, gnorm=0.455, loss_scale=4, train_wall=21, gb_free=29.5, wall=11090
2024-02-10 12:17:08 | INFO | train_inner | epoch 074:    308 / 685 loss=2.427, nll_loss=0.765, ppl=1.7, wps=65015.1, ups=4.44, wpb=14656, bsz=729.4, num_updates=50300, lr=0.000345376, gnorm=0.462, loss_scale=4, train_wall=22, gb_free=29.5, wall=11113
2024-02-10 12:17:29 | INFO | train_inner | epoch 074:    408 / 685 loss=2.425, nll_loss=0.763, ppl=1.7, wps=69386, ups=4.76, wpb=14577.4, bsz=764.2, num_updates=50400, lr=0.000345033, gnorm=0.458, loss_scale=4, train_wall=21, gb_free=29.5, wall=11134
2024-02-10 12:17:50 | INFO | train_inner | epoch 074:    508 / 685 loss=2.429, nll_loss=0.768, ppl=1.7, wps=69321.5, ups=4.77, wpb=14544.4, bsz=756.4, num_updates=50500, lr=0.000344691, gnorm=0.455, loss_scale=4, train_wall=21, gb_free=29.5, wall=11155
2024-02-10 12:18:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2024-02-10 12:18:11 | INFO | train_inner | epoch 074:    609 / 685 loss=2.43, nll_loss=0.77, ppl=1.7, wps=68257.6, ups=4.72, wpb=14474.4, bsz=742.3, num_updates=50600, lr=0.00034435, gnorm=0.46, loss_scale=2, train_wall=21, gb_free=29.5, wall=11176
2024-02-10 12:18:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:18:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:18:30 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.43 | nll_loss 1.832 | ppl 3.56 | wps 131685 | wpb 3189.1 | bsz 163.4 | num_updates 50676 | best_loss 3.396
2024-02-10 12:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 50676 updates
2024-02-10 12:18:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt
2024-02-10 12:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt
2024-02-10 12:18:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint74.pt (epoch 74 @ 50676 updates, score 3.43) (writing took 2.475046755047515 seconds)
2024-02-10 12:18:32 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-02-10 12:18:32 | INFO | train | epoch 074 | loss 2.422 | nll_loss 0.76 | ppl 1.69 | wps 66246.6 | ups 4.56 | wpb 14528.4 | bsz 748.4 | num_updates 50676 | lr 0.000344092 | gnorm 0.458 | loss_scale 2 | train_wall 143 | gb_free 29.8 | wall 11197
2024-02-10 12:18:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:18:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:18:32 | INFO | fairseq.trainer | begin training epoch 75
2024-02-10 12:18:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:18:37 | INFO | train_inner | epoch 075:     24 / 685 loss=2.422, nll_loss=0.76, ppl=1.69, wps=55950.4, ups=3.88, wpb=14404.4, bsz=735.8, num_updates=50700, lr=0.00034401, gnorm=0.467, loss_scale=2, train_wall=21, gb_free=29.5, wall=11202
2024-02-10 12:18:58 | INFO | train_inner | epoch 075:    124 / 685 loss=2.4, nll_loss=0.734, ppl=1.66, wps=69249.7, ups=4.78, wpb=14479.4, bsz=728.9, num_updates=50800, lr=0.000343672, gnorm=0.446, loss_scale=2, train_wall=21, gb_free=29.5, wall=11223
2024-02-10 12:19:19 | INFO | train_inner | epoch 075:    224 / 685 loss=2.407, nll_loss=0.742, ppl=1.67, wps=68219.7, ups=4.76, wpb=14336.1, bsz=772.3, num_updates=50900, lr=0.000343334, gnorm=0.483, loss_scale=2, train_wall=21, gb_free=29.6, wall=11244
2024-02-10 12:19:40 | INFO | train_inner | epoch 075:    324 / 685 loss=2.423, nll_loss=0.761, ppl=1.69, wps=70210.5, ups=4.78, wpb=14681.4, bsz=738.9, num_updates=51000, lr=0.000342997, gnorm=0.463, loss_scale=2, train_wall=21, gb_free=29.5, wall=11265
2024-02-10 12:20:01 | INFO | train_inner | epoch 075:    424 / 685 loss=2.424, nll_loss=0.763, ppl=1.7, wps=70470.6, ups=4.75, wpb=14820.8, bsz=772.6, num_updates=51100, lr=0.000342661, gnorm=0.491, loss_scale=2, train_wall=21, gb_free=29.5, wall=11286
2024-02-10 12:20:22 | INFO | train_inner | epoch 075:    524 / 685 loss=2.426, nll_loss=0.765, ppl=1.7, wps=69220.2, ups=4.77, wpb=14503.8, bsz=747, num_updates=51200, lr=0.000342327, gnorm=0.46, loss_scale=2, train_wall=21, gb_free=29.6, wall=11307
2024-02-10 12:20:43 | INFO | train_inner | epoch 075:    624 / 685 loss=2.435, nll_loss=0.775, ppl=1.71, wps=68722.5, ups=4.77, wpb=14404, bsz=744.4, num_updates=51300, lr=0.000341993, gnorm=0.471, loss_scale=2, train_wall=21, gb_free=29.5, wall=11328
2024-02-10 12:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:20:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:20:58 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.43 | nll_loss 1.831 | ppl 3.56 | wps 131850 | wpb 3189.1 | bsz 163.4 | num_updates 51361 | best_loss 3.396
2024-02-10 12:20:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 51361 updates
2024-02-10 12:20:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt
2024-02-10 12:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt
2024-02-10 12:21:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint75.pt (epoch 75 @ 51361 updates, score 3.43) (writing took 22.9846027251333 seconds)
2024-02-10 12:21:21 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-02-10 12:21:21 | INFO | train | epoch 075 | loss 2.419 | nll_loss 0.757 | ppl 1.69 | wps 58917.9 | ups 4.06 | wpb 14524.4 | bsz 748.5 | num_updates 51361 | lr 0.00034179 | gnorm 0.468 | loss_scale 2 | train_wall 142 | gb_free 29.8 | wall 11366
2024-02-10 12:21:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:21:24 | INFO | fairseq.trainer | begin training epoch 76
2024-02-10 12:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:21:44 | INFO | train_inner | epoch 076:     39 / 685 loss=2.415, nll_loss=0.752, ppl=1.68, wps=23600.2, ups=1.64, wpb=14347.1, bsz=749, num_updates=51400, lr=0.00034166, gnorm=0.461, loss_scale=2, train_wall=21, gb_free=29.5, wall=11388
2024-02-10 12:22:05 | INFO | train_inner | epoch 076:    139 / 685 loss=2.401, nll_loss=0.735, ppl=1.66, wps=69106.5, ups=4.76, wpb=14515.5, bsz=754.5, num_updates=51500, lr=0.000341328, gnorm=0.452, loss_scale=2, train_wall=21, gb_free=29.6, wall=11409
2024-02-10 12:22:26 | INFO | train_inner | epoch 076:    239 / 685 loss=2.408, nll_loss=0.743, ppl=1.67, wps=69924.3, ups=4.77, wpb=14666.5, bsz=757.7, num_updates=51600, lr=0.000340997, gnorm=0.451, loss_scale=2, train_wall=21, gb_free=29.5, wall=11430
2024-02-10 12:22:47 | INFO | train_inner | epoch 076:    339 / 685 loss=2.419, nll_loss=0.756, ppl=1.69, wps=69312.2, ups=4.77, wpb=14524.3, bsz=752.3, num_updates=51700, lr=0.000340667, gnorm=0.461, loss_scale=2, train_wall=21, gb_free=29.6, wall=11451
2024-02-10 12:23:08 | INFO | train_inner | epoch 076:    439 / 685 loss=2.42, nll_loss=0.757, ppl=1.69, wps=69746.3, ups=4.78, wpb=14598.2, bsz=740.2, num_updates=51800, lr=0.000340338, gnorm=0.453, loss_scale=2, train_wall=21, gb_free=29.6, wall=11472
2024-02-10 12:23:29 | INFO | train_inner | epoch 076:    539 / 685 loss=2.429, nll_loss=0.768, ppl=1.7, wps=69053.5, ups=4.77, wpb=14474.9, bsz=744.4, num_updates=51900, lr=0.00034001, gnorm=0.465, loss_scale=2, train_wall=21, gb_free=29.6, wall=11493
2024-02-10 12:23:50 | INFO | train_inner | epoch 076:    639 / 685 loss=2.425, nll_loss=0.764, ppl=1.7, wps=69038.4, ups=4.78, wpb=14447.6, bsz=735, num_updates=52000, lr=0.000339683, gnorm=0.48, loss_scale=2, train_wall=21, gb_free=29.5, wall=11514
2024-02-10 12:23:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:23:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:24:01 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.407 | nll_loss 1.805 | ppl 3.5 | wps 125847 | wpb 3189.1 | bsz 163.4 | num_updates 52046 | best_loss 3.396
2024-02-10 12:24:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 52046 updates
2024-02-10 12:24:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt
2024-02-10 12:24:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt
2024-02-10 12:24:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint76.pt (epoch 76 @ 52046 updates, score 3.407) (writing took 2.4273816188797355 seconds)
2024-02-10 12:24:04 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-02-10 12:24:04 | INFO | train | epoch 076 | loss 2.416 | nll_loss 0.753 | ppl 1.69 | wps 61051.8 | ups 4.2 | wpb 14524.4 | bsz 748.5 | num_updates 52046 | lr 0.000339533 | gnorm 0.461 | loss_scale 2 | train_wall 142 | gb_free 29.6 | wall 11529
2024-02-10 12:24:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:24:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:24:04 | INFO | fairseq.trainer | begin training epoch 77
2024-02-10 12:24:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:24:15 | INFO | train_inner | epoch 077:     54 / 685 loss=2.406, nll_loss=0.742, ppl=1.67, wps=55558.1, ups=3.87, wpb=14352.6, bsz=753.8, num_updates=52100, lr=0.000339357, gnorm=0.461, loss_scale=2, train_wall=21, gb_free=29.5, wall=11540
2024-02-10 12:24:36 | INFO | train_inner | epoch 077:    154 / 685 loss=2.399, nll_loss=0.734, ppl=1.66, wps=68966, ups=4.75, wpb=14515.1, bsz=763.6, num_updates=52200, lr=0.000339032, gnorm=0.456, loss_scale=2, train_wall=21, gb_free=29.5, wall=11561
2024-02-10 12:24:57 | INFO | train_inner | epoch 077:    254 / 685 loss=2.407, nll_loss=0.744, ppl=1.67, wps=70404, ups=4.77, wpb=14762.9, bsz=743.6, num_updates=52300, lr=0.000338707, gnorm=0.448, loss_scale=2, train_wall=21, gb_free=29.5, wall=11582
2024-02-10 12:25:18 | INFO | train_inner | epoch 077:    354 / 685 loss=2.418, nll_loss=0.755, ppl=1.69, wps=69113.9, ups=4.78, wpb=14463, bsz=755.8, num_updates=52400, lr=0.000338384, gnorm=0.463, loss_scale=2, train_wall=21, gb_free=29.5, wall=11603
2024-02-10 12:25:39 | INFO | train_inner | epoch 077:    454 / 685 loss=2.418, nll_loss=0.755, ppl=1.69, wps=68897.7, ups=4.79, wpb=14381, bsz=719.5, num_updates=52500, lr=0.000338062, gnorm=0.469, loss_scale=2, train_wall=21, gb_free=29.6, wall=11624
2024-02-10 12:26:00 | INFO | train_inner | epoch 077:    554 / 685 loss=2.418, nll_loss=0.757, ppl=1.69, wps=69580.3, ups=4.75, wpb=14637.4, bsz=766.6, num_updates=52600, lr=0.00033774, gnorm=0.462, loss_scale=2, train_wall=21, gb_free=29.5, wall=11645
2024-02-10 12:26:21 | INFO | train_inner | epoch 077:    654 / 685 loss=2.426, nll_loss=0.765, ppl=1.7, wps=70176.1, ups=4.77, wpb=14716.4, bsz=746.2, num_updates=52700, lr=0.00033742, gnorm=0.454, loss_scale=2, train_wall=21, gb_free=29.5, wall=11666
2024-02-10 12:26:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:26:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:26:30 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.425 | nll_loss 1.831 | ppl 3.56 | wps 131936 | wpb 3189.1 | bsz 163.4 | num_updates 52731 | best_loss 3.396
2024-02-10 12:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 52731 updates
2024-02-10 12:26:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt
2024-02-10 12:26:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt
2024-02-10 12:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt (epoch 77 @ 52731 updates, score 3.425) (writing took 2.37399920495227 seconds)
2024-02-10 12:26:32 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-02-10 12:26:32 | INFO | train | epoch 077 | loss 2.413 | nll_loss 0.75 | ppl 1.68 | wps 67060.4 | ups 4.62 | wpb 14524.4 | bsz 748.5 | num_updates 52731 | lr 0.00033732 | gnorm 0.459 | loss_scale 2 | train_wall 142 | gb_free 29.7 | wall 11677
2024-02-10 12:26:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:26:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:26:32 | INFO | fairseq.trainer | begin training epoch 78
2024-02-10 12:26:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:26:47 | INFO | train_inner | epoch 078:     69 / 685 loss=2.405, nll_loss=0.739, ppl=1.67, wps=55486.4, ups=3.89, wpb=14267.7, bsz=732.9, num_updates=52800, lr=0.0003371, gnorm=0.464, loss_scale=2, train_wall=21, gb_free=29.5, wall=11692
2024-02-10 12:27:08 | INFO | train_inner | epoch 078:    169 / 685 loss=2.395, nll_loss=0.729, ppl=1.66, wps=69147.1, ups=4.78, wpb=14479.1, bsz=738.6, num_updates=52900, lr=0.000336781, gnorm=0.463, loss_scale=2, train_wall=21, gb_free=29.5, wall=11713
2024-02-10 12:27:30 | INFO | train_inner | epoch 078:    269 / 685 loss=2.398, nll_loss=0.733, ppl=1.66, wps=65732, ups=4.57, wpb=14395.2, bsz=754.6, num_updates=53000, lr=0.000336463, gnorm=0.459, loss_scale=2, train_wall=22, gb_free=29.5, wall=11734
2024-02-10 12:27:54 | INFO | train_inner | epoch 078:    369 / 685 loss=2.414, nll_loss=0.752, ppl=1.68, wps=61179.1, ups=4.15, wpb=14743.2, bsz=764.5, num_updates=53100, lr=0.000336146, gnorm=0.454, loss_scale=2, train_wall=21, gb_free=29.5, wall=11759
2024-02-10 12:28:15 | INFO | train_inner | epoch 078:    469 / 685 loss=2.415, nll_loss=0.753, ppl=1.69, wps=69134.7, ups=4.78, wpb=14466.7, bsz=742.2, num_updates=53200, lr=0.00033583, gnorm=0.463, loss_scale=2, train_wall=21, gb_free=29.5, wall=11779
2024-02-10 12:28:36 | INFO | train_inner | epoch 078:    569 / 685 loss=2.424, nll_loss=0.763, ppl=1.7, wps=69959.2, ups=4.77, wpb=14679.5, bsz=753.8, num_updates=53300, lr=0.000335515, gnorm=0.461, loss_scale=2, train_wall=21, gb_free=29.5, wall=11800
2024-02-10 12:28:57 | INFO | train_inner | epoch 078:    669 / 685 loss=2.421, nll_loss=0.76, ppl=1.69, wps=68685.6, ups=4.74, wpb=14498.2, bsz=749.2, num_updates=53400, lr=0.000335201, gnorm=0.46, loss_scale=2, train_wall=21, gb_free=29.6, wall=11822
2024-02-10 12:29:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:29:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:29:02 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.418 | nll_loss 1.82 | ppl 3.53 | wps 131272 | wpb 3189.1 | bsz 163.4 | num_updates 53416 | best_loss 3.396
2024-02-10 12:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 53416 updates
2024-02-10 12:29:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt
2024-02-10 12:29:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt
2024-02-10 12:29:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt (epoch 78 @ 53416 updates, score 3.418) (writing took 2.5091597309801728 seconds)
2024-02-10 12:29:05 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-02-10 12:29:05 | INFO | train | epoch 078 | loss 2.41 | nll_loss 0.747 | ppl 1.68 | wps 65179 | ups 4.49 | wpb 14524.4 | bsz 748.5 | num_updates 53416 | lr 0.000335151 | gnorm 0.46 | loss_scale 2 | train_wall 143 | gb_free 29.7 | wall 11830
2024-02-10 12:29:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:29:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:29:05 | INFO | fairseq.trainer | begin training epoch 79
2024-02-10 12:29:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:29:23 | INFO | train_inner | epoch 079:     84 / 685 loss=2.392, nll_loss=0.726, ppl=1.65, wps=55214.6, ups=3.87, wpb=14284.1, bsz=732, num_updates=53500, lr=0.000334887, gnorm=0.462, loss_scale=2, train_wall=21, gb_free=29.6, wall=11847
2024-02-10 12:29:44 | INFO | train_inner | epoch 079:    184 / 685 loss=2.395, nll_loss=0.73, ppl=1.66, wps=69923.4, ups=4.77, wpb=14674.2, bsz=750.3, num_updates=53600, lr=0.000334575, gnorm=0.469, loss_scale=2, train_wall=21, gb_free=29.5, wall=11868
2024-02-10 12:30:05 | INFO | train_inner | epoch 079:    284 / 685 loss=2.397, nll_loss=0.732, ppl=1.66, wps=69125, ups=4.76, wpb=14507, bsz=743.1, num_updates=53700, lr=0.000334263, gnorm=0.45, loss_scale=2, train_wall=21, gb_free=29.5, wall=11889
2024-02-10 12:30:26 | INFO | train_inner | epoch 079:    384 / 685 loss=2.406, nll_loss=0.743, ppl=1.67, wps=68748, ups=4.76, wpb=14457.4, bsz=761, num_updates=53800, lr=0.000333952, gnorm=0.463, loss_scale=2, train_wall=21, gb_free=29.5, wall=11910
2024-02-10 12:30:47 | INFO | train_inner | epoch 079:    484 / 685 loss=2.418, nll_loss=0.756, ppl=1.69, wps=70224.5, ups=4.77, wpb=14706.8, bsz=750.7, num_updates=53900, lr=0.000333642, gnorm=0.46, loss_scale=2, train_wall=21, gb_free=29.6, wall=11931
2024-02-10 12:31:08 | INFO | train_inner | epoch 079:    584 / 685 loss=2.413, nll_loss=0.752, ppl=1.68, wps=69058.6, ups=4.75, wpb=14532.2, bsz=759, num_updates=54000, lr=0.000333333, gnorm=0.46, loss_scale=2, train_wall=21, gb_free=29.5, wall=11952
2024-02-10 12:31:29 | INFO | train_inner | epoch 079:    684 / 685 loss=2.425, nll_loss=0.765, ppl=1.7, wps=69583.9, ups=4.78, wpb=14555.2, bsz=748.2, num_updates=54100, lr=0.000333025, gnorm=0.501, loss_scale=2, train_wall=21, gb_free=29.5, wall=11973
2024-02-10 12:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:31:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:31:31 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 3.428 | nll_loss 1.836 | ppl 3.57 | wps 131032 | wpb 3189.1 | bsz 163.4 | num_updates 54101 | best_loss 3.396
2024-02-10 12:31:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 54101 updates
2024-02-10 12:31:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt
2024-02-10 12:31:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt
2024-02-10 12:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt (epoch 79 @ 54101 updates, score 3.428) (writing took 2.4535759321879596 seconds)
2024-02-10 12:31:34 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-02-10 12:31:34 | INFO | train | epoch 079 | loss 2.406 | nll_loss 0.743 | ppl 1.67 | wps 66926.6 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 54101 | lr 0.000333022 | gnorm 0.466 | loss_scale 2 | train_wall 142 | gb_free 29.6 | wall 11978
2024-02-10 12:31:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:31:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:31:34 | INFO | fairseq.trainer | begin training epoch 80
2024-02-10 12:31:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:31:55 | INFO | train_inner | epoch 080:     99 / 685 loss=2.382, nll_loss=0.715, ppl=1.64, wps=54555.2, ups=3.85, wpb=14152.9, bsz=739, num_updates=54200, lr=0.000332718, gnorm=0.467, loss_scale=2, train_wall=21, gb_free=29.5, wall=11999
2024-02-10 12:32:16 | INFO | train_inner | epoch 080:    199 / 685 loss=2.391, nll_loss=0.726, ppl=1.65, wps=69028.3, ups=4.75, wpb=14525.5, bsz=764.9, num_updates=54300, lr=0.000332411, gnorm=0.451, loss_scale=2, train_wall=21, gb_free=29.5, wall=12020
2024-02-10 12:32:37 | INFO | train_inner | epoch 080:    299 / 685 loss=2.406, nll_loss=0.743, ppl=1.67, wps=70194.3, ups=4.77, wpb=14711.5, bsz=743.9, num_updates=54400, lr=0.000332106, gnorm=0.459, loss_scale=2, train_wall=21, gb_free=29.5, wall=12041
2024-02-10 12:32:58 | INFO | train_inner | epoch 080:    399 / 685 loss=2.402, nll_loss=0.739, ppl=1.67, wps=69527.9, ups=4.77, wpb=14588.8, bsz=755.2, num_updates=54500, lr=0.000331801, gnorm=0.5, loss_scale=2, train_wall=21, gb_free=29.5, wall=12062
2024-02-10 12:33:19 | INFO | train_inner | epoch 080:    499 / 685 loss=2.41, nll_loss=0.748, ppl=1.68, wps=69359.4, ups=4.76, wpb=14578.6, bsz=756.2, num_updates=54600, lr=0.000331497, gnorm=0.461, loss_scale=2, train_wall=21, gb_free=29.5, wall=12083
2024-02-10 12:33:40 | INFO | train_inner | epoch 080:    599 / 685 loss=2.414, nll_loss=0.753, ppl=1.68, wps=69170.2, ups=4.76, wpb=14523.3, bsz=741.6, num_updates=54700, lr=0.000331194, gnorm=0.464, loss_scale=4, train_wall=21, gb_free=29.5, wall=12104
2024-02-10 12:33:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:33:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:34:00 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 3.424 | nll_loss 1.831 | ppl 3.56 | wps 131738 | wpb 3189.1 | bsz 163.4 | num_updates 54786 | best_loss 3.396
2024-02-10 12:34:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 54786 updates
2024-02-10 12:34:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt
2024-02-10 12:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt
2024-02-10 12:34:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt (epoch 80 @ 54786 updates, score 3.424) (writing took 2.4497911790385842 seconds)
2024-02-10 12:34:02 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-02-10 12:34:02 | INFO | train | epoch 080 | loss 2.403 | nll_loss 0.74 | ppl 1.67 | wps 66900.5 | ups 4.61 | wpb 14524.4 | bsz 748.5 | num_updates 54786 | lr 0.000330934 | gnorm 0.467 | loss_scale 4 | train_wall 142 | gb_free 29.7 | wall 12127
2024-02-10 12:34:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:34:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 685
2024-02-10 12:34:02 | INFO | fairseq.trainer | begin training epoch 81
2024-02-10 12:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-02-10 12:34:05 | INFO | train_inner | epoch 081:     14 / 685 loss=2.412, nll_loss=0.75, ppl=1.68, wps=56183.7, ups=3.88, wpb=14490.7, bsz=736.7, num_updates=54800, lr=0.000330891, gnorm=0.466, loss_scale=4, train_wall=21, gb_free=29.5, wall=12130
2024-02-10 12:34:26 | INFO | train_inner | epoch 081:    114 / 685 loss=2.389, nll_loss=0.722, ppl=1.65, wps=68537, ups=4.8, wpb=14288.4, bsz=721.3, num_updates=54900, lr=0.00033059, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=12151
2024-02-10 12:34:47 | INFO | train_inner | epoch 081:    214 / 685 loss=2.394, nll_loss=0.729, ppl=1.66, wps=69959.3, ups=4.76, wpb=14702.5, bsz=763, num_updates=55000, lr=0.000330289, gnorm=0.471, loss_scale=4, train_wall=21, gb_free=29.5, wall=12172
2024-02-10 12:35:08 | INFO | train_inner | epoch 081:    314 / 685 loss=2.398, nll_loss=0.734, ppl=1.66, wps=69474.1, ups=4.75, wpb=14625.2, bsz=758.7, num_updates=55100, lr=0.000329989, gnorm=0.459, loss_scale=4, train_wall=21, gb_free=29.5, wall=12193
2024-02-10 12:35:29 | INFO | train_inner | epoch 081:    414 / 685 loss=2.4, nll_loss=0.736, ppl=1.67, wps=69079.6, ups=4.73, wpb=14597, bsz=763.3, num_updates=55200, lr=0.00032969, gnorm=0.463, loss_scale=4, train_wall=21, gb_free=29.6, wall=12214
2024-02-10 12:35:51 | INFO | train_inner | epoch 081:    514 / 685 loss=2.407, nll_loss=0.745, ppl=1.68, wps=69208.9, ups=4.74, wpb=14603.3, bsz=757.4, num_updates=55300, lr=0.000329392, gnorm=0.458, loss_scale=4, train_wall=21, gb_free=29.5, wall=12235
2024-02-10 12:36:12 | INFO | train_inner | epoch 081:    614 / 685 loss=2.411, nll_loss=0.749, ppl=1.68, wps=69080, ups=4.77, wpb=14482.4, bsz=747, num_updates=55400, lr=0.000329095, gnorm=0.461, loss_scale=4, train_wall=21, gb_free=29.5, wall=12256
2024-02-10 12:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-02-10 12:36:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-02-10 12:36:29 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 3.422 | nll_loss 1.829 | ppl 3.55 | wps 125673 | wpb 3189.1 | bsz 163.4 | num_updates 55471 | best_loss 3.396
2024-02-10 12:36:29 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 25 runs
2024-02-10 12:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 55471 updates
2024-02-10 12:36:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt
2024-02-10 12:36:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt
2024-02-10 12:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt (epoch 81 @ 55471 updates, score 3.422) (writing took 2.4198362960014492 seconds)
2024-02-10 12:36:31 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-02-10 12:36:31 | INFO | train | epoch 081 | loss 2.401 | nll_loss 0.737 | ppl 1.67 | wps 66873.4 | ups 4.6 | wpb 14524.4 | bsz 748.5 | num_updates 55471 | lr 0.000328884 | gnorm 0.462 | loss_scale 4 | train_wall 142 | gb_free 29.6 | wall 12276
2024-02-10 12:36:31 | INFO | fairseq_cli.train | done training in 12270.8 seconds
Training complete.
Finetuning complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt' are the same file
cp: '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt' and '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt' are the same file
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6616 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test.en: 15516 sents, 256102 tokens, 0.0176% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 8648 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost_paraphrased/spm/test.de: 15516 sents, 269168 tokens, 0.0163% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
averaging checkpoints:  ['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint81.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint80.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint79.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint78.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/checkpoint77.pt']
Finished writing averaged checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 16, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 6616 types
INFO:fairseq.tasks.translation:[de] dictionary: 8648 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/models/avg_last_5_checkpoint.pt
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/test.en-de.en
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset/test.en-de.de
INFO:fairseq.tasks.translation:/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost_paraphrased/binarized_dataset test en-de 15516 examples
INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True
INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True
INFO:fairseq.tasks.fairseq_task:rebuild_batches = False
INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1
INFO:fairseq.logging.progress_bar::    101 / 243 wps=1483
INFO:fairseq.logging.progress_bar::    201 / 243 wps=1716
INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2
INFO:fairseq_cli.generate:Translated 15,512 sentences (259,579 tokens) in 87.5s (177.33 sentences/s, 2967.38 tokens/s)
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/hyp_mt.txt and /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/ref_mt.txt
Sampled predictions written to /home/kit/stud/uxude/predictions/finetune_mt_paraphrased_covost/hyp_mt.txt.sampled
Sample predictions:
Sample: Ich wei es nicht.
Reference: Ich wei nicht.
Sample: Lassen Sie es mich sehen!
Reference: Zeig mal!
Sample: Aber nicht lange.
Reference: Aber nicht lange.
Sample: Was machte er hingegen?
Reference: Wie war er?
Sample: Und der letzte?
Reference: Und zuletzt?
Sample: Damit ist das alles?
Reference: Mit allem?
Sample: Reichsbeginn
Reference: Reichstag
Sample: Arbeiten
Reference: Arbeitswelt
Sample: Das tue ich nach wie vor.
Reference: Das mache ich weiterhin.
Sample: Zeitverschiebung.
Reference: Die Zeit rast.
Sample: Du machst es.
Reference: Du machst es.
Sample: Sie ist gerecht.
Reference: Sie ist gerecht.
Sample: Ein Flug (musikalische Noten)
Reference: Eine Sinfonie
Sample: Das Leben ist schn.
Reference: Das Leben ist schn.
Sample: Das goldene Land
Reference: Das goldene Land
Sample: Gewitterte Erde
Reference: Gebackene Erde
Sample: Die Union ist krftig.
Reference: Die Union ist stark.
Sample: Siehst du Dienstag!
Reference: Sehen uns Dienstag!
Sample: Ein Akt des Glaubens
Reference: Ein Akt des Glaubens
Sample: Das kann nicht tun.
Reference: Das ist nicht mglich.
WER:
Generate test with beam=16: BLEU4 = 27.74, 55.2/34.5/22.8/15.5 (BP=0.968, ratio=0.969, syslen=135978, reflen=140361)
BLEU:
{
 "name": "BLEU",
 "score": 27.7,
 "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:none|version:2.4.0",
 "verbose_score": "55.2/34.5/22.8/15.5 (BP = 0.968 ratio = 0.969 hyp_len = 135978 ref_len = 140360)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "none",
 "smooth": "none",
 "version": "2.4.0"
}