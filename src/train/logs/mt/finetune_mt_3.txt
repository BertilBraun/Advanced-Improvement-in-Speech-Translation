(base) [uxude@uc2n994 train]$ cat finetune_mt_covost_23098029.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[2024-01-31 22:32:05] [INFO] [Dataset::Prepare Datasets]: Skipping dataset preparation, config file and MT spm data already exists
Finetuning the MT model...
rm: cannot remove '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/preprocess.log': No such file or directory
Binarized data already exists. Skipping binarization.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Training time: 6 hours
2024-01-31 22:32:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=500, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=50000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-01-31 22:32:26 | INFO | fairseq.tasks.translation | [en] dictionary: 6520 types
2024-01-31 22:32:26 | INFO | fairseq.tasks.translation | [de] dictionary: 7560 types
2024-01-31 22:32:29 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7560, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=7560, bias=False)
  )
)
2024-01-31 22:32:29 | INFO | fairseq_cli.train | task: TranslationTask
2024-01-31 22:32:29 | INFO | fairseq_cli.train | model: TransformerModel
2024-01-31 22:32:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-31 22:32:29 | INFO | fairseq_cli.train | num. shared model params: 51,347,456 (num. trained: 51,347,456)
2024-01-31 22:32:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-31 22:32:29 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.en
2024-01-31 22:32:29 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.de
2024-01-31 22:32:29 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset valid en-de 15520 examples
2024-01-31 22:32:29 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-31 22:32:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 22:32:29 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-01-31 22:32:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-31 22:32:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-31 22:32:29 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-01-31 22:32:29 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-01-31 22:32:33 | INFO | fairseq.trainer | Loaded checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt (epoch 174 @ 255161 updates)
2024-01-31 22:32:33 | INFO | fairseq.trainer | loading train data for epoch 174
2024-01-31 22:32:33 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.en
2024-01-31 22:32:33 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.de
2024-01-31 22:32:33 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset train en-de 289024 examples
2024-01-31 22:32:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:32:33 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 22:32:33 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 22:32:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 174
2024-01-31 22:32:34 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-31 22:32:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:32:34 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-31 22:32:34 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-31 22:32:34 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-31 22:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:32:37 | INFO | fairseq.trainer | begin training epoch 174
2024-01-31 22:32:37 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-01-31 22:32:47 | INFO | train_inner | epoch 174:     39 / 1475 loss=2.16, nll_loss=0.504, ppl=1.42, wps=50951.5, ups=13.82, wpb=3691, bsz=198.2, num_updates=255200, lr=6.25979e-05, gnorm=0.974, loss_scale=16, train_wall=10, gb_free=29.8, wall=18
2024-01-31 22:32:53 | INFO | train_inner | epoch 174:    139 / 1475 loss=2.166, nll_loss=0.512, ppl=1.43, wps=59506.9, ups=15.7, wpb=3790.3, bsz=195.4, num_updates=255300, lr=6.25856e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=24
2024-01-31 22:32:59 | INFO | train_inner | epoch 174:    239 / 1475 loss=2.165, nll_loss=0.51, ppl=1.42, wps=59364.3, ups=15.8, wpb=3757.2, bsz=205.6, num_updates=255400, lr=6.25734e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.7, wall=30
2024-01-31 22:33:06 | INFO | train_inner | epoch 174:    339 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=59594.7, ups=15.84, wpb=3761.1, bsz=200.6, num_updates=255500, lr=6.25611e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.7, wall=37
2024-01-31 22:33:12 | INFO | train_inner | epoch 174:    439 / 1475 loss=2.171, nll_loss=0.517, ppl=1.43, wps=58560, ups=15.76, wpb=3716.2, bsz=196.9, num_updates=255600, lr=6.25489e-05, gnorm=0.989, loss_scale=16, train_wall=6, gb_free=29.9, wall=43
2024-01-31 22:33:18 | INFO | train_inner | epoch 174:    539 / 1475 loss=2.164, nll_loss=0.511, ppl=1.42, wps=58782.6, ups=15.76, wpb=3730, bsz=209, num_updates=255700, lr=6.25367e-05, gnorm=0.989, loss_scale=16, train_wall=6, gb_free=29.7, wall=49
2024-01-31 22:33:25 | INFO | train_inner | epoch 174:    639 / 1475 loss=2.174, nll_loss=0.52, ppl=1.43, wps=60614.6, ups=15.73, wpb=3852.6, bsz=199.3, num_updates=255800, lr=6.25244e-05, gnorm=1.023, loss_scale=16, train_wall=6, gb_free=29.7, wall=56
2024-01-31 22:33:31 | INFO | train_inner | epoch 174:    739 / 1475 loss=2.184, nll_loss=0.531, ppl=1.44, wps=59374.1, ups=15.93, wpb=3727.2, bsz=186.9, num_updates=255900, lr=6.25122e-05, gnorm=1.04, loss_scale=16, train_wall=6, gb_free=29.7, wall=62
2024-01-31 22:33:37 | INFO | train_inner | epoch 174:    839 / 1475 loss=2.179, nll_loss=0.528, ppl=1.44, wps=59442.5, ups=15.8, wpb=3761.2, bsz=190.1, num_updates=256000, lr=6.25e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.8, wall=68
2024-01-31 22:33:44 | INFO | train_inner | epoch 174:    939 / 1475 loss=2.171, nll_loss=0.519, ppl=1.43, wps=58618.9, ups=15.68, wpb=3738, bsz=192.9, num_updates=256100, lr=6.24878e-05, gnorm=1.014, loss_scale=16, train_wall=6, gb_free=29.7, wall=75
2024-01-31 22:33:50 | INFO | train_inner | epoch 174:   1039 / 1475 loss=2.182, nll_loss=0.531, ppl=1.44, wps=59659.6, ups=15.75, wpb=3788.6, bsz=192.6, num_updates=256200, lr=6.24756e-05, gnorm=1.026, loss_scale=16, train_wall=6, gb_free=29.7, wall=81
2024-01-31 22:33:56 | INFO | train_inner | epoch 174:   1139 / 1475 loss=2.181, nll_loss=0.529, ppl=1.44, wps=59664.3, ups=15.7, wpb=3800.3, bsz=200.6, num_updates=256300, lr=6.24634e-05, gnorm=0.994, loss_scale=16, train_wall=6, gb_free=29.7, wall=87
2024-01-31 22:34:03 | INFO | train_inner | epoch 174:   1239 / 1475 loss=2.186, nll_loss=0.534, ppl=1.45, wps=59455, ups=15.78, wpb=3767.4, bsz=198.3, num_updates=256400, lr=6.24512e-05, gnorm=1.018, loss_scale=16, train_wall=6, gb_free=29.7, wall=94
2024-01-31 22:34:09 | INFO | train_inner | epoch 174:   1339 / 1475 loss=2.191, nll_loss=0.54, ppl=1.45, wps=60459.8, ups=15.88, wpb=3806.4, bsz=191, num_updates=256500, lr=6.24391e-05, gnorm=1.062, loss_scale=16, train_wall=6, gb_free=29.7, wall=100
2024-01-31 22:34:15 | INFO | train_inner | epoch 174:   1439 / 1475 loss=2.203, nll_loss=0.553, ppl=1.47, wps=59919.7, ups=15.82, wpb=3787.1, bsz=188.4, num_updates=256600, lr=6.24269e-05, gnorm=1.048, loss_scale=16, train_wall=6, gb_free=29.7, wall=106
2024-01-31 22:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:34:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:34:20 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 3.559 | nll_loss 1.969 | ppl 3.92 | wps 158827 | wpb 3189.1 | bsz 163.4 | num_updates 256636 | best_loss 3.385
2024-01-31 22:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 256636 updates
2024-01-31 22:34:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint174.pt
2024-01-31 22:34:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint174.pt
2024-01-31 22:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint174.pt (epoch 174 @ 256636 updates, score 3.559) (writing took 2.721659807022661 seconds)
2024-01-31 22:34:22 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2024-01-31 22:34:22 | INFO | train | epoch 174 | loss 2.177 | nll_loss 0.525 | ppl 1.44 | wps 56495.6 | ups 14.98 | wpb 3771.6 | bsz 195.9 | num_updates 256636 | lr 6.24225e-05 | gnorm 1.011 | loss_scale 16 | train_wall 96 | gb_free 29.8 | wall 113
2024-01-31 22:34:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:34:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:34:22 | INFO | fairseq.trainer | begin training epoch 175
2024-01-31 22:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:34:27 | INFO | train_inner | epoch 175:     64 / 1475 loss=2.177, nll_loss=0.523, ppl=1.44, wps=34640.1, ups=9.05, wpb=3827.8, bsz=187.2, num_updates=256700, lr=6.24147e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.7, wall=117
2024-01-31 22:34:33 | INFO | train_inner | epoch 175:    164 / 1475 loss=2.163, nll_loss=0.509, ppl=1.42, wps=59566, ups=15.68, wpb=3798.3, bsz=198.4, num_updates=256800, lr=6.24026e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.7, wall=124
2024-01-31 22:34:39 | INFO | train_inner | epoch 175:    264 / 1475 loss=2.163, nll_loss=0.508, ppl=1.42, wps=60167.1, ups=15.81, wpb=3804.9, bsz=203.8, num_updates=256900, lr=6.23904e-05, gnorm=0.972, loss_scale=16, train_wall=6, gb_free=29.7, wall=130
2024-01-31 22:34:46 | INFO | train_inner | epoch 175:    364 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=59613.4, ups=15.8, wpb=3774.1, bsz=195.6, num_updates=257000, lr=6.23783e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.6, wall=136
2024-01-31 22:34:52 | INFO | train_inner | epoch 175:    464 / 1475 loss=2.173, nll_loss=0.52, ppl=1.43, wps=60294.2, ups=15.78, wpb=3820.6, bsz=200.1, num_updates=257100, lr=6.23662e-05, gnorm=0.976, loss_scale=16, train_wall=6, gb_free=29.7, wall=143
2024-01-31 22:34:58 | INFO | train_inner | epoch 175:    564 / 1475 loss=2.173, nll_loss=0.52, ppl=1.43, wps=59403.8, ups=15.79, wpb=3761.8, bsz=192.2, num_updates=257200, lr=6.2354e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.7, wall=149
2024-01-31 22:35:05 | INFO | train_inner | epoch 175:    664 / 1475 loss=2.168, nll_loss=0.514, ppl=1.43, wps=59437.4, ups=15.88, wpb=3743.1, bsz=198.5, num_updates=257300, lr=6.23419e-05, gnorm=0.999, loss_scale=16, train_wall=6, gb_free=29.7, wall=155
2024-01-31 22:35:11 | INFO | train_inner | epoch 175:    764 / 1475 loss=2.176, nll_loss=0.523, ppl=1.44, wps=58534.1, ups=15.9, wpb=3682.5, bsz=196.6, num_updates=257400, lr=6.23298e-05, gnorm=1.024, loss_scale=16, train_wall=6, gb_free=29.7, wall=162
2024-01-31 22:35:17 | INFO | train_inner | epoch 175:    864 / 1475 loss=2.184, nll_loss=0.532, ppl=1.45, wps=60539.1, ups=15.82, wpb=3825.8, bsz=193.7, num_updates=257500, lr=6.23177e-05, gnorm=1.01, loss_scale=16, train_wall=6, gb_free=29.8, wall=168
2024-01-31 22:35:23 | INFO | train_inner | epoch 175:    964 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=59321.8, ups=15.7, wpb=3779.5, bsz=211.7, num_updates=257600, lr=6.23056e-05, gnorm=1.013, loss_scale=16, train_wall=6, gb_free=29.6, wall=174
2024-01-31 22:35:30 | INFO | train_inner | epoch 175:   1064 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=59723.8, ups=15.95, wpb=3744.9, bsz=184.6, num_updates=257700, lr=6.22935e-05, gnorm=1.047, loss_scale=16, train_wall=6, gb_free=29.8, wall=181
2024-01-31 22:35:36 | INFO | train_inner | epoch 175:   1164 / 1475 loss=2.181, nll_loss=0.529, ppl=1.44, wps=59164.1, ups=15.77, wpb=3750.6, bsz=196.8, num_updates=257800, lr=6.22814e-05, gnorm=1.011, loss_scale=16, train_wall=6, gb_free=29.7, wall=187
2024-01-31 22:35:42 | INFO | train_inner | epoch 175:   1264 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=59506.8, ups=15.88, wpb=3747.8, bsz=189.8, num_updates=257900, lr=6.22693e-05, gnorm=1.031, loss_scale=16, train_wall=6, gb_free=29.6, wall=193
2024-01-31 22:35:49 | INFO | train_inner | epoch 175:   1364 / 1475 loss=2.189, nll_loss=0.538, ppl=1.45, wps=60114.6, ups=15.77, wpb=3812.2, bsz=190.6, num_updates=258000, lr=6.22573e-05, gnorm=1.013, loss_scale=16, train_wall=6, gb_free=29.7, wall=200
2024-01-31 22:35:55 | INFO | train_inner | epoch 175:   1464 / 1475 loss=2.185, nll_loss=0.533, ppl=1.45, wps=60226.4, ups=15.83, wpb=3805.7, bsz=196.6, num_updates=258100, lr=6.22452e-05, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=29.7, wall=206
2024-01-31 22:35:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:35:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:35:58 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 3.564 | nll_loss 1.975 | ppl 3.93 | wps 159655 | wpb 3189.1 | bsz 163.4 | num_updates 258111 | best_loss 3.385
2024-01-31 22:35:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 258111 updates
2024-01-31 22:35:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint175.pt
2024-01-31 22:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint175.pt
2024-01-31 22:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint175.pt (epoch 175 @ 258111 updates, score 3.564) (writing took 2.3799893989926204 seconds)
2024-01-31 22:36:00 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2024-01-31 22:36:00 | INFO | train | epoch 175 | loss 2.176 | nll_loss 0.523 | ppl 1.44 | wps 56964.6 | ups 15.1 | wpb 3771.6 | bsz 195.9 | num_updates 258111 | lr 6.22439e-05 | gnorm 1.007 | loss_scale 16 | train_wall 88 | gb_free 29.6 | wall 211
2024-01-31 22:36:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:36:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:36:00 | INFO | fairseq.trainer | begin training epoch 176
2024-01-31 22:36:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:36:06 | INFO | train_inner | epoch 176:     89 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=35028.1, ups=9.29, wpb=3771.3, bsz=186.2, num_updates=258200, lr=6.22332e-05, gnorm=1.017, loss_scale=16, train_wall=6, gb_free=29.8, wall=217
2024-01-31 22:36:12 | INFO | train_inner | epoch 176:    189 / 1475 loss=2.158, nll_loss=0.503, ppl=1.42, wps=59411.4, ups=15.81, wpb=3756.7, bsz=197.1, num_updates=258300, lr=6.22211e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.8, wall=223
2024-01-31 22:36:18 | INFO | train_inner | epoch 176:    289 / 1475 loss=2.162, nll_loss=0.507, ppl=1.42, wps=59598.5, ups=15.77, wpb=3779, bsz=201, num_updates=258400, lr=6.22091e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.7, wall=229
2024-01-31 22:36:25 | INFO | train_inner | epoch 176:    389 / 1475 loss=2.178, nll_loss=0.524, ppl=1.44, wps=60283.7, ups=15.82, wpb=3810.8, bsz=188.3, num_updates=258500, lr=6.2197e-05, gnorm=1.008, loss_scale=16, train_wall=6, gb_free=29.7, wall=236
2024-01-31 22:36:31 | INFO | train_inner | epoch 176:    489 / 1475 loss=2.178, nll_loss=0.525, ppl=1.44, wps=60020.5, ups=15.78, wpb=3803.1, bsz=198.5, num_updates=258600, lr=6.2185e-05, gnorm=0.985, loss_scale=16, train_wall=6, gb_free=29.7, wall=242
2024-01-31 22:36:37 | INFO | train_inner | epoch 176:    589 / 1475 loss=2.166, nll_loss=0.512, ppl=1.43, wps=59672.6, ups=15.74, wpb=3791.7, bsz=209.4, num_updates=258700, lr=6.2173e-05, gnorm=0.965, loss_scale=16, train_wall=6, gb_free=29.8, wall=248
2024-01-31 22:36:44 | INFO | train_inner | epoch 176:    689 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=59402, ups=15.82, wpb=3755.4, bsz=201.7, num_updates=258800, lr=6.2161e-05, gnorm=1.008, loss_scale=16, train_wall=6, gb_free=29.6, wall=255
2024-01-31 22:36:50 | INFO | train_inner | epoch 176:    789 / 1475 loss=2.168, nll_loss=0.515, ppl=1.43, wps=58979.9, ups=15.76, wpb=3742, bsz=200.4, num_updates=258900, lr=6.2149e-05, gnorm=1.015, loss_scale=16, train_wall=6, gb_free=29.8, wall=261
2024-01-31 22:36:56 | INFO | train_inner | epoch 176:    889 / 1475 loss=2.168, nll_loss=0.515, ppl=1.43, wps=58737.7, ups=15.8, wpb=3717.8, bsz=210, num_updates=259000, lr=6.2137e-05, gnorm=0.987, loss_scale=16, train_wall=6, gb_free=29.8, wall=267
2024-01-31 22:37:03 | INFO | train_inner | epoch 176:    989 / 1475 loss=2.19, nll_loss=0.538, ppl=1.45, wps=60379.3, ups=15.88, wpb=3802.1, bsz=184.3, num_updates=259100, lr=6.2125e-05, gnorm=1.021, loss_scale=16, train_wall=6, gb_free=29.7, wall=274
2024-01-31 22:37:09 | INFO | train_inner | epoch 176:   1089 / 1475 loss=2.17, nll_loss=0.518, ppl=1.43, wps=59481.7, ups=15.74, wpb=3780, bsz=203.2, num_updates=259200, lr=6.2113e-05, gnorm=0.993, loss_scale=16, train_wall=6, gb_free=29.7, wall=280
2024-01-31 22:37:15 | INFO | train_inner | epoch 176:   1189 / 1475 loss=2.185, nll_loss=0.534, ppl=1.45, wps=60301.6, ups=15.85, wpb=3805.4, bsz=189.8, num_updates=259300, lr=6.2101e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.7, wall=286
2024-01-31 22:37:22 | INFO | train_inner | epoch 176:   1289 / 1475 loss=2.195, nll_loss=0.544, ppl=1.46, wps=58447.8, ups=15.87, wpb=3683, bsz=182.2, num_updates=259400, lr=6.2089e-05, gnorm=1.064, loss_scale=16, train_wall=6, gb_free=29.7, wall=293
2024-01-31 22:37:28 | INFO | train_inner | epoch 176:   1389 / 1475 loss=2.187, nll_loss=0.536, ppl=1.45, wps=59611, ups=15.9, wpb=3749.7, bsz=193, num_updates=259500, lr=6.20771e-05, gnorm=1.025, loss_scale=16, train_wall=6, gb_free=29.6, wall=299
2024-01-31 22:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:37:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:37:35 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 3.566 | nll_loss 1.977 | ppl 3.94 | wps 159118 | wpb 3189.1 | bsz 163.4 | num_updates 259586 | best_loss 3.385
2024-01-31 22:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 259586 updates
2024-01-31 22:37:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint176.pt
2024-01-31 22:37:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint176.pt
2024-01-31 22:37:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint176.pt (epoch 176 @ 259586 updates, score 3.566) (writing took 2.4761509409872815 seconds)
2024-01-31 22:37:38 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2024-01-31 22:37:38 | INFO | train | epoch 176 | loss 2.175 | nll_loss 0.523 | ppl 1.44 | wps 56897.1 | ups 15.09 | wpb 3771.6 | bsz 195.9 | num_updates 259586 | lr 6.20668e-05 | gnorm 1.004 | loss_scale 16 | train_wall 88 | gb_free 29.7 | wall 309
2024-01-31 22:37:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:37:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:37:38 | INFO | fairseq.trainer | begin training epoch 177
2024-01-31 22:37:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:37:39 | INFO | train_inner | epoch 177:     14 / 1475 loss=2.196, nll_loss=0.544, ppl=1.46, wps=34790.4, ups=9.31, wpb=3737.6, bsz=184.2, num_updates=259600, lr=6.20651e-05, gnorm=1.041, loss_scale=16, train_wall=6, gb_free=29.6, wall=310
2024-01-31 22:37:45 | INFO | train_inner | epoch 177:    114 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=59583.1, ups=16.02, wpb=3719.1, bsz=185.8, num_updates=259700, lr=6.20532e-05, gnorm=1.024, loss_scale=16, train_wall=6, gb_free=29.8, wall=316
2024-01-31 22:37:51 | INFO | train_inner | epoch 177:    214 / 1475 loss=2.17, nll_loss=0.515, ppl=1.43, wps=59226, ups=15.76, wpb=3756.9, bsz=190.2, num_updates=259800, lr=6.20412e-05, gnorm=1.001, loss_scale=16, train_wall=6, gb_free=29.7, wall=322
2024-01-31 22:37:58 | INFO | train_inner | epoch 177:    314 / 1475 loss=2.16, nll_loss=0.504, ppl=1.42, wps=58979, ups=15.82, wpb=3728.7, bsz=195, num_updates=259900, lr=6.20293e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.8, wall=329
2024-01-31 22:38:04 | INFO | train_inner | epoch 177:    414 / 1475 loss=2.172, nll_loss=0.519, ppl=1.43, wps=60087.5, ups=15.83, wpb=3796.7, bsz=195.1, num_updates=260000, lr=6.20174e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.8, wall=335
2024-01-31 22:38:10 | INFO | train_inner | epoch 177:    514 / 1475 loss=2.159, nll_loss=0.505, ppl=1.42, wps=60081, ups=15.74, wpb=3816.9, bsz=211.5, num_updates=260100, lr=6.20054e-05, gnorm=0.972, loss_scale=16, train_wall=6, gb_free=29.6, wall=341
2024-01-31 22:38:17 | INFO | train_inner | epoch 177:    614 / 1475 loss=2.161, nll_loss=0.507, ppl=1.42, wps=59918.5, ups=15.74, wpb=3807.3, bsz=210.6, num_updates=260200, lr=6.19935e-05, gnorm=0.978, loss_scale=16, train_wall=6, gb_free=29.7, wall=348
2024-01-31 22:38:23 | INFO | train_inner | epoch 177:    714 / 1475 loss=2.173, nll_loss=0.52, ppl=1.43, wps=59212.5, ups=15.74, wpb=3760.9, bsz=197.4, num_updates=260300, lr=6.19816e-05, gnorm=0.997, loss_scale=16, train_wall=6, gb_free=29.7, wall=354
2024-01-31 22:38:29 | INFO | train_inner | epoch 177:    814 / 1475 loss=2.18, nll_loss=0.528, ppl=1.44, wps=60413.2, ups=15.77, wpb=3831.5, bsz=198.2, num_updates=260400, lr=6.19697e-05, gnorm=1.03, loss_scale=16, train_wall=6, gb_free=29.6, wall=360
2024-01-31 22:38:36 | INFO | train_inner | epoch 177:    914 / 1475 loss=2.177, nll_loss=0.525, ppl=1.44, wps=59165.9, ups=15.81, wpb=3742.6, bsz=191.1, num_updates=260500, lr=6.19578e-05, gnorm=1.023, loss_scale=16, train_wall=6, gb_free=29.7, wall=367
2024-01-31 22:38:42 | INFO | train_inner | epoch 177:   1014 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=58065.8, ups=15.7, wpb=3697.4, bsz=203.3, num_updates=260600, lr=6.19459e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.7, wall=373
2024-01-31 22:38:48 | INFO | train_inner | epoch 177:   1114 / 1475 loss=2.182, nll_loss=0.53, ppl=1.44, wps=59920.7, ups=15.74, wpb=3805.9, bsz=192.5, num_updates=260700, lr=6.19341e-05, gnorm=1.02, loss_scale=16, train_wall=6, gb_free=29.7, wall=379
2024-01-31 22:38:55 | INFO | train_inner | epoch 177:   1214 / 1475 loss=2.185, nll_loss=0.534, ppl=1.45, wps=59125.4, ups=15.8, wpb=3742.9, bsz=186.3, num_updates=260800, lr=6.19222e-05, gnorm=1.045, loss_scale=16, train_wall=6, gb_free=29.7, wall=386
2024-01-31 22:39:01 | INFO | train_inner | epoch 177:   1314 / 1475 loss=2.192, nll_loss=0.542, ppl=1.46, wps=60228.4, ups=15.85, wpb=3799.8, bsz=191.9, num_updates=260900, lr=6.19103e-05, gnorm=1.025, loss_scale=16, train_wall=6, gb_free=29.8, wall=392
2024-01-31 22:39:07 | INFO | train_inner | epoch 177:   1414 / 1475 loss=2.191, nll_loss=0.54, ppl=1.45, wps=59737.7, ups=15.82, wpb=3776.1, bsz=190.7, num_updates=261000, lr=6.18984e-05, gnorm=1.042, loss_scale=16, train_wall=6, gb_free=29.7, wall=398
2024-01-31 22:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:39:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:39:13 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 3.566 | nll_loss 1.98 | ppl 3.94 | wps 158475 | wpb 3189.1 | bsz 163.4 | num_updates 261061 | best_loss 3.385
2024-01-31 22:39:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 261061 updates
2024-01-31 22:39:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint177.pt
2024-01-31 22:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint177.pt
2024-01-31 22:39:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint177.pt (epoch 177 @ 261061 updates, score 3.566) (writing took 2.42350803501904 seconds)
2024-01-31 22:39:16 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2024-01-31 22:39:16 | INFO | train | epoch 177 | loss 2.174 | nll_loss 0.521 | ppl 1.44 | wps 56864.7 | ups 15.08 | wpb 3771.6 | bsz 195.9 | num_updates 261061 | lr 6.18912e-05 | gnorm 1.009 | loss_scale 16 | train_wall 89 | gb_free 29.7 | wall 407
2024-01-31 22:39:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:39:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:39:16 | INFO | fairseq.trainer | begin training epoch 178
2024-01-31 22:39:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:39:18 | INFO | train_inner | epoch 178:     39 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=35590, ups=9.29, wpb=3831.1, bsz=208, num_updates=261100, lr=6.18866e-05, gnorm=0.978, loss_scale=16, train_wall=6, gb_free=29.8, wall=409
2024-01-31 22:39:25 | INFO | train_inner | epoch 178:    139 / 1475 loss=2.151, nll_loss=0.495, ppl=1.41, wps=59507.6, ups=15.81, wpb=3763, bsz=202.2, num_updates=261200, lr=6.18747e-05, gnorm=0.977, loss_scale=16, train_wall=6, gb_free=29.7, wall=415
2024-01-31 22:39:31 | INFO | train_inner | epoch 178:    239 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=59473.5, ups=15.77, wpb=3771.8, bsz=188.2, num_updates=261300, lr=6.18629e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.7, wall=422
2024-01-31 22:39:37 | INFO | train_inner | epoch 178:    339 / 1475 loss=2.157, nll_loss=0.502, ppl=1.42, wps=59376.3, ups=15.76, wpb=3766.6, bsz=200.9, num_updates=261400, lr=6.18511e-05, gnorm=0.992, loss_scale=16, train_wall=6, gb_free=29.7, wall=428
2024-01-31 22:39:44 | INFO | train_inner | epoch 178:    439 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=58608.9, ups=15.75, wpb=3721.8, bsz=199.4, num_updates=261500, lr=6.18392e-05, gnorm=0.999, loss_scale=16, train_wall=6, gb_free=29.7, wall=434
2024-01-31 22:39:50 | INFO | train_inner | epoch 178:    539 / 1475 loss=2.172, nll_loss=0.519, ppl=1.43, wps=59680.8, ups=15.78, wpb=3781.2, bsz=194.7, num_updates=261600, lr=6.18274e-05, gnorm=1.029, loss_scale=16, train_wall=6, gb_free=29.6, wall=441
2024-01-31 22:39:56 | INFO | train_inner | epoch 178:    639 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=59959.5, ups=15.88, wpb=3776, bsz=191.6, num_updates=261700, lr=6.18156e-05, gnorm=1.013, loss_scale=16, train_wall=6, gb_free=29.7, wall=447
2024-01-31 22:40:03 | INFO | train_inner | epoch 178:    739 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=59285.7, ups=15.62, wpb=3795.7, bsz=208.5, num_updates=261800, lr=6.18038e-05, gnorm=0.991, loss_scale=16, train_wall=6, gb_free=29.7, wall=454
2024-01-31 22:40:09 | INFO | train_inner | epoch 178:    839 / 1475 loss=2.18, nll_loss=0.527, ppl=1.44, wps=59018.5, ups=15.92, wpb=3707, bsz=185.7, num_updates=261900, lr=6.1792e-05, gnorm=1.043, loss_scale=16, train_wall=6, gb_free=29.7, wall=460
2024-01-31 22:40:15 | INFO | train_inner | epoch 178:    939 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=58376.3, ups=15.79, wpb=3697.6, bsz=189.2, num_updates=262000, lr=6.17802e-05, gnorm=1.044, loss_scale=16, train_wall=6, gb_free=29.7, wall=466
2024-01-31 22:40:22 | INFO | train_inner | epoch 178:   1039 / 1475 loss=2.19, nll_loss=0.539, ppl=1.45, wps=60170.5, ups=15.73, wpb=3826.3, bsz=190.4, num_updates=262100, lr=6.17684e-05, gnorm=1.029, loss_scale=16, train_wall=6, gb_free=29.7, wall=472
2024-01-31 22:40:28 | INFO | train_inner | epoch 178:   1139 / 1475 loss=2.177, nll_loss=0.524, ppl=1.44, wps=59613.6, ups=15.71, wpb=3794.3, bsz=202.1, num_updates=262200, lr=6.17566e-05, gnorm=1.009, loss_scale=16, train_wall=6, gb_free=29.7, wall=479
2024-01-31 22:40:34 | INFO | train_inner | epoch 178:   1239 / 1475 loss=2.186, nll_loss=0.535, ppl=1.45, wps=58923.1, ups=15.78, wpb=3734.2, bsz=199.1, num_updates=262300, lr=6.17449e-05, gnorm=1.037, loss_scale=16, train_wall=6, gb_free=29.7, wall=485
2024-01-31 22:40:41 | INFO | train_inner | epoch 178:   1339 / 1475 loss=2.194, nll_loss=0.543, ppl=1.46, wps=60847.4, ups=15.91, wpb=3824.2, bsz=187.2, num_updates=262400, lr=6.17331e-05, gnorm=1.045, loss_scale=16, train_wall=6, gb_free=29.8, wall=491
2024-01-31 22:40:47 | INFO | train_inner | epoch 178:   1439 / 1475 loss=2.184, nll_loss=0.534, ppl=1.45, wps=60160.1, ups=15.81, wpb=3805.4, bsz=195.9, num_updates=262500, lr=6.17213e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.7, wall=498
2024-01-31 22:40:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:40:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:40:51 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 3.567 | nll_loss 1.979 | ppl 3.94 | wps 158530 | wpb 3189.1 | bsz 163.4 | num_updates 262536 | best_loss 3.385
2024-01-31 22:40:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 262536 updates
2024-01-31 22:40:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint178.pt
2024-01-31 22:40:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint178.pt
2024-01-31 22:40:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint178.pt (epoch 178 @ 262536 updates, score 3.567) (writing took 2.436439797980711 seconds)
2024-01-31 22:40:54 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2024-01-31 22:40:54 | INFO | train | epoch 178 | loss 2.174 | nll_loss 0.521 | ppl 1.43 | wps 56843.3 | ups 15.07 | wpb 3771.6 | bsz 195.9 | num_updates 262536 | lr 6.17171e-05 | gnorm 1.014 | loss_scale 16 | train_wall 89 | gb_free 29.6 | wall 504
2024-01-31 22:40:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:40:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:40:54 | INFO | fairseq.trainer | begin training epoch 179
2024-01-31 22:40:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:40:58 | INFO | train_inner | epoch 179:     64 / 1475 loss=2.157, nll_loss=0.502, ppl=1.42, wps=35178.8, ups=9.28, wpb=3788.9, bsz=203.7, num_updates=262600, lr=6.17096e-05, gnorm=0.97, loss_scale=16, train_wall=6, gb_free=29.7, wall=509
2024-01-31 22:41:04 | INFO | train_inner | epoch 179:    164 / 1475 loss=2.173, nll_loss=0.518, ppl=1.43, wps=60518.2, ups=15.83, wpb=3821.9, bsz=187.1, num_updates=262700, lr=6.16978e-05, gnorm=1.003, loss_scale=16, train_wall=6, gb_free=29.7, wall=515
2024-01-31 22:41:10 | INFO | train_inner | epoch 179:    264 / 1475 loss=2.154, nll_loss=0.499, ppl=1.41, wps=59774.9, ups=15.75, wpb=3794.4, bsz=208.6, num_updates=262800, lr=6.16861e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.7, wall=521
2024-01-31 22:41:17 | INFO | train_inner | epoch 179:    364 / 1475 loss=2.171, nll_loss=0.517, ppl=1.43, wps=60370.1, ups=15.88, wpb=3802.1, bsz=184.9, num_updates=262900, lr=6.16744e-05, gnorm=1.039, loss_scale=16, train_wall=6, gb_free=29.7, wall=528
2024-01-31 22:41:23 | INFO | train_inner | epoch 179:    464 / 1475 loss=2.17, nll_loss=0.517, ppl=1.43, wps=60622.6, ups=15.79, wpb=3839.3, bsz=195.4, num_updates=263000, lr=6.16626e-05, gnorm=0.995, loss_scale=16, train_wall=6, gb_free=29.7, wall=534
2024-01-31 22:41:29 | INFO | train_inner | epoch 179:    564 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=58765.7, ups=15.68, wpb=3747.1, bsz=200, num_updates=263100, lr=6.16509e-05, gnorm=1.014, loss_scale=16, train_wall=6, gb_free=29.7, wall=540
2024-01-31 22:41:36 | INFO | train_inner | epoch 179:    664 / 1475 loss=2.17, nll_loss=0.518, ppl=1.43, wps=59646.7, ups=15.74, wpb=3788.6, bsz=199.1, num_updates=263200, lr=6.16392e-05, gnorm=0.996, loss_scale=16, train_wall=6, gb_free=29.7, wall=547
2024-01-31 22:41:42 | INFO | train_inner | epoch 179:    764 / 1475 loss=2.162, nll_loss=0.508, ppl=1.42, wps=59414, ups=15.71, wpb=3782.6, bsz=210.3, num_updates=263300, lr=6.16275e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.8, wall=553
2024-01-31 22:41:48 | INFO | train_inner | epoch 179:    864 / 1475 loss=2.171, nll_loss=0.518, ppl=1.43, wps=58067.6, ups=15.7, wpb=3697.8, bsz=199.8, num_updates=263400, lr=6.16158e-05, gnorm=1.021, loss_scale=16, train_wall=6, gb_free=29.7, wall=559
2024-01-31 22:41:55 | INFO | train_inner | epoch 179:    964 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=59430.9, ups=15.83, wpb=3755, bsz=197.9, num_updates=263500, lr=6.16041e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=566
2024-01-31 22:42:01 | INFO | train_inner | epoch 179:   1064 / 1475 loss=2.187, nll_loss=0.537, ppl=1.45, wps=60337.3, ups=15.75, wpb=3831.3, bsz=188.7, num_updates=263600, lr=6.15924e-05, gnorm=1.018, loss_scale=16, train_wall=6, gb_free=29.8, wall=572
2024-01-31 22:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 22:42:07 | INFO | train_inner | epoch 179:   1165 / 1475 loss=2.185, nll_loss=0.534, ppl=1.45, wps=59413, ups=15.76, wpb=3770.3, bsz=189.3, num_updates=263700, lr=6.15807e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.6, wall=578
2024-01-31 22:42:14 | INFO | train_inner | epoch 179:   1265 / 1475 loss=2.18, nll_loss=0.528, ppl=1.44, wps=58957.7, ups=15.91, wpb=3706, bsz=199.8, num_updates=263800, lr=6.15691e-05, gnorm=1.055, loss_scale=8, train_wall=6, gb_free=29.6, wall=585
2024-01-31 22:42:20 | INFO | train_inner | epoch 179:   1365 / 1475 loss=2.181, nll_loss=0.529, ppl=1.44, wps=58766.9, ups=15.78, wpb=3723.8, bsz=188.9, num_updates=263900, lr=6.15574e-05, gnorm=1.047, loss_scale=8, train_wall=6, gb_free=29.7, wall=591
2024-01-31 22:42:26 | INFO | train_inner | epoch 179:   1465 / 1475 loss=2.18, nll_loss=0.528, ppl=1.44, wps=59473.5, ups=15.83, wpb=3756.3, bsz=195.3, num_updates=264000, lr=6.15457e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.8, wall=597
2024-01-31 22:42:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:42:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:42:29 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 3.568 | nll_loss 1.977 | ppl 3.94 | wps 159525 | wpb 3189.1 | bsz 163.4 | num_updates 264010 | best_loss 3.385
2024-01-31 22:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 264010 updates
2024-01-31 22:42:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint179.pt
2024-01-31 22:42:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint179.pt
2024-01-31 22:42:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint179.pt (epoch 179 @ 264010 updates, score 3.568) (writing took 2.3486791159957647 seconds)
2024-01-31 22:42:31 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2024-01-31 22:42:31 | INFO | train | epoch 179 | loss 2.172 | nll_loss 0.519 | ppl 1.43 | wps 56841.7 | ups 15.07 | wpb 3771.4 | bsz 196 | num_updates 264010 | lr 6.15446e-05 | gnorm 1.012 | loss_scale 8 | train_wall 88 | gb_free 29.8 | wall 602
2024-01-31 22:42:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:42:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:42:31 | INFO | fairseq.trainer | begin training epoch 180
2024-01-31 22:42:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:42:37 | INFO | train_inner | epoch 180:     90 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=34966.2, ups=9.33, wpb=3747.7, bsz=187.7, num_updates=264100, lr=6.15341e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.6, wall=608
2024-01-31 22:42:43 | INFO | train_inner | epoch 180:    190 / 1475 loss=2.155, nll_loss=0.499, ppl=1.41, wps=58909.2, ups=15.8, wpb=3727.3, bsz=190.3, num_updates=264200, lr=6.15224e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.8, wall=614
2024-01-31 22:42:50 | INFO | train_inner | epoch 180:    290 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=60099.9, ups=15.88, wpb=3784.4, bsz=187.1, num_updates=264300, lr=6.15108e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=621
2024-01-31 22:42:56 | INFO | train_inner | epoch 180:    390 / 1475 loss=2.169, nll_loss=0.515, ppl=1.43, wps=61358.1, ups=15.83, wpb=3876.9, bsz=199, num_updates=264400, lr=6.14992e-05, gnorm=1.05, loss_scale=8, train_wall=6, gb_free=29.7, wall=627
2024-01-31 22:43:02 | INFO | train_inner | epoch 180:    490 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=59306.5, ups=15.75, wpb=3766.5, bsz=206.8, num_updates=264500, lr=6.14875e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=633
2024-01-31 22:43:09 | INFO | train_inner | epoch 180:    590 / 1475 loss=2.163, nll_loss=0.509, ppl=1.42, wps=59325.7, ups=15.71, wpb=3775.9, bsz=205.3, num_updates=264600, lr=6.14759e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=640
2024-01-31 22:43:15 | INFO | train_inner | epoch 180:    690 / 1475 loss=2.175, nll_loss=0.522, ppl=1.44, wps=59468.3, ups=15.85, wpb=3752.6, bsz=190, num_updates=264700, lr=6.14643e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=646
2024-01-31 22:43:21 | INFO | train_inner | epoch 180:    790 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=58727.5, ups=15.78, wpb=3722.8, bsz=204, num_updates=264800, lr=6.14527e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=652
2024-01-31 22:43:28 | INFO | train_inner | epoch 180:    890 / 1475 loss=2.18, nll_loss=0.528, ppl=1.44, wps=59786.3, ups=15.73, wpb=3800.5, bsz=190.5, num_updates=264900, lr=6.14411e-05, gnorm=1.027, loss_scale=8, train_wall=6, gb_free=29.7, wall=659
2024-01-31 22:43:34 | INFO | train_inner | epoch 180:    990 / 1475 loss=2.184, nll_loss=0.533, ppl=1.45, wps=60398.4, ups=15.77, wpb=3829.6, bsz=186.6, num_updates=265000, lr=6.14295e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.7, wall=665
2024-01-31 22:43:40 | INFO | train_inner | epoch 180:   1090 / 1475 loss=2.175, nll_loss=0.523, ppl=1.44, wps=59261.1, ups=15.87, wpb=3734.8, bsz=201.9, num_updates=265100, lr=6.14179e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.8, wall=671
2024-01-31 22:43:47 | INFO | train_inner | epoch 180:   1190 / 1475 loss=2.17, nll_loss=0.517, ppl=1.43, wps=59678.8, ups=15.8, wpb=3777.4, bsz=205.6, num_updates=265200, lr=6.14063e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.7, wall=678
2024-01-31 22:43:53 | INFO | train_inner | epoch 180:   1290 / 1475 loss=2.181, nll_loss=0.53, ppl=1.44, wps=58687.1, ups=15.82, wpb=3709.2, bsz=192.3, num_updates=265300, lr=6.13948e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.9, wall=684
2024-01-31 22:43:59 | INFO | train_inner | epoch 180:   1390 / 1475 loss=2.179, nll_loss=0.527, ppl=1.44, wps=59363.3, ups=15.74, wpb=3770.8, bsz=194.2, num_updates=265400, lr=6.13832e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=690
2024-01-31 22:44:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:44:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:44:07 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 3.564 | nll_loss 1.975 | ppl 3.93 | wps 160369 | wpb 3189.1 | bsz 163.4 | num_updates 265485 | best_loss 3.385
2024-01-31 22:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 265485 updates
2024-01-31 22:44:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint180.pt
2024-01-31 22:44:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint180.pt
2024-01-31 22:44:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint180.pt (epoch 180 @ 265485 updates, score 3.564) (writing took 2.4929596729343757 seconds)
2024-01-31 22:44:09 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2024-01-31 22:44:09 | INFO | train | epoch 180 | loss 2.171 | nll_loss 0.518 | ppl 1.43 | wps 56804.4 | ups 15.06 | wpb 3771.6 | bsz 195.9 | num_updates 265485 | lr 6.13734e-05 | gnorm 1.015 | loss_scale 8 | train_wall 89 | gb_free 29.7 | wall 700
2024-01-31 22:44:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:44:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:44:09 | INFO | fairseq.trainer | begin training epoch 181
2024-01-31 22:44:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:44:10 | INFO | train_inner | epoch 181:     15 / 1475 loss=2.179, nll_loss=0.527, ppl=1.44, wps=34923.1, ups=9.19, wpb=3800.4, bsz=195.1, num_updates=265500, lr=6.13716e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=701
2024-01-31 22:44:17 | INFO | train_inner | epoch 181:    115 / 1475 loss=2.16, nll_loss=0.505, ppl=1.42, wps=60538.3, ups=15.76, wpb=3841.5, bsz=200.1, num_updates=265600, lr=6.13601e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.7, wall=708
2024-01-31 22:44:23 | INFO | train_inner | epoch 181:    215 / 1475 loss=2.17, nll_loss=0.516, ppl=1.43, wps=60310.2, ups=15.85, wpb=3805.8, bsz=183.8, num_updates=265700, lr=6.13485e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.6, wall=714
2024-01-31 22:44:29 | INFO | train_inner | epoch 181:    315 / 1475 loss=2.165, nll_loss=0.511, ppl=1.42, wps=59146.7, ups=15.8, wpb=3743.9, bsz=196.1, num_updates=265800, lr=6.1337e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.9, wall=720
2024-01-31 22:44:36 | INFO | train_inner | epoch 181:    415 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=58394.6, ups=15.7, wpb=3718.6, bsz=206.4, num_updates=265900, lr=6.13255e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.6, wall=727
2024-01-31 22:44:42 | INFO | train_inner | epoch 181:    515 / 1475 loss=2.155, nll_loss=0.5, ppl=1.41, wps=59419.6, ups=15.74, wpb=3776, bsz=203.8, num_updates=266000, lr=6.13139e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.8, wall=733
2024-01-31 22:44:48 | INFO | train_inner | epoch 181:    615 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=60313.9, ups=15.77, wpb=3823.5, bsz=192.2, num_updates=266100, lr=6.13024e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=739
2024-01-31 22:44:55 | INFO | train_inner | epoch 181:    715 / 1475 loss=2.173, nll_loss=0.52, ppl=1.43, wps=59153.2, ups=15.87, wpb=3728.1, bsz=188.6, num_updates=266200, lr=6.12909e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=746
2024-01-31 22:45:01 | INFO | train_inner | epoch 181:    815 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=59895.7, ups=15.85, wpb=3779.5, bsz=195.8, num_updates=266300, lr=6.12794e-05, gnorm=1.027, loss_scale=8, train_wall=6, gb_free=29.7, wall=752
2024-01-31 22:45:07 | INFO | train_inner | epoch 181:    915 / 1475 loss=2.172, nll_loss=0.519, ppl=1.43, wps=60165.2, ups=15.83, wpb=3801.2, bsz=193.5, num_updates=266400, lr=6.12679e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=758
2024-01-31 22:45:14 | INFO | train_inner | epoch 181:   1015 / 1475 loss=2.185, nll_loss=0.534, ppl=1.45, wps=60243.8, ups=15.82, wpb=3807.8, bsz=187.8, num_updates=266500, lr=6.12564e-05, gnorm=1.033, loss_scale=8, train_wall=6, gb_free=29.7, wall=764
2024-01-31 22:45:20 | INFO | train_inner | epoch 181:   1115 / 1475 loss=2.178, nll_loss=0.526, ppl=1.44, wps=59779.8, ups=15.79, wpb=3785.7, bsz=194, num_updates=266600, lr=6.12449e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=771
2024-01-31 22:45:26 | INFO | train_inner | epoch 181:   1215 / 1475 loss=2.167, nll_loss=0.515, ppl=1.43, wps=59252.4, ups=15.73, wpb=3766.5, bsz=210.2, num_updates=266700, lr=6.12334e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.6, wall=777
2024-01-31 22:45:33 | INFO | train_inner | epoch 181:   1315 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=58382.8, ups=15.76, wpb=3705.1, bsz=197.8, num_updates=266800, lr=6.12219e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.6, wall=784
2024-01-31 22:45:39 | INFO | train_inner | epoch 181:   1415 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=58995.7, ups=15.82, wpb=3728.3, bsz=199.3, num_updates=266900, lr=6.12105e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.7, wall=790
2024-01-31 22:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:45:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:45:45 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 3.569 | nll_loss 1.98 | ppl 3.94 | wps 159986 | wpb 3189.1 | bsz 163.4 | num_updates 266960 | best_loss 3.385
2024-01-31 22:45:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 266960 updates
2024-01-31 22:45:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint181.pt
2024-01-31 22:45:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint181.pt
2024-01-31 22:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint181.pt (epoch 181 @ 266960 updates, score 3.569) (writing took 2.398413383984007 seconds)
2024-01-31 22:45:47 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2024-01-31 22:45:47 | INFO | train | epoch 181 | loss 2.17 | nll_loss 0.518 | ppl 1.43 | wps 56877.1 | ups 15.08 | wpb 3771.6 | bsz 195.9 | num_updates 266960 | lr 6.12036e-05 | gnorm 1.008 | loss_scale 8 | train_wall 88 | gb_free 29.7 | wall 798
2024-01-31 22:45:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:45:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:45:47 | INFO | fairseq.trainer | begin training epoch 182
2024-01-31 22:45:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:45:50 | INFO | train_inner | epoch 182:     40 / 1475 loss=2.17, nll_loss=0.517, ppl=1.43, wps=35102.2, ups=9.28, wpb=3781.3, bsz=192.7, num_updates=267000, lr=6.1199e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=801
2024-01-31 22:45:56 | INFO | train_inner | epoch 182:    140 / 1475 loss=2.158, nll_loss=0.503, ppl=1.42, wps=60499.9, ups=15.88, wpb=3809.6, bsz=198.6, num_updates=267100, lr=6.11875e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.8, wall=807
2024-01-31 22:46:02 | INFO | train_inner | epoch 182:    240 / 1475 loss=2.159, nll_loss=0.504, ppl=1.42, wps=59302.1, ups=15.7, wpb=3776.6, bsz=197, num_updates=267200, lr=6.11761e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.7, wall=813
2024-01-31 22:46:09 | INFO | train_inner | epoch 182:    340 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=59591.1, ups=15.78, wpb=3775.9, bsz=186.8, num_updates=267300, lr=6.11647e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.8, wall=820
2024-01-31 22:46:15 | INFO | train_inner | epoch 182:    440 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=58531.9, ups=15.69, wpb=3730.9, bsz=208.8, num_updates=267400, lr=6.11532e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.7, wall=826
2024-01-31 22:46:21 | INFO | train_inner | epoch 182:    540 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=58177.6, ups=15.93, wpb=3652.3, bsz=174.8, num_updates=267500, lr=6.11418e-05, gnorm=1.063, loss_scale=8, train_wall=6, gb_free=29.7, wall=832
2024-01-31 22:46:28 | INFO | train_inner | epoch 182:    640 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=60585.5, ups=15.73, wpb=3850.5, bsz=204.9, num_updates=267600, lr=6.11304e-05, gnorm=0.989, loss_scale=8, train_wall=6, gb_free=29.6, wall=839
2024-01-31 22:46:34 | INFO | train_inner | epoch 182:    740 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=59778.2, ups=15.82, wpb=3778.5, bsz=201.1, num_updates=267700, lr=6.11189e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.7, wall=845
2024-01-31 22:46:40 | INFO | train_inner | epoch 182:    840 / 1475 loss=2.187, nll_loss=0.535, ppl=1.45, wps=60674.3, ups=16.02, wpb=3787.3, bsz=185.8, num_updates=267800, lr=6.11075e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=851
2024-01-31 22:46:47 | INFO | train_inner | epoch 182:    940 / 1475 loss=2.172, nll_loss=0.518, ppl=1.43, wps=59647.4, ups=15.77, wpb=3781.4, bsz=193.7, num_updates=267900, lr=6.10961e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.8, wall=858
2024-01-31 22:46:53 | INFO | train_inner | epoch 182:   1040 / 1475 loss=2.173, nll_loss=0.521, ppl=1.43, wps=60777.6, ups=15.79, wpb=3849.1, bsz=204.9, num_updates=268000, lr=6.10847e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.8, wall=864
2024-01-31 22:46:59 | INFO | train_inner | epoch 182:   1140 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=58867.9, ups=15.76, wpb=3734.3, bsz=192.3, num_updates=268100, lr=6.10733e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=870
2024-01-31 22:47:06 | INFO | train_inner | epoch 182:   1240 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=59077.6, ups=15.82, wpb=3735.5, bsz=196.1, num_updates=268200, lr=6.10619e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.8, wall=877
2024-01-31 22:47:12 | INFO | train_inner | epoch 182:   1340 / 1475 loss=2.17, nll_loss=0.518, ppl=1.43, wps=59667.2, ups=15.79, wpb=3778.4, bsz=200, num_updates=268300, lr=6.10506e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=883
2024-01-31 22:47:18 | INFO | train_inner | epoch 182:   1440 / 1475 loss=2.179, nll_loss=0.528, ppl=1.44, wps=58748, ups=15.9, wpb=3694, bsz=191.4, num_updates=268400, lr=6.10392e-05, gnorm=1.066, loss_scale=8, train_wall=6, gb_free=29.9, wall=889
2024-01-31 22:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:47:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:47:22 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 3.566 | nll_loss 1.978 | ppl 3.94 | wps 159581 | wpb 3189.1 | bsz 163.4 | num_updates 268435 | best_loss 3.385
2024-01-31 22:47:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 268435 updates
2024-01-31 22:47:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint182.pt
2024-01-31 22:47:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint182.pt
2024-01-31 22:47:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint182.pt (epoch 182 @ 268435 updates, score 3.566) (writing took 2.3677870919927955 seconds)
2024-01-31 22:47:25 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2024-01-31 22:47:25 | INFO | train | epoch 182 | loss 2.169 | nll_loss 0.516 | ppl 1.43 | wps 56915.5 | ups 15.09 | wpb 3771.6 | bsz 195.9 | num_updates 268435 | lr 6.10352e-05 | gnorm 1.011 | loss_scale 8 | train_wall 88 | gb_free 29.8 | wall 896
2024-01-31 22:47:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:47:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:47:25 | INFO | fairseq.trainer | begin training epoch 183
2024-01-31 22:47:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:47:29 | INFO | train_inner | epoch 183:     65 / 1475 loss=2.158, nll_loss=0.504, ppl=1.42, wps=36004.8, ups=9.29, wpb=3876.9, bsz=207.4, num_updates=268500, lr=6.10278e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.6, wall=900
2024-01-31 22:47:35 | INFO | train_inner | epoch 183:    165 / 1475 loss=2.159, nll_loss=0.505, ppl=1.42, wps=59741.3, ups=15.78, wpb=3785.5, bsz=193.5, num_updates=268600, lr=6.10165e-05, gnorm=0.985, loss_scale=8, train_wall=6, gb_free=29.7, wall=906
2024-01-31 22:47:42 | INFO | train_inner | epoch 183:    265 / 1475 loss=2.147, nll_loss=0.491, ppl=1.41, wps=57380.2, ups=15.53, wpb=3694.2, bsz=207.5, num_updates=268700, lr=6.10051e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=913
2024-01-31 22:47:48 | INFO | train_inner | epoch 183:    365 / 1475 loss=2.16, nll_loss=0.505, ppl=1.42, wps=58304.9, ups=15.87, wpb=3673.7, bsz=190.4, num_updates=268800, lr=6.09938e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.8, wall=919
2024-01-31 22:47:54 | INFO | train_inner | epoch 183:    465 / 1475 loss=2.165, nll_loss=0.511, ppl=1.43, wps=60720.4, ups=15.85, wpb=3831.2, bsz=200, num_updates=268900, lr=6.09824e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.6, wall=925
2024-01-31 22:48:01 | INFO | train_inner | epoch 183:    565 / 1475 loss=2.17, nll_loss=0.517, ppl=1.43, wps=60320.1, ups=15.8, wpb=3818.2, bsz=193.2, num_updates=269000, lr=6.09711e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=932
2024-01-31 22:48:07 | INFO | train_inner | epoch 183:    665 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=60196.4, ups=15.78, wpb=3814.6, bsz=200.9, num_updates=269100, lr=6.09597e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.7, wall=938
2024-01-31 22:48:13 | INFO | train_inner | epoch 183:    765 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=59763.6, ups=15.71, wpb=3803.9, bsz=207.3, num_updates=269200, lr=6.09484e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=944
2024-01-31 22:48:20 | INFO | train_inner | epoch 183:    865 / 1475 loss=2.179, nll_loss=0.527, ppl=1.44, wps=59838, ups=15.81, wpb=3784.6, bsz=192.6, num_updates=269300, lr=6.09371e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=951
2024-01-31 22:48:26 | INFO | train_inner | epoch 183:    965 / 1475 loss=2.178, nll_loss=0.527, ppl=1.44, wps=59902.2, ups=15.81, wpb=3787.7, bsz=187.4, num_updates=269400, lr=6.09258e-05, gnorm=1.046, loss_scale=8, train_wall=6, gb_free=29.7, wall=957
2024-01-31 22:48:32 | INFO | train_inner | epoch 183:   1065 / 1475 loss=2.18, nll_loss=0.528, ppl=1.44, wps=58426.3, ups=15.97, wpb=3658.7, bsz=180.2, num_updates=269500, lr=6.09145e-05, gnorm=1.049, loss_scale=8, train_wall=6, gb_free=29.7, wall=963
2024-01-31 22:48:39 | INFO | train_inner | epoch 183:   1165 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=59981.3, ups=15.76, wpb=3805.4, bsz=192.2, num_updates=269600, lr=6.09032e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=970
2024-01-31 22:48:45 | INFO | train_inner | epoch 183:   1265 / 1475 loss=2.175, nll_loss=0.524, ppl=1.44, wps=59120.2, ups=15.84, wpb=3733, bsz=190.8, num_updates=269700, lr=6.08919e-05, gnorm=1.052, loss_scale=8, train_wall=6, gb_free=29.7, wall=976
2024-01-31 22:48:51 | INFO | train_inner | epoch 183:   1365 / 1475 loss=2.175, nll_loss=0.523, ppl=1.44, wps=60367.8, ups=15.8, wpb=3820.1, bsz=200.4, num_updates=269800, lr=6.08806e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.7, wall=982
2024-01-31 22:48:58 | INFO | train_inner | epoch 183:   1465 / 1475 loss=2.179, nll_loss=0.528, ppl=1.44, wps=59244.8, ups=15.89, wpb=3729, bsz=199.3, num_updates=269900, lr=6.08693e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.7, wall=989
2024-01-31 22:48:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:48:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:49:00 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 3.571 | nll_loss 1.984 | ppl 3.96 | wps 160829 | wpb 3189.1 | bsz 163.4 | num_updates 269910 | best_loss 3.385
2024-01-31 22:49:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 269910 updates
2024-01-31 22:49:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint183.pt
2024-01-31 22:49:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint183.pt
2024-01-31 22:49:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint183.pt (epoch 183 @ 269910 updates, score 3.571) (writing took 2.358560718013905 seconds)
2024-01-31 22:49:03 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2024-01-31 22:49:03 | INFO | train | epoch 183 | loss 2.168 | nll_loss 0.515 | ppl 1.43 | wps 56893 | ups 15.08 | wpb 3771.6 | bsz 195.9 | num_updates 269910 | lr 6.08682e-05 | gnorm 1.009 | loss_scale 8 | train_wall 89 | gb_free 29.8 | wall 993
2024-01-31 22:49:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:49:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:49:03 | INFO | fairseq.trainer | begin training epoch 184
2024-01-31 22:49:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:49:08 | INFO | train_inner | epoch 184:     90 / 1475 loss=2.15, nll_loss=0.494, ppl=1.41, wps=34807.5, ups=9.34, wpb=3728, bsz=199.8, num_updates=270000, lr=6.08581e-05, gnorm=0.975, loss_scale=8, train_wall=6, gb_free=29.7, wall=999
2024-01-31 22:49:15 | INFO | train_inner | epoch 184:    190 / 1475 loss=2.147, nll_loss=0.491, ppl=1.41, wps=59401.3, ups=15.86, wpb=3746.5, bsz=206.7, num_updates=270100, lr=6.08468e-05, gnorm=0.972, loss_scale=8, train_wall=6, gb_free=29.7, wall=1006
2024-01-31 22:49:21 | INFO | train_inner | epoch 184:    290 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=60039.8, ups=15.68, wpb=3828.9, bsz=190.1, num_updates=270200, lr=6.08355e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.7, wall=1012
2024-01-31 22:49:27 | INFO | train_inner | epoch 184:    390 / 1475 loss=2.164, nll_loss=0.509, ppl=1.42, wps=59819.8, ups=15.85, wpb=3773.5, bsz=190.6, num_updates=270300, lr=6.08243e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.7, wall=1018
2024-01-31 22:49:34 | INFO | train_inner | epoch 184:    490 / 1475 loss=2.151, nll_loss=0.496, ppl=1.41, wps=57839.9, ups=15.81, wpb=3659.6, bsz=197, num_updates=270400, lr=6.0813e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=1025
2024-01-31 22:49:40 | INFO | train_inner | epoch 184:    590 / 1475 loss=2.17, nll_loss=0.517, ppl=1.43, wps=59617.8, ups=15.79, wpb=3775.1, bsz=188.3, num_updates=270500, lr=6.08018e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.7, wall=1031
2024-01-31 22:49:46 | INFO | train_inner | epoch 184:    690 / 1475 loss=2.176, nll_loss=0.523, ppl=1.44, wps=59925.3, ups=15.85, wpb=3780.9, bsz=189.5, num_updates=270600, lr=6.07906e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.6, wall=1037
2024-01-31 22:49:53 | INFO | train_inner | epoch 184:    790 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=59532.5, ups=15.76, wpb=3776.5, bsz=192.3, num_updates=270700, lr=6.07793e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=1044
2024-01-31 22:49:59 | INFO | train_inner | epoch 184:    890 / 1475 loss=2.161, nll_loss=0.507, ppl=1.42, wps=58407, ups=15.73, wpb=3713, bsz=208.2, num_updates=270800, lr=6.07681e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.7, wall=1050
2024-01-31 22:50:05 | INFO | train_inner | epoch 184:    990 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=59699.5, ups=15.81, wpb=3775.6, bsz=211, num_updates=270900, lr=6.07569e-05, gnorm=1.109, loss_scale=8, train_wall=6, gb_free=29.7, wall=1056
2024-01-31 22:50:12 | INFO | train_inner | epoch 184:   1090 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=59944.4, ups=15.67, wpb=3825, bsz=197.4, num_updates=271000, lr=6.07457e-05, gnorm=1.017, loss_scale=8, train_wall=6, gb_free=29.6, wall=1063
2024-01-31 22:50:18 | INFO | train_inner | epoch 184:   1190 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=59889.8, ups=15.86, wpb=3776.5, bsz=197.5, num_updates=271100, lr=6.07345e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.7, wall=1069
2024-01-31 22:50:24 | INFO | train_inner | epoch 184:   1290 / 1475 loss=2.183, nll_loss=0.532, ppl=1.45, wps=59941.9, ups=15.91, wpb=3766.8, bsz=184.6, num_updates=271200, lr=6.07233e-05, gnorm=1.057, loss_scale=8, train_wall=6, gb_free=29.8, wall=1075
2024-01-31 22:50:31 | INFO | train_inner | epoch 184:   1390 / 1475 loss=2.179, nll_loss=0.528, ppl=1.44, wps=60481.7, ups=15.84, wpb=3819.1, bsz=189, num_updates=271300, lr=6.07121e-05, gnorm=1.026, loss_scale=8, train_wall=6, gb_free=29.7, wall=1082
2024-01-31 22:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:50:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:50:38 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 3.569 | nll_loss 1.982 | ppl 3.95 | wps 160081 | wpb 3189.1 | bsz 163.4 | num_updates 271385 | best_loss 3.385
2024-01-31 22:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 271385 updates
2024-01-31 22:50:38 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint184.pt
2024-01-31 22:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint184.pt
2024-01-31 22:50:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint184.pt (epoch 184 @ 271385 updates, score 3.569) (writing took 2.4228894209954888 seconds)
2024-01-31 22:50:40 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2024-01-31 22:50:40 | INFO | train | epoch 184 | loss 2.167 | nll_loss 0.514 | ppl 1.43 | wps 56887.9 | ups 15.08 | wpb 3771.6 | bsz 195.9 | num_updates 271385 | lr 6.07026e-05 | gnorm 1.017 | loss_scale 8 | train_wall 88 | gb_free 29.9 | wall 1091
2024-01-31 22:50:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:50:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:50:40 | INFO | fairseq.trainer | begin training epoch 185
2024-01-31 22:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:50:41 | INFO | train_inner | epoch 185:     15 / 1475 loss=2.171, nll_loss=0.518, ppl=1.43, wps=35156.2, ups=9.25, wpb=3799.9, bsz=198.4, num_updates=271400, lr=6.07009e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=1092
2024-01-31 22:50:48 | INFO | train_inner | epoch 185:    115 / 1475 loss=2.149, nll_loss=0.493, ppl=1.41, wps=60755.4, ups=15.81, wpb=3842.1, bsz=206.9, num_updates=271500, lr=6.06897e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=30.1, wall=1099
2024-01-31 22:50:54 | INFO | train_inner | epoch 185:    215 / 1475 loss=2.15, nll_loss=0.496, ppl=1.41, wps=59393.3, ups=15.76, wpb=3769.2, bsz=198.7, num_updates=271600, lr=6.06785e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.7, wall=1105
2024-01-31 22:51:00 | INFO | train_inner | epoch 185:    315 / 1475 loss=2.162, nll_loss=0.508, ppl=1.42, wps=59714.3, ups=15.8, wpb=3779.3, bsz=194.2, num_updates=271700, lr=6.06674e-05, gnorm=0.998, loss_scale=8, train_wall=6, gb_free=29.6, wall=1111
2024-01-31 22:51:07 | INFO | train_inner | epoch 185:    415 / 1475 loss=2.171, nll_loss=0.517, ppl=1.43, wps=60870.6, ups=15.87, wpb=3834.4, bsz=191.2, num_updates=271800, lr=6.06562e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.9, wall=1118
2024-01-31 22:51:13 | INFO | train_inner | epoch 185:    515 / 1475 loss=2.174, nll_loss=0.52, ppl=1.43, wps=60143.4, ups=15.89, wpb=3785.7, bsz=188.6, num_updates=271900, lr=6.06451e-05, gnorm=1.03, loss_scale=8, train_wall=6, gb_free=29.7, wall=1124
2024-01-31 22:51:19 | INFO | train_inner | epoch 185:    615 / 1475 loss=2.169, nll_loss=0.517, ppl=1.43, wps=59621.9, ups=15.76, wpb=3782.9, bsz=191, num_updates=272000, lr=6.06339e-05, gnorm=1.032, loss_scale=8, train_wall=6, gb_free=29.7, wall=1130
2024-01-31 22:51:26 | INFO | train_inner | epoch 185:    715 / 1475 loss=2.161, nll_loss=0.507, ppl=1.42, wps=57832.6, ups=15.95, wpb=3625.3, bsz=190.6, num_updates=272100, lr=6.06228e-05, gnorm=1.03, loss_scale=8, train_wall=6, gb_free=29.6, wall=1137
2024-01-31 22:51:32 | INFO | train_inner | epoch 185:    815 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=60287.7, ups=15.79, wpb=3817.7, bsz=192.2, num_updates=272200, lr=6.06116e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.7, wall=1143
2024-01-31 22:51:38 | INFO | train_inner | epoch 185:    915 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=58722.9, ups=15.78, wpb=3722.2, bsz=202.7, num_updates=272300, lr=6.06005e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=1149
2024-01-31 22:51:45 | INFO | train_inner | epoch 185:   1015 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=59300.8, ups=15.86, wpb=3738.8, bsz=193, num_updates=272400, lr=6.05894e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.7, wall=1156
2024-01-31 22:51:51 | INFO | train_inner | epoch 185:   1115 / 1475 loss=2.177, nll_loss=0.526, ppl=1.44, wps=60537.7, ups=15.75, wpb=3844.7, bsz=193.3, num_updates=272500, lr=6.05783e-05, gnorm=1.064, loss_scale=8, train_wall=6, gb_free=29.6, wall=1162
2024-01-31 22:51:57 | INFO | train_inner | epoch 185:   1215 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=60467.1, ups=15.8, wpb=3825.9, bsz=203.8, num_updates=272600, lr=6.05671e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.7, wall=1168
2024-01-31 22:52:04 | INFO | train_inner | epoch 185:   1315 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=59345.5, ups=15.77, wpb=3762.5, bsz=199.8, num_updates=272700, lr=6.0556e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.7, wall=1175
2024-01-31 22:52:10 | INFO | train_inner | epoch 185:   1415 / 1475 loss=2.172, nll_loss=0.52, ppl=1.43, wps=58239.5, ups=15.83, wpb=3678.7, bsz=203, num_updates=272800, lr=6.05449e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.9, wall=1181
2024-01-31 22:52:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:52:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:52:16 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 3.577 | nll_loss 1.992 | ppl 3.98 | wps 160433 | wpb 3189.1 | bsz 163.4 | num_updates 272860 | best_loss 3.385
2024-01-31 22:52:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 272860 updates
2024-01-31 22:52:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint185.pt
2024-01-31 22:52:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint185.pt
2024-01-31 22:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint185.pt (epoch 185 @ 272860 updates, score 3.577) (writing took 2.363531637005508 seconds)
2024-01-31 22:52:18 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2024-01-31 22:52:18 | INFO | train | epoch 185 | loss 2.166 | nll_loss 0.513 | ppl 1.43 | wps 56950.3 | ups 15.1 | wpb 3771.6 | bsz 195.9 | num_updates 272860 | lr 6.05383e-05 | gnorm 1.017 | loss_scale 8 | train_wall 88 | gb_free 29.7 | wall 1189
2024-01-31 22:52:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:52:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:52:18 | INFO | fairseq.trainer | begin training epoch 186
2024-01-31 22:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:52:21 | INFO | train_inner | epoch 186:     40 / 1475 loss=2.179, nll_loss=0.526, ppl=1.44, wps=35596.4, ups=9.35, wpb=3807.1, bsz=185, num_updates=272900, lr=6.05338e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=1192
2024-01-31 22:52:27 | INFO | train_inner | epoch 186:    140 / 1475 loss=2.16, nll_loss=0.505, ppl=1.42, wps=60615.1, ups=15.91, wpb=3810.9, bsz=186.2, num_updates=273000, lr=6.05228e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.8, wall=1198
2024-01-31 22:52:33 | INFO | train_inner | epoch 186:    240 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=59667.6, ups=15.76, wpb=3786.8, bsz=187.7, num_updates=273100, lr=6.05117e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.8, wall=1204
2024-01-31 22:52:40 | INFO | train_inner | epoch 186:    340 / 1475 loss=2.157, nll_loss=0.503, ppl=1.42, wps=59314.6, ups=15.75, wpb=3765.3, bsz=195, num_updates=273200, lr=6.05006e-05, gnorm=0.987, loss_scale=8, train_wall=6, gb_free=29.7, wall=1211
2024-01-31 22:52:46 | INFO | train_inner | epoch 186:    440 / 1475 loss=2.157, nll_loss=0.503, ppl=1.42, wps=59623.1, ups=15.92, wpb=3746.1, bsz=192.9, num_updates=273300, lr=6.04895e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.8, wall=1217
2024-01-31 22:52:52 | INFO | train_inner | epoch 186:    540 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=60087.6, ups=15.9, wpb=3778.1, bsz=203.4, num_updates=273400, lr=6.04785e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.8, wall=1223
2024-01-31 22:52:59 | INFO | train_inner | epoch 186:    640 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=59211.6, ups=15.84, wpb=3738.4, bsz=192.4, num_updates=273500, lr=6.04674e-05, gnorm=1.042, loss_scale=8, train_wall=6, gb_free=29.7, wall=1229
2024-01-31 22:53:05 | INFO | train_inner | epoch 186:    740 / 1475 loss=2.167, nll_loss=0.514, ppl=1.43, wps=59599, ups=15.9, wpb=3747.6, bsz=188.3, num_updates=273600, lr=6.04564e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.8, wall=1236
2024-01-31 22:53:11 | INFO | train_inner | epoch 186:    840 / 1475 loss=2.172, nll_loss=0.519, ppl=1.43, wps=59885.4, ups=15.62, wpb=3834, bsz=194.5, num_updates=273700, lr=6.04453e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.8, wall=1242
2024-01-31 22:53:18 | INFO | train_inner | epoch 186:    940 / 1475 loss=2.159, nll_loss=0.506, ppl=1.42, wps=57957.9, ups=15.38, wpb=3768.6, bsz=204.7, num_updates=273800, lr=6.04343e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.8, wall=1249
2024-01-31 22:53:24 | INFO | train_inner | epoch 186:   1040 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=59396.1, ups=15.72, wpb=3778.2, bsz=205.8, num_updates=273900, lr=6.04232e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.7, wall=1255
2024-01-31 22:53:30 | INFO | train_inner | epoch 186:   1140 / 1475 loss=2.174, nll_loss=0.523, ppl=1.44, wps=59271, ups=15.84, wpb=3742.3, bsz=193.5, num_updates=274000, lr=6.04122e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=1261
2024-01-31 22:53:37 | INFO | train_inner | epoch 186:   1240 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=59219.4, ups=15.72, wpb=3767.2, bsz=200.9, num_updates=274100, lr=6.04012e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.8, wall=1268
2024-01-31 22:53:43 | INFO | train_inner | epoch 186:   1340 / 1475 loss=2.172, nll_loss=0.519, ppl=1.43, wps=60136.1, ups=15.81, wpb=3802.9, bsz=201.2, num_updates=274200, lr=6.03902e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=30.1, wall=1274
2024-01-31 22:53:49 | INFO | train_inner | epoch 186:   1440 / 1475 loss=2.171, nll_loss=0.52, ppl=1.43, wps=58933.3, ups=15.71, wpb=3750.5, bsz=203.2, num_updates=274300, lr=6.03792e-05, gnorm=1.048, loss_scale=8, train_wall=6, gb_free=29.7, wall=1280
2024-01-31 22:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:53:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:53:53 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 3.573 | nll_loss 1.986 | ppl 3.96 | wps 161266 | wpb 3189.1 | bsz 163.4 | num_updates 274335 | best_loss 3.385
2024-01-31 22:53:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 274335 updates
2024-01-31 22:53:53 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint186.pt
2024-01-31 22:53:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint186.pt
2024-01-31 22:53:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint186.pt (epoch 186 @ 274335 updates, score 3.573) (writing took 2.387650104938075 seconds)
2024-01-31 22:53:56 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2024-01-31 22:53:56 | INFO | train | epoch 186 | loss 2.166 | nll_loss 0.513 | ppl 1.43 | wps 56825.1 | ups 15.07 | wpb 3771.6 | bsz 195.9 | num_updates 274335 | lr 6.03753e-05 | gnorm 1.016 | loss_scale 8 | train_wall 89 | gb_free 29.7 | wall 1287
2024-01-31 22:53:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:53:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:53:56 | INFO | fairseq.trainer | begin training epoch 187
2024-01-31 22:53:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:54:00 | INFO | train_inner | epoch 187:     65 / 1475 loss=2.163, nll_loss=0.509, ppl=1.42, wps=34659.5, ups=9.2, wpb=3766.7, bsz=195.1, num_updates=274400, lr=6.03682e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.7, wall=1291
2024-01-31 22:54:07 | INFO | train_inner | epoch 187:    165 / 1475 loss=2.141, nll_loss=0.485, ppl=1.4, wps=59041.4, ups=15.61, wpb=3782.1, bsz=206.6, num_updates=274500, lr=6.03572e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.7, wall=1298
2024-01-31 22:54:13 | INFO | train_inner | epoch 187:    265 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=59179.9, ups=15.76, wpb=3754.2, bsz=187.4, num_updates=274600, lr=6.03462e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.7, wall=1304
2024-01-31 22:54:19 | INFO | train_inner | epoch 187:    365 / 1475 loss=2.161, nll_loss=0.507, ppl=1.42, wps=58065, ups=15.9, wpb=3652.2, bsz=185.8, num_updates=274700, lr=6.03352e-05, gnorm=1.051, loss_scale=8, train_wall=6, gb_free=29.7, wall=1310
2024-01-31 22:54:26 | INFO | train_inner | epoch 187:    465 / 1475 loss=2.148, nll_loss=0.494, ppl=1.41, wps=59095.8, ups=15.63, wpb=3780.8, bsz=207.1, num_updates=274800, lr=6.03242e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.7, wall=1317
2024-01-31 22:54:32 | INFO | train_inner | epoch 187:    565 / 1475 loss=2.16, nll_loss=0.507, ppl=1.42, wps=59932.3, ups=15.82, wpb=3788, bsz=196.9, num_updates=274900, lr=6.03132e-05, gnorm=0.997, loss_scale=8, train_wall=6, gb_free=29.7, wall=1323
2024-01-31 22:54:38 | INFO | train_inner | epoch 187:    665 / 1475 loss=2.164, nll_loss=0.511, ppl=1.43, wps=59800.1, ups=15.82, wpb=3779.7, bsz=200.6, num_updates=275000, lr=6.03023e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.7, wall=1329
2024-01-31 22:54:45 | INFO | train_inner | epoch 187:    765 / 1475 loss=2.172, nll_loss=0.52, ppl=1.43, wps=60220.6, ups=15.83, wpb=3803.2, bsz=193, num_updates=275100, lr=6.02913e-05, gnorm=1.04, loss_scale=8, train_wall=6, gb_free=29.7, wall=1336
2024-01-31 22:54:51 | INFO | train_inner | epoch 187:    865 / 1475 loss=2.164, nll_loss=0.51, ppl=1.42, wps=60149.6, ups=15.74, wpb=3821.6, bsz=201, num_updates=275200, lr=6.02804e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.7, wall=1342
2024-01-31 22:54:57 | INFO | train_inner | epoch 187:    965 / 1475 loss=2.178, nll_loss=0.526, ppl=1.44, wps=60341.1, ups=15.83, wpb=3811.2, bsz=191.3, num_updates=275300, lr=6.02694e-05, gnorm=1.032, loss_scale=8, train_wall=6, gb_free=29.7, wall=1348
2024-01-31 22:55:04 | INFO | train_inner | epoch 187:   1065 / 1475 loss=2.161, nll_loss=0.507, ppl=1.42, wps=59893.5, ups=15.83, wpb=3784.5, bsz=204.2, num_updates=275400, lr=6.02585e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.9, wall=1355
2024-01-31 22:55:10 | INFO | train_inner | epoch 187:   1165 / 1475 loss=2.183, nll_loss=0.532, ppl=1.45, wps=60252.2, ups=15.85, wpb=3801.6, bsz=187.6, num_updates=275500, lr=6.02475e-05, gnorm=1.047, loss_scale=8, train_wall=6, gb_free=29.7, wall=1361
2024-01-31 22:55:16 | INFO | train_inner | epoch 187:   1265 / 1475 loss=2.171, nll_loss=0.519, ppl=1.43, wps=60046.7, ups=15.83, wpb=3792.5, bsz=198.2, num_updates=275600, lr=6.02366e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.8, wall=1367
2024-01-31 22:55:23 | INFO | train_inner | epoch 187:   1365 / 1475 loss=2.171, nll_loss=0.519, ppl=1.43, wps=58904, ups=15.86, wpb=3714.5, bsz=194.1, num_updates=275700, lr=6.02257e-05, gnorm=1.041, loss_scale=8, train_wall=6, gb_free=29.7, wall=1374
2024-01-31 22:55:29 | INFO | train_inner | epoch 187:   1465 / 1475 loss=2.183, nll_loss=0.532, ppl=1.45, wps=59457.9, ups=15.92, wpb=3735.6, bsz=187.5, num_updates=275800, lr=6.02147e-05, gnorm=1.046, loss_scale=8, train_wall=6, gb_free=29.7, wall=1380
2024-01-31 22:55:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:55:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:55:31 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 3.569 | nll_loss 1.982 | ppl 3.95 | wps 159512 | wpb 3189.1 | bsz 163.4 | num_updates 275810 | best_loss 3.385
2024-01-31 22:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 275810 updates
2024-01-31 22:55:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint187.pt
2024-01-31 22:55:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint187.pt
2024-01-31 22:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint187.pt (epoch 187 @ 275810 updates, score 3.569) (writing took 2.368873047060333 seconds)
2024-01-31 22:55:34 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2024-01-31 22:55:34 | INFO | train | epoch 187 | loss 2.165 | nll_loss 0.512 | ppl 1.43 | wps 56819.4 | ups 15.07 | wpb 3771.6 | bsz 195.9 | num_updates 275810 | lr 6.02137e-05 | gnorm 1.016 | loss_scale 8 | train_wall 89 | gb_free 29.7 | wall 1385
2024-01-31 22:55:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:55:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:55:34 | INFO | fairseq.trainer | begin training epoch 188
2024-01-31 22:55:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:55:40 | INFO | train_inner | epoch 188:     90 / 1475 loss=2.146, nll_loss=0.49, ppl=1.4, wps=33978.1, ups=9.12, wpb=3726.6, bsz=197.3, num_updates=275900, lr=6.02038e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=1391
2024-01-31 22:55:46 | INFO | train_inner | epoch 188:    190 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=60267.6, ups=15.71, wpb=3836.3, bsz=196.1, num_updates=276000, lr=6.01929e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=1397
2024-01-31 22:55:53 | INFO | train_inner | epoch 188:    290 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=60529.1, ups=15.83, wpb=3823.5, bsz=181, num_updates=276100, lr=6.0182e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.7, wall=1403
2024-01-31 22:55:59 | INFO | train_inner | epoch 188:    390 / 1475 loss=2.153, nll_loss=0.498, ppl=1.41, wps=58909.5, ups=15.86, wpb=3714.7, bsz=195.3, num_updates=276200, lr=6.01711e-05, gnorm=0.99, loss_scale=8, train_wall=6, gb_free=29.7, wall=1410
2024-01-31 22:56:05 | INFO | train_inner | epoch 188:    490 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=59639.9, ups=15.88, wpb=3756.1, bsz=188.2, num_updates=276300, lr=6.01602e-05, gnorm=1.047, loss_scale=8, train_wall=6, gb_free=29.6, wall=1416
2024-01-31 22:56:11 | INFO | train_inner | epoch 188:    590 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=60066.3, ups=15.8, wpb=3802.7, bsz=197.6, num_updates=276400, lr=6.01494e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=1422
2024-01-31 22:56:18 | INFO | train_inner | epoch 188:    690 / 1475 loss=2.159, nll_loss=0.505, ppl=1.42, wps=60230.6, ups=15.83, wpb=3804.8, bsz=207.9, num_updates=276500, lr=6.01385e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.6, wall=1429
2024-01-31 22:56:24 | INFO | train_inner | epoch 188:    790 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=59857.6, ups=15.78, wpb=3793.3, bsz=192.5, num_updates=276600, lr=6.01276e-05, gnorm=1.056, loss_scale=8, train_wall=6, gb_free=29.6, wall=1435
2024-01-31 22:56:30 | INFO | train_inner | epoch 188:    890 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=60473.1, ups=15.79, wpb=3829.5, bsz=198.5, num_updates=276700, lr=6.01167e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=1441
2024-01-31 22:56:37 | INFO | train_inner | epoch 188:    990 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=59805.3, ups=15.78, wpb=3790.5, bsz=198, num_updates=276800, lr=6.01059e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=1448
2024-01-31 22:56:43 | INFO | train_inner | epoch 188:   1090 / 1475 loss=2.167, nll_loss=0.515, ppl=1.43, wps=57884.7, ups=15.83, wpb=3655.9, bsz=205.2, num_updates=276900, lr=6.0095e-05, gnorm=1.102, loss_scale=8, train_wall=6, gb_free=29.7, wall=1454
2024-01-31 22:56:49 | INFO | train_inner | epoch 188:   1190 / 1475 loss=2.165, nll_loss=0.514, ppl=1.43, wps=58981.9, ups=15.74, wpb=3746.2, bsz=203.2, num_updates=277000, lr=6.00842e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.7, wall=1460
2024-01-31 22:56:56 | INFO | train_inner | epoch 188:   1290 / 1475 loss=2.179, nll_loss=0.528, ppl=1.44, wps=59495.1, ups=15.84, wpb=3756.5, bsz=191.4, num_updates=277100, lr=6.00733e-05, gnorm=1.044, loss_scale=8, train_wall=6, gb_free=29.7, wall=1467
2024-01-31 22:57:02 | INFO | train_inner | epoch 188:   1390 / 1475 loss=2.178, nll_loss=0.526, ppl=1.44, wps=58708.3, ups=15.82, wpb=3711, bsz=183.4, num_updates=277200, lr=6.00625e-05, gnorm=1.072, loss_scale=8, train_wall=6, gb_free=29.6, wall=1473
2024-01-31 22:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:57:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:57:09 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 3.577 | nll_loss 1.991 | ppl 3.98 | wps 160439 | wpb 3189.1 | bsz 163.4 | num_updates 277285 | best_loss 3.385
2024-01-31 22:57:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 277285 updates
2024-01-31 22:57:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint188.pt
2024-01-31 22:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint188.pt
2024-01-31 22:57:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint188.pt (epoch 188 @ 277285 updates, score 3.577) (writing took 2.4234724399866536 seconds)
2024-01-31 22:57:12 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2024-01-31 22:57:12 | INFO | train | epoch 188 | loss 2.164 | nll_loss 0.51 | ppl 1.42 | wps 56770.5 | ups 15.05 | wpb 3771.6 | bsz 195.9 | num_updates 277285 | lr 6.00533e-05 | gnorm 1.024 | loss_scale 8 | train_wall 89 | gb_free 29.7 | wall 1483
2024-01-31 22:57:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:57:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:57:12 | INFO | fairseq.trainer | begin training epoch 189
2024-01-31 22:57:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:57:13 | INFO | train_inner | epoch 189:     15 / 1475 loss=2.168, nll_loss=0.515, ppl=1.43, wps=35130.2, ups=9.28, wpb=3784.5, bsz=201.2, num_updates=277300, lr=6.00517e-05, gnorm=1.01, loss_scale=8, train_wall=6, gb_free=29.8, wall=1484
2024-01-31 22:57:19 | INFO | train_inner | epoch 189:    115 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=59846.5, ups=15.77, wpb=3795.6, bsz=202.6, num_updates=277400, lr=6.00408e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.7, wall=1490
2024-01-31 22:57:25 | INFO | train_inner | epoch 189:    215 / 1475 loss=2.156, nll_loss=0.501, ppl=1.42, wps=58734.8, ups=15.95, wpb=3682.5, bsz=186.9, num_updates=277500, lr=6.003e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=1496
2024-01-31 22:57:32 | INFO | train_inner | epoch 189:    315 / 1475 loss=2.152, nll_loss=0.496, ppl=1.41, wps=60001.1, ups=15.82, wpb=3793.1, bsz=196.8, num_updates=277600, lr=6.00192e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.7, wall=1503
2024-01-31 22:57:38 | INFO | train_inner | epoch 189:    415 / 1475 loss=2.157, nll_loss=0.503, ppl=1.42, wps=60663.2, ups=15.8, wpb=3838.9, bsz=204.1, num_updates=277700, lr=6.00084e-05, gnorm=0.991, loss_scale=8, train_wall=6, gb_free=29.7, wall=1509
2024-01-31 22:57:44 | INFO | train_inner | epoch 189:    515 / 1475 loss=2.154, nll_loss=0.5, ppl=1.41, wps=59904.3, ups=15.82, wpb=3787.7, bsz=193.2, num_updates=277800, lr=5.99976e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=1515
2024-01-31 22:57:51 | INFO | train_inner | epoch 189:    615 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=59919.5, ups=15.74, wpb=3807.1, bsz=191.8, num_updates=277900, lr=5.99868e-05, gnorm=1.008, loss_scale=8, train_wall=6, gb_free=29.7, wall=1522
2024-01-31 22:57:57 | INFO | train_inner | epoch 189:    715 / 1475 loss=2.17, nll_loss=0.517, ppl=1.43, wps=60360.7, ups=15.84, wpb=3811.3, bsz=184.9, num_updates=278000, lr=5.9976e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.7, wall=1528
2024-01-31 22:58:03 | INFO | train_inner | epoch 189:    815 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=60094.5, ups=15.83, wpb=3796.1, bsz=203, num_updates=278100, lr=5.99652e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.7, wall=1534
2024-01-31 22:58:10 | INFO | train_inner | epoch 189:    915 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=59471.5, ups=15.75, wpb=3775.8, bsz=202.1, num_updates=278200, lr=5.99545e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.9, wall=1541
2024-01-31 22:58:16 | INFO | train_inner | epoch 189:   1015 / 1475 loss=2.163, nll_loss=0.51, ppl=1.42, wps=58647.5, ups=15.69, wpb=3737.9, bsz=198, num_updates=278300, lr=5.99437e-05, gnorm=1.067, loss_scale=8, train_wall=6, gb_free=29.7, wall=1547
2024-01-31 22:58:23 | INFO | train_inner | epoch 189:   1115 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=59103.4, ups=15.67, wpb=3770.8, bsz=206.3, num_updates=278400, lr=5.99329e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.6, wall=1553
2024-01-31 22:58:29 | INFO | train_inner | epoch 189:   1215 / 1475 loss=2.166, nll_loss=0.514, ppl=1.43, wps=59916.2, ups=15.77, wpb=3799, bsz=207.8, num_updates=278500, lr=5.99222e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=1560
2024-01-31 22:58:35 | INFO | train_inner | epoch 189:   1315 / 1475 loss=2.181, nll_loss=0.531, ppl=1.44, wps=59723.5, ups=15.88, wpb=3762, bsz=190.9, num_updates=278600, lr=5.99114e-05, gnorm=1.033, loss_scale=8, train_wall=6, gb_free=29.8, wall=1566
2024-01-31 22:58:42 | INFO | train_inner | epoch 189:   1415 / 1475 loss=2.178, nll_loss=0.527, ppl=1.44, wps=58036, ups=15.8, wpb=3673.6, bsz=184.5, num_updates=278700, lr=5.99006e-05, gnorm=1.084, loss_scale=8, train_wall=6, gb_free=29.7, wall=1572
2024-01-31 22:58:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 22:58:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:58:47 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 3.575 | nll_loss 1.99 | ppl 3.97 | wps 160099 | wpb 3189.1 | bsz 163.4 | num_updates 278760 | best_loss 3.385
2024-01-31 22:58:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 278760 updates
2024-01-31 22:58:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint189.pt
2024-01-31 22:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint189.pt
2024-01-31 22:58:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint189.pt (epoch 189 @ 278760 updates, score 3.575) (writing took 2.407049856032245 seconds)
2024-01-31 22:58:50 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2024-01-31 22:58:50 | INFO | train | epoch 189 | loss 2.163 | nll_loss 0.51 | ppl 1.42 | wps 56889.3 | ups 15.08 | wpb 3771.6 | bsz 195.9 | num_updates 278760 | lr 5.98942e-05 | gnorm 1.022 | loss_scale 8 | train_wall 88 | gb_free 29.7 | wall 1581
2024-01-31 22:58:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 22:58:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 22:58:50 | INFO | fairseq.trainer | begin training epoch 190
2024-01-31 22:58:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 22:58:52 | INFO | train_inner | epoch 190:     40 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=34830.5, ups=9.31, wpb=3742.4, bsz=191.8, num_updates=278800, lr=5.98899e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.7, wall=1583
2024-01-31 22:58:59 | INFO | train_inner | epoch 190:    140 / 1475 loss=2.14, nll_loss=0.484, ppl=1.4, wps=59870.7, ups=15.7, wpb=3812.8, bsz=208.2, num_updates=278900, lr=5.98792e-05, gnorm=0.978, loss_scale=8, train_wall=6, gb_free=29.8, wall=1590
2024-01-31 22:59:05 | INFO | train_inner | epoch 190:    240 / 1475 loss=2.144, nll_loss=0.489, ppl=1.4, wps=59980.1, ups=15.74, wpb=3810.8, bsz=205.4, num_updates=279000, lr=5.98684e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=1596
2024-01-31 22:59:11 | INFO | train_inner | epoch 190:    340 / 1475 loss=2.139, nll_loss=0.484, ppl=1.4, wps=59064.3, ups=15.63, wpb=3778.9, bsz=214.7, num_updates=279100, lr=5.98577e-05, gnorm=1.065, loss_scale=8, train_wall=6, gb_free=29.7, wall=1602
2024-01-31 22:59:18 | INFO | train_inner | epoch 190:    440 / 1475 loss=2.163, nll_loss=0.509, ppl=1.42, wps=59935.6, ups=15.94, wpb=3760.4, bsz=185.2, num_updates=279200, lr=5.9847e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.9, wall=1609
2024-01-31 22:59:24 | INFO | train_inner | epoch 190:    540 / 1475 loss=2.163, nll_loss=0.508, ppl=1.42, wps=60914.9, ups=15.92, wpb=3826.8, bsz=192.2, num_updates=279300, lr=5.98363e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.7, wall=1615
2024-01-31 22:59:30 | INFO | train_inner | epoch 190:    640 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=58663.1, ups=15.87, wpb=3696, bsz=198.8, num_updates=279400, lr=5.98256e-05, gnorm=1.058, loss_scale=8, train_wall=6, gb_free=29.7, wall=1621
2024-01-31 22:59:37 | INFO | train_inner | epoch 190:    740 / 1475 loss=2.159, nll_loss=0.506, ppl=1.42, wps=58485, ups=15.74, wpb=3715.5, bsz=191.9, num_updates=279500, lr=5.98149e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=1628
2024-01-31 22:59:43 | INFO | train_inner | epoch 190:    840 / 1475 loss=2.174, nll_loss=0.521, ppl=1.44, wps=59757.8, ups=15.87, wpb=3764.4, bsz=187.2, num_updates=279600, lr=5.98042e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.8, wall=1634
2024-01-31 22:59:49 | INFO | train_inner | epoch 190:    940 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=59514.6, ups=15.79, wpb=3768.8, bsz=200.5, num_updates=279700, lr=5.97935e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.7, wall=1640
2024-01-31 22:59:56 | INFO | train_inner | epoch 190:   1040 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=60095.8, ups=15.67, wpb=3835.2, bsz=209.2, num_updates=279800, lr=5.97828e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.8, wall=1647
2024-01-31 23:00:02 | INFO | train_inner | epoch 190:   1140 / 1475 loss=2.176, nll_loss=0.525, ppl=1.44, wps=59544.5, ups=15.63, wpb=3810.1, bsz=188.8, num_updates=279900, lr=5.97721e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.7, wall=1653
2024-01-31 23:00:08 | INFO | train_inner | epoch 190:   1240 / 1475 loss=2.164, nll_loss=0.511, ppl=1.43, wps=59162.4, ups=15.78, wpb=3748.7, bsz=200.2, num_updates=280000, lr=5.97614e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.6, wall=1659
2024-01-31 23:00:15 | INFO | train_inner | epoch 190:   1340 / 1475 loss=2.191, nll_loss=0.541, ppl=1.45, wps=59895.4, ups=15.91, wpb=3765.6, bsz=172.7, num_updates=280100, lr=5.97508e-05, gnorm=1.072, loss_scale=16, train_wall=6, gb_free=29.7, wall=1666
2024-01-31 23:00:21 | INFO | train_inner | epoch 190:   1440 / 1475 loss=2.176, nll_loss=0.524, ppl=1.44, wps=59760.8, ups=15.84, wpb=3772.6, bsz=193.8, num_updates=280200, lr=5.97401e-05, gnorm=1.032, loss_scale=16, train_wall=6, gb_free=29.7, wall=1672
2024-01-31 23:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:00:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:00:25 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 3.571 | nll_loss 1.985 | ppl 3.96 | wps 159891 | wpb 3189.1 | bsz 163.4 | num_updates 280235 | best_loss 3.385
2024-01-31 23:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 280235 updates
2024-01-31 23:00:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint190.pt
2024-01-31 23:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint190.pt
2024-01-31 23:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint190.pt (epoch 190 @ 280235 updates, score 3.571) (writing took 2.360612535965629 seconds)
2024-01-31 23:00:27 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2024-01-31 23:00:27 | INFO | train | epoch 190 | loss 2.162 | nll_loss 0.509 | ppl 1.42 | wps 56873.1 | ups 15.08 | wpb 3771.6 | bsz 195.9 | num_updates 280235 | lr 5.97364e-05 | gnorm 1.025 | loss_scale 16 | train_wall 89 | gb_free 29.7 | wall 1678
2024-01-31 23:00:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:00:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:00:27 | INFO | fairseq.trainer | begin training epoch 191
2024-01-31 23:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:00:32 | INFO | train_inner | epoch 191:     65 / 1475 loss=2.157, nll_loss=0.503, ppl=1.42, wps=35501.8, ups=9.32, wpb=3809, bsz=194.3, num_updates=280300, lr=5.97294e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.8, wall=1683
2024-01-31 23:00:38 | INFO | train_inner | epoch 191:    165 / 1475 loss=2.144, nll_loss=0.489, ppl=1.4, wps=58341.5, ups=15.9, wpb=3668.6, bsz=197.4, num_updates=280400, lr=5.97188e-05, gnorm=1.018, loss_scale=16, train_wall=6, gb_free=29.8, wall=1689
2024-01-31 23:00:44 | INFO | train_inner | epoch 191:    265 / 1475 loss=2.14, nll_loss=0.484, ppl=1.4, wps=58723.2, ups=15.77, wpb=3723.9, bsz=202.6, num_updates=280500, lr=5.97081e-05, gnorm=0.999, loss_scale=16, train_wall=6, gb_free=29.6, wall=1695
2024-01-31 23:00:51 | INFO | train_inner | epoch 191:    365 / 1475 loss=2.158, nll_loss=0.504, ppl=1.42, wps=60072.9, ups=15.87, wpb=3785.6, bsz=196.5, num_updates=280600, lr=5.96975e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.6, wall=1702
2024-01-31 23:00:57 | INFO | train_inner | epoch 191:    465 / 1475 loss=2.154, nll_loss=0.5, ppl=1.41, wps=59659, ups=15.8, wpb=3776.7, bsz=194.4, num_updates=280700, lr=5.96869e-05, gnorm=1.014, loss_scale=16, train_wall=6, gb_free=29.6, wall=1708
2024-01-31 23:01:03 | INFO | train_inner | epoch 191:    565 / 1475 loss=2.159, nll_loss=0.505, ppl=1.42, wps=59352.6, ups=15.84, wpb=3747.9, bsz=191, num_updates=280800, lr=5.96762e-05, gnorm=1.017, loss_scale=16, train_wall=6, gb_free=29.7, wall=1714
2024-01-31 23:01:10 | INFO | train_inner | epoch 191:    665 / 1475 loss=2.154, nll_loss=0.501, ppl=1.41, wps=60263.7, ups=15.72, wpb=3833.2, bsz=201.9, num_updates=280900, lr=5.96656e-05, gnorm=1.011, loss_scale=16, train_wall=6, gb_free=29.7, wall=1721
2024-01-31 23:01:16 | INFO | train_inner | epoch 191:    765 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=60086, ups=15.79, wpb=3804.9, bsz=192.2, num_updates=281000, lr=5.9655e-05, gnorm=1.059, loss_scale=16, train_wall=6, gb_free=29.6, wall=1727
2024-01-31 23:01:22 | INFO | train_inner | epoch 191:    865 / 1475 loss=2.17, nll_loss=0.518, ppl=1.43, wps=59806.1, ups=15.84, wpb=3774.6, bsz=191.4, num_updates=281100, lr=5.96444e-05, gnorm=1.039, loss_scale=16, train_wall=6, gb_free=29.8, wall=1733
2024-01-31 23:01:29 | INFO | train_inner | epoch 191:    965 / 1475 loss=2.168, nll_loss=0.516, ppl=1.43, wps=59574, ups=15.83, wpb=3763.8, bsz=192.2, num_updates=281200, lr=5.96338e-05, gnorm=1.043, loss_scale=16, train_wall=6, gb_free=29.7, wall=1739
2024-01-31 23:01:35 | INFO | train_inner | epoch 191:   1065 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=57562, ups=15.28, wpb=3767.7, bsz=187.7, num_updates=281300, lr=5.96232e-05, gnorm=1.06, loss_scale=16, train_wall=6, gb_free=29.8, wall=1746
2024-01-31 23:01:41 | INFO | train_inner | epoch 191:   1165 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=59971.8, ups=15.69, wpb=3822, bsz=215, num_updates=281400, lr=5.96126e-05, gnorm=1.01, loss_scale=16, train_wall=6, gb_free=29.7, wall=1752
2024-01-31 23:01:48 | INFO | train_inner | epoch 191:   1265 / 1475 loss=2.165, nll_loss=0.511, ppl=1.43, wps=59022.8, ups=15.82, wpb=3732, bsz=199.7, num_updates=281500, lr=5.9602e-05, gnorm=1.035, loss_scale=16, train_wall=6, gb_free=29.7, wall=1759
2024-01-31 23:01:54 | INFO | train_inner | epoch 191:   1365 / 1475 loss=2.175, nll_loss=0.524, ppl=1.44, wps=60072.5, ups=15.77, wpb=3808.1, bsz=190.5, num_updates=281600, lr=5.95914e-05, gnorm=1.049, loss_scale=16, train_wall=6, gb_free=29.7, wall=1765
2024-01-31 23:02:00 | INFO | train_inner | epoch 191:   1465 / 1475 loss=2.174, nll_loss=0.523, ppl=1.44, wps=59773.4, ups=15.88, wpb=3764.1, bsz=189.6, num_updates=281700, lr=5.95808e-05, gnorm=1.041, loss_scale=16, train_wall=6, gb_free=29.8, wall=1771
2024-01-31 23:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:02:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:02:03 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 3.579 | nll_loss 1.993 | ppl 3.98 | wps 160661 | wpb 3189.1 | bsz 163.4 | num_updates 281710 | best_loss 3.385
2024-01-31 23:02:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 281710 updates
2024-01-31 23:02:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint191.pt
2024-01-31 23:02:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint191.pt
2024-01-31 23:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint191.pt (epoch 191 @ 281710 updates, score 3.579) (writing took 2.349278003908694 seconds)
2024-01-31 23:02:05 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2024-01-31 23:02:05 | INFO | train | epoch 191 | loss 2.161 | nll_loss 0.507 | ppl 1.42 | wps 56805.4 | ups 15.06 | wpb 3771.6 | bsz 195.9 | num_updates 281710 | lr 5.95798e-05 | gnorm 1.027 | loss_scale 16 | train_wall 89 | gb_free 29.8 | wall 1776
2024-01-31 23:02:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:02:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:02:05 | INFO | fairseq.trainer | begin training epoch 192
2024-01-31 23:02:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:02:11 | INFO | train_inner | epoch 192:     90 / 1475 loss=2.155, nll_loss=0.5, ppl=1.41, wps=34858, ups=9.34, wpb=3733.9, bsz=186.1, num_updates=281800, lr=5.95703e-05, gnorm=1.016, loss_scale=16, train_wall=6, gb_free=29.7, wall=1782
2024-01-31 23:02:17 | INFO | train_inner | epoch 192:    190 / 1475 loss=2.153, nll_loss=0.498, ppl=1.41, wps=59380.6, ups=15.83, wpb=3750, bsz=202.3, num_updates=281900, lr=5.95597e-05, gnorm=0.985, loss_scale=16, train_wall=6, gb_free=29.7, wall=1788
2024-01-31 23:02:24 | INFO | train_inner | epoch 192:    290 / 1475 loss=2.155, nll_loss=0.5, ppl=1.41, wps=59142, ups=15.81, wpb=3741.7, bsz=189, num_updates=282000, lr=5.95491e-05, gnorm=1.01, loss_scale=16, train_wall=6, gb_free=29.6, wall=1795
2024-01-31 23:02:30 | INFO | train_inner | epoch 192:    390 / 1475 loss=2.154, nll_loss=0.5, ppl=1.41, wps=59110.6, ups=15.92, wpb=3712.4, bsz=189.7, num_updates=282100, lr=5.95386e-05, gnorm=1.046, loss_scale=16, train_wall=6, gb_free=29.7, wall=1801
2024-01-31 23:02:36 | INFO | train_inner | epoch 192:    490 / 1475 loss=2.154, nll_loss=0.5, ppl=1.41, wps=59417.9, ups=15.73, wpb=3778.1, bsz=206.2, num_updates=282200, lr=5.9528e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.7, wall=1807
2024-01-31 23:02:43 | INFO | train_inner | epoch 192:    590 / 1475 loss=2.163, nll_loss=0.51, ppl=1.42, wps=60835.3, ups=15.79, wpb=3852.2, bsz=195.2, num_updates=282300, lr=5.95175e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=1814
2024-01-31 23:02:49 | INFO | train_inner | epoch 192:    690 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=58868, ups=15.84, wpb=3715.4, bsz=184.2, num_updates=282400, lr=5.95069e-05, gnorm=1.042, loss_scale=16, train_wall=6, gb_free=29.8, wall=1820
2024-01-31 23:02:55 | INFO | train_inner | epoch 192:    790 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=59020.2, ups=15.78, wpb=3740.2, bsz=202.3, num_updates=282500, lr=5.94964e-05, gnorm=1.009, loss_scale=16, train_wall=6, gb_free=29.8, wall=1826
2024-01-31 23:03:02 | INFO | train_inner | epoch 192:    890 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=59739.4, ups=15.71, wpb=3803.1, bsz=195.4, num_updates=282600, lr=5.94859e-05, gnorm=0.998, loss_scale=16, train_wall=6, gb_free=29.6, wall=1833
2024-01-31 23:03:08 | INFO | train_inner | epoch 192:    990 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=59477.8, ups=15.91, wpb=3737.5, bsz=194.1, num_updates=282700, lr=5.94754e-05, gnorm=1.02, loss_scale=16, train_wall=6, gb_free=29.8, wall=1839
2024-01-31 23:03:14 | INFO | train_inner | epoch 192:   1090 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=60360.1, ups=15.75, wpb=3831.9, bsz=198.1, num_updates=282800, lr=5.94648e-05, gnorm=1.002, loss_scale=16, train_wall=6, gb_free=29.7, wall=1845
2024-01-31 23:03:21 | INFO | train_inner | epoch 192:   1190 / 1475 loss=2.173, nll_loss=0.522, ppl=1.44, wps=59477.3, ups=15.74, wpb=3778.2, bsz=189.3, num_updates=282900, lr=5.94543e-05, gnorm=1.034, loss_scale=16, train_wall=6, gb_free=29.7, wall=1852
2024-01-31 23:03:27 | INFO | train_inner | epoch 192:   1290 / 1475 loss=2.163, nll_loss=0.511, ppl=1.42, wps=57787, ups=15.32, wpb=3771.3, bsz=202.6, num_updates=283000, lr=5.94438e-05, gnorm=1.025, loss_scale=16, train_wall=6, gb_free=29.7, wall=1858
2024-01-31 23:03:34 | INFO | train_inner | epoch 192:   1390 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=60132, ups=15.75, wpb=3818.5, bsz=203.8, num_updates=283100, lr=5.94333e-05, gnorm=1.004, loss_scale=16, train_wall=6, gb_free=29.7, wall=1865
2024-01-31 23:03:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:03:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:03:41 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 3.577 | nll_loss 1.992 | ppl 3.98 | wps 149769 | wpb 3189.1 | bsz 163.4 | num_updates 283185 | best_loss 3.385
2024-01-31 23:03:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 283185 updates
2024-01-31 23:03:41 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint192.pt
2024-01-31 23:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint192.pt
2024-01-31 23:03:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint192.pt (epoch 192 @ 283185 updates, score 3.577) (writing took 2.3785308949882165 seconds)
2024-01-31 23:03:44 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2024-01-31 23:03:44 | INFO | train | epoch 192 | loss 2.16 | nll_loss 0.506 | ppl 1.42 | wps 56685.6 | ups 15.03 | wpb 3771.6 | bsz 195.9 | num_updates 283185 | lr 5.94244e-05 | gnorm 1.013 | loss_scale 16 | train_wall 89 | gb_free 29.7 | wall 1874
2024-01-31 23:03:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:03:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:03:44 | INFO | fairseq.trainer | begin training epoch 193
2024-01-31 23:03:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:03:45 | INFO | train_inner | epoch 193:     15 / 1475 loss=2.164, nll_loss=0.512, ppl=1.43, wps=34452.8, ups=9.17, wpb=3757.2, bsz=197.8, num_updates=283200, lr=5.94228e-05, gnorm=1.019, loss_scale=16, train_wall=6, gb_free=29.6, wall=1875
2024-01-31 23:03:51 | INFO | train_inner | epoch 193:    115 / 1475 loss=2.149, nll_loss=0.493, ppl=1.41, wps=60023.6, ups=15.78, wpb=3802.9, bsz=192.2, num_updates=283300, lr=5.94123e-05, gnorm=0.989, loss_scale=16, train_wall=6, gb_free=29.6, wall=1882
2024-01-31 23:03:57 | INFO | train_inner | epoch 193:    215 / 1475 loss=2.149, nll_loss=0.494, ppl=1.41, wps=59531.4, ups=15.8, wpb=3767.4, bsz=197.5, num_updates=283400, lr=5.94019e-05, gnorm=1.003, loss_scale=16, train_wall=6, gb_free=29.7, wall=1888
2024-01-31 23:04:04 | INFO | train_inner | epoch 193:    315 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=60028.8, ups=15.74, wpb=3813.3, bsz=201.4, num_updates=283500, lr=5.93914e-05, gnorm=0.982, loss_scale=16, train_wall=6, gb_free=29.7, wall=1895
2024-01-31 23:04:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-31 23:04:10 | INFO | train_inner | epoch 193:    416 / 1475 loss=2.156, nll_loss=0.502, ppl=1.42, wps=59299.7, ups=15.66, wpb=3785.8, bsz=194.6, num_updates=283600, lr=5.93809e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.7, wall=1901
2024-01-31 23:04:16 | INFO | train_inner | epoch 193:    516 / 1475 loss=2.153, nll_loss=0.499, ppl=1.41, wps=59125.3, ups=15.78, wpb=3748, bsz=203.1, num_updates=283700, lr=5.93704e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.6, wall=1907
2024-01-31 23:04:23 | INFO | train_inner | epoch 193:    616 / 1475 loss=2.154, nll_loss=0.501, ppl=1.41, wps=58728.5, ups=15.78, wpb=3721.3, bsz=199.7, num_updates=283800, lr=5.936e-05, gnorm=1.202, loss_scale=8, train_wall=6, gb_free=29.8, wall=1914
2024-01-31 23:04:29 | INFO | train_inner | epoch 193:    716 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=58937.2, ups=16, wpb=3683, bsz=192.5, num_updates=283900, lr=5.93495e-05, gnorm=1.06, loss_scale=8, train_wall=6, gb_free=29.7, wall=1920
2024-01-31 23:04:35 | INFO | train_inner | epoch 193:    816 / 1475 loss=2.158, nll_loss=0.504, ppl=1.42, wps=59090, ups=15.82, wpb=3734.2, bsz=196.8, num_updates=284000, lr=5.93391e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.8, wall=1926
2024-01-31 23:04:42 | INFO | train_inner | epoch 193:    916 / 1475 loss=2.163, nll_loss=0.51, ppl=1.42, wps=60340.9, ups=15.87, wpb=3802.3, bsz=190.2, num_updates=284100, lr=5.93286e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=1932
2024-01-31 23:04:48 | INFO | train_inner | epoch 193:   1016 / 1475 loss=2.169, nll_loss=0.517, ppl=1.43, wps=60259.6, ups=15.83, wpb=3806.6, bsz=188.3, num_updates=284200, lr=5.93182e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.9, wall=1939
2024-01-31 23:04:54 | INFO | train_inner | epoch 193:   1116 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=59752.9, ups=15.77, wpb=3789.7, bsz=202.2, num_updates=284300, lr=5.93078e-05, gnorm=1.03, loss_scale=8, train_wall=6, gb_free=29.6, wall=1945
2024-01-31 23:05:00 | INFO | train_inner | epoch 193:   1216 / 1475 loss=2.168, nll_loss=0.516, ppl=1.43, wps=59653.9, ups=15.88, wpb=3756.3, bsz=196.6, num_updates=284400, lr=5.92973e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.7, wall=1951
2024-01-31 23:05:07 | INFO | train_inner | epoch 193:   1316 / 1475 loss=2.163, nll_loss=0.511, ppl=1.43, wps=59428.3, ups=15.68, wpb=3789.7, bsz=201.9, num_updates=284500, lr=5.92869e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.8, wall=1958
2024-01-31 23:05:13 | INFO | train_inner | epoch 193:   1416 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=59716.1, ups=15.9, wpb=3755.7, bsz=193.8, num_updates=284600, lr=5.92765e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.7, wall=1964
2024-01-31 23:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:05:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:05:19 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 3.573 | nll_loss 1.988 | ppl 3.97 | wps 154476 | wpb 3189.1 | bsz 163.4 | num_updates 284659 | best_loss 3.385
2024-01-31 23:05:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 284659 updates
2024-01-31 23:05:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint193.pt
2024-01-31 23:05:20 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint193.pt
2024-01-31 23:05:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint193.pt (epoch 193 @ 284659 updates, score 3.573) (writing took 2.45378520200029 seconds)
2024-01-31 23:05:21 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2024-01-31 23:05:21 | INFO | train | epoch 193 | loss 2.16 | nll_loss 0.506 | ppl 1.42 | wps 56831.1 | ups 15.07 | wpb 3771.7 | bsz 195.9 | num_updates 284659 | lr 5.92704e-05 | gnorm 1.033 | loss_scale 8 | train_wall 88 | gb_free 29.7 | wall 1972
2024-01-31 23:05:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:05:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:05:21 | INFO | fairseq.trainer | begin training epoch 194
2024-01-31 23:05:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:05:24 | INFO | train_inner | epoch 194:     41 / 1475 loss=2.163, nll_loss=0.511, ppl=1.42, wps=34697.1, ups=9.18, wpb=3780.1, bsz=197.6, num_updates=284700, lr=5.92661e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.7, wall=1975
2024-01-31 23:05:30 | INFO | train_inner | epoch 194:    141 / 1475 loss=2.138, nll_loss=0.483, ppl=1.4, wps=59579, ups=15.72, wpb=3790.5, bsz=203.4, num_updates=284800, lr=5.92557e-05, gnorm=0.981, loss_scale=8, train_wall=6, gb_free=29.7, wall=1981
2024-01-31 23:05:37 | INFO | train_inner | epoch 194:    241 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=60133.6, ups=15.83, wpb=3798.1, bsz=205.1, num_updates=284900, lr=5.92453e-05, gnorm=0.984, loss_scale=8, train_wall=6, gb_free=29.7, wall=1988
2024-01-31 23:05:43 | INFO | train_inner | epoch 194:    341 / 1475 loss=2.148, nll_loss=0.493, ppl=1.41, wps=60155.3, ups=15.77, wpb=3814.5, bsz=193, num_updates=285000, lr=5.92349e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.6, wall=1994
2024-01-31 23:05:49 | INFO | train_inner | epoch 194:    441 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=58459.3, ups=15.67, wpb=3729.8, bsz=201, num_updates=285100, lr=5.92245e-05, gnorm=1.012, loss_scale=8, train_wall=6, gb_free=29.6, wall=2000
2024-01-31 23:05:56 | INFO | train_inner | epoch 194:    541 / 1475 loss=2.169, nll_loss=0.516, ppl=1.43, wps=60104.8, ups=15.91, wpb=3778.2, bsz=185.8, num_updates=285200, lr=5.92141e-05, gnorm=1.045, loss_scale=8, train_wall=6, gb_free=29.6, wall=2007
2024-01-31 23:06:02 | INFO | train_inner | epoch 194:    641 / 1475 loss=2.166, nll_loss=0.514, ppl=1.43, wps=60080.6, ups=15.85, wpb=3791.4, bsz=184.2, num_updates=285300, lr=5.92037e-05, gnorm=1.03, loss_scale=8, train_wall=6, gb_free=29.7, wall=2013
2024-01-31 23:06:08 | INFO | train_inner | epoch 194:    741 / 1475 loss=2.155, nll_loss=0.502, ppl=1.42, wps=59700.8, ups=15.71, wpb=3799.6, bsz=203.5, num_updates=285400, lr=5.91934e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.7, wall=2019
2024-01-31 23:06:15 | INFO | train_inner | epoch 194:    841 / 1475 loss=2.164, nll_loss=0.511, ppl=1.43, wps=59675.6, ups=15.84, wpb=3766.7, bsz=191, num_updates=285500, lr=5.9183e-05, gnorm=1.04, loss_scale=8, train_wall=6, gb_free=29.7, wall=2026
2024-01-31 23:06:21 | INFO | train_inner | epoch 194:    941 / 1475 loss=2.168, nll_loss=0.515, ppl=1.43, wps=60511.7, ups=15.91, wpb=3802.7, bsz=188.2, num_updates=285600, lr=5.91726e-05, gnorm=1.03, loss_scale=8, train_wall=6, gb_free=29.7, wall=2032
2024-01-31 23:06:27 | INFO | train_inner | epoch 194:   1041 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=60087.8, ups=15.77, wpb=3811.2, bsz=198.7, num_updates=285700, lr=5.91623e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.8, wall=2038
2024-01-31 23:06:34 | INFO | train_inner | epoch 194:   1141 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=59049.1, ups=15.73, wpb=3755, bsz=201.4, num_updates=285800, lr=5.91519e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=2045
2024-01-31 23:06:40 | INFO | train_inner | epoch 194:   1241 / 1475 loss=2.156, nll_loss=0.503, ppl=1.42, wps=59422.8, ups=15.75, wpb=3773.8, bsz=207, num_updates=285900, lr=5.91416e-05, gnorm=1.033, loss_scale=8, train_wall=6, gb_free=29.7, wall=2051
2024-01-31 23:06:46 | INFO | train_inner | epoch 194:   1341 / 1475 loss=2.17, nll_loss=0.519, ppl=1.43, wps=58749, ups=15.91, wpb=3693.3, bsz=184.7, num_updates=286000, lr=5.91312e-05, gnorm=1.061, loss_scale=8, train_wall=6, gb_free=29.7, wall=2057
2024-01-31 23:06:53 | INFO | train_inner | epoch 194:   1441 / 1475 loss=2.173, nll_loss=0.522, ppl=1.44, wps=59670.8, ups=15.85, wpb=3764.4, bsz=188.7, num_updates=286100, lr=5.91209e-05, gnorm=1.051, loss_scale=8, train_wall=6, gb_free=29.7, wall=2064
2024-01-31 23:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:06:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:06:57 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 3.579 | nll_loss 1.993 | ppl 3.98 | wps 159954 | wpb 3189.1 | bsz 163.4 | num_updates 286134 | best_loss 3.385
2024-01-31 23:06:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 286134 updates
2024-01-31 23:06:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint194.pt
2024-01-31 23:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint194.pt
2024-01-31 23:06:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint194.pt (epoch 194 @ 286134 updates, score 3.579) (writing took 2.4015566390007734 seconds)
2024-01-31 23:06:59 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2024-01-31 23:06:59 | INFO | train | epoch 194 | loss 2.158 | nll_loss 0.505 | ppl 1.42 | wps 56894.4 | ups 15.09 | wpb 3771.6 | bsz 195.9 | num_updates 286134 | lr 5.91174e-05 | gnorm 1.023 | loss_scale 8 | train_wall 88 | gb_free 29.7 | wall 2070
2024-01-31 23:06:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:06:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:06:59 | INFO | fairseq.trainer | begin training epoch 195
2024-01-31 23:06:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:07:03 | INFO | train_inner | epoch 195:     66 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=34236.3, ups=9.31, wpb=3678.1, bsz=192.9, num_updates=286200, lr=5.91106e-05, gnorm=1.027, loss_scale=8, train_wall=6, gb_free=29.7, wall=2074
2024-01-31 23:07:10 | INFO | train_inner | epoch 195:    166 / 1475 loss=2.146, nll_loss=0.49, ppl=1.4, wps=59743.3, ups=15.93, wpb=3750.9, bsz=186.5, num_updates=286300, lr=5.91003e-05, gnorm=1, loss_scale=8, train_wall=6, gb_free=29.8, wall=2081
2024-01-31 23:07:16 | INFO | train_inner | epoch 195:    266 / 1475 loss=2.155, nll_loss=0.501, ppl=1.41, wps=59967.7, ups=15.76, wpb=3806, bsz=197, num_updates=286400, lr=5.90899e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.7, wall=2087
2024-01-31 23:07:22 | INFO | train_inner | epoch 195:    366 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=60300.5, ups=15.74, wpb=3831.8, bsz=200.8, num_updates=286500, lr=5.90796e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.8, wall=2093
2024-01-31 23:07:29 | INFO | train_inner | epoch 195:    466 / 1475 loss=2.151, nll_loss=0.496, ppl=1.41, wps=59036, ups=15.77, wpb=3744.6, bsz=195.6, num_updates=286600, lr=5.90693e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.8, wall=2100
2024-01-31 23:07:35 | INFO | train_inner | epoch 195:    566 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=56945.6, ups=15.08, wpb=3775.5, bsz=189.6, num_updates=286700, lr=5.9059e-05, gnorm=1.026, loss_scale=8, train_wall=6, gb_free=29.9, wall=2106
2024-01-31 23:07:42 | INFO | train_inner | epoch 195:    666 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=57791.3, ups=15.3, wpb=3777.9, bsz=190.9, num_updates=286800, lr=5.90487e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.8, wall=2113
2024-01-31 23:07:49 | INFO | train_inner | epoch 195:    766 / 1475 loss=2.153, nll_loss=0.499, ppl=1.41, wps=56510.8, ups=15.1, wpb=3743.6, bsz=207, num_updates=286900, lr=5.90384e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.6, wall=2119
2024-01-31 23:07:55 | INFO | train_inner | epoch 195:    866 / 1475 loss=2.151, nll_loss=0.498, ppl=1.41, wps=55970.4, ups=15.07, wpb=3712.8, bsz=199.9, num_updates=287000, lr=5.90281e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=2126
2024-01-31 23:08:01 | INFO | train_inner | epoch 195:    966 / 1475 loss=2.153, nll_loss=0.501, ppl=1.41, wps=59486.4, ups=15.91, wpb=3738, bsz=208.4, num_updates=287100, lr=5.90179e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=2132
2024-01-31 23:08:08 | INFO | train_inner | epoch 195:   1066 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=63010.6, ups=16.43, wpb=3835.4, bsz=187.9, num_updates=287200, lr=5.90076e-05, gnorm=1.05, loss_scale=8, train_wall=6, gb_free=29.7, wall=2138
2024-01-31 23:08:14 | INFO | train_inner | epoch 195:   1166 / 1475 loss=2.174, nll_loss=0.522, ppl=1.44, wps=62867.1, ups=16.52, wpb=3806.2, bsz=187.5, num_updates=287300, lr=5.89973e-05, gnorm=1.046, loss_scale=8, train_wall=6, gb_free=29.6, wall=2144
2024-01-31 23:08:20 | INFO | train_inner | epoch 195:   1266 / 1475 loss=2.164, nll_loss=0.512, ppl=1.43, wps=61502.2, ups=16.37, wpb=3756.3, bsz=197.6, num_updates=287400, lr=5.8987e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=2151
2024-01-31 23:08:26 | INFO | train_inner | epoch 195:   1366 / 1475 loss=2.175, nll_loss=0.523, ppl=1.44, wps=62435.5, ups=16.45, wpb=3794.4, bsz=194.1, num_updates=287500, lr=5.89768e-05, gnorm=1.045, loss_scale=8, train_wall=6, gb_free=29.7, wall=2157
2024-01-31 23:08:32 | INFO | train_inner | epoch 195:   1466 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=61693, ups=16.35, wpb=3773.8, bsz=200, num_updates=287600, lr=5.89665e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.7, wall=2163
2024-01-31 23:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:08:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:08:34 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 3.578 | nll_loss 1.994 | ppl 3.98 | wps 165223 | wpb 3189.1 | bsz 163.4 | num_updates 287609 | best_loss 3.385
2024-01-31 23:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 287609 updates
2024-01-31 23:08:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint195.pt
2024-01-31 23:08:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint195.pt
2024-01-31 23:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint195.pt (epoch 195 @ 287609 updates, score 3.578) (writing took 2.411316283978522 seconds)
2024-01-31 23:08:37 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2024-01-31 23:08:37 | INFO | train | epoch 195 | loss 2.158 | nll_loss 0.504 | ppl 1.42 | wps 56989.2 | ups 15.11 | wpb 3771.6 | bsz 195.9 | num_updates 287609 | lr 5.89656e-05 | gnorm 1.021 | loss_scale 8 | train_wall 89 | gb_free 29.7 | wall 2168
2024-01-31 23:08:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:08:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:08:37 | INFO | fairseq.trainer | begin training epoch 196
2024-01-31 23:08:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:08:42 | INFO | train_inner | epoch 196:     91 / 1475 loss=2.144, nll_loss=0.488, ppl=1.4, wps=35913, ups=9.52, wpb=3770.9, bsz=196.6, num_updates=287700, lr=5.89563e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=2173
2024-01-31 23:08:48 | INFO | train_inner | epoch 196:    191 / 1475 loss=2.143, nll_loss=0.488, ppl=1.4, wps=62633.6, ups=16.36, wpb=3829.2, bsz=204.8, num_updates=287800, lr=5.8946e-05, gnorm=0.992, loss_scale=8, train_wall=6, gb_free=29.7, wall=2179
2024-01-31 23:08:55 | INFO | train_inner | epoch 196:    291 / 1475 loss=2.155, nll_loss=0.5, ppl=1.41, wps=62645, ups=16.4, wpb=3819.7, bsz=191.5, num_updates=287900, lr=5.89358e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=2185
2024-01-31 23:09:01 | INFO | train_inner | epoch 196:    391 / 1475 loss=2.137, nll_loss=0.482, ppl=1.4, wps=61661.3, ups=16.36, wpb=3768.5, bsz=208.9, num_updates=288000, lr=5.89256e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.9, wall=2192
2024-01-31 23:09:07 | INFO | train_inner | epoch 196:    491 / 1475 loss=2.16, nll_loss=0.506, ppl=1.42, wps=62836.1, ups=16.5, wpb=3808.8, bsz=193.4, num_updates=288100, lr=5.89153e-05, gnorm=1.099, loss_scale=8, train_wall=6, gb_free=29.6, wall=2198
2024-01-31 23:09:13 | INFO | train_inner | epoch 196:    591 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=61811.9, ups=16.46, wpb=3754.3, bsz=189.7, num_updates=288200, lr=5.89051e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.7, wall=2204
2024-01-31 23:09:19 | INFO | train_inner | epoch 196:    691 / 1475 loss=2.156, nll_loss=0.503, ppl=1.42, wps=62227.9, ups=16.35, wpb=3806.8, bsz=199, num_updates=288300, lr=5.88949e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=2210
2024-01-31 23:09:25 | INFO | train_inner | epoch 196:    791 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=62293.6, ups=16.41, wpb=3797.2, bsz=201.1, num_updates=288400, lr=5.88847e-05, gnorm=1.003, loss_scale=8, train_wall=6, gb_free=29.7, wall=2216
2024-01-31 23:09:31 | INFO | train_inner | epoch 196:    891 / 1475 loss=2.169, nll_loss=0.517, ppl=1.43, wps=63003, ups=16.47, wpb=3824.9, bsz=189.8, num_updates=288500, lr=5.88745e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=2222
2024-01-31 23:09:37 | INFO | train_inner | epoch 196:    991 / 1475 loss=2.157, nll_loss=0.503, ppl=1.42, wps=60826.2, ups=16.43, wpb=3702.5, bsz=199.7, num_updates=288600, lr=5.88643e-05, gnorm=1.036, loss_scale=8, train_wall=6, gb_free=29.7, wall=2228
2024-01-31 23:09:43 | INFO | train_inner | epoch 196:   1091 / 1475 loss=2.154, nll_loss=0.5, ppl=1.41, wps=61251.7, ups=16.44, wpb=3725.7, bsz=192.9, num_updates=288700, lr=5.88541e-05, gnorm=1.026, loss_scale=8, train_wall=6, gb_free=29.7, wall=2234
2024-01-31 23:09:49 | INFO | train_inner | epoch 196:   1191 / 1475 loss=2.161, nll_loss=0.509, ppl=1.42, wps=62252.2, ups=16.41, wpb=3793.2, bsz=196.4, num_updates=288800, lr=5.88439e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.7, wall=2240
2024-01-31 23:09:55 | INFO | train_inner | epoch 196:   1291 / 1475 loss=2.166, nll_loss=0.514, ppl=1.43, wps=61604.7, ups=16.44, wpb=3748, bsz=190.5, num_updates=288900, lr=5.88337e-05, gnorm=1.053, loss_scale=8, train_wall=6, gb_free=29.8, wall=2246
2024-01-31 23:10:02 | INFO | train_inner | epoch 196:   1391 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=60418.4, ups=16.44, wpb=3676.2, bsz=193, num_updates=289000, lr=5.88235e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=2252
2024-01-31 23:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:10:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:10:08 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 3.576 | nll_loss 1.991 | ppl 3.98 | wps 165980 | wpb 3189.1 | bsz 163.4 | num_updates 289084 | best_loss 3.385
2024-01-31 23:10:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 289084 updates
2024-01-31 23:10:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint196.pt
2024-01-31 23:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint196.pt
2024-01-31 23:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint196.pt (epoch 196 @ 289084 updates, score 3.576) (writing took 2.5182455059839413 seconds)
2024-01-31 23:10:11 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2024-01-31 23:10:11 | INFO | train | epoch 196 | loss 2.156 | nll_loss 0.502 | ppl 1.42 | wps 58985.2 | ups 15.64 | wpb 3771.6 | bsz 195.9 | num_updates 289084 | lr 5.8815e-05 | gnorm 1.028 | loss_scale 8 | train_wall 86 | gb_free 29.8 | wall 2262
2024-01-31 23:10:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:10:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:10:11 | INFO | fairseq.trainer | begin training epoch 197
2024-01-31 23:10:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:10:12 | INFO | train_inner | epoch 197:     16 / 1475 loss=2.167, nll_loss=0.515, ppl=1.43, wps=35699.2, ups=9.46, wpb=3775.5, bsz=191.8, num_updates=289100, lr=5.88134e-05, gnorm=1.04, loss_scale=8, train_wall=6, gb_free=29.7, wall=2263
2024-01-31 23:10:18 | INFO | train_inner | epoch 197:    116 / 1475 loss=2.147, nll_loss=0.492, ppl=1.41, wps=62390.4, ups=16.46, wpb=3791.3, bsz=200.3, num_updates=289200, lr=5.88032e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.8, wall=2269
2024-01-31 23:10:24 | INFO | train_inner | epoch 197:    216 / 1475 loss=2.151, nll_loss=0.496, ppl=1.41, wps=62031, ups=16.45, wpb=3769.8, bsz=186.1, num_updates=289300, lr=5.8793e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.8, wall=2275
2024-01-31 23:10:30 | INFO | train_inner | epoch 197:    316 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=61582.6, ups=16.49, wpb=3733.6, bsz=202, num_updates=289400, lr=5.87829e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.6, wall=2281
2024-01-31 23:10:36 | INFO | train_inner | epoch 197:    416 / 1475 loss=2.147, nll_loss=0.493, ppl=1.41, wps=61315.8, ups=16.26, wpb=3770.2, bsz=195.2, num_updates=289500, lr=5.87727e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.7, wall=2287
2024-01-31 23:10:43 | INFO | train_inner | epoch 197:    516 / 1475 loss=2.146, nll_loss=0.491, ppl=1.41, wps=61207.7, ups=16.29, wpb=3757.7, bsz=202.7, num_updates=289600, lr=5.87626e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.7, wall=2294
2024-01-31 23:10:49 | INFO | train_inner | epoch 197:    616 / 1475 loss=2.156, nll_loss=0.502, ppl=1.42, wps=62646, ups=16.58, wpb=3777.6, bsz=193, num_updates=289700, lr=5.87524e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=2300
2024-01-31 23:10:55 | INFO | train_inner | epoch 197:    716 / 1475 loss=2.164, nll_loss=0.511, ppl=1.42, wps=61898.4, ups=16.17, wpb=3827.6, bsz=188.8, num_updates=289800, lr=5.87423e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.7, wall=2306
2024-01-31 23:11:01 | INFO | train_inner | epoch 197:    816 / 1475 loss=2.15, nll_loss=0.496, ppl=1.41, wps=59120.2, ups=15.65, wpb=3776.9, bsz=202.3, num_updates=289900, lr=5.87321e-05, gnorm=1.005, loss_scale=8, train_wall=6, gb_free=29.7, wall=2312
2024-01-31 23:11:07 | INFO | train_inner | epoch 197:    916 / 1475 loss=2.169, nll_loss=0.518, ppl=1.43, wps=62636.1, ups=16.53, wpb=3789.2, bsz=188.3, num_updates=290000, lr=5.8722e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.7, wall=2318
2024-01-31 23:11:13 | INFO | train_inner | epoch 197:   1016 / 1475 loss=2.149, nll_loss=0.494, ppl=1.41, wps=61691.6, ups=16.33, wpb=3777.4, bsz=208.7, num_updates=290100, lr=5.87119e-05, gnorm=1.013, loss_scale=8, train_wall=6, gb_free=29.7, wall=2324
2024-01-31 23:11:19 | INFO | train_inner | epoch 197:   1116 / 1475 loss=2.152, nll_loss=0.499, ppl=1.41, wps=59449.4, ups=16.5, wpb=3602.5, bsz=191.1, num_updates=290200, lr=5.87018e-05, gnorm=1.073, loss_scale=8, train_wall=6, gb_free=29.7, wall=2330
2024-01-31 23:11:26 | INFO | train_inner | epoch 197:   1216 / 1475 loss=2.164, nll_loss=0.513, ppl=1.43, wps=62754.6, ups=16.38, wpb=3831.7, bsz=194.9, num_updates=290300, lr=5.86917e-05, gnorm=1.001, loss_scale=8, train_wall=6, gb_free=29.7, wall=2336
2024-01-31 23:11:32 | INFO | train_inner | epoch 197:   1316 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=62255.9, ups=16.47, wpb=3780.4, bsz=194, num_updates=290400, lr=5.86816e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.7, wall=2343
2024-01-31 23:11:38 | INFO | train_inner | epoch 197:   1416 / 1475 loss=2.161, nll_loss=0.51, ppl=1.42, wps=62270.7, ups=16.42, wpb=3792, bsz=201.6, num_updates=290500, lr=5.86715e-05, gnorm=1.043, loss_scale=8, train_wall=6, gb_free=29.7, wall=2349
2024-01-31 23:11:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:11:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:11:43 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 3.583 | nll_loss 1.997 | ppl 3.99 | wps 166288 | wpb 3189.1 | bsz 163.4 | num_updates 290559 | best_loss 3.385
2024-01-31 23:11:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 290559 updates
2024-01-31 23:11:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint197.pt
2024-01-31 23:11:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint197.pt
2024-01-31 23:11:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint197.pt (epoch 197 @ 290559 updates, score 3.583) (writing took 2.3598825510125607 seconds)
2024-01-31 23:11:46 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2024-01-31 23:11:46 | INFO | train | epoch 197 | loss 2.156 | nll_loss 0.502 | ppl 1.42 | wps 58873.2 | ups 15.61 | wpb 3771.6 | bsz 195.9 | num_updates 290559 | lr 5.86655e-05 | gnorm 1.025 | loss_scale 8 | train_wall 86 | gb_free 29.7 | wall 2356
2024-01-31 23:11:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:11:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:11:46 | INFO | fairseq.trainer | begin training epoch 198
2024-01-31 23:11:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:11:48 | INFO | train_inner | epoch 198:     41 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=36476.4, ups=9.61, wpb=3795.1, bsz=187.6, num_updates=290600, lr=5.86614e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.7, wall=2359
2024-01-31 23:11:54 | INFO | train_inner | epoch 198:    141 / 1475 loss=2.143, nll_loss=0.488, ppl=1.4, wps=61869.1, ups=16.45, wpb=3761.3, bsz=196.8, num_updates=290700, lr=5.86513e-05, gnorm=1.006, loss_scale=8, train_wall=6, gb_free=29.7, wall=2365
2024-01-31 23:12:00 | INFO | train_inner | epoch 198:    241 / 1475 loss=2.138, nll_loss=0.481, ppl=1.4, wps=60749.1, ups=16.48, wpb=3686.5, bsz=200.9, num_updates=290800, lr=5.86412e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.9, wall=2371
2024-01-31 23:12:06 | INFO | train_inner | epoch 198:    341 / 1475 loss=2.145, nll_loss=0.491, ppl=1.41, wps=62359.5, ups=16.42, wpb=3798.4, bsz=197.9, num_updates=290900, lr=5.86311e-05, gnorm=1.015, loss_scale=8, train_wall=6, gb_free=29.6, wall=2377
2024-01-31 23:12:12 | INFO | train_inner | epoch 198:    441 / 1475 loss=2.155, nll_loss=0.5, ppl=1.41, wps=62019.6, ups=16.52, wpb=3755, bsz=191.3, num_updates=291000, lr=5.8621e-05, gnorm=1.02, loss_scale=8, train_wall=6, gb_free=29.6, wall=2383
2024-01-31 23:12:19 | INFO | train_inner | epoch 198:    541 / 1475 loss=2.147, nll_loss=0.493, ppl=1.41, wps=62291.8, ups=16.42, wpb=3793.7, bsz=202.2, num_updates=291100, lr=5.8611e-05, gnorm=0.996, loss_scale=8, train_wall=6, gb_free=29.7, wall=2389
2024-01-31 23:12:25 | INFO | train_inner | epoch 198:    641 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=62222.1, ups=16.4, wpb=3794.3, bsz=192.6, num_updates=291200, lr=5.86009e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.7, wall=2396
2024-01-31 23:12:31 | INFO | train_inner | epoch 198:    741 / 1475 loss=2.166, nll_loss=0.513, ppl=1.43, wps=62224.2, ups=16.5, wpb=3772.3, bsz=184.2, num_updates=291300, lr=5.85908e-05, gnorm=1.054, loss_scale=8, train_wall=6, gb_free=29.7, wall=2402
2024-01-31 23:12:37 | INFO | train_inner | epoch 198:    841 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=63297.4, ups=16.33, wpb=3876.1, bsz=201.4, num_updates=291400, lr=5.85808e-05, gnorm=0.999, loss_scale=8, train_wall=6, gb_free=29.7, wall=2408
2024-01-31 23:12:43 | INFO | train_inner | epoch 198:    941 / 1475 loss=2.156, nll_loss=0.503, ppl=1.42, wps=61038, ups=16.4, wpb=3721.8, bsz=201.4, num_updates=291500, lr=5.85707e-05, gnorm=1.033, loss_scale=8, train_wall=6, gb_free=29.7, wall=2414
2024-01-31 23:12:49 | INFO | train_inner | epoch 198:   1041 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=61253.2, ups=16.48, wpb=3716.1, bsz=196.1, num_updates=291600, lr=5.85607e-05, gnorm=1.022, loss_scale=8, train_wall=6, gb_free=29.7, wall=2420
2024-01-31 23:12:55 | INFO | train_inner | epoch 198:   1141 / 1475 loss=2.159, nll_loss=0.507, ppl=1.42, wps=61494, ups=16.38, wpb=3755.3, bsz=196.2, num_updates=291700, lr=5.85507e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.7, wall=2426
2024-01-31 23:13:01 | INFO | train_inner | epoch 198:   1241 / 1475 loss=2.167, nll_loss=0.516, ppl=1.43, wps=62042.2, ups=16.48, wpb=3765.5, bsz=194.5, num_updates=291800, lr=5.85406e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.8, wall=2432
2024-01-31 23:13:07 | INFO | train_inner | epoch 198:   1341 / 1475 loss=2.175, nll_loss=0.524, ppl=1.44, wps=63042, ups=16.61, wpb=3796.4, bsz=184.5, num_updates=291900, lr=5.85306e-05, gnorm=1.065, loss_scale=8, train_wall=6, gb_free=29.7, wall=2438
2024-01-31 23:13:13 | INFO | train_inner | epoch 198:   1441 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=61997.9, ups=16.41, wpb=3778.8, bsz=208.6, num_updates=292000, lr=5.85206e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=2444
2024-01-31 23:13:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:13:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:13:17 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 3.574 | nll_loss 1.991 | ppl 3.97 | wps 165997 | wpb 3189.1 | bsz 163.4 | num_updates 292034 | best_loss 3.385
2024-01-31 23:13:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 292034 updates
2024-01-31 23:13:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint198.pt
2024-01-31 23:13:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint198.pt
2024-01-31 23:13:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint198.pt (epoch 198 @ 292034 updates, score 3.574) (writing took 2.4129896400263533 seconds)
2024-01-31 23:13:20 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2024-01-31 23:13:20 | INFO | train | epoch 198 | loss 2.155 | nll_loss 0.501 | ppl 1.42 | wps 59139.4 | ups 15.68 | wpb 3771.6 | bsz 195.9 | num_updates 292034 | lr 5.85172e-05 | gnorm 1.023 | loss_scale 8 | train_wall 86 | gb_free 29.7 | wall 2451
2024-01-31 23:13:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:13:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:13:20 | INFO | fairseq.trainer | begin training epoch 199
2024-01-31 23:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:13:24 | INFO | train_inner | epoch 199:     66 / 1475 loss=2.151, nll_loss=0.497, ppl=1.41, wps=36412.9, ups=9.51, wpb=3830.8, bsz=192.2, num_updates=292100, lr=5.85106e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.7, wall=2455
2024-01-31 23:13:30 | INFO | train_inner | epoch 199:    166 / 1475 loss=2.147, nll_loss=0.492, ppl=1.41, wps=62919, ups=16.42, wpb=3831.5, bsz=194.5, num_updates=292200, lr=5.85005e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=2461
2024-01-31 23:13:36 | INFO | train_inner | epoch 199:    266 / 1475 loss=2.147, nll_loss=0.492, ppl=1.41, wps=62088.8, ups=16.58, wpb=3744.7, bsz=190.9, num_updates=292300, lr=5.84905e-05, gnorm=1.016, loss_scale=8, train_wall=6, gb_free=29.9, wall=2467
2024-01-31 23:13:42 | INFO | train_inner | epoch 199:    366 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=62844.1, ups=16.41, wpb=3829.8, bsz=187.9, num_updates=292400, lr=5.84805e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=2473
2024-01-31 23:13:48 | INFO | train_inner | epoch 199:    466 / 1475 loss=2.14, nll_loss=0.485, ppl=1.4, wps=61378.8, ups=16.39, wpb=3744.3, bsz=209.3, num_updates=292500, lr=5.84705e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.8, wall=2479
2024-01-31 23:13:54 | INFO | train_inner | epoch 199:    566 / 1475 loss=2.151, nll_loss=0.497, ppl=1.41, wps=61533.7, ups=16.45, wpb=3740.9, bsz=193.3, num_updates=292600, lr=5.84605e-05, gnorm=1.042, loss_scale=8, train_wall=6, gb_free=29.6, wall=2485
2024-01-31 23:14:00 | INFO | train_inner | epoch 199:    666 / 1475 loss=2.145, nll_loss=0.491, ppl=1.41, wps=61761.3, ups=16.37, wpb=3772.1, bsz=206.2, num_updates=292700, lr=5.84506e-05, gnorm=0.994, loss_scale=8, train_wall=6, gb_free=29.7, wall=2491
2024-01-31 23:14:06 | INFO | train_inner | epoch 199:    766 / 1475 loss=2.148, nll_loss=0.494, ppl=1.41, wps=60443.8, ups=16.11, wpb=3751, bsz=198.1, num_updates=292800, lr=5.84406e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=2497
2024-01-31 23:14:13 | INFO | train_inner | epoch 199:    866 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=60364.3, ups=16.31, wpb=3701.4, bsz=201.5, num_updates=292900, lr=5.84306e-05, gnorm=1.011, loss_scale=8, train_wall=6, gb_free=29.7, wall=2504
2024-01-31 23:14:19 | INFO | train_inner | epoch 199:    966 / 1475 loss=2.163, nll_loss=0.511, ppl=1.42, wps=62589.4, ups=16.29, wpb=3842.7, bsz=188.9, num_updates=293000, lr=5.84206e-05, gnorm=1.039, loss_scale=8, train_wall=6, gb_free=29.8, wall=2510
2024-01-31 23:14:25 | INFO | train_inner | epoch 199:   1066 / 1475 loss=2.158, nll_loss=0.506, ppl=1.42, wps=59953.3, ups=16.48, wpb=3638.6, bsz=200.4, num_updates=293100, lr=5.84107e-05, gnorm=1.051, loss_scale=8, train_wall=6, gb_free=29.7, wall=2516
2024-01-31 23:14:31 | INFO | train_inner | epoch 199:   1166 / 1475 loss=2.176, nll_loss=0.525, ppl=1.44, wps=62779.2, ups=16.48, wpb=3809.4, bsz=185.7, num_updates=293200, lr=5.84007e-05, gnorm=1.058, loss_scale=8, train_wall=6, gb_free=29.8, wall=2522
2024-01-31 23:14:37 | INFO | train_inner | epoch 199:   1266 / 1475 loss=2.166, nll_loss=0.514, ppl=1.43, wps=62577, ups=16.4, wpb=3815.2, bsz=200.5, num_updates=293300, lr=5.83907e-05, gnorm=1.045, loss_scale=8, train_wall=6, gb_free=29.8, wall=2528
2024-01-31 23:14:43 | INFO | train_inner | epoch 199:   1366 / 1475 loss=2.167, nll_loss=0.515, ppl=1.43, wps=62115.3, ups=16.5, wpb=3765.2, bsz=192.6, num_updates=293400, lr=5.83808e-05, gnorm=1.057, loss_scale=8, train_wall=6, gb_free=29.7, wall=2534
2024-01-31 23:14:49 | INFO | train_inner | epoch 199:   1466 / 1475 loss=2.163, nll_loss=0.511, ppl=1.43, wps=62496.2, ups=16.5, wpb=3788, bsz=197, num_updates=293500, lr=5.83708e-05, gnorm=1.038, loss_scale=8, train_wall=6, gb_free=29.7, wall=2540
2024-01-31 23:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:14:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:14:52 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 3.583 | nll_loss 1.997 | ppl 3.99 | wps 162300 | wpb 3189.1 | bsz 163.4 | num_updates 293509 | best_loss 3.385
2024-01-31 23:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 293509 updates
2024-01-31 23:14:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint199.pt
2024-01-31 23:14:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint199.pt
2024-01-31 23:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint199.pt (epoch 199 @ 293509 updates, score 3.583) (writing took 2.379446560051292 seconds)
2024-01-31 23:14:54 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2024-01-31 23:14:54 | INFO | train | epoch 199 | loss 2.155 | nll_loss 0.501 | ppl 1.42 | wps 58952.5 | ups 15.63 | wpb 3771.6 | bsz 195.9 | num_updates 293509 | lr 5.83699e-05 | gnorm 1.026 | loss_scale 8 | train_wall 86 | gb_free 29.8 | wall 2545
2024-01-31 23:14:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:14:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:14:54 | INFO | fairseq.trainer | begin training epoch 200
2024-01-31 23:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:15:00 | INFO | train_inner | epoch 200:     91 / 1475 loss=2.142, nll_loss=0.485, ppl=1.4, wps=35914.9, ups=9.53, wpb=3767, bsz=197.8, num_updates=293600, lr=5.83609e-05, gnorm=1.086, loss_scale=8, train_wall=6, gb_free=29.8, wall=2551
2024-01-31 23:15:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2024-01-31 23:15:06 | INFO | train_inner | epoch 200:    192 / 1475 loss=2.14, nll_loss=0.485, ppl=1.4, wps=61757.1, ups=16.22, wpb=3808.4, bsz=199.4, num_updates=293700, lr=5.8351e-05, gnorm=0.997, loss_scale=4, train_wall=6, gb_free=29.7, wall=2557
2024-01-31 23:15:12 | INFO | train_inner | epoch 200:    292 / 1475 loss=2.145, nll_loss=0.49, ppl=1.4, wps=62693.5, ups=16.36, wpb=3831.6, bsz=204.7, num_updates=293800, lr=5.8341e-05, gnorm=1.002, loss_scale=4, train_wall=6, gb_free=29.7, wall=2563
2024-01-31 23:15:18 | INFO | train_inner | epoch 200:    392 / 1475 loss=2.143, nll_loss=0.488, ppl=1.4, wps=61999.3, ups=16.52, wpb=3752.6, bsz=197.4, num_updates=293900, lr=5.83311e-05, gnorm=1.034, loss_scale=4, train_wall=6, gb_free=29.9, wall=2569
2024-01-31 23:15:24 | INFO | train_inner | epoch 200:    492 / 1475 loss=2.147, nll_loss=0.493, ppl=1.41, wps=61590, ups=16.41, wpb=3752.7, bsz=192.3, num_updates=294000, lr=5.83212e-05, gnorm=1.031, loss_scale=4, train_wall=6, gb_free=29.7, wall=2575
2024-01-31 23:15:30 | INFO | train_inner | epoch 200:    592 / 1475 loss=2.147, nll_loss=0.493, ppl=1.41, wps=61589.1, ups=16.45, wpb=3743.1, bsz=198.7, num_updates=294100, lr=5.83113e-05, gnorm=1.008, loss_scale=4, train_wall=6, gb_free=29.7, wall=2581
2024-01-31 23:15:36 | INFO | train_inner | epoch 200:    692 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=61333.9, ups=16.42, wpb=3735.8, bsz=198.2, num_updates=294200, lr=5.83014e-05, gnorm=1.059, loss_scale=4, train_wall=6, gb_free=29.8, wall=2587
2024-01-31 23:15:42 | INFO | train_inner | epoch 200:    792 / 1475 loss=2.153, nll_loss=0.499, ppl=1.41, wps=61853.8, ups=16.48, wpb=3752.6, bsz=195.8, num_updates=294300, lr=5.82915e-05, gnorm=1.028, loss_scale=4, train_wall=6, gb_free=29.7, wall=2593
2024-01-31 23:15:48 | INFO | train_inner | epoch 200:    892 / 1475 loss=2.162, nll_loss=0.51, ppl=1.42, wps=62915.3, ups=16.38, wpb=3840.7, bsz=194.8, num_updates=294400, lr=5.82816e-05, gnorm=1.01, loss_scale=4, train_wall=6, gb_free=29.6, wall=2599
2024-01-31 23:15:54 | INFO | train_inner | epoch 200:    992 / 1475 loss=2.157, nll_loss=0.503, ppl=1.42, wps=60858.1, ups=16.47, wpb=3694.3, bsz=192.8, num_updates=294500, lr=5.82717e-05, gnorm=1.018, loss_scale=4, train_wall=6, gb_free=29.8, wall=2605
2024-01-31 23:16:01 | INFO | train_inner | epoch 200:   1092 / 1475 loss=2.154, nll_loss=0.502, ppl=1.42, wps=61815.8, ups=16.33, wpb=3786.3, bsz=196.6, num_updates=294600, lr=5.82618e-05, gnorm=1.013, loss_scale=4, train_wall=6, gb_free=29.7, wall=2611
2024-01-31 23:16:07 | INFO | train_inner | epoch 200:   1192 / 1475 loss=2.172, nll_loss=0.521, ppl=1.43, wps=61781.1, ups=16.52, wpb=3739.5, bsz=177.4, num_updates=294700, lr=5.82519e-05, gnorm=1.089, loss_scale=4, train_wall=6, gb_free=29.7, wall=2618
2024-01-31 23:16:13 | INFO | train_inner | epoch 200:   1292 / 1475 loss=2.164, nll_loss=0.513, ppl=1.43, wps=60303, ups=15.86, wpb=3801.5, bsz=191.4, num_updates=294800, lr=5.8242e-05, gnorm=1.05, loss_scale=4, train_wall=6, gb_free=29.7, wall=2624
2024-01-31 23:16:19 | INFO | train_inner | epoch 200:   1392 / 1475 loss=2.159, nll_loss=0.507, ppl=1.42, wps=62729.5, ups=16.48, wpb=3805.9, bsz=207, num_updates=294900, lr=5.82321e-05, gnorm=1.035, loss_scale=4, train_wall=6, gb_free=29.7, wall=2630
2024-01-31 23:16:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:16:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:16:26 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 3.581 | nll_loss 1.997 | ppl 3.99 | wps 165315 | wpb 3189.1 | bsz 163.4 | num_updates 294983 | best_loss 3.385
2024-01-31 23:16:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 294983 updates
2024-01-31 23:16:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint200.pt
2024-01-31 23:16:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint200.pt
2024-01-31 23:16:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint200.pt (epoch 200 @ 294983 updates, score 3.581) (writing took 2.3851748509332538 seconds)
2024-01-31 23:16:28 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2024-01-31 23:16:28 | INFO | train | epoch 200 | loss 2.153 | nll_loss 0.5 | ppl 1.41 | wps 58917.6 | ups 15.62 | wpb 3771.5 | bsz 195.9 | num_updates 294983 | lr 5.82239e-05 | gnorm 1.033 | loss_scale 4 | train_wall 86 | gb_free 29.8 | wall 2639
2024-01-31 23:16:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:16:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:16:28 | INFO | fairseq.trainer | begin training epoch 201
2024-01-31 23:16:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:16:29 | INFO | train_inner | epoch 201:     17 / 1475 loss=2.164, nll_loss=0.512, ppl=1.43, wps=36001.9, ups=9.54, wpb=3773.2, bsz=190, num_updates=295000, lr=5.82223e-05, gnorm=1.041, loss_scale=4, train_wall=6, gb_free=29.6, wall=2640
2024-01-31 23:16:36 | INFO | train_inner | epoch 201:    117 / 1475 loss=2.132, nll_loss=0.475, ppl=1.39, wps=60829.6, ups=16.42, wpb=3703.8, bsz=198.4, num_updates=295100, lr=5.82124e-05, gnorm=0.989, loss_scale=4, train_wall=6, gb_free=29.7, wall=2646
2024-01-31 23:16:42 | INFO | train_inner | epoch 201:    217 / 1475 loss=2.136, nll_loss=0.48, ppl=1.4, wps=61903.5, ups=16.44, wpb=3765.5, bsz=201.2, num_updates=295200, lr=5.82025e-05, gnorm=0.989, loss_scale=4, train_wall=6, gb_free=29.8, wall=2653
2024-01-31 23:16:48 | INFO | train_inner | epoch 201:    317 / 1475 loss=2.141, nll_loss=0.486, ppl=1.4, wps=61436.9, ups=16.42, wpb=3741.1, bsz=205.6, num_updates=295300, lr=5.81927e-05, gnorm=1, loss_scale=4, train_wall=6, gb_free=29.6, wall=2659
2024-01-31 23:16:54 | INFO | train_inner | epoch 201:    417 / 1475 loss=2.159, nll_loss=0.505, ppl=1.42, wps=62036.7, ups=16.5, wpb=3760.1, bsz=183.6, num_updates=295400, lr=5.81828e-05, gnorm=1.04, loss_scale=4, train_wall=6, gb_free=29.7, wall=2665
2024-01-31 23:17:00 | INFO | train_inner | epoch 201:    517 / 1475 loss=2.138, nll_loss=0.482, ppl=1.4, wps=61770.7, ups=16.42, wpb=3762.8, bsz=207.4, num_updates=295500, lr=5.8173e-05, gnorm=1.004, loss_scale=4, train_wall=6, gb_free=29.7, wall=2671
2024-01-31 23:17:06 | INFO | train_inner | epoch 201:    617 / 1475 loss=2.156, nll_loss=0.502, ppl=1.42, wps=62869.9, ups=16.56, wpb=3795.7, bsz=191.2, num_updates=295600, lr=5.81631e-05, gnorm=1.03, loss_scale=4, train_wall=6, gb_free=29.7, wall=2677
2024-01-31 23:17:12 | INFO | train_inner | epoch 201:    717 / 1475 loss=2.162, nll_loss=0.509, ppl=1.42, wps=61792, ups=16.51, wpb=3742.7, bsz=183.8, num_updates=295700, lr=5.81533e-05, gnorm=1.057, loss_scale=4, train_wall=6, gb_free=29.7, wall=2683
2024-01-31 23:17:18 | INFO | train_inner | epoch 201:    817 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62558.6, ups=16.49, wpb=3794.2, bsz=190.8, num_updates=295800, lr=5.81435e-05, gnorm=1.052, loss_scale=4, train_wall=6, gb_free=29.8, wall=2689
2024-01-31 23:17:24 | INFO | train_inner | epoch 201:    917 / 1475 loss=2.164, nll_loss=0.511, ppl=1.43, wps=61270.7, ups=16.4, wpb=3737, bsz=187.2, num_updates=295900, lr=5.81336e-05, gnorm=1.037, loss_scale=4, train_wall=6, gb_free=29.7, wall=2695
2024-01-31 23:17:30 | INFO | train_inner | epoch 201:   1017 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62814.6, ups=16.34, wpb=3845.2, bsz=195.4, num_updates=296000, lr=5.81238e-05, gnorm=1.001, loss_scale=4, train_wall=6, gb_free=29.7, wall=2701
2024-01-31 23:17:36 | INFO | train_inner | epoch 201:   1117 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=62148.6, ups=16.4, wpb=3789.8, bsz=196.1, num_updates=296100, lr=5.8114e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.8, wall=2707
2024-01-31 23:17:42 | INFO | train_inner | epoch 201:   1217 / 1475 loss=2.149, nll_loss=0.495, ppl=1.41, wps=60967.7, ups=16.4, wpb=3716.9, bsz=202.2, num_updates=296200, lr=5.81042e-05, gnorm=1.02, loss_scale=4, train_wall=6, gb_free=29.7, wall=2713
2024-01-31 23:17:49 | INFO | train_inner | epoch 201:   1317 / 1475 loss=2.161, nll_loss=0.509, ppl=1.42, wps=61561.2, ups=16.49, wpb=3733.3, bsz=193.9, num_updates=296300, lr=5.80944e-05, gnorm=1.039, loss_scale=4, train_wall=6, gb_free=29.7, wall=2719
2024-01-31 23:17:55 | INFO | train_inner | epoch 201:   1417 / 1475 loss=2.156, nll_loss=0.504, ppl=1.42, wps=62242.9, ups=16.26, wpb=3828.4, bsz=202.2, num_updates=296400, lr=5.80846e-05, gnorm=1.019, loss_scale=4, train_wall=6, gb_free=29.6, wall=2726
2024-01-31 23:17:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:17:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:18:00 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 3.578 | nll_loss 1.997 | ppl 3.99 | wps 166330 | wpb 3189.1 | bsz 163.4 | num_updates 296458 | best_loss 3.385
2024-01-31 23:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 296458 updates
2024-01-31 23:18:00 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint201.pt
2024-01-31 23:18:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint201.pt
2024-01-31 23:18:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint201.pt (epoch 201 @ 296458 updates, score 3.578) (writing took 2.3795190940145403 seconds)
2024-01-31 23:18:02 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2024-01-31 23:18:02 | INFO | train | epoch 201 | loss 2.152 | nll_loss 0.499 | ppl 1.41 | wps 59086.8 | ups 15.67 | wpb 3771.6 | bsz 195.9 | num_updates 296458 | lr 5.80789e-05 | gnorm 1.021 | loss_scale 4 | train_wall 86 | gb_free 29.6 | wall 2733
2024-01-31 23:18:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:18:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:18:03 | INFO | fairseq.trainer | begin training epoch 202
2024-01-31 23:18:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:18:05 | INFO | train_inner | epoch 202:     42 / 1475 loss=2.155, nll_loss=0.501, ppl=1.42, wps=36435.9, ups=9.58, wpb=3802.3, bsz=200.7, num_updates=296500, lr=5.80748e-05, gnorm=1.026, loss_scale=4, train_wall=6, gb_free=29.6, wall=2736
2024-01-31 23:18:11 | INFO | train_inner | epoch 202:    142 / 1475 loss=2.128, nll_loss=0.472, ppl=1.39, wps=61881.2, ups=16.38, wpb=3777.5, bsz=203.6, num_updates=296600, lr=5.8065e-05, gnorm=0.988, loss_scale=4, train_wall=6, gb_free=29.7, wall=2742
2024-01-31 23:18:17 | INFO | train_inner | epoch 202:    242 / 1475 loss=2.137, nll_loss=0.482, ppl=1.4, wps=62222.5, ups=16.45, wpb=3781.5, bsz=205.7, num_updates=296700, lr=5.80552e-05, gnorm=1.017, loss_scale=4, train_wall=6, gb_free=29.9, wall=2748
2024-01-31 23:18:23 | INFO | train_inner | epoch 202:    342 / 1475 loss=2.148, nll_loss=0.494, ppl=1.41, wps=62151.5, ups=16.35, wpb=3800.5, bsz=196.1, num_updates=296800, lr=5.80454e-05, gnorm=0.999, loss_scale=4, train_wall=6, gb_free=29.6, wall=2754
2024-01-31 23:18:29 | INFO | train_inner | epoch 202:    442 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=61241.3, ups=16.52, wpb=3706.9, bsz=188.7, num_updates=296900, lr=5.80357e-05, gnorm=1.07, loss_scale=4, train_wall=6, gb_free=29.9, wall=2760
2024-01-31 23:18:36 | INFO | train_inner | epoch 202:    542 / 1475 loss=2.141, nll_loss=0.486, ppl=1.4, wps=61757.7, ups=16.41, wpb=3764.4, bsz=199.4, num_updates=297000, lr=5.80259e-05, gnorm=1.013, loss_scale=4, train_wall=6, gb_free=29.7, wall=2766
2024-01-31 23:18:42 | INFO | train_inner | epoch 202:    642 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=62092.6, ups=16.57, wpb=3746.5, bsz=188.3, num_updates=297100, lr=5.80161e-05, gnorm=1.046, loss_scale=4, train_wall=6, gb_free=29.7, wall=2773
2024-01-31 23:18:48 | INFO | train_inner | epoch 202:    742 / 1475 loss=2.154, nll_loss=0.501, ppl=1.42, wps=63209.4, ups=16.41, wpb=3851.1, bsz=191, num_updates=297200, lr=5.80064e-05, gnorm=1.009, loss_scale=4, train_wall=6, gb_free=29.7, wall=2779
2024-01-31 23:18:54 | INFO | train_inner | epoch 202:    842 / 1475 loss=2.167, nll_loss=0.515, ppl=1.43, wps=63154.7, ups=16.51, wpb=3824.1, bsz=194.1, num_updates=297300, lr=5.79966e-05, gnorm=1.041, loss_scale=4, train_wall=6, gb_free=29.6, wall=2785
2024-01-31 23:19:00 | INFO | train_inner | epoch 202:    942 / 1475 loss=2.174, nll_loss=0.523, ppl=1.44, wps=62565.7, ups=16.68, wpb=3751.1, bsz=179, num_updates=297400, lr=5.79869e-05, gnorm=1.086, loss_scale=4, train_wall=6, gb_free=29.8, wall=2791
2024-01-31 23:19:06 | INFO | train_inner | epoch 202:   1042 / 1475 loss=2.155, nll_loss=0.502, ppl=1.42, wps=61016.4, ups=16.38, wpb=3724, bsz=193.8, num_updates=297500, lr=5.79771e-05, gnorm=1.017, loss_scale=4, train_wall=6, gb_free=29.7, wall=2797
2024-01-31 23:19:12 | INFO | train_inner | epoch 202:   1142 / 1475 loss=2.147, nll_loss=0.494, ppl=1.41, wps=61207.7, ups=16.45, wpb=3720.5, bsz=208.1, num_updates=297600, lr=5.79674e-05, gnorm=1.026, loss_scale=4, train_wall=6, gb_free=29.7, wall=2803
2024-01-31 23:19:18 | INFO | train_inner | epoch 202:   1242 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62066.6, ups=16.5, wpb=3760.7, bsz=198.6, num_updates=297700, lr=5.79576e-05, gnorm=1.036, loss_scale=4, train_wall=6, gb_free=29.7, wall=2809
2024-01-31 23:19:24 | INFO | train_inner | epoch 202:   1342 / 1475 loss=2.155, nll_loss=0.502, ppl=1.42, wps=62853.2, ups=16.43, wpb=3825, bsz=203.8, num_updates=297800, lr=5.79479e-05, gnorm=1.187, loss_scale=4, train_wall=6, gb_free=29.7, wall=2815
2024-01-31 23:19:30 | INFO | train_inner | epoch 202:   1442 / 1475 loss=2.159, nll_loss=0.506, ppl=1.42, wps=62233.9, ups=16.45, wpb=3782.9, bsz=195, num_updates=297900, lr=5.79382e-05, gnorm=1.044, loss_scale=4, train_wall=6, gb_free=29.9, wall=2821
2024-01-31 23:19:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:19:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:19:34 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 3.572 | nll_loss 1.99 | ppl 3.97 | wps 165458 | wpb 3189.1 | bsz 163.4 | num_updates 297933 | best_loss 3.385
2024-01-31 23:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 297933 updates
2024-01-31 23:19:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint202.pt
2024-01-31 23:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint202.pt
2024-01-31 23:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint202.pt (epoch 202 @ 297933 updates, score 3.572) (writing took 2.3489547909703106 seconds)
2024-01-31 23:19:36 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2024-01-31 23:19:36 | INFO | train | epoch 202 | loss 2.152 | nll_loss 0.498 | ppl 1.41 | wps 59248.9 | ups 15.71 | wpb 3771.6 | bsz 195.9 | num_updates 297933 | lr 5.7935e-05 | gnorm 1.042 | loss_scale 4 | train_wall 86 | gb_free 29.8 | wall 2827
2024-01-31 23:19:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:19:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:19:36 | INFO | fairseq.trainer | begin training epoch 203
2024-01-31 23:19:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:19:41 | INFO | train_inner | epoch 203:     67 / 1475 loss=2.152, nll_loss=0.497, ppl=1.41, wps=36448.5, ups=9.64, wpb=3781.7, bsz=192.1, num_updates=298000, lr=5.79284e-05, gnorm=1.032, loss_scale=4, train_wall=6, gb_free=29.7, wall=2831
2024-01-31 23:19:47 | INFO | train_inner | epoch 203:    167 / 1475 loss=2.139, nll_loss=0.483, ppl=1.4, wps=63009.4, ups=16.46, wpb=3828.1, bsz=197.8, num_updates=298100, lr=5.79187e-05, gnorm=0.993, loss_scale=4, train_wall=6, gb_free=29.8, wall=2838
2024-01-31 23:19:53 | INFO | train_inner | epoch 203:    267 / 1475 loss=2.137, nll_loss=0.482, ppl=1.4, wps=60705.3, ups=15.8, wpb=3842.2, bsz=203.7, num_updates=298200, lr=5.7909e-05, gnorm=1.012, loss_scale=4, train_wall=6, gb_free=29.7, wall=2844
2024-01-31 23:19:59 | INFO | train_inner | epoch 203:    367 / 1475 loss=2.141, nll_loss=0.486, ppl=1.4, wps=62331.7, ups=16.5, wpb=3777.3, bsz=201.1, num_updates=298300, lr=5.78993e-05, gnorm=1.041, loss_scale=4, train_wall=6, gb_free=29.7, wall=2850
2024-01-31 23:20:05 | INFO | train_inner | epoch 203:    467 / 1475 loss=2.153, nll_loss=0.499, ppl=1.41, wps=60678.4, ups=16.44, wpb=3691.3, bsz=190.4, num_updates=298400, lr=5.78896e-05, gnorm=1.056, loss_scale=4, train_wall=6, gb_free=29.8, wall=2856
2024-01-31 23:20:11 | INFO | train_inner | epoch 203:    567 / 1475 loss=2.142, nll_loss=0.488, ppl=1.4, wps=61547, ups=16.41, wpb=3750, bsz=199.9, num_updates=298500, lr=5.78799e-05, gnorm=1.036, loss_scale=4, train_wall=6, gb_free=29.9, wall=2862
2024-01-31 23:20:17 | INFO | train_inner | epoch 203:    667 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62977.2, ups=16.51, wpb=3814.6, bsz=191.5, num_updates=298600, lr=5.78702e-05, gnorm=1.04, loss_scale=4, train_wall=6, gb_free=29.7, wall=2868
2024-01-31 23:20:23 | INFO | train_inner | epoch 203:    767 / 1475 loss=2.149, nll_loss=0.495, ppl=1.41, wps=61898.7, ups=16.58, wpb=3733.7, bsz=194, num_updates=298700, lr=5.78605e-05, gnorm=1.014, loss_scale=4, train_wall=6, gb_free=29.8, wall=2874
2024-01-31 23:20:29 | INFO | train_inner | epoch 203:    867 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=61326.3, ups=16.42, wpb=3735, bsz=210.9, num_updates=298800, lr=5.78508e-05, gnorm=1.026, loss_scale=4, train_wall=6, gb_free=29.7, wall=2880
2024-01-31 23:20:35 | INFO | train_inner | epoch 203:    967 / 1475 loss=2.155, nll_loss=0.502, ppl=1.42, wps=62562, ups=16.53, wpb=3784.1, bsz=191.5, num_updates=298900, lr=5.78412e-05, gnorm=1.045, loss_scale=4, train_wall=6, gb_free=29.8, wall=2886
2024-01-31 23:20:42 | INFO | train_inner | epoch 203:   1067 / 1475 loss=2.153, nll_loss=0.5, ppl=1.41, wps=62050.8, ups=16.35, wpb=3795.1, bsz=194.3, num_updates=299000, lr=5.78315e-05, gnorm=1.023, loss_scale=4, train_wall=6, gb_free=29.7, wall=2892
2024-01-31 23:20:48 | INFO | train_inner | epoch 203:   1167 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=62104.4, ups=16.44, wpb=3778.7, bsz=195.6, num_updates=299100, lr=5.78218e-05, gnorm=1.022, loss_scale=4, train_wall=6, gb_free=29.7, wall=2899
2024-01-31 23:20:54 | INFO | train_inner | epoch 203:   1267 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=62277.3, ups=16.49, wpb=3776.1, bsz=191.5, num_updates=299200, lr=5.78122e-05, gnorm=1.052, loss_scale=4, train_wall=6, gb_free=29.7, wall=2905
2024-01-31 23:21:00 | INFO | train_inner | epoch 203:   1367 / 1475 loss=2.159, nll_loss=0.506, ppl=1.42, wps=61839, ups=16.58, wpb=3728.7, bsz=195.6, num_updates=299300, lr=5.78025e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.8, wall=2911
2024-01-31 23:21:06 | INFO | train_inner | epoch 203:   1467 / 1475 loss=2.158, nll_loss=0.506, ppl=1.42, wps=61848.5, ups=16.44, wpb=3763.2, bsz=189.4, num_updates=299400, lr=5.77928e-05, gnorm=1.066, loss_scale=4, train_wall=6, gb_free=29.7, wall=2917
2024-01-31 23:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:21:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:21:08 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 3.583 | nll_loss 1.999 | ppl 4 | wps 165882 | wpb 3189.1 | bsz 163.4 | num_updates 299408 | best_loss 3.385
2024-01-31 23:21:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 299408 updates
2024-01-31 23:21:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint203.pt
2024-01-31 23:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint203.pt
2024-01-31 23:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint203.pt (epoch 203 @ 299408 updates, score 3.583) (writing took 2.367833413067274 seconds)
2024-01-31 23:21:11 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2024-01-31 23:21:11 | INFO | train | epoch 203 | loss 2.15 | nll_loss 0.497 | ppl 1.41 | wps 59077.6 | ups 15.66 | wpb 3771.6 | bsz 195.9 | num_updates 299408 | lr 5.77921e-05 | gnorm 1.032 | loss_scale 4 | train_wall 86 | gb_free 29.7 | wall 2921
2024-01-31 23:21:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:21:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:21:11 | INFO | fairseq.trainer | begin training epoch 204
2024-01-31 23:21:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:21:16 | INFO | train_inner | epoch 204:     92 / 1475 loss=2.144, nll_loss=0.489, ppl=1.4, wps=35708.1, ups=9.59, wpb=3724.1, bsz=183.8, num_updates=299500, lr=5.77832e-05, gnorm=1.049, loss_scale=4, train_wall=6, gb_free=29.7, wall=2927
2024-01-31 23:21:22 | INFO | train_inner | epoch 204:    192 / 1475 loss=2.146, nll_loss=0.491, ppl=1.41, wps=62045.5, ups=16.45, wpb=3771.4, bsz=190.2, num_updates=299600, lr=5.77736e-05, gnorm=1.03, loss_scale=4, train_wall=6, gb_free=29.8, wall=2933
2024-01-31 23:21:28 | INFO | train_inner | epoch 204:    292 / 1475 loss=2.144, nll_loss=0.49, ppl=1.4, wps=62110.9, ups=16.46, wpb=3772.9, bsz=193.2, num_updates=299700, lr=5.77639e-05, gnorm=1.055, loss_scale=4, train_wall=6, gb_free=29.6, wall=2939
2024-01-31 23:21:34 | INFO | train_inner | epoch 204:    392 / 1475 loss=2.136, nll_loss=0.481, ppl=1.4, wps=62171.7, ups=16.32, wpb=3808.6, bsz=206.1, num_updates=299800, lr=5.77543e-05, gnorm=1.006, loss_scale=4, train_wall=6, gb_free=29.7, wall=2945
2024-01-31 23:21:41 | INFO | train_inner | epoch 204:    492 / 1475 loss=2.134, nll_loss=0.479, ppl=1.39, wps=61635.3, ups=16.38, wpb=3763.1, bsz=210.7, num_updates=299900, lr=5.77447e-05, gnorm=1.026, loss_scale=4, train_wall=6, gb_free=29.7, wall=2952
2024-01-31 23:21:47 | INFO | train_inner | epoch 204:    592 / 1475 loss=2.148, nll_loss=0.494, ppl=1.41, wps=63059, ups=16.41, wpb=3842.9, bsz=199.8, num_updates=300000, lr=5.7735e-05, gnorm=1.007, loss_scale=4, train_wall=6, gb_free=29.7, wall=2958
2024-01-31 23:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:21:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:21:49 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 3.58 | nll_loss 1.999 | ppl 4 | wps 166131 | wpb 3189.1 | bsz 163.4 | num_updates 300000 | best_loss 3.385
2024-01-31 23:21:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 300000 updates
2024-01-31 23:21:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_204_300000.pt
2024-01-31 23:21:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_204_300000.pt
2024-01-31 23:21:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_204_300000.pt (epoch 204 @ 300000 updates, score 3.58) (writing took 2.437891867943108 seconds)
2024-01-31 23:21:57 | INFO | train_inner | epoch 204:    692 / 1475 loss=2.143, nll_loss=0.489, ppl=1.4, wps=35309, ups=9.42, wpb=3747.9, bsz=204.6, num_updates=300100, lr=5.77254e-05, gnorm=1.052, loss_scale=4, train_wall=6, gb_free=29.7, wall=2968
2024-01-31 23:22:03 | INFO | train_inner | epoch 204:    792 / 1475 loss=2.155, nll_loss=0.501, ppl=1.42, wps=62574.8, ups=16.49, wpb=3795.2, bsz=191.6, num_updates=300200, lr=5.77158e-05, gnorm=1.038, loss_scale=4, train_wall=6, gb_free=29.9, wall=2974
2024-01-31 23:22:09 | INFO | train_inner | epoch 204:    892 / 1475 loss=2.151, nll_loss=0.498, ppl=1.41, wps=61211.7, ups=16.55, wpb=3698.1, bsz=191.4, num_updates=300300, lr=5.77062e-05, gnorm=1.024, loss_scale=4, train_wall=6, gb_free=29.7, wall=2980
2024-01-31 23:22:15 | INFO | train_inner | epoch 204:    992 / 1475 loss=2.155, nll_loss=0.501, ppl=1.42, wps=62667, ups=16.55, wpb=3786.2, bsz=199.6, num_updates=300400, lr=5.76966e-05, gnorm=1.027, loss_scale=4, train_wall=6, gb_free=29.7, wall=2986
2024-01-31 23:22:22 | INFO | train_inner | epoch 204:   1092 / 1475 loss=2.155, nll_loss=0.503, ppl=1.42, wps=60703.1, ups=16.45, wpb=3690.9, bsz=192.6, num_updates=300500, lr=5.7687e-05, gnorm=1.052, loss_scale=4, train_wall=6, gb_free=29.7, wall=2992
2024-01-31 23:22:28 | INFO | train_inner | epoch 204:   1192 / 1475 loss=2.16, nll_loss=0.507, ppl=1.42, wps=62665.9, ups=16.48, wpb=3803.2, bsz=194.8, num_updates=300600, lr=5.76774e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.7, wall=2999
2024-01-31 23:22:34 | INFO | train_inner | epoch 204:   1292 / 1475 loss=2.166, nll_loss=0.514, ppl=1.43, wps=62530.1, ups=16.54, wpb=3780.1, bsz=184.6, num_updates=300700, lr=5.76678e-05, gnorm=1.062, loss_scale=4, train_wall=6, gb_free=29.7, wall=3005
2024-01-31 23:22:40 | INFO | train_inner | epoch 204:   1392 / 1475 loss=2.155, nll_loss=0.503, ppl=1.42, wps=62100.1, ups=16.42, wpb=3782.2, bsz=201.4, num_updates=300800, lr=5.76582e-05, gnorm=1.021, loss_scale=4, train_wall=6, gb_free=29.7, wall=3011
2024-01-31 23:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:22:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:22:47 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 3.576 | nll_loss 1.995 | ppl 3.99 | wps 165783 | wpb 3189.1 | bsz 163.4 | num_updates 300883 | best_loss 3.385
2024-01-31 23:22:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 300883 updates
2024-01-31 23:22:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint204.pt
2024-01-31 23:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint204.pt
2024-01-31 23:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint204.pt (epoch 204 @ 300883 updates, score 3.576) (writing took 2.3759066029451787 seconds)
2024-01-31 23:22:49 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2024-01-31 23:22:49 | INFO | train | epoch 204 | loss 2.15 | nll_loss 0.496 | ppl 1.41 | wps 56476.1 | ups 14.97 | wpb 3771.6 | bsz 195.9 | num_updates 300883 | lr 5.76502e-05 | gnorm 1.035 | loss_scale 4 | train_wall 86 | gb_free 29.7 | wall 3020
2024-01-31 23:22:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:22:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:22:49 | INFO | fairseq.trainer | begin training epoch 205
2024-01-31 23:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:22:50 | INFO | train_inner | epoch 205:     17 / 1475 loss=2.155, nll_loss=0.503, ppl=1.42, wps=36616.7, ups=9.58, wpb=3822.1, bsz=191.7, num_updates=300900, lr=5.76486e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.6, wall=3021
2024-01-31 23:22:56 | INFO | train_inner | epoch 205:    117 / 1475 loss=2.131, nll_loss=0.475, ppl=1.39, wps=62043.2, ups=16.4, wpb=3783.7, bsz=199.8, num_updates=301000, lr=5.7639e-05, gnorm=0.99, loss_scale=4, train_wall=6, gb_free=29.7, wall=3027
2024-01-31 23:23:02 | INFO | train_inner | epoch 205:    217 / 1475 loss=2.135, nll_loss=0.479, ppl=1.39, wps=61215.4, ups=16.5, wpb=3709.2, bsz=196.2, num_updates=301100, lr=5.76295e-05, gnorm=1.021, loss_scale=4, train_wall=6, gb_free=29.7, wall=3033
2024-01-31 23:23:08 | INFO | train_inner | epoch 205:    317 / 1475 loss=2.146, nll_loss=0.491, ppl=1.41, wps=62634.9, ups=16.52, wpb=3790.8, bsz=199.5, num_updates=301200, lr=5.76199e-05, gnorm=1.011, loss_scale=4, train_wall=6, gb_free=29.7, wall=3039
2024-01-31 23:23:14 | INFO | train_inner | epoch 205:    417 / 1475 loss=2.145, nll_loss=0.49, ppl=1.4, wps=62082.5, ups=16.43, wpb=3778.4, bsz=197, num_updates=301300, lr=5.76103e-05, gnorm=1.011, loss_scale=4, train_wall=6, gb_free=29.8, wall=3045
2024-01-31 23:23:21 | INFO | train_inner | epoch 205:    517 / 1475 loss=2.145, nll_loss=0.49, ppl=1.4, wps=61990.1, ups=16.43, wpb=3773.1, bsz=193.5, num_updates=301400, lr=5.76008e-05, gnorm=1.018, loss_scale=4, train_wall=6, gb_free=29.7, wall=3051
2024-01-31 23:23:27 | INFO | train_inner | epoch 205:    617 / 1475 loss=2.145, nll_loss=0.491, ppl=1.41, wps=61859.7, ups=16.32, wpb=3791.6, bsz=197.4, num_updates=301500, lr=5.75912e-05, gnorm=1.007, loss_scale=4, train_wall=6, gb_free=29.7, wall=3058
2024-01-31 23:23:33 | INFO | train_inner | epoch 205:    717 / 1475 loss=2.147, nll_loss=0.493, ppl=1.41, wps=62786.7, ups=16.55, wpb=3793.2, bsz=199.7, num_updates=301600, lr=5.75817e-05, gnorm=1.032, loss_scale=4, train_wall=6, gb_free=29.8, wall=3064
2024-01-31 23:23:39 | INFO | train_inner | epoch 205:    817 / 1475 loss=2.147, nll_loss=0.492, ppl=1.41, wps=61399.2, ups=16.53, wpb=3713.6, bsz=199.8, num_updates=301700, lr=5.75721e-05, gnorm=1.053, loss_scale=4, train_wall=6, gb_free=29.8, wall=3070
2024-01-31 23:23:45 | INFO | train_inner | epoch 205:    917 / 1475 loss=2.153, nll_loss=0.5, ppl=1.41, wps=62202.1, ups=16.49, wpb=3772.1, bsz=194.1, num_updates=301800, lr=5.75626e-05, gnorm=1.038, loss_scale=4, train_wall=6, gb_free=29.7, wall=3076
2024-01-31 23:23:51 | INFO | train_inner | epoch 205:   1017 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=62124.8, ups=16.4, wpb=3788.5, bsz=190.6, num_updates=301900, lr=5.75531e-05, gnorm=1.065, loss_scale=4, train_wall=6, gb_free=29.7, wall=3082
2024-01-31 23:23:57 | INFO | train_inner | epoch 205:   1117 / 1475 loss=2.159, nll_loss=0.507, ppl=1.42, wps=61822.2, ups=16.52, wpb=3742.2, bsz=191.8, num_updates=302000, lr=5.75435e-05, gnorm=1.072, loss_scale=4, train_wall=6, gb_free=29.7, wall=3088
2024-01-31 23:24:03 | INFO | train_inner | epoch 205:   1217 / 1475 loss=2.152, nll_loss=0.499, ppl=1.41, wps=62237.8, ups=16.56, wpb=3757.9, bsz=199.6, num_updates=302100, lr=5.7534e-05, gnorm=1.035, loss_scale=4, train_wall=6, gb_free=29.6, wall=3094
2024-01-31 23:24:09 | INFO | train_inner | epoch 205:   1317 / 1475 loss=2.157, nll_loss=0.506, ppl=1.42, wps=61956.2, ups=16.4, wpb=3777.2, bsz=194.3, num_updates=302200, lr=5.75245e-05, gnorm=1.038, loss_scale=4, train_wall=6, gb_free=29.7, wall=3100
2024-01-31 23:24:15 | INFO | train_inner | epoch 205:   1417 / 1475 loss=2.161, nll_loss=0.509, ppl=1.42, wps=62477.7, ups=16.43, wpb=3803.6, bsz=195.4, num_updates=302300, lr=5.7515e-05, gnorm=1.043, loss_scale=4, train_wall=6, gb_free=29.8, wall=3106
2024-01-31 23:24:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:24:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:24:21 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 3.578 | nll_loss 1.999 | ppl 4 | wps 167036 | wpb 3189.1 | bsz 163.4 | num_updates 302358 | best_loss 3.385
2024-01-31 23:24:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 302358 updates
2024-01-31 23:24:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint205.pt
2024-01-31 23:24:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint205.pt
2024-01-31 23:24:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint205.pt (epoch 205 @ 302358 updates, score 3.578) (writing took 2.3907264519948512 seconds)
2024-01-31 23:24:23 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2024-01-31 23:24:23 | INFO | train | epoch 205 | loss 2.149 | nll_loss 0.496 | ppl 1.41 | wps 59217.2 | ups 15.7 | wpb 3771.6 | bsz 195.9 | num_updates 302358 | lr 5.75095e-05 | gnorm 1.032 | loss_scale 4 | train_wall 86 | gb_free 29.7 | wall 3114
2024-01-31 23:24:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:24:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:24:23 | INFO | fairseq.trainer | begin training epoch 206
2024-01-31 23:24:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:24:26 | INFO | train_inner | epoch 206:     42 / 1475 loss=2.152, nll_loss=0.499, ppl=1.41, wps=36159.4, ups=9.59, wpb=3772.5, bsz=195.1, num_updates=302400, lr=5.75055e-05, gnorm=1.043, loss_scale=4, train_wall=6, gb_free=29.7, wall=3117
2024-01-31 23:24:32 | INFO | train_inner | epoch 206:    142 / 1475 loss=2.135, nll_loss=0.478, ppl=1.39, wps=62871.9, ups=16.52, wpb=3806.7, bsz=199.9, num_updates=302500, lr=5.7496e-05, gnorm=1.069, loss_scale=4, train_wall=6, gb_free=29.7, wall=3123
2024-01-31 23:24:38 | INFO | train_inner | epoch 206:    242 / 1475 loss=2.132, nll_loss=0.476, ppl=1.39, wps=60491.3, ups=16.53, wpb=3658.7, bsz=194.7, num_updates=302600, lr=5.74865e-05, gnorm=1.038, loss_scale=4, train_wall=6, gb_free=29.7, wall=3129
2024-01-31 23:24:44 | INFO | train_inner | epoch 206:    342 / 1475 loss=2.129, nll_loss=0.474, ppl=1.39, wps=62376.5, ups=16.29, wpb=3829.9, bsz=212.2, num_updates=302700, lr=5.7477e-05, gnorm=0.968, loss_scale=4, train_wall=6, gb_free=29.7, wall=3135
2024-01-31 23:24:50 | INFO | train_inner | epoch 206:    442 / 1475 loss=2.143, nll_loss=0.488, ppl=1.4, wps=61656.7, ups=16.59, wpb=3717.3, bsz=193.4, num_updates=302800, lr=5.74675e-05, gnorm=1.035, loss_scale=4, train_wall=6, gb_free=29.7, wall=3141
2024-01-31 23:24:56 | INFO | train_inner | epoch 206:    542 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=61703.5, ups=16.38, wpb=3767.4, bsz=197.6, num_updates=302900, lr=5.7458e-05, gnorm=1.028, loss_scale=4, train_wall=6, gb_free=29.7, wall=3147
2024-01-31 23:25:02 | INFO | train_inner | epoch 206:    642 / 1475 loss=2.162, nll_loss=0.51, ppl=1.42, wps=61871.8, ups=16.65, wpb=3717.1, bsz=184.6, num_updates=303000, lr=5.74485e-05, gnorm=1.086, loss_scale=4, train_wall=6, gb_free=29.7, wall=3153
2024-01-31 23:25:08 | INFO | train_inner | epoch 206:    742 / 1475 loss=2.153, nll_loss=0.5, ppl=1.41, wps=62861.2, ups=16.39, wpb=3835.9, bsz=197, num_updates=303100, lr=5.7439e-05, gnorm=1.027, loss_scale=4, train_wall=6, gb_free=29.7, wall=3159
2024-01-31 23:25:14 | INFO | train_inner | epoch 206:    842 / 1475 loss=2.145, nll_loss=0.491, ppl=1.41, wps=62186, ups=16.43, wpb=3783.8, bsz=202.8, num_updates=303200, lr=5.74295e-05, gnorm=1.044, loss_scale=4, train_wall=6, gb_free=29.7, wall=3165
2024-01-31 23:25:20 | INFO | train_inner | epoch 206:    942 / 1475 loss=2.152, nll_loss=0.499, ppl=1.41, wps=62181.1, ups=16.5, wpb=3769.6, bsz=202.5, num_updates=303300, lr=5.74201e-05, gnorm=1.058, loss_scale=4, train_wall=6, gb_free=29.8, wall=3171
2024-01-31 23:25:26 | INFO | train_inner | epoch 206:   1042 / 1475 loss=2.161, nll_loss=0.509, ppl=1.42, wps=62227.2, ups=16.42, wpb=3788.8, bsz=188.7, num_updates=303400, lr=5.74106e-05, gnorm=1.042, loss_scale=4, train_wall=6, gb_free=29.7, wall=3177
2024-01-31 23:25:32 | INFO | train_inner | epoch 206:   1142 / 1475 loss=2.155, nll_loss=0.503, ppl=1.42, wps=61504.6, ups=16.46, wpb=3735.6, bsz=189.1, num_updates=303500, lr=5.74012e-05, gnorm=1.072, loss_scale=4, train_wall=6, gb_free=29.8, wall=3183
2024-01-31 23:25:38 | INFO | train_inner | epoch 206:   1242 / 1475 loss=2.17, nll_loss=0.518, ppl=1.43, wps=62739.2, ups=16.59, wpb=3780.7, bsz=183, num_updates=303600, lr=5.73917e-05, gnorm=1.061, loss_scale=4, train_wall=6, gb_free=29.8, wall=3189
2024-01-31 23:25:45 | INFO | train_inner | epoch 206:   1342 / 1475 loss=2.155, nll_loss=0.503, ppl=1.42, wps=61823.8, ups=16.37, wpb=3776, bsz=197, num_updates=303700, lr=5.73823e-05, gnorm=1.062, loss_scale=4, train_wall=6, gb_free=29.8, wall=3196
2024-01-31 23:25:51 | INFO | train_inner | epoch 206:   1442 / 1475 loss=2.157, nll_loss=0.505, ppl=1.42, wps=63103.5, ups=16.36, wpb=3856.8, bsz=195.3, num_updates=303800, lr=5.73728e-05, gnorm=1.02, loss_scale=4, train_wall=6, gb_free=29.7, wall=3202
2024-01-31 23:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:25:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:25:54 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 3.584 | nll_loss 1.999 | ppl 4 | wps 167030 | wpb 3189.1 | bsz 163.4 | num_updates 303833 | best_loss 3.385
2024-01-31 23:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 303833 updates
2024-01-31 23:25:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint206.pt
2024-01-31 23:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint206.pt
2024-01-31 23:25:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint206.pt (epoch 206 @ 303833 updates, score 3.584) (writing took 2.475301258964464 seconds)
2024-01-31 23:25:57 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2024-01-31 23:25:57 | INFO | train | epoch 206 | loss 2.149 | nll_loss 0.496 | ppl 1.41 | wps 59156.7 | ups 15.68 | wpb 3771.6 | bsz 195.9 | num_updates 303833 | lr 5.73697e-05 | gnorm 1.042 | loss_scale 4 | train_wall 86 | gb_free 29.7 | wall 3208
2024-01-31 23:25:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:25:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:25:57 | INFO | fairseq.trainer | begin training epoch 207
2024-01-31 23:25:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:26:01 | INFO | train_inner | epoch 207:     67 / 1475 loss=2.145, nll_loss=0.49, ppl=1.4, wps=36223.2, ups=9.52, wpb=3806.5, bsz=197.4, num_updates=303900, lr=5.73634e-05, gnorm=1.043, loss_scale=4, train_wall=6, gb_free=29.7, wall=3212
2024-01-31 23:26:07 | INFO | train_inner | epoch 207:    167 / 1475 loss=2.136, nll_loss=0.48, ppl=1.39, wps=62466.5, ups=16.44, wpb=3800.8, bsz=196.6, num_updates=304000, lr=5.73539e-05, gnorm=1.023, loss_scale=4, train_wall=6, gb_free=29.7, wall=3218
2024-01-31 23:26:14 | INFO | train_inner | epoch 207:    267 / 1475 loss=2.148, nll_loss=0.493, ppl=1.41, wps=60539.1, ups=16.09, wpb=3762.6, bsz=184.1, num_updates=304100, lr=5.73445e-05, gnorm=1.03, loss_scale=4, train_wall=6, gb_free=29.7, wall=3224
2024-01-31 23:26:20 | INFO | train_inner | epoch 207:    367 / 1475 loss=2.14, nll_loss=0.485, ppl=1.4, wps=60904.6, ups=16.41, wpb=3710.9, bsz=196.1, num_updates=304200, lr=5.73351e-05, gnorm=1.028, loss_scale=4, train_wall=6, gb_free=29.7, wall=3231
2024-01-31 23:26:26 | INFO | train_inner | epoch 207:    467 / 1475 loss=2.139, nll_loss=0.485, ppl=1.4, wps=62617.2, ups=16.27, wpb=3848.9, bsz=203.9, num_updates=304300, lr=5.73257e-05, gnorm=1.008, loss_scale=4, train_wall=6, gb_free=29.6, wall=3237
2024-01-31 23:26:32 | INFO | train_inner | epoch 207:    567 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=61905.5, ups=16.42, wpb=3770.5, bsz=198.1, num_updates=304400, lr=5.73162e-05, gnorm=1.036, loss_scale=4, train_wall=6, gb_free=29.9, wall=3243
2024-01-31 23:26:38 | INFO | train_inner | epoch 207:    667 / 1475 loss=2.154, nll_loss=0.501, ppl=1.42, wps=62068.3, ups=16.6, wpb=3739.5, bsz=191.5, num_updates=304500, lr=5.73068e-05, gnorm=1.065, loss_scale=4, train_wall=6, gb_free=29.7, wall=3249
2024-01-31 23:26:44 | INFO | train_inner | epoch 207:    767 / 1475 loss=2.149, nll_loss=0.496, ppl=1.41, wps=61637.9, ups=16.42, wpb=3753.3, bsz=198.5, num_updates=304600, lr=5.72974e-05, gnorm=1.039, loss_scale=4, train_wall=6, gb_free=29.7, wall=3255
2024-01-31 23:26:50 | INFO | train_inner | epoch 207:    867 / 1475 loss=2.154, nll_loss=0.501, ppl=1.41, wps=61852.6, ups=16.49, wpb=3751.2, bsz=193.4, num_updates=304700, lr=5.7288e-05, gnorm=1.059, loss_scale=4, train_wall=6, gb_free=29.7, wall=3261
2024-01-31 23:26:56 | INFO | train_inner | epoch 207:    967 / 1475 loss=2.145, nll_loss=0.492, ppl=1.41, wps=62355.1, ups=16.35, wpb=3813.3, bsz=201, num_updates=304800, lr=5.72786e-05, gnorm=1.003, loss_scale=4, train_wall=6, gb_free=29.8, wall=3267
2024-01-31 23:27:02 | INFO | train_inner | epoch 207:   1067 / 1475 loss=2.152, nll_loss=0.499, ppl=1.41, wps=62570.1, ups=16.4, wpb=3815.2, bsz=201, num_updates=304900, lr=5.72692e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.8, wall=3273
2024-01-31 23:27:08 | INFO | train_inner | epoch 207:   1167 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=61303, ups=16.58, wpb=3697.8, bsz=187, num_updates=305000, lr=5.72598e-05, gnorm=1.05, loss_scale=4, train_wall=6, gb_free=29.6, wall=3279
2024-01-31 23:27:14 | INFO | train_inner | epoch 207:   1267 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62732.8, ups=16.46, wpb=3811.4, bsz=192.4, num_updates=305100, lr=5.72504e-05, gnorm=1.041, loss_scale=4, train_wall=6, gb_free=29.7, wall=3285
2024-01-31 23:27:20 | INFO | train_inner | epoch 207:   1367 / 1475 loss=2.139, nll_loss=0.485, ppl=1.4, wps=61172.2, ups=16.42, wpb=3725.4, bsz=213, num_updates=305200, lr=5.72411e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.8, wall=3291
2024-01-31 23:27:26 | INFO | train_inner | epoch 207:   1467 / 1475 loss=2.156, nll_loss=0.504, ppl=1.42, wps=62165.9, ups=16.53, wpb=3761.3, bsz=191.5, num_updates=305300, lr=5.72317e-05, gnorm=1.047, loss_scale=4, train_wall=6, gb_free=29.6, wall=3297
2024-01-31 23:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:27:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:27:29 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 3.58 | nll_loss 1.996 | ppl 3.99 | wps 166057 | wpb 3189.1 | bsz 163.4 | num_updates 305308 | best_loss 3.385
2024-01-31 23:27:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 305308 updates
2024-01-31 23:27:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint207.pt
2024-01-31 23:27:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint207.pt
2024-01-31 23:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint207.pt (epoch 207 @ 305308 updates, score 3.58) (writing took 2.4375038609141484 seconds)
2024-01-31 23:27:31 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2024-01-31 23:27:31 | INFO | train | epoch 207 | loss 2.148 | nll_loss 0.494 | ppl 1.41 | wps 59012 | ups 15.65 | wpb 3771.6 | bsz 195.9 | num_updates 305308 | lr 5.72309e-05 | gnorm 1.036 | loss_scale 4 | train_wall 86 | gb_free 29.7 | wall 3302
2024-01-31 23:27:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:27:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:27:31 | INFO | fairseq.trainer | begin training epoch 208
2024-01-31 23:27:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:27:37 | INFO | train_inner | epoch 208:     92 / 1475 loss=2.137, nll_loss=0.482, ppl=1.4, wps=36387, ups=9.51, wpb=3824.5, bsz=199.5, num_updates=305400, lr=5.72223e-05, gnorm=0.993, loss_scale=4, train_wall=6, gb_free=29.7, wall=3308
2024-01-31 23:27:43 | INFO | train_inner | epoch 208:    192 / 1475 loss=2.141, nll_loss=0.486, ppl=1.4, wps=62714.6, ups=16.41, wpb=3820.7, bsz=188.4, num_updates=305500, lr=5.7213e-05, gnorm=1.101, loss_scale=4, train_wall=6, gb_free=29.8, wall=3314
2024-01-31 23:27:49 | INFO | train_inner | epoch 208:    292 / 1475 loss=2.139, nll_loss=0.484, ppl=1.4, wps=62062.7, ups=16.49, wpb=3763.8, bsz=198.3, num_updates=305600, lr=5.72036e-05, gnorm=1.054, loss_scale=4, train_wall=6, gb_free=29.7, wall=3320
2024-01-31 23:27:55 | INFO | train_inner | epoch 208:    392 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=62070.3, ups=16.49, wpb=3764, bsz=186.3, num_updates=305700, lr=5.71942e-05, gnorm=1.029, loss_scale=4, train_wall=6, gb_free=29.7, wall=3326
2024-01-31 23:28:01 | INFO | train_inner | epoch 208:    492 / 1475 loss=2.139, nll_loss=0.484, ppl=1.4, wps=62081.3, ups=16.44, wpb=3775.4, bsz=201.8, num_updates=305800, lr=5.71849e-05, gnorm=1.013, loss_scale=4, train_wall=6, gb_free=29.7, wall=3332
2024-01-31 23:28:07 | INFO | train_inner | epoch 208:    592 / 1475 loss=2.156, nll_loss=0.502, ppl=1.42, wps=63363.4, ups=16.44, wpb=3854.8, bsz=190.1, num_updates=305900, lr=5.71755e-05, gnorm=1.048, loss_scale=4, train_wall=6, gb_free=29.7, wall=3338
2024-01-31 23:28:14 | INFO | train_inner | epoch 208:    692 / 1475 loss=2.141, nll_loss=0.487, ppl=1.4, wps=58882.3, ups=15.9, wpb=3703.8, bsz=195, num_updates=306000, lr=5.71662e-05, gnorm=1.035, loss_scale=4, train_wall=6, gb_free=29.7, wall=3345
2024-01-31 23:28:20 | INFO | train_inner | epoch 208:    792 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=61459.8, ups=16.56, wpb=3711.6, bsz=194.1, num_updates=306100, lr=5.71569e-05, gnorm=1.058, loss_scale=4, train_wall=6, gb_free=29.7, wall=3351
2024-01-31 23:28:26 | INFO | train_inner | epoch 208:    892 / 1475 loss=2.153, nll_loss=0.5, ppl=1.41, wps=61065.5, ups=16.36, wpb=3732.6, bsz=191.5, num_updates=306200, lr=5.71475e-05, gnorm=1.069, loss_scale=4, train_wall=6, gb_free=29.7, wall=3357
2024-01-31 23:28:32 | INFO | train_inner | epoch 208:    992 / 1475 loss=2.158, nll_loss=0.505, ppl=1.42, wps=61895.7, ups=16.52, wpb=3745.8, bsz=190.7, num_updates=306300, lr=5.71382e-05, gnorm=1.088, loss_scale=4, train_wall=6, gb_free=29.8, wall=3363
2024-01-31 23:28:38 | INFO | train_inner | epoch 208:   1092 / 1475 loss=2.162, nll_loss=0.51, ppl=1.42, wps=63500, ups=16.45, wpb=3860.8, bsz=189.8, num_updates=306400, lr=5.71289e-05, gnorm=1.046, loss_scale=4, train_wall=6, gb_free=29.6, wall=3369
2024-01-31 23:28:44 | INFO | train_inner | epoch 208:   1192 / 1475 loss=2.141, nll_loss=0.487, ppl=1.4, wps=60741.8, ups=16.42, wpb=3698.4, bsz=210.9, num_updates=306500, lr=5.71195e-05, gnorm=1.063, loss_scale=4, train_wall=6, gb_free=29.6, wall=3375
2024-01-31 23:28:50 | INFO | train_inner | epoch 208:   1292 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62208.4, ups=16.5, wpb=3769.5, bsz=193, num_updates=306600, lr=5.71102e-05, gnorm=1.054, loss_scale=4, train_wall=6, gb_free=29.7, wall=3381
2024-01-31 23:28:56 | INFO | train_inner | epoch 208:   1392 / 1475 loss=2.143, nll_loss=0.489, ppl=1.4, wps=61399.2, ups=16.29, wpb=3769.6, bsz=212, num_updates=306700, lr=5.71009e-05, gnorm=1.022, loss_scale=4, train_wall=6, gb_free=29.9, wall=3387
2024-01-31 23:29:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:29:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:29:03 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 3.579 | nll_loss 1.998 | ppl 4 | wps 166921 | wpb 3189.1 | bsz 163.4 | num_updates 306783 | best_loss 3.385
2024-01-31 23:29:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 306783 updates
2024-01-31 23:29:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint208.pt
2024-01-31 23:29:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint208.pt
2024-01-31 23:29:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint208.pt (epoch 208 @ 306783 updates, score 3.579) (writing took 2.435814253985882 seconds)
2024-01-31 23:29:06 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2024-01-31 23:29:06 | INFO | train | epoch 208 | loss 2.147 | nll_loss 0.493 | ppl 1.41 | wps 59063.8 | ups 15.66 | wpb 3771.6 | bsz 195.9 | num_updates 306783 | lr 5.70932e-05 | gnorm 1.049 | loss_scale 4 | train_wall 86 | gb_free 29.6 | wall 3396
2024-01-31 23:29:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:29:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:29:06 | INFO | fairseq.trainer | begin training epoch 209
2024-01-31 23:29:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:29:07 | INFO | train_inner | epoch 209:     17 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=36597.1, ups=9.64, wpb=3795.1, bsz=193.1, num_updates=306800, lr=5.70916e-05, gnorm=1.056, loss_scale=4, train_wall=6, gb_free=29.6, wall=3398
2024-01-31 23:29:13 | INFO | train_inner | epoch 209:    117 / 1475 loss=2.129, nll_loss=0.473, ppl=1.39, wps=61139.8, ups=16.47, wpb=3711.7, bsz=201, num_updates=306900, lr=5.70823e-05, gnorm=1.041, loss_scale=4, train_wall=6, gb_free=29.7, wall=3404
2024-01-31 23:29:19 | INFO | train_inner | epoch 209:    217 / 1475 loss=2.138, nll_loss=0.483, ppl=1.4, wps=62841.5, ups=16.4, wpb=3831.2, bsz=201.3, num_updates=307000, lr=5.7073e-05, gnorm=1.011, loss_scale=4, train_wall=6, gb_free=29.8, wall=3410
2024-01-31 23:29:25 | INFO | train_inner | epoch 209:    317 / 1475 loss=2.138, nll_loss=0.482, ppl=1.4, wps=62164.7, ups=16.3, wpb=3814.8, bsz=197, num_updates=307100, lr=5.70637e-05, gnorm=1.036, loss_scale=4, train_wall=6, gb_free=29.6, wall=3416
2024-01-31 23:29:31 | INFO | train_inner | epoch 209:    417 / 1475 loss=2.143, nll_loss=0.489, ppl=1.4, wps=60746.9, ups=15.87, wpb=3826.9, bsz=195.6, num_updates=307200, lr=5.70544e-05, gnorm=1.118, loss_scale=4, train_wall=6, gb_free=29.7, wall=3422
2024-01-31 23:29:37 | INFO | train_inner | epoch 209:    517 / 1475 loss=2.148, nll_loss=0.494, ppl=1.41, wps=60291.2, ups=16.52, wpb=3650, bsz=183, num_updates=307300, lr=5.70451e-05, gnorm=1.073, loss_scale=4, train_wall=6, gb_free=29.8, wall=3428
2024-01-31 23:29:43 | INFO | train_inner | epoch 209:    617 / 1475 loss=2.141, nll_loss=0.486, ppl=1.4, wps=62218.2, ups=16.42, wpb=3789.7, bsz=199.3, num_updates=307400, lr=5.70359e-05, gnorm=1.034, loss_scale=4, train_wall=6, gb_free=29.7, wall=3434
2024-01-31 23:29:49 | INFO | train_inner | epoch 209:    717 / 1475 loss=2.149, nll_loss=0.496, ppl=1.41, wps=62593.3, ups=16.38, wpb=3820.6, bsz=197, num_updates=307500, lr=5.70266e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.7, wall=3440
2024-01-31 23:29:56 | INFO | train_inner | epoch 209:    817 / 1475 loss=2.144, nll_loss=0.49, ppl=1.4, wps=61264.5, ups=16.45, wpb=3724.8, bsz=199, num_updates=307600, lr=5.70173e-05, gnorm=1.038, loss_scale=4, train_wall=6, gb_free=29.7, wall=3446
2024-01-31 23:30:02 | INFO | train_inner | epoch 209:    917 / 1475 loss=2.142, nll_loss=0.488, ppl=1.4, wps=60806.9, ups=16.39, wpb=3709.6, bsz=199.2, num_updates=307700, lr=5.70081e-05, gnorm=1.056, loss_scale=4, train_wall=6, gb_free=30, wall=3453
2024-01-31 23:30:08 | INFO | train_inner | epoch 209:   1017 / 1475 loss=2.149, nll_loss=0.495, ppl=1.41, wps=61635.7, ups=16.57, wpb=3720.6, bsz=198.9, num_updates=307800, lr=5.69988e-05, gnorm=1.049, loss_scale=4, train_wall=6, gb_free=29.7, wall=3459
2024-01-31 23:30:14 | INFO | train_inner | epoch 209:   1117 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=62948, ups=16.47, wpb=3822.5, bsz=192.8, num_updates=307900, lr=5.69895e-05, gnorm=1.063, loss_scale=4, train_wall=6, gb_free=29.7, wall=3465
2024-01-31 23:30:20 | INFO | train_inner | epoch 209:   1217 / 1475 loss=2.157, nll_loss=0.504, ppl=1.42, wps=63834.8, ups=16.63, wpb=3838.9, bsz=194.3, num_updates=308000, lr=5.69803e-05, gnorm=1.049, loss_scale=4, train_wall=6, gb_free=29.7, wall=3471
2024-01-31 23:30:26 | INFO | train_inner | epoch 209:   1317 / 1475 loss=2.148, nll_loss=0.495, ppl=1.41, wps=61658.6, ups=16.41, wpb=3757.6, bsz=196.8, num_updates=308100, lr=5.6971e-05, gnorm=1.043, loss_scale=4, train_wall=6, gb_free=29.7, wall=3477
2024-01-31 23:30:32 | INFO | train_inner | epoch 209:   1417 / 1475 loss=2.163, nll_loss=0.511, ppl=1.43, wps=62607.1, ups=16.52, wpb=3790.2, bsz=191.8, num_updates=308200, lr=5.69618e-05, gnorm=1.095, loss_scale=4, train_wall=6, gb_free=29.8, wall=3483
2024-01-31 23:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:30:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:30:37 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 3.584 | nll_loss 2.004 | ppl 4.01 | wps 166515 | wpb 3189.1 | bsz 163.4 | num_updates 308258 | best_loss 3.385
2024-01-31 23:30:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 308258 updates
2024-01-31 23:30:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint209.pt
2024-01-31 23:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint209.pt
2024-01-31 23:30:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint209.pt (epoch 209 @ 308258 updates, score 3.584) (writing took 2.365031344932504 seconds)
2024-01-31 23:30:40 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2024-01-31 23:30:40 | INFO | train | epoch 209 | loss 2.146 | nll_loss 0.492 | ppl 1.41 | wps 59120 | ups 15.68 | wpb 3771.6 | bsz 195.9 | num_updates 308258 | lr 5.69564e-05 | gnorm 1.053 | loss_scale 4 | train_wall 86 | gb_free 29.8 | wall 3491
2024-01-31 23:30:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:30:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:30:40 | INFO | fairseq.trainer | begin training epoch 210
2024-01-31 23:30:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:30:42 | INFO | train_inner | epoch 210:     42 / 1475 loss=2.152, nll_loss=0.498, ppl=1.41, wps=36536.5, ups=9.7, wpb=3767.1, bsz=184.2, num_updates=308300, lr=5.69526e-05, gnorm=1.055, loss_scale=4, train_wall=6, gb_free=29.7, wall=3493
2024-01-31 23:30:48 | INFO | train_inner | epoch 210:    142 / 1475 loss=2.129, nll_loss=0.473, ppl=1.39, wps=62542.5, ups=16.38, wpb=3818.4, bsz=200.1, num_updates=308400, lr=5.69433e-05, gnorm=0.988, loss_scale=4, train_wall=6, gb_free=29.7, wall=3499
2024-01-31 23:30:54 | INFO | train_inner | epoch 210:    242 / 1475 loss=2.136, nll_loss=0.481, ppl=1.4, wps=62780.8, ups=16.46, wpb=3813.4, bsz=195.8, num_updates=308500, lr=5.69341e-05, gnorm=1.013, loss_scale=4, train_wall=6, gb_free=29.7, wall=3505
2024-01-31 23:31:01 | INFO | train_inner | epoch 210:    342 / 1475 loss=2.137, nll_loss=0.481, ppl=1.4, wps=62539.4, ups=16.39, wpb=3816.2, bsz=200.5, num_updates=308600, lr=5.69249e-05, gnorm=1.01, loss_scale=4, train_wall=6, gb_free=29.7, wall=3511
2024-01-31 23:31:07 | INFO | train_inner | epoch 210:    442 / 1475 loss=2.136, nll_loss=0.481, ppl=1.4, wps=61825.9, ups=16.39, wpb=3772.6, bsz=199.1, num_updates=308700, lr=5.69156e-05, gnorm=1.027, loss_scale=4, train_wall=6, gb_free=29.6, wall=3518
2024-01-31 23:31:13 | INFO | train_inner | epoch 210:    542 / 1475 loss=2.148, nll_loss=0.494, ppl=1.41, wps=62374.1, ups=16.53, wpb=3774.3, bsz=184.3, num_updates=308800, lr=5.69064e-05, gnorm=1.033, loss_scale=4, train_wall=6, gb_free=29.7, wall=3524
2024-01-31 23:31:19 | INFO | train_inner | epoch 210:    642 / 1475 loss=2.136, nll_loss=0.482, ppl=1.4, wps=61469.3, ups=16.39, wpb=3750.8, bsz=202.1, num_updates=308900, lr=5.68972e-05, gnorm=1.032, loss_scale=4, train_wall=6, gb_free=29.8, wall=3530
2024-01-31 23:31:25 | INFO | train_inner | epoch 210:    742 / 1475 loss=2.147, nll_loss=0.494, ppl=1.41, wps=61972, ups=16.4, wpb=3778, bsz=195, num_updates=309000, lr=5.6888e-05, gnorm=1.035, loss_scale=4, train_wall=6, gb_free=29.7, wall=3536
2024-01-31 23:31:31 | INFO | train_inner | epoch 210:    842 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=61084, ups=16.47, wpb=3709.4, bsz=199, num_updates=309100, lr=5.68788e-05, gnorm=1.037, loss_scale=4, train_wall=6, gb_free=29.6, wall=3542
2024-01-31 23:31:37 | INFO | train_inner | epoch 210:    942 / 1475 loss=2.145, nll_loss=0.492, ppl=1.41, wps=62059.8, ups=16.43, wpb=3778.1, bsz=205.4, num_updates=309200, lr=5.68696e-05, gnorm=1.052, loss_scale=4, train_wall=6, gb_free=29.8, wall=3548
2024-01-31 23:31:43 | INFO | train_inner | epoch 210:   1042 / 1475 loss=2.154, nll_loss=0.501, ppl=1.42, wps=61166.1, ups=16.6, wpb=3685.2, bsz=187.8, num_updates=309300, lr=5.68604e-05, gnorm=1.055, loss_scale=4, train_wall=6, gb_free=29.8, wall=3554
2024-01-31 23:31:49 | INFO | train_inner | epoch 210:   1142 / 1475 loss=2.165, nll_loss=0.512, ppl=1.43, wps=61945.7, ups=16.65, wpb=3721.1, bsz=182.2, num_updates=309400, lr=5.68512e-05, gnorm=1.087, loss_scale=4, train_wall=6, gb_free=29.9, wall=3560
2024-01-31 23:31:55 | INFO | train_inner | epoch 210:   1242 / 1475 loss=2.141, nll_loss=0.487, ppl=1.4, wps=61880.8, ups=16.37, wpb=3780.9, bsz=207.8, num_updates=309500, lr=5.6842e-05, gnorm=1.018, loss_scale=4, train_wall=6, gb_free=29.7, wall=3566
2024-01-31 23:32:01 | INFO | train_inner | epoch 210:   1342 / 1475 loss=2.149, nll_loss=0.497, ppl=1.41, wps=61385.9, ups=16.4, wpb=3743.6, bsz=202.2, num_updates=309600, lr=5.68329e-05, gnorm=1.054, loss_scale=4, train_wall=6, gb_free=29.9, wall=3572
2024-01-31 23:32:08 | INFO | train_inner | epoch 210:   1442 / 1475 loss=2.161, nll_loss=0.508, ppl=1.42, wps=61406.4, ups=15.97, wpb=3844.3, bsz=193.7, num_updates=309700, lr=5.68237e-05, gnorm=1.059, loss_scale=4, train_wall=6, gb_free=29.7, wall=3578
2024-01-31 23:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:32:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:32:11 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 3.58 | nll_loss 2 | ppl 4 | wps 165372 | wpb 3189.1 | bsz 163.4 | num_updates 309733 | best_loss 3.385
2024-01-31 23:32:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 309733 updates
2024-01-31 23:32:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint210.pt
2024-01-31 23:32:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint210.pt
2024-01-31 23:32:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint210.pt (epoch 210 @ 309733 updates, score 3.58) (writing took 2.378172804019414 seconds)
2024-01-31 23:32:14 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2024-01-31 23:32:14 | INFO | train | epoch 210 | loss 2.146 | nll_loss 0.492 | ppl 1.41 | wps 59068.5 | ups 15.66 | wpb 3771.6 | bsz 195.9 | num_updates 309733 | lr 5.68207e-05 | gnorm 1.037 | loss_scale 4 | train_wall 86 | gb_free 29.8 | wall 3585
2024-01-31 23:32:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:32:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:32:14 | INFO | fairseq.trainer | begin training epoch 211
2024-01-31 23:32:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:32:18 | INFO | train_inner | epoch 211:     67 / 1475 loss=2.151, nll_loss=0.496, ppl=1.41, wps=35998.4, ups=9.61, wpb=3746.6, bsz=188.7, num_updates=309800, lr=5.68145e-05, gnorm=1.052, loss_scale=4, train_wall=6, gb_free=29.8, wall=3589
2024-01-31 23:32:24 | INFO | train_inner | epoch 211:    167 / 1475 loss=2.137, nll_loss=0.482, ppl=1.4, wps=60589.6, ups=16.48, wpb=3676.7, bsz=186.3, num_updates=309900, lr=5.68053e-05, gnorm=1.035, loss_scale=4, train_wall=6, gb_free=29.9, wall=3595
2024-01-31 23:32:30 | INFO | train_inner | epoch 211:    267 / 1475 loss=2.149, nll_loss=0.494, ppl=1.41, wps=62504, ups=16.55, wpb=3775.7, bsz=187, num_updates=310000, lr=5.67962e-05, gnorm=1.061, loss_scale=4, train_wall=6, gb_free=29.8, wall=3601
2024-01-31 23:32:36 | INFO | train_inner | epoch 211:    367 / 1475 loss=2.132, nll_loss=0.477, ppl=1.39, wps=63100.9, ups=16.35, wpb=3859.4, bsz=205.8, num_updates=310100, lr=5.6787e-05, gnorm=1.021, loss_scale=8, train_wall=6, gb_free=29.7, wall=3607
2024-01-31 23:32:42 | INFO | train_inner | epoch 211:    467 / 1475 loss=2.145, nll_loss=0.49, ppl=1.4, wps=63165.9, ups=16.48, wpb=3833.5, bsz=196.6, num_updates=310200, lr=5.67779e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.7, wall=3613
2024-01-31 23:32:48 | INFO | train_inner | epoch 211:    567 / 1475 loss=2.144, nll_loss=0.489, ppl=1.4, wps=62635.2, ups=16.5, wpb=3796, bsz=192.2, num_updates=310300, lr=5.67687e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=3619
2024-01-31 23:32:54 | INFO | train_inner | epoch 211:    667 / 1475 loss=2.135, nll_loss=0.479, ppl=1.39, wps=60877.2, ups=16.4, wpb=3712.6, bsz=192.8, num_updates=310400, lr=5.67596e-05, gnorm=1.046, loss_scale=8, train_wall=6, gb_free=29.7, wall=3625
2024-01-31 23:33:00 | INFO | train_inner | epoch 211:    767 / 1475 loss=2.142, nll_loss=0.488, ppl=1.4, wps=61880.8, ups=16.38, wpb=3778, bsz=206.1, num_updates=310500, lr=5.67504e-05, gnorm=1.025, loss_scale=8, train_wall=6, gb_free=29.8, wall=3631
2024-01-31 23:33:07 | INFO | train_inner | epoch 211:    867 / 1475 loss=2.148, nll_loss=0.495, ppl=1.41, wps=61795.1, ups=16.49, wpb=3747.6, bsz=191.6, num_updates=310600, lr=5.67413e-05, gnorm=1.051, loss_scale=8, train_wall=6, gb_free=29.8, wall=3637
2024-01-31 23:33:13 | INFO | train_inner | epoch 211:    967 / 1475 loss=2.142, nll_loss=0.488, ppl=1.4, wps=61746.4, ups=16.43, wpb=3759, bsz=204.2, num_updates=310700, lr=5.67322e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=3644
2024-01-31 23:33:19 | INFO | train_inner | epoch 211:   1067 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=62370.6, ups=16.41, wpb=3801.6, bsz=199.4, num_updates=310800, lr=5.6723e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.7, wall=3650
2024-01-31 23:33:25 | INFO | train_inner | epoch 211:   1167 / 1475 loss=2.167, nll_loss=0.515, ppl=1.43, wps=62471.9, ups=16.65, wpb=3751.9, bsz=179.4, num_updates=310900, lr=5.67139e-05, gnorm=1.101, loss_scale=8, train_wall=6, gb_free=29.8, wall=3656
2024-01-31 23:33:31 | INFO | train_inner | epoch 211:   1267 / 1475 loss=2.147, nll_loss=0.493, ppl=1.41, wps=61665.4, ups=16.56, wpb=3724, bsz=199.4, num_updates=311000, lr=5.67048e-05, gnorm=1.031, loss_scale=8, train_wall=6, gb_free=29.7, wall=3662
2024-01-31 23:33:37 | INFO | train_inner | epoch 211:   1367 / 1475 loss=2.147, nll_loss=0.494, ppl=1.41, wps=62636.6, ups=16.39, wpb=3822.2, bsz=197.2, num_updates=311100, lr=5.66957e-05, gnorm=1.044, loss_scale=8, train_wall=6, gb_free=29.7, wall=3668
2024-01-31 23:33:43 | INFO | train_inner | epoch 211:   1467 / 1475 loss=2.148, nll_loss=0.495, ppl=1.41, wps=62397.8, ups=16.5, wpb=3782.7, bsz=207.9, num_updates=311200, lr=5.66866e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=30, wall=3674
2024-01-31 23:33:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:33:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:33:45 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 3.582 | nll_loss 2.001 | ppl 4 | wps 166170 | wpb 3189.1 | bsz 163.4 | num_updates 311208 | best_loss 3.385
2024-01-31 23:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 311208 updates
2024-01-31 23:33:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint211.pt
2024-01-31 23:33:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint211.pt
2024-01-31 23:33:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint211.pt (epoch 211 @ 311208 updates, score 3.582) (writing took 2.386498120962642 seconds)
2024-01-31 23:33:48 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2024-01-31 23:33:48 | INFO | train | epoch 211 | loss 2.145 | nll_loss 0.491 | ppl 1.4 | wps 59265 | ups 15.71 | wpb 3771.6 | bsz 195.9 | num_updates 311208 | lr 5.66858e-05 | gnorm 1.039 | loss_scale 8 | train_wall 86 | gb_free 29.8 | wall 3679
2024-01-31 23:33:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:33:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:33:48 | INFO | fairseq.trainer | begin training epoch 212
2024-01-31 23:33:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:33:53 | INFO | train_inner | epoch 212:     92 / 1475 loss=2.131, nll_loss=0.475, ppl=1.39, wps=36199.4, ups=9.63, wpb=3759.8, bsz=199.5, num_updates=311300, lr=5.66775e-05, gnorm=1.007, loss_scale=8, train_wall=6, gb_free=29.7, wall=3684
2024-01-31 23:33:59 | INFO | train_inner | epoch 212:    192 / 1475 loss=2.138, nll_loss=0.482, ppl=1.4, wps=61382.1, ups=16.57, wpb=3704.1, bsz=188, num_updates=311400, lr=5.66684e-05, gnorm=1.046, loss_scale=8, train_wall=6, gb_free=29.8, wall=3690
2024-01-31 23:34:05 | INFO | train_inner | epoch 212:    292 / 1475 loss=2.145, nll_loss=0.49, ppl=1.4, wps=61985.3, ups=16.64, wpb=3725.7, bsz=183.8, num_updates=311500, lr=5.66593e-05, gnorm=1.075, loss_scale=8, train_wall=6, gb_free=29.7, wall=3696
2024-01-31 23:34:12 | INFO | train_inner | epoch 212:    392 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=59910.8, ups=15.97, wpb=3751.5, bsz=188.5, num_updates=311600, lr=5.66502e-05, gnorm=1.042, loss_scale=8, train_wall=6, gb_free=29.6, wall=3703
2024-01-31 23:34:18 | INFO | train_inner | epoch 212:    492 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=62465, ups=16.45, wpb=3796.2, bsz=188.3, num_updates=311700, lr=5.66411e-05, gnorm=1.05, loss_scale=8, train_wall=6, gb_free=29.7, wall=3709
2024-01-31 23:34:24 | INFO | train_inner | epoch 212:    592 / 1475 loss=2.131, nll_loss=0.476, ppl=1.39, wps=61072.9, ups=16.29, wpb=3749.8, bsz=203.2, num_updates=311800, lr=5.6632e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.7, wall=3715
2024-01-31 23:34:34 | INFO | train_inner | epoch 212:    692 / 1475 loss=2.14, nll_loss=0.486, ppl=1.4, wps=36729.2, ups=9.73, wpb=3774.5, bsz=196.8, num_updates=311900, lr=5.66229e-05, gnorm=1.023, loss_scale=8, train_wall=6, gb_free=29.9, wall=3725
2024-01-31 23:34:40 | INFO | train_inner | epoch 212:    792 / 1475 loss=2.15, nll_loss=0.497, ppl=1.41, wps=63078.3, ups=16.46, wpb=3833.3, bsz=189.8, num_updates=312000, lr=5.66139e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.7, wall=3731
2024-01-31 23:34:46 | INFO | train_inner | epoch 212:    892 / 1475 loss=2.144, nll_loss=0.49, ppl=1.4, wps=62910.3, ups=16.44, wpb=3826.4, bsz=204.9, num_updates=312100, lr=5.66048e-05, gnorm=1.019, loss_scale=8, train_wall=6, gb_free=29.7, wall=3737
2024-01-31 23:34:52 | INFO | train_inner | epoch 212:    992 / 1475 loss=2.149, nll_loss=0.496, ppl=1.41, wps=62978.3, ups=16.32, wpb=3858.4, bsz=202.7, num_updates=312200, lr=5.65957e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.7, wall=3743
2024-01-31 23:34:59 | INFO | train_inner | epoch 212:   1092 / 1475 loss=2.141, nll_loss=0.488, ppl=1.4, wps=62438.5, ups=16.42, wpb=3802.8, bsz=205.1, num_updates=312300, lr=5.65867e-05, gnorm=1.053, loss_scale=8, train_wall=6, gb_free=29.6, wall=3749
2024-01-31 23:35:05 | INFO | train_inner | epoch 212:   1192 / 1475 loss=2.151, nll_loss=0.498, ppl=1.41, wps=60845.7, ups=16.5, wpb=3686.9, bsz=198.1, num_updates=312400, lr=5.65776e-05, gnorm=1.084, loss_scale=8, train_wall=6, gb_free=29.8, wall=3755
2024-01-31 23:35:11 | INFO | train_inner | epoch 212:   1292 / 1475 loss=2.145, nll_loss=0.492, ppl=1.41, wps=62120.4, ups=16.52, wpb=3760.5, bsz=197, num_updates=312500, lr=5.65685e-05, gnorm=1.058, loss_scale=8, train_wall=6, gb_free=29.7, wall=3762
2024-01-31 23:35:17 | INFO | train_inner | epoch 212:   1392 / 1475 loss=2.153, nll_loss=0.501, ppl=1.42, wps=61980.2, ups=16.52, wpb=3751.1, bsz=197.7, num_updates=312600, lr=5.65595e-05, gnorm=1.072, loss_scale=8, train_wall=6, gb_free=29.8, wall=3768
2024-01-31 23:35:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:35:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:35:24 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 3.583 | nll_loss 2.004 | ppl 4.01 | wps 166999 | wpb 3189.1 | bsz 163.4 | num_updates 312683 | best_loss 3.385
2024-01-31 23:35:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 312683 updates
2024-01-31 23:35:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint212.pt
2024-01-31 23:36:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint212.pt
2024-01-31 23:36:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint212.pt (epoch 212 @ 312683 updates, score 3.583) (writing took 44.241035750019364 seconds)
2024-01-31 23:36:08 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2024-01-31 23:36:08 | INFO | train | epoch 212 | loss 2.144 | nll_loss 0.49 | ppl 1.4 | wps 39616.2 | ups 10.5 | wpb 3771.6 | bsz 195.9 | num_updates 312683 | lr 5.6552e-05 | gnorm 1.046 | loss_scale 8 | train_wall 86 | gb_free 29.8 | wall 3819
2024-01-31 23:36:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:36:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:36:08 | INFO | fairseq.trainer | begin training epoch 213
2024-01-31 23:36:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:36:09 | INFO | train_inner | epoch 213:     17 / 1475 loss=2.148, nll_loss=0.496, ppl=1.41, wps=7226.2, ups=1.9, wpb=3796.2, bsz=202.6, num_updates=312700, lr=5.65504e-05, gnorm=1.03, loss_scale=8, train_wall=6, gb_free=29.7, wall=3820
2024-01-31 23:36:15 | INFO | train_inner | epoch 213:    117 / 1475 loss=2.137, nll_loss=0.48, ppl=1.4, wps=62027.4, ups=16.46, wpb=3767.8, bsz=190.2, num_updates=312800, lr=5.65414e-05, gnorm=1.034, loss_scale=8, train_wall=6, gb_free=29.7, wall=3826
2024-01-31 23:36:21 | INFO | train_inner | epoch 213:    217 / 1475 loss=2.137, nll_loss=0.481, ppl=1.4, wps=61203.7, ups=16.44, wpb=3722.3, bsz=186.8, num_updates=312900, lr=5.65324e-05, gnorm=1.064, loss_scale=8, train_wall=6, gb_free=29.8, wall=3832
2024-01-31 23:36:27 | INFO | train_inner | epoch 213:    317 / 1475 loss=2.141, nll_loss=0.487, ppl=1.4, wps=63788.2, ups=16.48, wpb=3870.5, bsz=200.9, num_updates=313000, lr=5.65233e-05, gnorm=1.028, loss_scale=8, train_wall=6, gb_free=29.7, wall=3838
2024-01-31 23:36:34 | INFO | train_inner | epoch 213:    417 / 1475 loss=2.136, nll_loss=0.482, ppl=1.4, wps=62075.5, ups=16.47, wpb=3768.8, bsz=195.6, num_updates=313100, lr=5.65143e-05, gnorm=1.024, loss_scale=8, train_wall=6, gb_free=29.7, wall=3844
2024-01-31 23:36:40 | INFO | train_inner | epoch 213:    517 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=62413.2, ups=16.45, wpb=3794.6, bsz=195, num_updates=313200, lr=5.65053e-05, gnorm=1.041, loss_scale=8, train_wall=6, gb_free=29.8, wall=3851
2024-01-31 23:36:46 | INFO | train_inner | epoch 213:    617 / 1475 loss=2.128, nll_loss=0.473, ppl=1.39, wps=62153.4, ups=16.37, wpb=3795.9, bsz=204.7, num_updates=313300, lr=5.64963e-05, gnorm=1.009, loss_scale=8, train_wall=6, gb_free=29.6, wall=3857
2024-01-31 23:36:52 | INFO | train_inner | epoch 213:    717 / 1475 loss=2.145, nll_loss=0.491, ppl=1.41, wps=60959.6, ups=16.57, wpb=3679.9, bsz=193.3, num_updates=313400, lr=5.64873e-05, gnorm=1.069, loss_scale=8, train_wall=6, gb_free=29.6, wall=3863
2024-01-31 23:36:58 | INFO | train_inner | epoch 213:    817 / 1475 loss=2.143, nll_loss=0.489, ppl=1.4, wps=62838.4, ups=16.47, wpb=3814.8, bsz=201.8, num_updates=313500, lr=5.64782e-05, gnorm=1.052, loss_scale=8, train_wall=6, gb_free=29.7, wall=3869
2024-01-31 23:37:04 | INFO | train_inner | epoch 213:    917 / 1475 loss=2.146, nll_loss=0.492, ppl=1.41, wps=61148, ups=16.55, wpb=3695.3, bsz=188.9, num_updates=313600, lr=5.64692e-05, gnorm=1.052, loss_scale=8, train_wall=6, gb_free=29.8, wall=3875
2024-01-31 23:37:12 | INFO | train_inner | epoch 213:   1017 / 1475 loss=2.143, nll_loss=0.49, ppl=1.4, wps=48855.7, ups=12.83, wpb=3808.6, bsz=201.7, num_updates=313700, lr=5.64602e-05, gnorm=1.029, loss_scale=8, train_wall=6, gb_free=29.7, wall=3883
2024-01-31 23:37:18 | INFO | train_inner | epoch 213:   1117 / 1475 loss=2.149, nll_loss=0.497, ppl=1.41, wps=61897, ups=16.47, wpb=3757.6, bsz=198.3, num_updates=313800, lr=5.64512e-05, gnorm=1.049, loss_scale=8, train_wall=6, gb_free=29.6, wall=3889
2024-01-31 23:37:25 | INFO | train_inner | epoch 213:   1217 / 1475 loss=2.153, nll_loss=0.5, ppl=1.41, wps=51551.9, ups=13.64, wpb=3779.7, bsz=197.2, num_updates=313900, lr=5.64423e-05, gnorm=1.079, loss_scale=8, train_wall=7, gb_free=29.8, wall=3896
2024-01-31 23:37:33 | INFO | train_inner | epoch 213:   1317 / 1475 loss=2.143, nll_loss=0.49, ppl=1.4, wps=45016, ups=11.91, wpb=3779, bsz=199.4, num_updates=314000, lr=5.64333e-05, gnorm=1.043, loss_scale=8, train_wall=6, gb_free=29.8, wall=3904
2024-01-31 23:37:51 | INFO | train_inner | epoch 213:   1417 / 1475 loss=2.158, nll_loss=0.506, ppl=1.42, wps=22265.3, ups=5.86, wpb=3798.5, bsz=190.7, num_updates=314100, lr=5.64243e-05, gnorm=1.043, loss_scale=8, train_wall=7, gb_free=29.7, wall=3921
2024-01-31 23:37:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:37:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:37:56 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 3.591 | nll_loss 2.009 | ppl 4.02 | wps 166078 | wpb 3189.1 | bsz 163.4 | num_updates 314158 | best_loss 3.385
2024-01-31 23:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 314158 updates
2024-01-31 23:37:57 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint213.pt
2024-01-31 23:37:58 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint213.pt
2024-01-31 23:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint213.pt (epoch 213 @ 314158 updates, score 3.591) (writing took 2.498591296025552 seconds)
2024-01-31 23:37:59 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2024-01-31 23:37:59 | INFO | train | epoch 213 | loss 2.143 | nll_loss 0.489 | ppl 1.4 | wps 50159.8 | ups 13.3 | wpb 3771.6 | bsz 195.9 | num_updates 314158 | lr 5.64191e-05 | gnorm 1.045 | loss_scale 8 | train_wall 88 | gb_free 29.9 | wall 3930
2024-01-31 23:37:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:37:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:37:59 | INFO | fairseq.trainer | begin training epoch 214
2024-01-31 23:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:38:02 | INFO | train_inner | epoch 214:     42 / 1475 loss=2.143, nll_loss=0.489, ppl=1.4, wps=33267, ups=8.99, wpb=3700.5, bsz=188.4, num_updates=314200, lr=5.64153e-05, gnorm=1.068, loss_scale=8, train_wall=6, gb_free=29.6, wall=3933
2024-01-31 23:38:08 | INFO | train_inner | epoch 214:    142 / 1475 loss=2.123, nll_loss=0.466, ppl=1.38, wps=62180.3, ups=16.51, wpb=3765.3, bsz=199.2, num_updates=314300, lr=5.64063e-05, gnorm=1.004, loss_scale=8, train_wall=6, gb_free=29.7, wall=3939
2024-01-31 23:38:14 | INFO | train_inner | epoch 214:    242 / 1475 loss=2.138, nll_loss=0.483, ppl=1.4, wps=62641.3, ups=16.5, wpb=3796.9, bsz=197.3, num_updates=314400, lr=5.63974e-05, gnorm=1.06, loss_scale=8, train_wall=6, gb_free=29.8, wall=3945
2024-01-31 23:38:20 | INFO | train_inner | epoch 214:    342 / 1475 loss=2.142, nll_loss=0.488, ppl=1.4, wps=62010.2, ups=16.5, wpb=3757.9, bsz=186.1, num_updates=314500, lr=5.63884e-05, gnorm=1.054, loss_scale=8, train_wall=6, gb_free=29.8, wall=3951
2024-01-31 23:38:26 | INFO | train_inner | epoch 214:    442 / 1475 loss=2.144, nll_loss=0.489, ppl=1.4, wps=63775.5, ups=16.57, wpb=3848.8, bsz=191.4, num_updates=314600, lr=5.63794e-05, gnorm=1.036, loss_scale=8, train_wall=6, gb_free=29.7, wall=3957
2024-01-31 23:38:32 | INFO | train_inner | epoch 214:    542 / 1475 loss=2.138, nll_loss=0.484, ppl=1.4, wps=62170.1, ups=16.41, wpb=3788.5, bsz=197.4, num_updates=314700, lr=5.63705e-05, gnorm=1.018, loss_scale=8, train_wall=6, gb_free=29.8, wall=3963
2024-01-31 23:38:38 | INFO | train_inner | epoch 214:    642 / 1475 loss=2.142, nll_loss=0.487, ppl=1.4, wps=62288.8, ups=16.52, wpb=3770.3, bsz=196.7, num_updates=314800, lr=5.63615e-05, gnorm=1.049, loss_scale=8, train_wall=6, gb_free=29.7, wall=3969
2024-01-31 23:38:44 | INFO | train_inner | epoch 214:    742 / 1475 loss=2.136, nll_loss=0.482, ppl=1.4, wps=61124.7, ups=16.49, wpb=3706.2, bsz=199.2, num_updates=314900, lr=5.63526e-05, gnorm=1.06, loss_scale=8, train_wall=6, gb_free=29.7, wall=3975
2024-01-31 23:38:50 | INFO | train_inner | epoch 214:    842 / 1475 loss=2.147, nll_loss=0.494, ppl=1.41, wps=62605.5, ups=16.56, wpb=3780.5, bsz=191, num_updates=315000, lr=5.63436e-05, gnorm=1.049, loss_scale=8, train_wall=6, gb_free=29.7, wall=3981
2024-01-31 23:38:56 | INFO | train_inner | epoch 214:    942 / 1475 loss=2.14, nll_loss=0.486, ppl=1.4, wps=61471.4, ups=16.39, wpb=3749.6, bsz=201.7, num_updates=315100, lr=5.63347e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=3987
2024-01-31 23:39:02 | INFO | train_inner | epoch 214:   1042 / 1475 loss=2.148, nll_loss=0.495, ppl=1.41, wps=61852.9, ups=16.51, wpb=3746.9, bsz=189.8, num_updates=315200, lr=5.63257e-05, gnorm=1.057, loss_scale=8, train_wall=6, gb_free=29.7, wall=3993
2024-01-31 23:39:08 | INFO | train_inner | epoch 214:   1142 / 1475 loss=2.143, nll_loss=0.49, ppl=1.4, wps=62372.1, ups=16.36, wpb=3811.7, bsz=202.3, num_updates=315300, lr=5.63168e-05, gnorm=1.035, loss_scale=8, train_wall=6, gb_free=29.7, wall=3999
2024-01-31 23:39:14 | INFO | train_inner | epoch 214:   1242 / 1475 loss=2.146, nll_loss=0.493, ppl=1.41, wps=62014.1, ups=16.45, wpb=3770, bsz=205.6, num_updates=315400, lr=5.63079e-05, gnorm=1.063, loss_scale=8, train_wall=6, gb_free=29.6, wall=4005
2024-01-31 23:39:20 | INFO | train_inner | epoch 214:   1342 / 1475 loss=2.159, nll_loss=0.507, ppl=1.42, wps=62715.9, ups=16.58, wpb=3783.6, bsz=190.6, num_updates=315500, lr=5.6299e-05, gnorm=1.074, loss_scale=8, train_wall=6, gb_free=29.7, wall=4011
2024-01-31 23:39:27 | INFO | train_inner | epoch 214:   1442 / 1475 loss=2.152, nll_loss=0.5, ppl=1.41, wps=61213.5, ups=16.43, wpb=3726.1, bsz=196.8, num_updates=315600, lr=5.629e-05, gnorm=1.094, loss_scale=8, train_wall=6, gb_free=29.6, wall=4017
2024-01-31 23:39:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:39:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:39:30 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 3.585 | nll_loss 2.005 | ppl 4.01 | wps 167482 | wpb 3189.1 | bsz 163.4 | num_updates 315633 | best_loss 3.385
2024-01-31 23:39:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 315633 updates
2024-01-31 23:39:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint214.pt
2024-01-31 23:39:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint214.pt
2024-01-31 23:39:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint214.pt (epoch 214 @ 315633 updates, score 3.585) (writing took 2.319415831938386 seconds)
2024-01-31 23:39:33 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2024-01-31 23:39:33 | INFO | train | epoch 214 | loss 2.143 | nll_loss 0.489 | ppl 1.4 | wps 59352.9 | ups 15.74 | wpb 3771.6 | bsz 195.9 | num_updates 315633 | lr 5.62871e-05 | gnorm 1.049 | loss_scale 8 | train_wall 86 | gb_free 29.7 | wall 4024
2024-01-31 23:39:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:39:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-31 23:39:33 | INFO | fairseq.trainer | begin training epoch 215
2024-01-31 23:39:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-31 23:39:37 | INFO | train_inner | epoch 215:     67 / 1475 loss=2.138, nll_loss=0.483, ppl=1.4, wps=36554.9, ups=9.74, wpb=3752.4, bsz=194.7, num_updates=315700, lr=5.62811e-05, gnorm=1.04, loss_scale=8, train_wall=6, gb_free=29.7, wall=4028
2024-01-31 23:39:43 | INFO | train_inner | epoch 215:    167 / 1475 loss=2.133, nll_loss=0.477, ppl=1.39, wps=61522.9, ups=16.49, wpb=3729.8, bsz=184.4, num_updates=315800, lr=5.62722e-05, gnorm=1.062, loss_scale=8, train_wall=6, gb_free=29.8, wall=4034
2024-01-31 23:39:49 | INFO | train_inner | epoch 215:    267 / 1475 loss=2.142, nll_loss=0.488, ppl=1.4, wps=62134.2, ups=16.48, wpb=3770.7, bsz=185.4, num_updates=315900, lr=5.62633e-05, gnorm=1.044, loss_scale=8, train_wall=6, gb_free=29.6, wall=4040
2024-01-31 23:39:55 | INFO | train_inner | epoch 215:    367 / 1475 loss=2.119, nll_loss=0.464, ppl=1.38, wps=62433.9, ups=16.31, wpb=3828.1, bsz=219.7, num_updates=316000, lr=5.62544e-05, gnorm=0.995, loss_scale=8, train_wall=6, gb_free=29.7, wall=4046
2024-01-31 23:40:01 | INFO | train_inner | epoch 215:    467 / 1475 loss=2.138, nll_loss=0.483, ppl=1.4, wps=62485, ups=16.43, wpb=3804.2, bsz=196.2, num_updates=316100, lr=5.62455e-05, gnorm=1.037, loss_scale=8, train_wall=6, gb_free=29.6, wall=4052
2024-01-31 23:40:07 | INFO | train_inner | epoch 215:    567 / 1475 loss=2.135, nll_loss=0.48, ppl=1.39, wps=61873.7, ups=16.44, wpb=3762.9, bsz=205, num_updates=316200, lr=5.62366e-05, gnorm=1.047, loss_scale=8, train_wall=6, gb_free=29.7, wall=4058
2024-01-31 23:40:10 | INFO | fairseq_cli.train | Stopping training due to cumulative_training_time: 6.000009442965189 > stop_time_hours: 6.0 hour(s)
2024-01-31 23:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-31 23:40:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-31 23:40:12 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 3.594 | nll_loss 2.018 | ppl 4.05 | wps 165809 | wpb 3189.1 | bsz 163.4 | num_updates 316244 | best_loss 3.385
2024-01-31 23:40:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 316244 updates
2024-01-31 23:40:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-01-31 23:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-01-31 23:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt (epoch 215 @ 316244 updates, score 3.594) (writing took 0.9609825590159744 seconds)
2024-01-31 23:40:13 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2024-01-31 23:40:13 | INFO | train | epoch 215 | loss 2.132 | nll_loss 0.477 | ppl 1.39 | wps 57292 | ups 15.24 | wpb 3759.3 | bsz 199 | num_updates 316244 | lr 5.62327e-05 | gnorm 1.034 | loss_scale 8 | train_wall 36 | gb_free 29.7 | wall 4064
2024-01-31 23:40:13 | INFO | fairseq_cli.train | done training in 4056.2 seconds
Training complete.
Finetuning complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.en: 15516 sents, 256102 tokens, 0.0215% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 7560 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.de: 15516 sents, 269168 tokens, 0.0241% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
averaging checkpoints:  ['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint214.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint213.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint212.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint211.pt', '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint210.pt']
Finished writing averaged checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/finetune_mt_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 16, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 6520 types
INFO:fairseq.tasks.translation:[de] dictionary: 7560 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/test.en-de.en
INFO:fairseq.data.data_utils:loaded 15,516 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/test.en-de.de
INFO:fairseq.tasks.translation:/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset test en-de 15516 examples
INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True
INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True
INFO:fairseq.tasks.fairseq_task:rebuild_batches = False
INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1
INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2
INFO:fairseq_cli.generate:Translated 15,512 sentences (261,066 tokens) in 61.5s (252.32 sentences/s, 4246.52 tokens/s)
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/finetune_mt_covost/hyp_mt.txt and /home/kit/stud/uxude/predictions/finetune_mt_covost/ref_mt.txt
Sample predictions:
Sample: Wie fhlst du dich?
Reference: Wie fhlen Sie sich?
Sample: Das war in Ordnung.
Reference: Das war alles richtig.
WER:
Generate test with beam=16: BLEU4 = 28.80, 56.7/35.6/23.5/16.0 (BP=0.976, ratio=0.976, syslen=137031, reflen=140351)
BLEU:
{
 "name": "BLEU",
 "score": 28.8,
 "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:none|version:2.4.0",
 "verbose_score": "56.7/35.6/23.5/16.0 (BP = 0.976 ratio = 0.976 hyp_len = 137031 ref_len = 140350)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "none",
 "smooth": "none",
 "version": "2.4.0"
}