(base) [uxude@uc2n994 train]$ cat finetune_mt_covost_23092673.txt
No extension needed for workspace ASR.
No extension needed for workspace MT.
Fairseq directory exists. Checking if installed...
fairseq                  0.12.2       /home/kit/stud/uxude/fairseq
Fairseq is already installed. Skipping installation.
Setup complete. Starting script execution...
[2024-01-30 15:33:07] [INFO] [Dataset::Prepare Datasets]: Skipping dataset preparation, config file and MT spm data already exists
Finetuning the MT model...
Binarized data already exists. Skipping binarization.
Training the model...
Model will be stored in /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Training time: 6 hours
2024-01-30 15:33:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 6.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=6.0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=50000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=1, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-01-30 15:33:20 | INFO | fairseq.tasks.translation | [en] dictionary: 6520 types
2024-01-30 15:33:20 | INFO | fairseq.tasks.translation | [de] dictionary: 7560 types
2024-01-30 15:33:21 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6520, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7560, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=7560, bias=False)
  )
)
2024-01-30 15:33:21 | INFO | fairseq_cli.train | task: TranslationTask
2024-01-30 15:33:21 | INFO | fairseq_cli.train | model: TransformerModel
2024-01-30 15:33:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-30 15:33:21 | INFO | fairseq_cli.train | num. shared model params: 51,347,456 (num. trained: 51,347,456)
2024-01-30 15:33:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-30 15:33:21 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.en
2024-01-30 15:33:21 | INFO | fairseq.data.data_utils | loaded 15,520 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/valid.en-de.de
2024-01-30 15:33:21 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset valid en-de 15520 examples
2024-01-30 15:33:21 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-30 15:33:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-30 15:33:21 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-SXM2-32GB
2024-01-30 15:33:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-30 15:33:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-30 15:33:21 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2024-01-30 15:33:21 | INFO | fairseq.trainer | Preparing to load checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-01-30 15:33:21 | INFO | fairseq.trainer | No existing checkpoint found /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_last.pt
2024-01-30 15:33:21 | INFO | fairseq.trainer | loading train data for epoch 1
2024-01-30 15:33:21 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.en
2024-01-30 15:33:21 | INFO | fairseq.data.data_utils | loaded 289,024 examples from: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/train.en-de.de
2024-01-30 15:33:21 | INFO | fairseq.tasks.translation | /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset train en-de 289024 examples
2024-01-30 15:33:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:33:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-30 15:33:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-30 15:33:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-30 15:33:24 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-30 15:33:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:33:24 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-30 15:33:24 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-30 15:33:24 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-30 15:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:33:27 | INFO | fairseq.trainer | begin training epoch 1
2024-01-30 15:33:27 | INFO | fairseq_cli.train | Start iterating over samples
/home/kit/stud/uxude/miniconda3/envs/nmt/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-01-30 15:33:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2024-01-30 15:33:41 | INFO | train_inner | epoch 001:    101 / 1475 loss=12.303, nll_loss=12.151, ppl=4546.45, wps=55747.3, ups=14.7, wpb=3783.2, bsz=200.6, num_updates=100, lr=1.25e-05, gnorm=3.252, loss_scale=64, train_wall=14, gb_free=29.7, wall=20
2024-01-30 15:33:48 | INFO | train_inner | epoch 001:    201 / 1475 loss=11.117, nll_loss=10.829, ppl=1818.79, wps=57869.8, ups=15.31, wpb=3780.6, bsz=179.8, num_updates=200, lr=2.5e-05, gnorm=1.751, loss_scale=64, train_wall=6, gb_free=29.7, wall=27
2024-01-30 15:33:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-01-30 15:33:54 | INFO | train_inner | epoch 001:    302 / 1475 loss=10.276, nll_loss=9.861, ppl=929.78, wps=56951.1, ups=15.23, wpb=3739.9, bsz=195.8, num_updates=300, lr=3.75e-05, gnorm=1.758, loss_scale=32, train_wall=6, gb_free=29.7, wall=33
2024-01-30 15:34:01 | INFO | train_inner | epoch 001:    402 / 1475 loss=9.923, nll_loss=9.427, ppl=688.43, wps=57411.5, ups=15.23, wpb=3770.7, bsz=201.4, num_updates=400, lr=5e-05, gnorm=1.639, loss_scale=32, train_wall=6, gb_free=29.7, wall=40
2024-01-30 15:34:07 | INFO | train_inner | epoch 001:    502 / 1475 loss=9.808, nll_loss=9.279, ppl=621.1, wps=58284.9, ups=15.26, wpb=3818.8, bsz=196, num_updates=500, lr=6.25e-05, gnorm=1.56, loss_scale=32, train_wall=6, gb_free=29.7, wall=46
2024-01-30 15:34:14 | INFO | train_inner | epoch 001:    602 / 1475 loss=9.754, nll_loss=9.211, ppl=592.82, wps=57852.9, ups=15.29, wpb=3784.7, bsz=195, num_updates=600, lr=7.5e-05, gnorm=1.65, loss_scale=32, train_wall=6, gb_free=29.7, wall=53
2024-01-30 15:34:20 | INFO | train_inner | epoch 001:    702 / 1475 loss=9.595, nll_loss=9.03, ppl=522.74, wps=57985.5, ups=15.39, wpb=3767.4, bsz=198.2, num_updates=700, lr=8.75e-05, gnorm=1.457, loss_scale=32, train_wall=6, gb_free=29.7, wall=59
2024-01-30 15:34:27 | INFO | train_inner | epoch 001:    802 / 1475 loss=9.474, nll_loss=8.89, ppl=474.26, wps=57260.6, ups=15.37, wpb=3725, bsz=191.8, num_updates=800, lr=0.0001, gnorm=1.583, loss_scale=32, train_wall=6, gb_free=29.8, wall=66
2024-01-30 15:34:33 | INFO | train_inner | epoch 001:    902 / 1475 loss=9.332, nll_loss=8.726, ppl=423.41, wps=57883.5, ups=15.35, wpb=3771.8, bsz=190.9, num_updates=900, lr=0.0001125, gnorm=1.563, loss_scale=32, train_wall=6, gb_free=29.6, wall=72
2024-01-30 15:34:40 | INFO | train_inner | epoch 001:   1002 / 1475 loss=9.106, nll_loss=8.465, ppl=353.39, wps=57516.7, ups=15.35, wpb=3745.9, bsz=203.6, num_updates=1000, lr=0.000125, gnorm=1.612, loss_scale=32, train_wall=6, gb_free=29.9, wall=79
2024-01-30 15:34:46 | INFO | train_inner | epoch 001:   1102 / 1475 loss=8.903, nll_loss=8.23, ppl=300.2, wps=57942.8, ups=15.3, wpb=3788, bsz=204.7, num_updates=1100, lr=0.0001375, gnorm=1.555, loss_scale=32, train_wall=6, gb_free=29.8, wall=85
2024-01-30 15:34:53 | INFO | train_inner | epoch 001:   1202 / 1475 loss=8.766, nll_loss=8.07, ppl=268.66, wps=57463, ups=15.15, wpb=3793.3, bsz=200.6, num_updates=1200, lr=0.00015, gnorm=1.454, loss_scale=32, train_wall=6, gb_free=29.7, wall=92
2024-01-30 15:34:59 | INFO | train_inner | epoch 001:   1302 / 1475 loss=8.638, nll_loss=7.92, ppl=242.25, wps=57854.4, ups=15.4, wpb=3757.9, bsz=197.5, num_updates=1300, lr=0.0001625, gnorm=1.611, loss_scale=32, train_wall=6, gb_free=29.6, wall=99
2024-01-30 15:35:06 | INFO | train_inner | epoch 001:   1402 / 1475 loss=8.588, nll_loss=7.858, ppl=232.03, wps=57777.7, ups=15.33, wpb=3768.4, bsz=187.9, num_updates=1400, lr=0.000175, gnorm=1.426, loss_scale=32, train_wall=6, gb_free=29.7, wall=105
2024-01-30 15:35:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:35:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:35:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.209 | nll_loss 7.411 | ppl 170.22 | wps 130388 | wpb 3189.1 | bsz 163.4 | num_updates 1473
2024-01-30 15:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2024-01-30 15:35:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint1.pt
2024-01-30 15:35:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint1.pt
2024-01-30 15:35:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint1.pt (epoch 1 @ 1473 updates, score 8.209) (writing took 5.277005393058062 seconds)
2024-01-30 15:35:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-01-30 15:35:18 | INFO | train | epoch 001 | loss 9.623 | nll_loss 9.067 | ppl 536.48 | wps 53384.9 | ups 14.15 | wpb 3771.4 | bsz 195.9 | num_updates 1473 | lr 0.000184125 | gnorm 1.689 | loss_scale 32 | train_wall 100 | gb_free 29.7 | wall 117
2024-01-30 15:35:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:35:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:35:18 | INFO | fairseq.trainer | begin training epoch 2
2024-01-30 15:35:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:35:20 | INFO | train_inner | epoch 002:     27 / 1475 loss=8.419, nll_loss=7.664, ppl=202.83, wps=26515.8, ups=7.02, wpb=3775.9, bsz=194.4, num_updates=1500, lr=0.0001875, gnorm=1.406, loss_scale=32, train_wall=6, gb_free=29.8, wall=119
2024-01-30 15:35:27 | INFO | train_inner | epoch 002:    127 / 1475 loss=8.282, nll_loss=7.505, ppl=181.65, wps=57274.6, ups=15.27, wpb=3751, bsz=196.5, num_updates=1600, lr=0.0002, gnorm=1.413, loss_scale=32, train_wall=6, gb_free=29.7, wall=126
2024-01-30 15:35:33 | INFO | train_inner | epoch 002:    227 / 1475 loss=8.125, nll_loss=7.321, ppl=159.94, wps=58254.2, ups=15.35, wpb=3795.3, bsz=201.9, num_updates=1700, lr=0.0002125, gnorm=1.375, loss_scale=32, train_wall=6, gb_free=29.8, wall=132
2024-01-30 15:35:40 | INFO | train_inner | epoch 002:    327 / 1475 loss=7.979, nll_loss=7.154, ppl=142.38, wps=57949.3, ups=15.29, wpb=3789.4, bsz=214.4, num_updates=1800, lr=0.000225, gnorm=1.436, loss_scale=32, train_wall=6, gb_free=29.7, wall=139
2024-01-30 15:35:46 | INFO | train_inner | epoch 002:    427 / 1475 loss=7.896, nll_loss=7.055, ppl=132.95, wps=57789, ups=15.4, wpb=3751.6, bsz=203.1, num_updates=1900, lr=0.0002375, gnorm=1.391, loss_scale=32, train_wall=6, gb_free=29.7, wall=145
2024-01-30 15:35:53 | INFO | train_inner | epoch 002:    527 / 1475 loss=7.842, nll_loss=6.991, ppl=127.25, wps=58569, ups=15.35, wpb=3815.8, bsz=193, num_updates=2000, lr=0.00025, gnorm=1.245, loss_scale=32, train_wall=6, gb_free=29.7, wall=152
2024-01-30 15:35:59 | INFO | train_inner | epoch 002:    627 / 1475 loss=7.736, nll_loss=6.869, ppl=116.89, wps=59040.8, ups=15.41, wpb=3830.5, bsz=196.2, num_updates=2100, lr=0.0002625, gnorm=1.359, loss_scale=32, train_wall=6, gb_free=29.8, wall=158
2024-01-30 15:36:06 | INFO | train_inner | epoch 002:    727 / 1475 loss=7.564, nll_loss=6.672, ppl=101.94, wps=57155.4, ups=15.39, wpb=3714.7, bsz=209.4, num_updates=2200, lr=0.000275, gnorm=1.393, loss_scale=32, train_wall=6, gb_free=29.7, wall=165
2024-01-30 15:36:12 | INFO | train_inner | epoch 002:    827 / 1475 loss=7.61, nll_loss=6.721, ppl=105.53, wps=58382.9, ups=15.56, wpb=3751.4, bsz=174.9, num_updates=2300, lr=0.0002875, gnorm=1.268, loss_scale=32, train_wall=6, gb_free=29.7, wall=171
2024-01-30 15:36:19 | INFO | train_inner | epoch 002:    927 / 1475 loss=7.448, nll_loss=6.536, ppl=92.78, wps=59344.2, ups=15.5, wpb=3829.9, bsz=181.9, num_updates=2400, lr=0.0003, gnorm=1.251, loss_scale=32, train_wall=6, gb_free=29.7, wall=178
2024-01-30 15:36:25 | INFO | train_inner | epoch 002:   1027 / 1475 loss=7.252, nll_loss=6.308, ppl=79.25, wps=58913.9, ups=15.44, wpb=3815.4, bsz=193.4, num_updates=2500, lr=0.0003125, gnorm=1.315, loss_scale=32, train_wall=6, gb_free=29.7, wall=184
2024-01-30 15:36:32 | INFO | train_inner | epoch 002:   1127 / 1475 loss=7.111, nll_loss=6.147, ppl=70.88, wps=58599, ups=15.37, wpb=3812.2, bsz=200.9, num_updates=2600, lr=0.000325, gnorm=1.316, loss_scale=32, train_wall=6, gb_free=29.7, wall=191
2024-01-30 15:36:38 | INFO | train_inner | epoch 002:   1227 / 1475 loss=7.002, nll_loss=6.019, ppl=64.85, wps=57780.8, ups=15.52, wpb=3723.3, bsz=197.8, num_updates=2700, lr=0.0003375, gnorm=1.406, loss_scale=32, train_wall=6, gb_free=29.6, wall=197
2024-01-30 15:36:45 | INFO | train_inner | epoch 002:   1327 / 1475 loss=7.002, nll_loss=6.019, ppl=64.83, wps=57560.7, ups=15.53, wpb=3707.4, bsz=188.2, num_updates=2800, lr=0.00035, gnorm=1.405, loss_scale=32, train_wall=6, gb_free=29.7, wall=204
2024-01-30 15:36:51 | INFO | train_inner | epoch 002:   1427 / 1475 loss=6.822, nll_loss=5.811, ppl=56.16, wps=57431.7, ups=15.37, wpb=3736.3, bsz=201, num_updates=2900, lr=0.0003625, gnorm=1.315, loss_scale=32, train_wall=6, gb_free=29.8, wall=210
2024-01-30 15:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:36:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:36:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.38 | nll_loss 5.251 | ppl 38.09 | wps 131740 | wpb 3189.1 | bsz 163.4 | num_updates 2948 | best_loss 6.38
2024-01-30 15:36:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2948 updates
2024-01-30 15:36:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint2.pt
2024-01-30 15:36:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint2.pt
2024-01-30 15:37:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint2.pt (epoch 2 @ 2948 updates, score 6.38) (writing took 5.050265136989765 seconds)
2024-01-30 15:37:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-01-30 15:37:02 | INFO | train | epoch 002 | loss 7.539 | nll_loss 6.642 | ppl 99.86 | wps 53862 | ups 14.28 | wpb 3771.6 | bsz 195.9 | num_updates 2948 | lr 0.0003685 | gnorm 1.349 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 221
2024-01-30 15:37:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:37:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:37:02 | INFO | fairseq.trainer | begin training epoch 3
2024-01-30 15:37:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:37:05 | INFO | train_inner | epoch 003:     52 / 1475 loss=6.7, nll_loss=5.67, ppl=50.93, wps=26756, ups=7.14, wpb=3745.7, bsz=177.7, num_updates=3000, lr=0.000375, gnorm=1.264, loss_scale=32, train_wall=6, gb_free=29.7, wall=224
2024-01-30 15:37:12 | INFO | train_inner | epoch 003:    152 / 1475 loss=6.491, nll_loss=5.429, ppl=43.09, wps=57211.1, ups=15.45, wpb=3702.5, bsz=194, num_updates=3100, lr=0.0003875, gnorm=1.364, loss_scale=32, train_wall=6, gb_free=29.6, wall=231
2024-01-30 15:37:18 | INFO | train_inner | epoch 003:    252 / 1475 loss=6.439, nll_loss=5.367, ppl=41.26, wps=57492.1, ups=15.54, wpb=3699.5, bsz=193.8, num_updates=3200, lr=0.0004, gnorm=1.377, loss_scale=32, train_wall=6, gb_free=29.7, wall=237
2024-01-30 15:37:24 | INFO | train_inner | epoch 003:    352 / 1475 loss=6.313, nll_loss=5.221, ppl=37.3, wps=57832.7, ups=15.45, wpb=3742.7, bsz=199, num_updates=3300, lr=0.0004125, gnorm=1.33, loss_scale=32, train_wall=6, gb_free=29.7, wall=244
2024-01-30 15:37:31 | INFO | train_inner | epoch 003:    452 / 1475 loss=6.131, nll_loss=5.011, ppl=32.24, wps=58353.8, ups=15.47, wpb=3773, bsz=200, num_updates=3400, lr=0.000425, gnorm=1.33, loss_scale=32, train_wall=6, gb_free=29.7, wall=250
2024-01-30 15:37:37 | INFO | train_inner | epoch 003:    552 / 1475 loss=6.058, nll_loss=4.923, ppl=30.34, wps=58851.8, ups=15.44, wpb=3812.1, bsz=198.6, num_updates=3500, lr=0.0004375, gnorm=1.345, loss_scale=32, train_wall=6, gb_free=29.7, wall=256
2024-01-30 15:37:44 | INFO | train_inner | epoch 003:    652 / 1475 loss=5.969, nll_loss=4.818, ppl=28.22, wps=58856.2, ups=15.4, wpb=3821.6, bsz=200.7, num_updates=3600, lr=0.00045, gnorm=1.32, loss_scale=32, train_wall=6, gb_free=29.6, wall=263
2024-01-30 15:37:50 | INFO | train_inner | epoch 003:    752 / 1475 loss=5.905, nll_loss=4.742, ppl=26.77, wps=58862.9, ups=15.42, wpb=3816.2, bsz=192, num_updates=3700, lr=0.0004625, gnorm=1.346, loss_scale=32, train_wall=6, gb_free=29.8, wall=269
2024-01-30 15:37:57 | INFO | train_inner | epoch 003:    852 / 1475 loss=5.811, nll_loss=4.634, ppl=24.83, wps=58131.5, ups=15.42, wpb=3769.4, bsz=186.7, num_updates=3800, lr=0.000475, gnorm=1.374, loss_scale=32, train_wall=6, gb_free=29.7, wall=276
2024-01-30 15:38:03 | INFO | train_inner | epoch 003:    952 / 1475 loss=5.651, nll_loss=4.448, ppl=21.83, wps=58419.5, ups=15.4, wpb=3794.5, bsz=202.7, num_updates=3900, lr=0.0004875, gnorm=1.337, loss_scale=32, train_wall=6, gb_free=29.8, wall=282
2024-01-30 15:38:10 | INFO | train_inner | epoch 003:   1052 / 1475 loss=5.571, nll_loss=4.353, ppl=20.44, wps=59200.4, ups=15.5, wpb=3819.3, bsz=200.3, num_updates=4000, lr=0.0005, gnorm=1.361, loss_scale=32, train_wall=6, gb_free=29.7, wall=289
2024-01-30 15:38:16 | INFO | train_inner | epoch 003:   1152 / 1475 loss=5.412, nll_loss=4.17, ppl=18, wps=58413.8, ups=15.42, wpb=3789.1, bsz=208.6, num_updates=4100, lr=0.000493865, gnorm=1.351, loss_scale=32, train_wall=6, gb_free=29.7, wall=295
2024-01-30 15:38:23 | INFO | train_inner | epoch 003:   1252 / 1475 loss=5.437, nll_loss=4.199, ppl=18.36, wps=57593.3, ups=15.44, wpb=3731.1, bsz=189.6, num_updates=4200, lr=0.00048795, gnorm=1.359, loss_scale=32, train_wall=6, gb_free=29.7, wall=302
2024-01-30 15:38:29 | INFO | train_inner | epoch 003:   1352 / 1475 loss=5.336, nll_loss=4.08, ppl=16.92, wps=57832.8, ups=15.49, wpb=3734.7, bsz=183.6, num_updates=4300, lr=0.000482243, gnorm=1.357, loss_scale=32, train_wall=6, gb_free=29.7, wall=308
2024-01-30 15:38:36 | INFO | train_inner | epoch 003:   1452 / 1475 loss=5.184, nll_loss=3.906, ppl=14.99, wps=58490.3, ups=15.44, wpb=3789.2, bsz=202.4, num_updates=4400, lr=0.000476731, gnorm=1.333, loss_scale=32, train_wall=6, gb_free=29.7, wall=315
2024-01-30 15:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:38:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:38:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.996 | nll_loss 3.55 | ppl 11.71 | wps 132042 | wpb 3189.1 | bsz 163.4 | num_updates 4423 | best_loss 4.996
2024-01-30 15:38:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4423 updates
2024-01-30 15:38:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint3.pt
2024-01-30 15:38:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint3.pt
2024-01-30 15:38:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint3.pt (epoch 3 @ 4423 updates, score 4.996) (writing took 5.040111684938893 seconds)
2024-01-30 15:38:45 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-01-30 15:38:45 | INFO | train | epoch 003 | loss 5.852 | nll_loss 4.683 | ppl 25.68 | wps 53996.5 | ups 14.32 | wpb 3771.6 | bsz 195.9 | num_updates 4423 | lr 0.00047549 | gnorm 1.345 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 324
2024-01-30 15:38:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:38:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:38:45 | INFO | fairseq.trainer | begin training epoch 4
2024-01-30 15:38:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:38:50 | INFO | train_inner | epoch 004:     77 / 1475 loss=5.027, nll_loss=3.725, ppl=13.22, wps=27535.7, ups=7.13, wpb=3864.6, bsz=194.7, num_updates=4500, lr=0.000471405, gnorm=1.287, loss_scale=32, train_wall=6, gb_free=29.8, wall=329
2024-01-30 15:38:56 | INFO | train_inner | epoch 004:    177 / 1475 loss=5.011, nll_loss=3.706, ppl=13.05, wps=57797.3, ups=15.47, wpb=3735.7, bsz=184.7, num_updates=4600, lr=0.000466252, gnorm=1.322, loss_scale=32, train_wall=6, gb_free=29.7, wall=335
2024-01-30 15:39:03 | INFO | train_inner | epoch 004:    277 / 1475 loss=4.944, nll_loss=3.629, ppl=12.37, wps=58215.7, ups=15.47, wpb=3763.8, bsz=201.8, num_updates=4700, lr=0.000461266, gnorm=1.331, loss_scale=32, train_wall=6, gb_free=29.8, wall=342
2024-01-30 15:39:09 | INFO | train_inner | epoch 004:    377 / 1475 loss=4.883, nll_loss=3.558, ppl=11.78, wps=58911.8, ups=15.47, wpb=3808.3, bsz=195.4, num_updates=4800, lr=0.000456435, gnorm=1.279, loss_scale=32, train_wall=6, gb_free=29.7, wall=348
2024-01-30 15:39:16 | INFO | train_inner | epoch 004:    477 / 1475 loss=4.773, nll_loss=3.431, ppl=10.78, wps=58782.5, ups=15.33, wpb=3835.3, bsz=195.8, num_updates=4900, lr=0.000451754, gnorm=1.247, loss_scale=32, train_wall=6, gb_free=29.8, wall=355
2024-01-30 15:39:22 | INFO | train_inner | epoch 004:    577 / 1475 loss=4.774, nll_loss=3.433, ppl=10.8, wps=58827.7, ups=15.47, wpb=3802.4, bsz=180.2, num_updates=5000, lr=0.000447214, gnorm=1.224, loss_scale=32, train_wall=6, gb_free=29.7, wall=361
2024-01-30 15:39:29 | INFO | train_inner | epoch 004:    677 / 1475 loss=4.732, nll_loss=3.385, ppl=10.45, wps=57956.5, ups=15.42, wpb=3758.4, bsz=190.4, num_updates=5100, lr=0.000442807, gnorm=1.252, loss_scale=32, train_wall=6, gb_free=29.9, wall=368
2024-01-30 15:39:35 | INFO | train_inner | epoch 004:    777 / 1475 loss=4.67, nll_loss=3.315, ppl=9.95, wps=57679.4, ups=15.6, wpb=3697.7, bsz=185.9, num_updates=5200, lr=0.000438529, gnorm=1.24, loss_scale=32, train_wall=6, gb_free=29.7, wall=374
2024-01-30 15:39:41 | INFO | train_inner | epoch 004:    877 / 1475 loss=4.582, nll_loss=3.215, ppl=9.28, wps=58232.4, ups=15.44, wpb=3771.4, bsz=198.6, num_updates=5300, lr=0.000434372, gnorm=1.245, loss_scale=32, train_wall=6, gb_free=29.7, wall=381
2024-01-30 15:39:48 | INFO | train_inner | epoch 004:    977 / 1475 loss=4.638, nll_loss=3.279, ppl=9.7, wps=57568.2, ups=15.49, wpb=3716.8, bsz=192.9, num_updates=5400, lr=0.000430331, gnorm=1.269, loss_scale=32, train_wall=6, gb_free=29.7, wall=387
2024-01-30 15:39:54 | INFO | train_inner | epoch 004:   1077 / 1475 loss=4.497, nll_loss=3.12, ppl=8.69, wps=56639.9, ups=15.4, wpb=3678.8, bsz=210.6, num_updates=5500, lr=0.000426401, gnorm=1.239, loss_scale=32, train_wall=6, gb_free=29.8, wall=394
2024-01-30 15:40:01 | INFO | train_inner | epoch 004:   1177 / 1475 loss=4.461, nll_loss=3.078, ppl=8.44, wps=58331.9, ups=15.45, wpb=3775.1, bsz=205.4, num_updates=5600, lr=0.000422577, gnorm=1.187, loss_scale=32, train_wall=6, gb_free=29.7, wall=400
2024-01-30 15:40:07 | INFO | train_inner | epoch 004:   1277 / 1475 loss=4.491, nll_loss=3.111, ppl=8.64, wps=58422.2, ups=15.48, wpb=3774.6, bsz=197, num_updates=5700, lr=0.000418854, gnorm=1.18, loss_scale=32, train_wall=6, gb_free=29.7, wall=406
2024-01-30 15:40:14 | INFO | train_inner | epoch 004:   1377 / 1475 loss=4.431, nll_loss=3.045, ppl=8.25, wps=58566.8, ups=15.49, wpb=3781.5, bsz=197.8, num_updates=5800, lr=0.000415227, gnorm=1.134, loss_scale=32, train_wall=6, gb_free=29.7, wall=413
2024-01-30 15:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:40:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:40:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.309 | nll_loss 2.735 | ppl 6.66 | wps 131729 | wpb 3189.1 | bsz 163.4 | num_updates 5898 | best_loss 4.309
2024-01-30 15:40:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5898 updates
2024-01-30 15:40:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint4.pt
2024-01-30 15:40:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint4.pt
2024-01-30 15:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint4.pt (epoch 4 @ 5898 updates, score 4.309) (writing took 5.19259921903722 seconds)
2024-01-30 15:40:28 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-01-30 15:40:28 | INFO | train | epoch 004 | loss 4.679 | nll_loss 3.325 | ppl 10.02 | wps 53926.9 | ups 14.3 | wpb 3771.6 | bsz 195.9 | num_updates 5898 | lr 0.000411763 | gnorm 1.239 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 427
2024-01-30 15:40:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:40:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:40:28 | INFO | fairseq.trainer | begin training epoch 5
2024-01-30 15:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:40:28 | INFO | train_inner | epoch 005:      2 / 1475 loss=4.352, nll_loss=2.955, ppl=7.75, wps=26878.8, ups=7.06, wpb=3805.5, bsz=208.9, num_updates=5900, lr=0.000411693, gnorm=1.157, loss_scale=32, train_wall=6, gb_free=29.8, wall=427
2024-01-30 15:40:34 | INFO | train_inner | epoch 005:    102 / 1475 loss=4.268, nll_loss=2.858, ppl=7.25, wps=58228.7, ups=15.35, wpb=3793.1, bsz=193.3, num_updates=6000, lr=0.000408248, gnorm=1.123, loss_scale=32, train_wall=6, gb_free=29.7, wall=434
2024-01-30 15:40:41 | INFO | train_inner | epoch 005:    202 / 1475 loss=4.241, nll_loss=2.828, ppl=7.1, wps=57932.3, ups=15.37, wpb=3768.6, bsz=203.3, num_updates=6100, lr=0.000404888, gnorm=1.141, loss_scale=32, train_wall=6, gb_free=29.7, wall=440
2024-01-30 15:40:47 | INFO | train_inner | epoch 005:    302 / 1475 loss=4.208, nll_loss=2.789, ppl=6.91, wps=58742.8, ups=15.45, wpb=3802.8, bsz=204.4, num_updates=6200, lr=0.00040161, gnorm=1.117, loss_scale=32, train_wall=6, gb_free=29.7, wall=447
2024-01-30 15:40:54 | INFO | train_inner | epoch 005:    402 / 1475 loss=4.262, nll_loss=2.853, ppl=7.22, wps=56559.7, ups=15.42, wpb=3668.1, bsz=190, num_updates=6300, lr=0.00039841, gnorm=1.179, loss_scale=32, train_wall=6, gb_free=29.7, wall=453
2024-01-30 15:41:00 | INFO | train_inner | epoch 005:    502 / 1475 loss=4.185, nll_loss=2.764, ppl=6.79, wps=57978.2, ups=15.46, wpb=3750.8, bsz=192.7, num_updates=6400, lr=0.000395285, gnorm=1.132, loss_scale=32, train_wall=6, gb_free=29.7, wall=460
2024-01-30 15:41:07 | INFO | train_inner | epoch 005:    602 / 1475 loss=4.218, nll_loss=2.801, ppl=6.97, wps=58770.4, ups=15.62, wpb=3762.1, bsz=191, num_updates=6500, lr=0.000392232, gnorm=1.136, loss_scale=32, train_wall=6, gb_free=29.7, wall=466
2024-01-30 15:41:13 | INFO | train_inner | epoch 005:    702 / 1475 loss=4.151, nll_loss=2.725, ppl=6.61, wps=58145, ups=15.42, wpb=3771.4, bsz=193.3, num_updates=6600, lr=0.000389249, gnorm=1.113, loss_scale=32, train_wall=6, gb_free=29.7, wall=472
2024-01-30 15:41:20 | INFO | train_inner | epoch 005:    802 / 1475 loss=4.114, nll_loss=2.683, ppl=6.42, wps=58735.4, ups=15.46, wpb=3798.5, bsz=194.2, num_updates=6700, lr=0.000386334, gnorm=1.105, loss_scale=32, train_wall=6, gb_free=29.7, wall=479
2024-01-30 15:41:26 | INFO | train_inner | epoch 005:    902 / 1475 loss=4.135, nll_loss=2.708, ppl=6.54, wps=58120.6, ups=15.48, wpb=3753.3, bsz=188.4, num_updates=6800, lr=0.000383482, gnorm=1.12, loss_scale=32, train_wall=6, gb_free=29.7, wall=485
2024-01-30 15:41:33 | INFO | train_inner | epoch 005:   1002 / 1475 loss=4.082, nll_loss=2.648, ppl=6.27, wps=58509, ups=15.36, wpb=3808.7, bsz=203, num_updates=6900, lr=0.000380693, gnorm=1.073, loss_scale=32, train_wall=6, gb_free=29.7, wall=492
2024-01-30 15:41:39 | INFO | train_inner | epoch 005:   1102 / 1475 loss=4.021, nll_loss=2.582, ppl=5.99, wps=57767.9, ups=15.42, wpb=3747.3, bsz=216.7, num_updates=7000, lr=0.000377964, gnorm=1.108, loss_scale=32, train_wall=6, gb_free=29.7, wall=498
2024-01-30 15:41:46 | INFO | train_inner | epoch 005:   1202 / 1475 loss=4.143, nll_loss=2.719, ppl=6.58, wps=59058.5, ups=15.58, wpb=3789.5, bsz=185, num_updates=7100, lr=0.000375293, gnorm=1.095, loss_scale=32, train_wall=6, gb_free=29.8, wall=505
2024-01-30 15:41:52 | INFO | train_inner | epoch 005:   1302 / 1475 loss=4.042, nll_loss=2.605, ppl=6.08, wps=58256.2, ups=15.44, wpb=3772.8, bsz=197.7, num_updates=7200, lr=0.000372678, gnorm=1.079, loss_scale=32, train_wall=6, gb_free=29.8, wall=511
2024-01-30 15:41:59 | INFO | train_inner | epoch 005:   1402 / 1475 loss=4.036, nll_loss=2.597, ppl=6.05, wps=58137.1, ups=15.46, wpb=3759.9, bsz=185.8, num_updates=7300, lr=0.000370117, gnorm=1.11, loss_scale=32, train_wall=6, gb_free=29.9, wall=518
2024-01-30 15:42:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:42:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:42:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.062 | nll_loss 2.471 | ppl 5.54 | wps 133615 | wpb 3189.1 | bsz 163.4 | num_updates 7373 | best_loss 4.062
2024-01-30 15:42:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7373 updates
2024-01-30 15:42:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint5.pt
2024-01-30 15:42:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint5.pt
2024-01-30 15:42:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint5.pt (epoch 5 @ 7373 updates, score 4.062) (writing took 5.1254116870695725 seconds)
2024-01-30 15:42:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-01-30 15:42:11 | INFO | train | epoch 005 | loss 4.141 | nll_loss 2.715 | ppl 6.57 | wps 53972.7 | ups 14.31 | wpb 3771.6 | bsz 195.9 | num_updates 7373 | lr 0.00036828 | gnorm 1.113 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 530
2024-01-30 15:42:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:42:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:42:11 | INFO | fairseq.trainer | begin training epoch 6
2024-01-30 15:42:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:42:13 | INFO | train_inner | epoch 006:     27 / 1475 loss=3.958, nll_loss=2.509, ppl=5.69, wps=27138.4, ups=7.09, wpb=3828.5, bsz=198.2, num_updates=7400, lr=0.000367607, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=29.7, wall=532
2024-01-30 15:42:19 | INFO | train_inner | epoch 006:    127 / 1475 loss=3.876, nll_loss=2.416, ppl=5.34, wps=58692.1, ups=15.43, wpb=3802.9, bsz=199.8, num_updates=7500, lr=0.000365148, gnorm=1.038, loss_scale=32, train_wall=6, gb_free=29.7, wall=538
2024-01-30 15:42:26 | INFO | train_inner | epoch 006:    227 / 1475 loss=3.878, nll_loss=2.419, ppl=5.35, wps=58450, ups=15.36, wpb=3805.3, bsz=199.8, num_updates=7600, lr=0.000362738, gnorm=1.036, loss_scale=32, train_wall=6, gb_free=29.7, wall=545
2024-01-30 15:42:32 | INFO | train_inner | epoch 006:    327 / 1475 loss=3.942, nll_loss=2.49, ppl=5.62, wps=59249.6, ups=15.61, wpb=3795.8, bsz=188.6, num_updates=7700, lr=0.000360375, gnorm=1.076, loss_scale=32, train_wall=6, gb_free=29.7, wall=551
2024-01-30 15:42:39 | INFO | train_inner | epoch 006:    427 / 1475 loss=3.865, nll_loss=2.404, ppl=5.29, wps=58347.5, ups=15.5, wpb=3763.6, bsz=195.5, num_updates=7800, lr=0.000358057, gnorm=1.068, loss_scale=32, train_wall=6, gb_free=29.6, wall=558
2024-01-30 15:42:45 | INFO | train_inner | epoch 006:    527 / 1475 loss=3.876, nll_loss=2.417, ppl=5.34, wps=58679.3, ups=15.5, wpb=3785.1, bsz=195.1, num_updates=7900, lr=0.000355784, gnorm=1.034, loss_scale=32, train_wall=6, gb_free=29.7, wall=564
2024-01-30 15:42:51 | INFO | train_inner | epoch 006:    627 / 1475 loss=3.857, nll_loss=2.395, ppl=5.26, wps=59117.3, ups=15.44, wpb=3829.9, bsz=196.5, num_updates=8000, lr=0.000353553, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=29.7, wall=571
2024-01-30 15:42:58 | INFO | train_inner | epoch 006:    727 / 1475 loss=3.82, nll_loss=2.354, ppl=5.11, wps=58044.6, ups=15.47, wpb=3753.1, bsz=205.4, num_updates=8100, lr=0.000351364, gnorm=1.051, loss_scale=32, train_wall=6, gb_free=29.7, wall=577
2024-01-30 15:43:04 | INFO | train_inner | epoch 006:    827 / 1475 loss=3.86, nll_loss=2.399, ppl=5.28, wps=58355.8, ups=15.43, wpb=3781.7, bsz=193, num_updates=8200, lr=0.000349215, gnorm=1.045, loss_scale=32, train_wall=6, gb_free=29.7, wall=584
2024-01-30 15:43:11 | INFO | train_inner | epoch 006:    927 / 1475 loss=3.867, nll_loss=2.409, ppl=5.31, wps=57744.3, ups=15.5, wpb=3725.4, bsz=186.2, num_updates=8300, lr=0.000347105, gnorm=1.048, loss_scale=32, train_wall=6, gb_free=29.7, wall=590
2024-01-30 15:43:17 | INFO | train_inner | epoch 006:   1027 / 1475 loss=3.838, nll_loss=2.376, ppl=5.19, wps=57263.8, ups=15.39, wpb=3721.8, bsz=197.4, num_updates=8400, lr=0.000345033, gnorm=1.044, loss_scale=32, train_wall=6, gb_free=29.8, wall=597
2024-01-30 15:43:24 | INFO | train_inner | epoch 006:   1127 / 1475 loss=3.812, nll_loss=2.347, ppl=5.09, wps=57594.6, ups=15.36, wpb=3749.1, bsz=199.6, num_updates=8500, lr=0.000342997, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=29.7, wall=603
2024-01-30 15:43:30 | INFO | train_inner | epoch 006:   1227 / 1475 loss=3.809, nll_loss=2.343, ppl=5.07, wps=58025.3, ups=15.48, wpb=3747.2, bsz=200.6, num_updates=8600, lr=0.000340997, gnorm=1.037, loss_scale=32, train_wall=6, gb_free=29.7, wall=609
2024-01-30 15:43:37 | INFO | train_inner | epoch 006:   1327 / 1475 loss=3.834, nll_loss=2.372, ppl=5.18, wps=57553.3, ups=15.47, wpb=3719.5, bsz=188, num_updates=8700, lr=0.000339032, gnorm=1.069, loss_scale=32, train_wall=6, gb_free=29.7, wall=616
2024-01-30 15:43:43 | INFO | train_inner | epoch 006:   1427 / 1475 loss=3.756, nll_loss=2.285, ppl=4.87, wps=58320.7, ups=15.4, wpb=3787.4, bsz=207.4, num_updates=8800, lr=0.0003371, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=29.7, wall=622
2024-01-30 15:43:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:43:49 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.858 | nll_loss 2.23 | ppl 4.69 | wps 133621 | wpb 3189.1 | bsz 163.4 | num_updates 8848 | best_loss 3.858
2024-01-30 15:43:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8848 updates
2024-01-30 15:43:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint6.pt
2024-01-30 15:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint6.pt
2024-01-30 15:43:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint6.pt (epoch 6 @ 8848 updates, score 3.858) (writing took 5.191093373927288 seconds)
2024-01-30 15:43:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-01-30 15:43:54 | INFO | train | epoch 006 | loss 3.85 | nll_loss 2.388 | ppl 5.23 | wps 53907.4 | ups 14.29 | wpb 3771.6 | bsz 195.9 | num_updates 8848 | lr 0.000336184 | gnorm 1.048 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 633
2024-01-30 15:43:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:43:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:43:54 | INFO | fairseq.trainer | begin training epoch 7
2024-01-30 15:43:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:43:58 | INFO | train_inner | epoch 007:     52 / 1475 loss=3.733, nll_loss=2.257, ppl=4.78, wps=27158.6, ups=7.03, wpb=3862, bsz=183.6, num_updates=8900, lr=0.000335201, gnorm=0.997, loss_scale=32, train_wall=6, gb_free=29.7, wall=637
2024-01-30 15:44:04 | INFO | train_inner | epoch 007:    152 / 1475 loss=3.636, nll_loss=2.146, ppl=4.43, wps=58980.6, ups=15.44, wpb=3819.9, bsz=210.4, num_updates=9000, lr=0.000333333, gnorm=0.974, loss_scale=32, train_wall=6, gb_free=29.7, wall=643
2024-01-30 15:44:10 | INFO | train_inner | epoch 007:    252 / 1475 loss=3.666, nll_loss=2.179, ppl=4.53, wps=58649.7, ups=15.48, wpb=3787.7, bsz=197.7, num_updates=9100, lr=0.000331497, gnorm=0.98, loss_scale=32, train_wall=6, gb_free=29.8, wall=650
2024-01-30 15:44:17 | INFO | train_inner | epoch 007:    352 / 1475 loss=3.66, nll_loss=2.174, ppl=4.51, wps=58955.6, ups=15.45, wpb=3814.8, bsz=199.4, num_updates=9200, lr=0.00032969, gnorm=0.985, loss_scale=32, train_wall=6, gb_free=29.8, wall=656
2024-01-30 15:44:23 | INFO | train_inner | epoch 007:    452 / 1475 loss=3.694, nll_loss=2.213, ppl=4.64, wps=56658, ups=15.47, wpb=3661.3, bsz=200.6, num_updates=9300, lr=0.000327913, gnorm=1.06, loss_scale=32, train_wall=6, gb_free=29.7, wall=663
2024-01-30 15:44:30 | INFO | train_inner | epoch 007:    552 / 1475 loss=3.66, nll_loss=2.174, ppl=4.51, wps=58614.2, ups=15.49, wpb=3784.8, bsz=194.5, num_updates=9400, lr=0.000326164, gnorm=0.996, loss_scale=32, train_wall=6, gb_free=29.7, wall=669
2024-01-30 15:44:36 | INFO | train_inner | epoch 007:    652 / 1475 loss=3.666, nll_loss=2.181, ppl=4.53, wps=58892.1, ups=15.54, wpb=3790.9, bsz=192.7, num_updates=9500, lr=0.000324443, gnorm=1.001, loss_scale=32, train_wall=6, gb_free=29.7, wall=675
2024-01-30 15:44:43 | INFO | train_inner | epoch 007:    752 / 1475 loss=3.667, nll_loss=2.182, ppl=4.54, wps=59596.8, ups=15.48, wpb=3850.8, bsz=196.8, num_updates=9600, lr=0.000322749, gnorm=0.965, loss_scale=32, train_wall=6, gb_free=29.8, wall=682
2024-01-30 15:44:49 | INFO | train_inner | epoch 007:    852 / 1475 loss=3.662, nll_loss=2.177, ppl=4.52, wps=58154.6, ups=15.56, wpb=3738.6, bsz=192.2, num_updates=9700, lr=0.000321081, gnorm=1.008, loss_scale=32, train_wall=6, gb_free=29.7, wall=688
2024-01-30 15:44:56 | INFO | train_inner | epoch 007:    952 / 1475 loss=3.673, nll_loss=2.191, ppl=4.57, wps=57112.5, ups=15.42, wpb=3704, bsz=188.2, num_updates=9800, lr=0.000319438, gnorm=1.042, loss_scale=32, train_wall=6, gb_free=29.7, wall=695
2024-01-30 15:45:02 | INFO | train_inner | epoch 007:   1052 / 1475 loss=3.632, nll_loss=2.145, ppl=4.42, wps=58258.2, ups=15.42, wpb=3778.4, bsz=187.8, num_updates=9900, lr=0.000317821, gnorm=0.999, loss_scale=32, train_wall=6, gb_free=29.7, wall=701
2024-01-30 15:45:09 | INFO | train_inner | epoch 007:   1152 / 1475 loss=3.632, nll_loss=2.145, ppl=4.42, wps=57947.3, ups=15.51, wpb=3736.6, bsz=200.8, num_updates=10000, lr=0.000316228, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=29.7, wall=708
2024-01-30 15:45:15 | INFO | train_inner | epoch 007:   1252 / 1475 loss=3.654, nll_loss=2.17, ppl=4.5, wps=58481.9, ups=15.51, wpb=3771.4, bsz=186.6, num_updates=10100, lr=0.000314658, gnorm=1.022, loss_scale=32, train_wall=6, gb_free=29.8, wall=714
2024-01-30 15:45:22 | INFO | train_inner | epoch 007:   1352 / 1475 loss=3.598, nll_loss=2.107, ppl=4.31, wps=57011.4, ups=15.3, wpb=3727.2, bsz=205.3, num_updates=10200, lr=0.000313112, gnorm=1.01, loss_scale=32, train_wall=6, gb_free=29.7, wall=721
2024-01-30 15:45:28 | INFO | train_inner | epoch 007:   1452 / 1475 loss=3.634, nll_loss=2.148, ppl=4.43, wps=58519, ups=15.33, wpb=3816.2, bsz=194.1, num_updates=10300, lr=0.000311588, gnorm=0.981, loss_scale=32, train_wall=6, gb_free=29.8, wall=727
2024-01-30 15:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:45:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:45:32 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.762 | nll_loss 2.131 | ppl 4.38 | wps 125478 | wpb 3189.1 | bsz 163.4 | num_updates 10323 | best_loss 3.762
2024-01-30 15:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10323 updates
2024-01-30 15:45:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint7.pt
2024-01-30 15:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint7.pt
2024-01-30 15:45:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint7.pt (epoch 7 @ 10323 updates, score 3.762) (writing took 5.3528879339573905 seconds)
2024-01-30 15:45:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-01-30 15:45:38 | INFO | train | epoch 007 | loss 3.652 | nll_loss 2.166 | ppl 4.49 | wps 53715.9 | ups 14.24 | wpb 3771.6 | bsz 195.9 | num_updates 10323 | lr 0.000311241 | gnorm 1.003 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 737
2024-01-30 15:45:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:45:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:45:38 | INFO | fairseq.trainer | begin training epoch 8
2024-01-30 15:45:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:45:43 | INFO | train_inner | epoch 008:     77 / 1475 loss=3.542, nll_loss=2.043, ppl=4.12, wps=25534.5, ups=6.77, wpb=3773.1, bsz=190, num_updates=10400, lr=0.000310087, gnorm=1.003, loss_scale=32, train_wall=6, gb_free=29.7, wall=742
2024-01-30 15:45:50 | INFO | train_inner | epoch 008:    177 / 1475 loss=3.5, nll_loss=1.994, ppl=3.98, wps=57112.9, ups=14.97, wpb=3816.2, bsz=194.2, num_updates=10500, lr=0.000308607, gnorm=0.952, loss_scale=32, train_wall=6, gb_free=29.9, wall=749
2024-01-30 15:45:56 | INFO | train_inner | epoch 008:    277 / 1475 loss=3.485, nll_loss=1.979, ppl=3.94, wps=56363.7, ups=15.03, wpb=3751, bsz=202.3, num_updates=10600, lr=0.000307148, gnorm=0.964, loss_scale=32, train_wall=6, gb_free=29.7, wall=755
2024-01-30 15:46:03 | INFO | train_inner | epoch 008:    377 / 1475 loss=3.545, nll_loss=2.045, ppl=4.13, wps=58989.9, ups=15.58, wpb=3786.3, bsz=187.5, num_updates=10700, lr=0.000305709, gnorm=0.974, loss_scale=32, train_wall=6, gb_free=29.8, wall=762
2024-01-30 15:46:09 | INFO | train_inner | epoch 008:    477 / 1475 loss=3.514, nll_loss=2.011, ppl=4.03, wps=58135.4, ups=15.55, wpb=3737.6, bsz=191.5, num_updates=10800, lr=0.00030429, gnorm=0.987, loss_scale=32, train_wall=6, gb_free=29.7, wall=768
2024-01-30 15:46:16 | INFO | train_inner | epoch 008:    577 / 1475 loss=3.483, nll_loss=1.976, ppl=3.93, wps=58472.8, ups=15.44, wpb=3787, bsz=205.2, num_updates=10900, lr=0.000302891, gnorm=0.952, loss_scale=32, train_wall=6, gb_free=29.7, wall=775
2024-01-30 15:46:22 | INFO | train_inner | epoch 008:    677 / 1475 loss=3.52, nll_loss=2.019, ppl=4.05, wps=57924.4, ups=15.47, wpb=3743.8, bsz=197.3, num_updates=11000, lr=0.000301511, gnorm=1.001, loss_scale=32, train_wall=6, gb_free=29.7, wall=781
2024-01-30 15:46:28 | INFO | train_inner | epoch 008:    777 / 1475 loss=3.571, nll_loss=2.075, ppl=4.21, wps=59074.2, ups=15.59, wpb=3788.2, bsz=182.6, num_updates=11100, lr=0.00030015, gnorm=0.997, loss_scale=32, train_wall=6, gb_free=29.7, wall=788
2024-01-30 15:46:35 | INFO | train_inner | epoch 008:    877 / 1475 loss=3.501, nll_loss=1.998, ppl=3.99, wps=59019.2, ups=15.5, wpb=3806.7, bsz=208.3, num_updates=11200, lr=0.000298807, gnorm=0.958, loss_scale=32, train_wall=6, gb_free=29.8, wall=794
2024-01-30 15:46:41 | INFO | train_inner | epoch 008:    977 / 1475 loss=3.47, nll_loss=1.963, ppl=3.9, wps=58786.9, ups=15.46, wpb=3801.3, bsz=208.8, num_updates=11300, lr=0.000297482, gnorm=0.953, loss_scale=32, train_wall=6, gb_free=29.8, wall=800
2024-01-30 15:46:48 | INFO | train_inner | epoch 008:   1077 / 1475 loss=3.488, nll_loss=1.983, ppl=3.95, wps=58169.3, ups=15.54, wpb=3742.9, bsz=194.8, num_updates=11400, lr=0.000296174, gnorm=0.983, loss_scale=32, train_wall=6, gb_free=29.7, wall=807
2024-01-30 15:46:54 | INFO | train_inner | epoch 008:   1177 / 1475 loss=3.514, nll_loss=2.014, ppl=4.04, wps=57791.8, ups=15.38, wpb=3758.3, bsz=198.2, num_updates=11500, lr=0.000294884, gnorm=1.004, loss_scale=32, train_wall=6, gb_free=29.7, wall=813
2024-01-30 15:47:01 | INFO | train_inner | epoch 008:   1277 / 1475 loss=3.51, nll_loss=2.009, ppl=4.02, wps=56618.6, ups=15.17, wpb=3732, bsz=193.8, num_updates=11600, lr=0.00029361, gnorm=1.007, loss_scale=32, train_wall=6, gb_free=29.7, wall=820
2024-01-30 15:47:07 | INFO | train_inner | epoch 008:   1377 / 1475 loss=3.546, nll_loss=2.051, ppl=4.14, wps=56858.8, ups=15.18, wpb=3744.8, bsz=190.4, num_updates=11700, lr=0.000292353, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=29.7, wall=827
2024-01-30 15:47:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:47:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:47:16 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.694 | nll_loss 2.052 | ppl 4.15 | wps 129611 | wpb 3189.1 | bsz 163.4 | num_updates 11798 | best_loss 3.694
2024-01-30 15:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11798 updates
2024-01-30 15:47:16 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint8.pt
2024-01-30 15:47:17 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint8.pt
2024-01-30 15:47:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint8.pt (epoch 8 @ 11798 updates, score 3.694) (writing took 5.406758495955728 seconds)
2024-01-30 15:47:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-01-30 15:47:22 | INFO | train | epoch 008 | loss 3.512 | nll_loss 2.009 | ppl 4.03 | wps 53349.9 | ups 14.15 | wpb 3771.6 | bsz 195.9 | num_updates 11798 | lr 0.000291136 | gnorm 0.982 | loss_scale 32 | train_wall 93 | gb_free 29.6 | wall 841
2024-01-30 15:47:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:47:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:47:22 | INFO | fairseq.trainer | begin training epoch 9
2024-01-30 15:47:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:47:22 | INFO | train_inner | epoch 009:      2 / 1475 loss=3.499, nll_loss=1.997, ppl=3.99, wps=25792.6, ups=6.82, wpb=3782.8, bsz=196.9, num_updates=11800, lr=0.000291111, gnorm=0.978, loss_scale=32, train_wall=6, gb_free=29.7, wall=841
2024-01-30 15:47:29 | INFO | train_inner | epoch 009:    102 / 1475 loss=3.378, nll_loss=1.858, ppl=3.63, wps=53731.4, ups=14.21, wpb=3781.4, bsz=199.3, num_updates=11900, lr=0.000289886, gnorm=0.946, loss_scale=32, train_wall=7, gb_free=29.7, wall=848
2024-01-30 15:47:36 | INFO | train_inner | epoch 009:    202 / 1475 loss=3.397, nll_loss=1.88, ppl=3.68, wps=57154, ups=15.36, wpb=3721.3, bsz=200.8, num_updates=12000, lr=0.000288675, gnorm=0.963, loss_scale=32, train_wall=6, gb_free=29.8, wall=855
2024-01-30 15:47:42 | INFO | train_inner | epoch 009:    302 / 1475 loss=3.385, nll_loss=1.865, ppl=3.64, wps=58106.6, ups=15.39, wpb=3775.8, bsz=194.2, num_updates=12100, lr=0.00028748, gnorm=0.951, loss_scale=32, train_wall=6, gb_free=29.6, wall=861
2024-01-30 15:47:49 | INFO | train_inner | epoch 009:    402 / 1475 loss=3.421, nll_loss=1.906, ppl=3.75, wps=58037.3, ups=15.41, wpb=3766.4, bsz=190.1, num_updates=12200, lr=0.000286299, gnorm=0.971, loss_scale=32, train_wall=6, gb_free=29.7, wall=868
2024-01-30 15:47:55 | INFO | train_inner | epoch 009:    502 / 1475 loss=3.409, nll_loss=1.892, ppl=3.71, wps=58363.3, ups=15.36, wpb=3799.2, bsz=188.6, num_updates=12300, lr=0.000285133, gnorm=0.968, loss_scale=32, train_wall=6, gb_free=29.7, wall=874
2024-01-30 15:48:02 | INFO | train_inner | epoch 009:    602 / 1475 loss=3.403, nll_loss=1.886, ppl=3.7, wps=58278.5, ups=15.37, wpb=3791.5, bsz=195.5, num_updates=12400, lr=0.000283981, gnorm=0.938, loss_scale=32, train_wall=6, gb_free=29.7, wall=881
2024-01-30 15:48:08 | INFO | train_inner | epoch 009:    702 / 1475 loss=3.39, nll_loss=1.872, ppl=3.66, wps=58260.5, ups=15.38, wpb=3787.3, bsz=198.3, num_updates=12500, lr=0.000282843, gnorm=0.967, loss_scale=32, train_wall=6, gb_free=29.6, wall=887
2024-01-30 15:48:15 | INFO | train_inner | epoch 009:    802 / 1475 loss=3.396, nll_loss=1.88, ppl=3.68, wps=57900, ups=15.32, wpb=3779.2, bsz=197.4, num_updates=12600, lr=0.000281718, gnorm=0.952, loss_scale=32, train_wall=6, gb_free=29.7, wall=894
2024-01-30 15:48:21 | INFO | train_inner | epoch 009:    902 / 1475 loss=3.424, nll_loss=1.913, ppl=3.77, wps=58201.8, ups=15.53, wpb=3748.4, bsz=192.7, num_updates=12700, lr=0.000280607, gnorm=0.984, loss_scale=32, train_wall=6, gb_free=29.7, wall=900
2024-01-30 15:48:28 | INFO | train_inner | epoch 009:   1002 / 1475 loss=3.436, nll_loss=1.927, ppl=3.8, wps=57105.5, ups=15.52, wpb=3680.3, bsz=190.1, num_updates=12800, lr=0.000279508, gnorm=1.016, loss_scale=32, train_wall=6, gb_free=29.6, wall=907
2024-01-30 15:48:34 | INFO | train_inner | epoch 009:   1102 / 1475 loss=3.38, nll_loss=1.861, ppl=3.63, wps=59719, ups=15.54, wpb=3843.4, bsz=196.8, num_updates=12900, lr=0.000278423, gnorm=0.93, loss_scale=32, train_wall=6, gb_free=29.6, wall=913
2024-01-30 15:48:40 | INFO | train_inner | epoch 009:   1202 / 1475 loss=3.374, nll_loss=1.856, ppl=3.62, wps=58654.8, ups=15.5, wpb=3783.5, bsz=206.2, num_updates=13000, lr=0.00027735, gnorm=0.969, loss_scale=32, train_wall=6, gb_free=29.7, wall=920
2024-01-30 15:48:47 | INFO | train_inner | epoch 009:   1302 / 1475 loss=3.404, nll_loss=1.89, ppl=3.71, wps=57830.3, ups=15.57, wpb=3715.2, bsz=200.8, num_updates=13100, lr=0.000276289, gnorm=0.981, loss_scale=32, train_wall=6, gb_free=29.7, wall=926
2024-01-30 15:48:53 | INFO | train_inner | epoch 009:   1402 / 1475 loss=3.391, nll_loss=1.876, ppl=3.67, wps=59771.8, ups=15.48, wpb=3860.7, bsz=194.6, num_updates=13200, lr=0.000275241, gnorm=0.931, loss_scale=32, train_wall=6, gb_free=29.7, wall=932
2024-01-30 15:48:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:48:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:49:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.635 | nll_loss 2.002 | ppl 4 | wps 106907 | wpb 3189.1 | bsz 163.4 | num_updates 13273 | best_loss 3.635
2024-01-30 15:49:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13273 updates
2024-01-30 15:49:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint9.pt
2024-01-30 15:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint9.pt
2024-01-30 15:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint9.pt (epoch 9 @ 13273 updates, score 3.635) (writing took 5.519051674986258 seconds)
2024-01-30 15:49:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-01-30 15:49:07 | INFO | train | epoch 009 | loss 3.398 | nll_loss 1.882 | ppl 3.69 | wps 53051.4 | ups 14.07 | wpb 3771.6 | bsz 195.9 | num_updates 13273 | lr 0.000274483 | gnorm 0.962 | loss_scale 32 | train_wall 93 | gb_free 29.7 | wall 946
2024-01-30 15:49:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:49:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:49:07 | INFO | fairseq.trainer | begin training epoch 10
2024-01-30 15:49:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:49:09 | INFO | train_inner | epoch 010:     27 / 1475 loss=3.351, nll_loss=1.83, ppl=3.56, wps=24577.8, ups=6.49, wpb=3784.3, bsz=196.5, num_updates=13300, lr=0.000274204, gnorm=0.95, loss_scale=32, train_wall=6, gb_free=29.7, wall=948
2024-01-30 15:49:16 | INFO | train_inner | epoch 010:    127 / 1475 loss=3.291, nll_loss=1.761, ppl=3.39, wps=52953.6, ups=14.24, wpb=3717.8, bsz=188.9, num_updates=13400, lr=0.000273179, gnorm=0.964, loss_scale=32, train_wall=7, gb_free=29.7, wall=955
2024-01-30 15:49:23 | INFO | train_inner | epoch 010:    227 / 1475 loss=3.321, nll_loss=1.794, ppl=3.47, wps=52071.7, ups=13.9, wpb=3745.2, bsz=190.7, num_updates=13500, lr=0.000272166, gnorm=0.959, loss_scale=32, train_wall=7, gb_free=29.7, wall=962
2024-01-30 15:49:30 | INFO | train_inner | epoch 010:    327 / 1475 loss=3.249, nll_loss=1.714, ppl=3.28, wps=52691.8, ups=13.91, wpb=3787.8, bsz=210.4, num_updates=13600, lr=0.000271163, gnorm=0.906, loss_scale=32, train_wall=7, gb_free=29.6, wall=969
2024-01-30 15:49:37 | INFO | train_inner | epoch 010:    427 / 1475 loss=3.317, nll_loss=1.79, ppl=3.46, wps=52224.1, ups=13.9, wpb=3757.8, bsz=184.9, num_updates=13700, lr=0.000270172, gnorm=0.96, loss_scale=32, train_wall=7, gb_free=29.8, wall=976
2024-01-30 15:49:44 | INFO | train_inner | epoch 010:    527 / 1475 loss=3.356, nll_loss=1.834, ppl=3.57, wps=52608.5, ups=14.04, wpb=3746.5, bsz=182.2, num_updates=13800, lr=0.000269191, gnorm=0.967, loss_scale=32, train_wall=7, gb_free=29.8, wall=984
2024-01-30 15:49:52 | INFO | train_inner | epoch 010:    627 / 1475 loss=3.308, nll_loss=1.78, ppl=3.43, wps=53102.2, ups=13.88, wpb=3827, bsz=195.7, num_updates=13900, lr=0.000268221, gnorm=0.933, loss_scale=32, train_wall=7, gb_free=29.7, wall=991
2024-01-30 15:49:59 | INFO | train_inner | epoch 010:    727 / 1475 loss=3.296, nll_loss=1.766, ppl=3.4, wps=53185, ups=14.07, wpb=3779.7, bsz=198.9, num_updates=14000, lr=0.000267261, gnorm=0.963, loss_scale=32, train_wall=7, gb_free=29.7, wall=998
2024-01-30 15:50:06 | INFO | train_inner | epoch 010:    827 / 1475 loss=3.323, nll_loss=1.798, ppl=3.48, wps=54383.3, ups=14.31, wpb=3801, bsz=199.8, num_updates=14100, lr=0.000266312, gnorm=0.96, loss_scale=32, train_wall=7, gb_free=29.9, wall=1005
2024-01-30 15:50:13 | INFO | train_inner | epoch 010:    927 / 1475 loss=3.313, nll_loss=1.786, ppl=3.45, wps=52701.7, ups=13.96, wpb=3776, bsz=196.8, num_updates=14200, lr=0.000265372, gnorm=0.943, loss_scale=32, train_wall=7, gb_free=29.8, wall=1012
2024-01-30 15:50:20 | INFO | train_inner | epoch 010:   1027 / 1475 loss=3.336, nll_loss=1.814, ppl=3.52, wps=51954.2, ups=14.1, wpb=3684.5, bsz=189.5, num_updates=14300, lr=0.000264443, gnorm=0.973, loss_scale=32, train_wall=7, gb_free=29.6, wall=1019
2024-01-30 15:50:27 | INFO | train_inner | epoch 010:   1127 / 1475 loss=3.288, nll_loss=1.758, ppl=3.38, wps=52679.9, ups=13.86, wpb=3800.7, bsz=211.7, num_updates=14400, lr=0.000263523, gnorm=0.935, loss_scale=32, train_wall=7, gb_free=29.7, wall=1026
2024-01-30 15:50:34 | INFO | train_inner | epoch 010:   1227 / 1475 loss=3.329, nll_loss=1.805, ppl=3.49, wps=55341.5, ups=14.52, wpb=3812, bsz=193.8, num_updates=14500, lr=0.000262613, gnorm=0.957, loss_scale=32, train_wall=7, gb_free=29.7, wall=1033
2024-01-30 15:50:41 | INFO | train_inner | epoch 010:   1327 / 1475 loss=3.29, nll_loss=1.762, ppl=3.39, wps=52694.8, ups=14.09, wpb=3738.6, bsz=197.9, num_updates=14600, lr=0.000261712, gnorm=0.95, loss_scale=32, train_wall=7, gb_free=29.7, wall=1040
2024-01-30 15:50:48 | INFO | train_inner | epoch 010:   1427 / 1475 loss=3.288, nll_loss=1.76, ppl=3.39, wps=53625.2, ups=14.1, wpb=3802.5, bsz=206.2, num_updates=14700, lr=0.00026082, gnorm=0.93, loss_scale=32, train_wall=7, gb_free=29.7, wall=1047
2024-01-30 15:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:50:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:50:54 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.574 | nll_loss 1.929 | ppl 3.81 | wps 119737 | wpb 3189.1 | bsz 163.4 | num_updates 14748 | best_loss 3.574
2024-01-30 15:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14748 updates
2024-01-30 15:50:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint10.pt
2024-01-30 15:50:55 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint10.pt
2024-01-30 15:51:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint10.pt (epoch 10 @ 14748 updates, score 3.574) (writing took 5.471021319041029 seconds)
2024-01-30 15:51:00 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-01-30 15:51:00 | INFO | train | epoch 010 | loss 3.308 | nll_loss 1.781 | ppl 3.44 | wps 49166.5 | ups 13.04 | wpb 3771.6 | bsz 195.9 | num_updates 14748 | lr 0.000260395 | gnorm 0.95 | loss_scale 32 | train_wall 101 | gb_free 29.7 | wall 1059
2024-01-30 15:51:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:51:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:51:00 | INFO | fairseq.trainer | begin training epoch 11
2024-01-30 15:51:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:51:04 | INFO | train_inner | epoch 011:     52 / 1475 loss=3.279, nll_loss=1.75, ppl=3.36, wps=24252.2, ups=6.49, wpb=3735.5, bsz=190.6, num_updates=14800, lr=0.000259938, gnorm=0.949, loss_scale=32, train_wall=7, gb_free=29.7, wall=1063
2024-01-30 15:51:11 | INFO | train_inner | epoch 011:    152 / 1475 loss=3.22, nll_loss=1.682, ppl=3.21, wps=52546, ups=13.9, wpb=3779.7, bsz=189.3, num_updates=14900, lr=0.000259064, gnorm=0.913, loss_scale=32, train_wall=7, gb_free=29.7, wall=1070
2024-01-30 15:51:18 | INFO | train_inner | epoch 011:    252 / 1475 loss=3.274, nll_loss=1.743, ppl=3.35, wps=52583.4, ups=14.19, wpb=3706.3, bsz=188.5, num_updates=15000, lr=0.000258199, gnorm=0.953, loss_scale=32, train_wall=7, gb_free=29.7, wall=1077
2024-01-30 15:51:25 | INFO | train_inner | epoch 011:    352 / 1475 loss=3.197, nll_loss=1.655, ppl=3.15, wps=55528.3, ups=14.42, wpb=3851.5, bsz=204.5, num_updates=15100, lr=0.000257343, gnorm=0.88, loss_scale=32, train_wall=7, gb_free=29.7, wall=1084
2024-01-30 15:51:32 | INFO | train_inner | epoch 011:    452 / 1475 loss=3.246, nll_loss=1.711, ppl=3.27, wps=52122.7, ups=13.87, wpb=3759.3, bsz=188.3, num_updates=15200, lr=0.000256495, gnorm=0.929, loss_scale=32, train_wall=7, gb_free=29.7, wall=1091
2024-01-30 15:51:39 | INFO | train_inner | epoch 011:    552 / 1475 loss=3.203, nll_loss=1.663, ppl=3.17, wps=52622.6, ups=13.8, wpb=3813.6, bsz=200.4, num_updates=15300, lr=0.000255655, gnorm=0.92, loss_scale=32, train_wall=7, gb_free=29.7, wall=1098
2024-01-30 15:51:47 | INFO | train_inner | epoch 011:    652 / 1475 loss=3.264, nll_loss=1.732, ppl=3.32, wps=51923.4, ups=13.74, wpb=3780.3, bsz=189, num_updates=15400, lr=0.000254824, gnorm=0.946, loss_scale=32, train_wall=7, gb_free=29.7, wall=1106
2024-01-30 15:51:54 | INFO | train_inner | epoch 011:    752 / 1475 loss=3.228, nll_loss=1.69, ppl=3.23, wps=51137.8, ups=13.62, wpb=3754.3, bsz=197.5, num_updates=15500, lr=0.000254, gnorm=0.961, loss_scale=32, train_wall=7, gb_free=29.9, wall=1113
2024-01-30 15:52:01 | INFO | train_inner | epoch 011:    852 / 1475 loss=3.238, nll_loss=1.703, ppl=3.26, wps=51685.8, ups=13.45, wpb=3843.9, bsz=202.2, num_updates=15600, lr=0.000253185, gnorm=0.928, loss_scale=32, train_wall=7, gb_free=29.8, wall=1121
2024-01-30 15:52:09 | INFO | train_inner | epoch 011:    952 / 1475 loss=3.186, nll_loss=1.645, ppl=3.13, wps=51835.7, ups=13.76, wpb=3768.3, bsz=209.7, num_updates=15700, lr=0.000252377, gnorm=0.923, loss_scale=32, train_wall=7, gb_free=29.8, wall=1128
2024-01-30 15:52:15 | INFO | train_inner | epoch 011:   1052 / 1475 loss=3.259, nll_loss=1.727, ppl=3.31, wps=59486.3, ups=15.59, wpb=3816.2, bsz=194.2, num_updates=15800, lr=0.000251577, gnorm=0.933, loss_scale=32, train_wall=6, gb_free=29.7, wall=1134
2024-01-30 15:52:22 | INFO | train_inner | epoch 011:   1152 / 1475 loss=3.242, nll_loss=1.709, ppl=3.27, wps=57176.6, ups=15.52, wpb=3685, bsz=199.8, num_updates=15900, lr=0.000250785, gnorm=0.964, loss_scale=32, train_wall=6, gb_free=29.6, wall=1141
2024-01-30 15:52:28 | INFO | train_inner | epoch 011:   1252 / 1475 loss=3.244, nll_loss=1.711, ppl=3.27, wps=58044.7, ups=15.4, wpb=3769.1, bsz=199.5, num_updates=16000, lr=0.00025, gnorm=0.937, loss_scale=32, train_wall=6, gb_free=30, wall=1147
2024-01-30 15:52:35 | INFO | train_inner | epoch 011:   1352 / 1475 loss=3.234, nll_loss=1.701, ppl=3.25, wps=57222.9, ups=15.53, wpb=3684, bsz=190.6, num_updates=16100, lr=0.000249222, gnorm=0.948, loss_scale=32, train_wall=6, gb_free=29.7, wall=1154
2024-01-30 15:52:41 | INFO | train_inner | epoch 011:   1452 / 1475 loss=3.235, nll_loss=1.701, ppl=3.25, wps=59332.4, ups=15.52, wpb=3822.5, bsz=189.1, num_updates=16200, lr=0.000248452, gnorm=0.916, loss_scale=32, train_wall=6, gb_free=29.7, wall=1160
2024-01-30 15:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:52:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:52:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.554 | nll_loss 1.908 | ppl 3.75 | wps 134179 | wpb 3189.1 | bsz 163.4 | num_updates 16223 | best_loss 3.554
2024-01-30 15:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16223 updates
2024-01-30 15:52:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint11.pt
2024-01-30 15:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint11.pt
2024-01-30 15:52:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint11.pt (epoch 11 @ 16223 updates, score 3.554) (writing took 5.029944039997645 seconds)
2024-01-30 15:52:50 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-01-30 15:52:50 | INFO | train | epoch 011 | loss 3.233 | nll_loss 1.697 | ppl 3.24 | wps 50585.2 | ups 13.41 | wpb 3771.6 | bsz 195.9 | num_updates 16223 | lr 0.000248276 | gnorm 0.932 | loss_scale 32 | train_wall 99 | gb_free 29.8 | wall 1169
2024-01-30 15:52:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:52:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:52:50 | INFO | fairseq.trainer | begin training epoch 12
2024-01-30 15:52:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:52:55 | INFO | train_inner | epoch 012:     77 / 1475 loss=3.197, nll_loss=1.657, ppl=3.15, wps=26747.2, ups=7.14, wpb=3747, bsz=183.9, num_updates=16300, lr=0.000247689, gnorm=0.924, loss_scale=32, train_wall=6, gb_free=29.7, wall=1174
2024-01-30 15:53:01 | INFO | train_inner | epoch 012:    177 / 1475 loss=3.171, nll_loss=1.626, ppl=3.09, wps=59878.9, ups=15.6, wpb=3838.8, bsz=188.3, num_updates=16400, lr=0.000246932, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.7, wall=1180
2024-01-30 15:53:08 | INFO | train_inner | epoch 012:    277 / 1475 loss=3.105, nll_loss=1.553, ppl=2.93, wps=56187.8, ups=15.52, wpb=3619.4, bsz=209, num_updates=16500, lr=0.000246183, gnorm=0.947, loss_scale=32, train_wall=6, gb_free=29.7, wall=1187
2024-01-30 15:53:14 | INFO | train_inner | epoch 012:    377 / 1475 loss=3.146, nll_loss=1.599, ppl=3.03, wps=59314.3, ups=15.46, wpb=3837.7, bsz=209, num_updates=16600, lr=0.00024544, gnorm=0.898, loss_scale=32, train_wall=6, gb_free=29.7, wall=1193
2024-01-30 15:53:21 | INFO | train_inner | epoch 012:    477 / 1475 loss=3.145, nll_loss=1.598, ppl=3.03, wps=58205.1, ups=15.48, wpb=3760.3, bsz=201.4, num_updates=16700, lr=0.000244704, gnorm=0.926, loss_scale=64, train_wall=6, gb_free=29.7, wall=1200
2024-01-30 15:53:27 | INFO | train_inner | epoch 012:    577 / 1475 loss=3.146, nll_loss=1.6, ppl=3.03, wps=57821.1, ups=15.47, wpb=3738, bsz=200.6, num_updates=16800, lr=0.000243975, gnorm=0.943, loss_scale=64, train_wall=6, gb_free=29.6, wall=1206
2024-01-30 15:53:34 | INFO | train_inner | epoch 012:    677 / 1475 loss=3.165, nll_loss=1.621, ppl=3.08, wps=57927.8, ups=15.53, wpb=3729, bsz=197, num_updates=16900, lr=0.000243252, gnorm=0.946, loss_scale=64, train_wall=6, gb_free=29.7, wall=1213
2024-01-30 15:53:40 | INFO | train_inner | epoch 012:    777 / 1475 loss=3.179, nll_loss=1.637, ppl=3.11, wps=59521.8, ups=15.56, wpb=3824.9, bsz=191.3, num_updates=17000, lr=0.000242536, gnorm=0.914, loss_scale=64, train_wall=6, gb_free=29.8, wall=1219
2024-01-30 15:53:47 | INFO | train_inner | epoch 012:    877 / 1475 loss=3.175, nll_loss=1.632, ppl=3.1, wps=59250.8, ups=15.54, wpb=3812.3, bsz=190.5, num_updates=17100, lr=0.000241825, gnorm=0.925, loss_scale=64, train_wall=6, gb_free=29.8, wall=1226
2024-01-30 15:53:53 | INFO | train_inner | epoch 012:    977 / 1475 loss=3.17, nll_loss=1.627, ppl=3.09, wps=57329.9, ups=15.47, wpb=3706.1, bsz=202.6, num_updates=17200, lr=0.000241121, gnorm=0.951, loss_scale=64, train_wall=6, gb_free=29.7, wall=1232
2024-01-30 15:53:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-01-30 15:53:59 | INFO | train_inner | epoch 012:   1078 / 1475 loss=3.192, nll_loss=1.652, ppl=3.14, wps=59432.6, ups=15.39, wpb=3862.9, bsz=187.8, num_updates=17300, lr=0.000240424, gnorm=0.914, loss_scale=32, train_wall=6, gb_free=29.7, wall=1239
2024-01-30 15:54:06 | INFO | train_inner | epoch 012:   1178 / 1475 loss=3.172, nll_loss=1.63, ppl=3.1, wps=58397.8, ups=15.5, wpb=3766.4, bsz=202.2, num_updates=17400, lr=0.000239732, gnorm=0.926, loss_scale=32, train_wall=6, gb_free=29.7, wall=1245
2024-01-30 15:54:12 | INFO | train_inner | epoch 012:   1278 / 1475 loss=3.216, nll_loss=1.681, ppl=3.21, wps=59455.4, ups=15.55, wpb=3822.6, bsz=189.6, num_updates=17500, lr=0.000239046, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=1251
2024-01-30 15:54:19 | INFO | train_inner | epoch 012:   1378 / 1475 loss=3.182, nll_loss=1.642, ppl=3.12, wps=58264.2, ups=15.59, wpb=3736.4, bsz=194.6, num_updates=17600, lr=0.000238366, gnorm=0.935, loss_scale=32, train_wall=6, gb_free=29.7, wall=1258
2024-01-30 15:54:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:54:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:54:27 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.528 | nll_loss 1.873 | ppl 3.66 | wps 134198 | wpb 3189.1 | bsz 163.4 | num_updates 17697 | best_loss 3.528
2024-01-30 15:54:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17697 updates
2024-01-30 15:54:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint12.pt
2024-01-30 15:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint12.pt
2024-01-30 15:54:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint12.pt (epoch 12 @ 17697 updates, score 3.528) (writing took 5.078331483993679 seconds)
2024-01-30 15:54:32 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-01-30 15:54:32 | INFO | train | epoch 012 | loss 3.17 | nll_loss 1.627 | ppl 3.09 | wps 54165.2 | ups 14.36 | wpb 3771.4 | bsz 195.9 | num_updates 17697 | lr 0.000237711 | gnorm 0.928 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 1272
2024-01-30 15:54:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:54:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:54:33 | INFO | fairseq.trainer | begin training epoch 13
2024-01-30 15:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:54:33 | INFO | train_inner | epoch 013:      3 / 1475 loss=3.204, nll_loss=1.667, ppl=3.18, wps=26823.2, ups=7.14, wpb=3759, bsz=188, num_updates=17700, lr=0.000237691, gnorm=0.945, loss_scale=32, train_wall=6, gb_free=29.7, wall=1272
2024-01-30 15:54:39 | INFO | train_inner | epoch 013:    103 / 1475 loss=3.126, nll_loss=1.577, ppl=2.98, wps=58809.6, ups=15.53, wpb=3787.2, bsz=177.4, num_updates=17800, lr=0.000237023, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.7, wall=1278
2024-01-30 15:54:46 | INFO | train_inner | epoch 013:    203 / 1475 loss=3.085, nll_loss=1.53, ppl=2.89, wps=57944, ups=15.56, wpb=3723.4, bsz=193.2, num_updates=17900, lr=0.00023636, gnorm=0.935, loss_scale=32, train_wall=6, gb_free=29.7, wall=1285
2024-01-30 15:54:52 | INFO | train_inner | epoch 013:    303 / 1475 loss=3.081, nll_loss=1.526, ppl=2.88, wps=59116.5, ups=15.55, wpb=3801.7, bsz=195.8, num_updates=18000, lr=0.000235702, gnorm=0.884, loss_scale=32, train_wall=6, gb_free=29.8, wall=1291
2024-01-30 15:54:59 | INFO | train_inner | epoch 013:    403 / 1475 loss=3.109, nll_loss=1.558, ppl=2.94, wps=59574.6, ups=15.54, wpb=3832.8, bsz=195, num_updates=18100, lr=0.00023505, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.7, wall=1298
2024-01-30 15:55:05 | INFO | train_inner | epoch 013:    503 / 1475 loss=3.135, nll_loss=1.587, ppl=3, wps=58394.3, ups=15.6, wpb=3744, bsz=194.1, num_updates=18200, lr=0.000234404, gnorm=0.943, loss_scale=32, train_wall=6, gb_free=29.8, wall=1304
2024-01-30 15:55:11 | INFO | train_inner | epoch 013:    603 / 1475 loss=3.121, nll_loss=1.572, ppl=2.97, wps=58323, ups=15.5, wpb=3762.4, bsz=195.8, num_updates=18300, lr=0.000233762, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.7, wall=1310
2024-01-30 15:55:18 | INFO | train_inner | epoch 013:    703 / 1475 loss=3.12, nll_loss=1.572, ppl=2.97, wps=57922, ups=15.51, wpb=3735.7, bsz=198.2, num_updates=18400, lr=0.000233126, gnorm=0.93, loss_scale=32, train_wall=6, gb_free=29.7, wall=1317
2024-01-30 15:55:24 | INFO | train_inner | epoch 013:    803 / 1475 loss=3.091, nll_loss=1.537, ppl=2.9, wps=58181.9, ups=15.53, wpb=3745.4, bsz=203.3, num_updates=18500, lr=0.000232495, gnorm=0.916, loss_scale=32, train_wall=6, gb_free=29.7, wall=1323
2024-01-30 15:55:31 | INFO | train_inner | epoch 013:    903 / 1475 loss=3.113, nll_loss=1.563, ppl=2.95, wps=58261.5, ups=15.6, wpb=3735.3, bsz=195.7, num_updates=18600, lr=0.000231869, gnorm=0.944, loss_scale=32, train_wall=6, gb_free=29.8, wall=1330
2024-01-30 15:55:37 | INFO | train_inner | epoch 013:   1003 / 1475 loss=3.138, nll_loss=1.592, ppl=3.01, wps=58898.2, ups=15.67, wpb=3757.9, bsz=190.6, num_updates=18700, lr=0.000231249, gnorm=0.942, loss_scale=32, train_wall=6, gb_free=29.7, wall=1336
2024-01-30 15:55:43 | INFO | train_inner | epoch 013:   1103 / 1475 loss=3.145, nll_loss=1.599, ppl=3.03, wps=60161.9, ups=15.63, wpb=3849.3, bsz=193.7, num_updates=18800, lr=0.000230633, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.8, wall=1343
2024-01-30 15:55:50 | INFO | train_inner | epoch 013:   1203 / 1475 loss=3.119, nll_loss=1.572, ppl=2.97, wps=57955.7, ups=15.59, wpb=3717.6, bsz=195.1, num_updates=18900, lr=0.000230022, gnorm=0.914, loss_scale=32, train_wall=6, gb_free=29.7, wall=1349
2024-01-30 15:55:56 | INFO | train_inner | epoch 013:   1303 / 1475 loss=3.101, nll_loss=1.551, ppl=2.93, wps=58024.8, ups=15.46, wpb=3753, bsz=201.8, num_updates=19000, lr=0.000229416, gnorm=0.927, loss_scale=32, train_wall=6, gb_free=29.7, wall=1355
2024-01-30 15:56:03 | INFO | train_inner | epoch 013:   1403 / 1475 loss=3.121, nll_loss=1.574, ppl=2.98, wps=59152.2, ups=15.49, wpb=3818.4, bsz=207.9, num_updates=19100, lr=0.000228814, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.6, wall=1362
2024-01-30 15:56:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:56:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:56:10 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.497 | nll_loss 1.85 | ppl 3.6 | wps 134097 | wpb 3189.1 | bsz 163.4 | num_updates 19172 | best_loss 3.497
2024-01-30 15:56:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19172 updates
2024-01-30 15:56:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint13.pt
2024-01-30 15:56:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint13.pt
2024-01-30 15:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint13.pt (epoch 13 @ 19172 updates, score 3.497) (writing took 5.052704864065163 seconds)
2024-01-30 15:56:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-01-30 15:56:15 | INFO | train | epoch 013 | loss 3.115 | nll_loss 1.565 | ppl 2.96 | wps 54326.9 | ups 14.4 | wpb 3771.6 | bsz 195.9 | num_updates 19172 | lr 0.000228384 | gnorm 0.919 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 1374
2024-01-30 15:56:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:56:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:56:15 | INFO | fairseq.trainer | begin training epoch 14
2024-01-30 15:56:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:56:17 | INFO | train_inner | epoch 014:     28 / 1475 loss=3.112, nll_loss=1.562, ppl=2.95, wps=26680, ups=7.13, wpb=3740.4, bsz=192.4, num_updates=19200, lr=0.000228218, gnorm=0.929, loss_scale=32, train_wall=6, gb_free=29.9, wall=1376
2024-01-30 15:56:23 | INFO | train_inner | epoch 014:    128 / 1475 loss=2.994, nll_loss=1.427, ppl=2.69, wps=57972, ups=15.49, wpb=3742.2, bsz=207.9, num_updates=19300, lr=0.000227626, gnorm=0.882, loss_scale=32, train_wall=6, gb_free=29.7, wall=1382
2024-01-30 15:56:30 | INFO | train_inner | epoch 014:    228 / 1475 loss=3.054, nll_loss=1.495, ppl=2.82, wps=58180, ups=15.72, wpb=3700.5, bsz=186.5, num_updates=19400, lr=0.000227038, gnorm=0.926, loss_scale=32, train_wall=6, gb_free=29.7, wall=1389
2024-01-30 15:56:36 | INFO | train_inner | epoch 014:    328 / 1475 loss=3.081, nll_loss=1.526, ppl=2.88, wps=58474.3, ups=15.58, wpb=3753.7, bsz=186.8, num_updates=19500, lr=0.000226455, gnorm=0.925, loss_scale=32, train_wall=6, gb_free=29.7, wall=1395
2024-01-30 15:56:43 | INFO | train_inner | epoch 014:    428 / 1475 loss=3.026, nll_loss=1.465, ppl=2.76, wps=58898.7, ups=15.43, wpb=3816, bsz=199.2, num_updates=19600, lr=0.000225877, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.8, wall=1402
2024-01-30 15:56:49 | INFO | train_inner | epoch 014:    528 / 1475 loss=2.997, nll_loss=1.433, ppl=2.7, wps=58288.5, ups=15.55, wpb=3749.2, bsz=215.2, num_updates=19700, lr=0.000225303, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.8, wall=1408
2024-01-30 15:56:55 | INFO | train_inner | epoch 014:    628 / 1475 loss=3.088, nll_loss=1.536, ppl=2.9, wps=58219.5, ups=15.5, wpb=3756.2, bsz=186.8, num_updates=19800, lr=0.000224733, gnorm=0.938, loss_scale=32, train_wall=6, gb_free=29.7, wall=1415
2024-01-30 15:57:02 | INFO | train_inner | epoch 014:    728 / 1475 loss=3.105, nll_loss=1.554, ppl=2.94, wps=59354.6, ups=15.67, wpb=3788.2, bsz=191.7, num_updates=19900, lr=0.000224168, gnorm=0.915, loss_scale=32, train_wall=6, gb_free=29.8, wall=1421
2024-01-30 15:57:08 | INFO | train_inner | epoch 014:    828 / 1475 loss=3.065, nll_loss=1.51, ppl=2.85, wps=58932.4, ups=15.59, wpb=3781, bsz=200.5, num_updates=20000, lr=0.000223607, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.8, wall=1427
2024-01-30 15:57:15 | INFO | train_inner | epoch 014:    928 / 1475 loss=3.106, nll_loss=1.555, ppl=2.94, wps=58132.3, ups=15.54, wpb=3741.7, bsz=191.4, num_updates=20100, lr=0.00022305, gnorm=0.944, loss_scale=32, train_wall=6, gb_free=29.8, wall=1434
2024-01-30 15:57:21 | INFO | train_inner | epoch 014:   1028 / 1475 loss=3.093, nll_loss=1.542, ppl=2.91, wps=59945.5, ups=15.55, wpb=3854.4, bsz=192.8, num_updates=20200, lr=0.000222497, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.7, wall=1440
2024-01-30 15:57:28 | INFO | train_inner | epoch 014:   1128 / 1475 loss=3.085, nll_loss=1.533, ppl=2.89, wps=58296.3, ups=15.54, wpb=3752.3, bsz=188.4, num_updates=20300, lr=0.000221948, gnorm=0.933, loss_scale=32, train_wall=6, gb_free=29.7, wall=1447
2024-01-30 15:57:34 | INFO | train_inner | epoch 014:   1228 / 1475 loss=3.045, nll_loss=1.488, ppl=2.8, wps=58433.3, ups=15.54, wpb=3760.3, bsz=209.2, num_updates=20400, lr=0.000221404, gnorm=0.921, loss_scale=32, train_wall=6, gb_free=29.7, wall=1453
2024-01-30 15:57:40 | INFO | train_inner | epoch 014:   1328 / 1475 loss=3.124, nll_loss=1.576, ppl=2.98, wps=60356.9, ups=15.66, wpb=3854.2, bsz=184.1, num_updates=20500, lr=0.000220863, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.8, wall=1459
2024-01-30 15:57:47 | INFO | train_inner | epoch 014:   1428 / 1475 loss=3.068, nll_loss=1.515, ppl=2.86, wps=59205.2, ups=15.51, wpb=3817.7, bsz=204.2, num_updates=20600, lr=0.000220326, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.7, wall=1466
2024-01-30 15:57:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:57:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:57:52 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.487 | nll_loss 1.837 | ppl 3.57 | wps 132987 | wpb 3189.1 | bsz 163.4 | num_updates 20647 | best_loss 3.487
2024-01-30 15:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20647 updates
2024-01-30 15:57:52 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint14.pt
2024-01-30 15:57:53 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint14.pt
2024-01-30 15:57:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint14.pt (epoch 14 @ 20647 updates, score 3.487) (writing took 5.037034815992229 seconds)
2024-01-30 15:57:57 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-01-30 15:57:57 | INFO | train | epoch 014 | loss 3.066 | nll_loss 1.511 | ppl 2.85 | wps 54336 | ups 14.41 | wpb 3771.6 | bsz 195.9 | num_updates 20647 | lr 0.000220075 | gnorm 0.914 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 1476
2024-01-30 15:57:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:57:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:57:57 | INFO | fairseq.trainer | begin training epoch 15
2024-01-30 15:57:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:58:01 | INFO | train_inner | epoch 015:     53 / 1475 loss=3.012, nll_loss=1.45, ppl=2.73, wps=26880.6, ups=7.12, wpb=3776.6, bsz=203.6, num_updates=20700, lr=0.000219793, gnorm=0.892, loss_scale=32, train_wall=6, gb_free=29.7, wall=1480
2024-01-30 15:58:07 | INFO | train_inner | epoch 015:    153 / 1475 loss=2.982, nll_loss=1.415, ppl=2.67, wps=59267.4, ups=15.46, wpb=3833.8, bsz=201.4, num_updates=20800, lr=0.000219265, gnorm=0.865, loss_scale=32, train_wall=6, gb_free=29.7, wall=1486
2024-01-30 15:58:14 | INFO | train_inner | epoch 015:    253 / 1475 loss=3.014, nll_loss=1.452, ppl=2.74, wps=58456.5, ups=15.45, wpb=3783.9, bsz=191, num_updates=20900, lr=0.000218739, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.8, wall=1493
2024-01-30 15:58:20 | INFO | train_inner | epoch 015:    353 / 1475 loss=2.996, nll_loss=1.432, ppl=2.7, wps=57856.9, ups=15.55, wpb=3720.7, bsz=198.5, num_updates=21000, lr=0.000218218, gnorm=0.911, loss_scale=32, train_wall=6, gb_free=29.8, wall=1499
2024-01-30 15:58:27 | INFO | train_inner | epoch 015:    453 / 1475 loss=3.019, nll_loss=1.457, ppl=2.75, wps=57751.2, ups=15.6, wpb=3702.8, bsz=195, num_updates=21100, lr=0.0002177, gnorm=0.934, loss_scale=32, train_wall=6, gb_free=29.7, wall=1506
2024-01-30 15:58:33 | INFO | train_inner | epoch 015:    553 / 1475 loss=3.033, nll_loss=1.473, ppl=2.78, wps=57622.6, ups=15.57, wpb=3701.7, bsz=190.2, num_updates=21200, lr=0.000217186, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=1512
2024-01-30 15:58:39 | INFO | train_inner | epoch 015:    653 / 1475 loss=3.027, nll_loss=1.467, ppl=2.76, wps=58776.4, ups=15.66, wpb=3753.9, bsz=191.2, num_updates=21300, lr=0.000216676, gnorm=0.924, loss_scale=32, train_wall=6, gb_free=29.7, wall=1519
2024-01-30 15:58:46 | INFO | train_inner | epoch 015:    753 / 1475 loss=3.04, nll_loss=1.482, ppl=2.79, wps=59585.4, ups=15.52, wpb=3839.4, bsz=198.2, num_updates=21400, lr=0.000216169, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.7, wall=1525
2024-01-30 15:58:52 | INFO | train_inner | epoch 015:    853 / 1475 loss=3.008, nll_loss=1.446, ppl=2.72, wps=59001.3, ups=15.48, wpb=3810.3, bsz=208, num_updates=21500, lr=0.000215666, gnorm=0.879, loss_scale=32, train_wall=6, gb_free=29.8, wall=1531
2024-01-30 15:58:59 | INFO | train_inner | epoch 015:    953 / 1475 loss=3.03, nll_loss=1.471, ppl=2.77, wps=58937.9, ups=15.59, wpb=3781.2, bsz=194.6, num_updates=21600, lr=0.000215166, gnorm=0.909, loss_scale=32, train_wall=6, gb_free=29.7, wall=1538
2024-01-30 15:59:05 | INFO | train_inner | epoch 015:   1053 / 1475 loss=3.023, nll_loss=1.462, ppl=2.76, wps=59179.5, ups=15.62, wpb=3789.5, bsz=201.1, num_updates=21700, lr=0.000214669, gnorm=0.937, loss_scale=32, train_wall=6, gb_free=29.8, wall=1544
2024-01-30 15:59:12 | INFO | train_inner | epoch 015:   1153 / 1475 loss=3.049, nll_loss=1.492, ppl=2.81, wps=58771.6, ups=15.58, wpb=3773, bsz=191.4, num_updates=21800, lr=0.000214176, gnorm=0.923, loss_scale=32, train_wall=6, gb_free=29.7, wall=1551
2024-01-30 15:59:18 | INFO | train_inner | epoch 015:   1253 / 1475 loss=3.064, nll_loss=1.51, ppl=2.85, wps=58841.2, ups=15.6, wpb=3772.1, bsz=187.8, num_updates=21900, lr=0.000213687, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.7, wall=1557
2024-01-30 15:59:24 | INFO | train_inner | epoch 015:   1353 / 1475 loss=3.033, nll_loss=1.475, ppl=2.78, wps=58317.1, ups=15.48, wpb=3767.1, bsz=198.6, num_updates=22000, lr=0.000213201, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.7, wall=1564
2024-01-30 15:59:31 | INFO | train_inner | epoch 015:   1453 / 1475 loss=3.037, nll_loss=1.48, ppl=2.79, wps=58585.7, ups=15.61, wpb=3752.5, bsz=198.8, num_updates=22100, lr=0.000212718, gnorm=0.923, loss_scale=32, train_wall=6, gb_free=29.7, wall=1570
2024-01-30 15:59:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 15:59:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:59:34 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.474 | nll_loss 1.825 | ppl 3.54 | wps 134810 | wpb 3189.1 | bsz 163.4 | num_updates 22122 | best_loss 3.474
2024-01-30 15:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22122 updates
2024-01-30 15:59:34 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint15.pt
2024-01-30 15:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint15.pt
2024-01-30 15:59:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint15.pt (epoch 15 @ 22122 updates, score 3.474) (writing took 5.039002603967674 seconds)
2024-01-30 15:59:40 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-01-30 15:59:40 | INFO | train | epoch 015 | loss 3.024 | nll_loss 1.464 | ppl 2.76 | wps 54310.8 | ups 14.4 | wpb 3771.6 | bsz 195.9 | num_updates 22122 | lr 0.000212612 | gnorm 0.911 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 1579
2024-01-30 15:59:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 15:59:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 15:59:40 | INFO | fairseq.trainer | begin training epoch 16
2024-01-30 15:59:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 15:59:45 | INFO | train_inner | epoch 016:     78 / 1475 loss=2.985, nll_loss=1.418, ppl=2.67, wps=26980.2, ups=7.14, wpb=3777.9, bsz=189.7, num_updates=22200, lr=0.000212238, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.7, wall=1584
2024-01-30 15:59:51 | INFO | train_inner | epoch 016:    178 / 1475 loss=2.949, nll_loss=1.377, ppl=2.6, wps=58359.3, ups=15.59, wpb=3743.5, bsz=203, num_updates=22300, lr=0.000211762, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.6, wall=1590
2024-01-30 15:59:58 | INFO | train_inner | epoch 016:    278 / 1475 loss=2.97, nll_loss=1.402, ppl=2.64, wps=59050.7, ups=15.49, wpb=3811.7, bsz=199.1, num_updates=22400, lr=0.000211289, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.7, wall=1597
2024-01-30 16:00:04 | INFO | train_inner | epoch 016:    378 / 1475 loss=2.952, nll_loss=1.382, ppl=2.61, wps=58301.4, ups=15.54, wpb=3752.7, bsz=204.3, num_updates=22500, lr=0.000210819, gnorm=0.889, loss_scale=32, train_wall=6, gb_free=29.7, wall=1603
2024-01-30 16:00:11 | INFO | train_inner | epoch 016:    478 / 1475 loss=2.974, nll_loss=1.407, ppl=2.65, wps=58629.6, ups=15.57, wpb=3766.1, bsz=195.4, num_updates=22600, lr=0.000210352, gnorm=0.887, loss_scale=32, train_wall=6, gb_free=29.7, wall=1610
2024-01-30 16:00:17 | INFO | train_inner | epoch 016:    578 / 1475 loss=2.985, nll_loss=1.42, ppl=2.68, wps=58594.6, ups=15.51, wpb=3778.6, bsz=199.3, num_updates=22700, lr=0.000209888, gnorm=0.915, loss_scale=32, train_wall=6, gb_free=29.7, wall=1616
2024-01-30 16:00:23 | INFO | train_inner | epoch 016:    678 / 1475 loss=2.987, nll_loss=1.421, ppl=2.68, wps=58054.8, ups=15.55, wpb=3733.9, bsz=195.4, num_updates=22800, lr=0.000209427, gnorm=0.938, loss_scale=32, train_wall=6, gb_free=29.7, wall=1623
2024-01-30 16:00:30 | INFO | train_inner | epoch 016:    778 / 1475 loss=2.981, nll_loss=1.415, ppl=2.67, wps=59438.6, ups=15.53, wpb=3827.3, bsz=200, num_updates=22900, lr=0.000208969, gnorm=0.888, loss_scale=32, train_wall=6, gb_free=29.7, wall=1629
2024-01-30 16:00:36 | INFO | train_inner | epoch 016:    878 / 1475 loss=2.988, nll_loss=1.424, ppl=2.68, wps=59112.4, ups=15.46, wpb=3824.7, bsz=198, num_updates=23000, lr=0.000208514, gnorm=0.886, loss_scale=32, train_wall=6, gb_free=29.6, wall=1635
2024-01-30 16:00:43 | INFO | train_inner | epoch 016:    978 / 1475 loss=2.985, nll_loss=1.42, ppl=2.68, wps=58684.4, ups=15.52, wpb=3780.8, bsz=199.1, num_updates=23100, lr=0.000208063, gnorm=0.889, loss_scale=32, train_wall=6, gb_free=29.8, wall=1642
2024-01-30 16:00:49 | INFO | train_inner | epoch 016:   1078 / 1475 loss=3.008, nll_loss=1.447, ppl=2.73, wps=58902.8, ups=15.59, wpb=3778.1, bsz=189.8, num_updates=23200, lr=0.000207614, gnorm=0.93, loss_scale=32, train_wall=6, gb_free=29.7, wall=1648
2024-01-30 16:00:56 | INFO | train_inner | epoch 016:   1178 / 1475 loss=3.018, nll_loss=1.458, ppl=2.75, wps=59453.1, ups=15.53, wpb=3827.6, bsz=192, num_updates=23300, lr=0.000207168, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.6, wall=1655
2024-01-30 16:01:02 | INFO | train_inner | epoch 016:   1278 / 1475 loss=2.988, nll_loss=1.425, ppl=2.69, wps=57899.3, ups=15.54, wpb=3724.7, bsz=194.2, num_updates=23400, lr=0.000206725, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.7, wall=1661
2024-01-30 16:01:08 | INFO | train_inner | epoch 016:   1378 / 1475 loss=3.034, nll_loss=1.478, ppl=2.78, wps=58044.7, ups=15.64, wpb=3712.1, bsz=183, num_updates=23500, lr=0.000206284, gnorm=0.957, loss_scale=32, train_wall=6, gb_free=29.7, wall=1668
2024-01-30 16:01:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:01:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:01:17 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.461 | nll_loss 1.804 | ppl 3.49 | wps 134776 | wpb 3189.1 | bsz 163.4 | num_updates 23597 | best_loss 3.461
2024-01-30 16:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23597 updates
2024-01-30 16:01:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint16.pt
2024-01-30 16:01:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint16.pt
2024-01-30 16:01:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint16.pt (epoch 16 @ 23597 updates, score 3.461) (writing took 5.065340414061211 seconds)
2024-01-30 16:01:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-01-30 16:01:22 | INFO | train | epoch 016 | loss 2.986 | nll_loss 1.421 | ppl 2.68 | wps 54282.2 | ups 14.39 | wpb 3771.6 | bsz 195.9 | num_updates 23597 | lr 0.00020586 | gnorm 0.907 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 1681
2024-01-30 16:01:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:01:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:01:22 | INFO | fairseq.trainer | begin training epoch 17
2024-01-30 16:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:01:22 | INFO | train_inner | epoch 017:      3 / 1475 loss=3.003, nll_loss=1.442, ppl=2.72, wps=26828.4, ups=7.15, wpb=3752.7, bsz=193.8, num_updates=23600, lr=0.000205847, gnorm=0.925, loss_scale=32, train_wall=6, gb_free=29.7, wall=1682
2024-01-30 16:01:29 | INFO | train_inner | epoch 017:    103 / 1475 loss=2.906, nll_loss=1.33, ppl=2.51, wps=57921.8, ups=15.4, wpb=3761.2, bsz=201.3, num_updates=23700, lr=0.000205412, gnorm=0.872, loss_scale=32, train_wall=6, gb_free=29.7, wall=1688
2024-01-30 16:01:35 | INFO | train_inner | epoch 017:    203 / 1475 loss=2.925, nll_loss=1.351, ppl=2.55, wps=59342.1, ups=15.57, wpb=3812, bsz=192.6, num_updates=23800, lr=0.00020498, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=1695
2024-01-30 16:01:42 | INFO | train_inner | epoch 017:    303 / 1475 loss=2.949, nll_loss=1.378, ppl=2.6, wps=59604.7, ups=15.63, wpb=3812.9, bsz=186.7, num_updates=23900, lr=0.000204551, gnorm=0.888, loss_scale=32, train_wall=6, gb_free=29.7, wall=1701
2024-01-30 16:01:48 | INFO | train_inner | epoch 017:    403 / 1475 loss=2.925, nll_loss=1.352, ppl=2.55, wps=59147.1, ups=15.56, wpb=3801.4, bsz=198.9, num_updates=24000, lr=0.000204124, gnorm=0.875, loss_scale=32, train_wall=6, gb_free=29.7, wall=1707
2024-01-30 16:01:55 | INFO | train_inner | epoch 017:    503 / 1475 loss=2.924, nll_loss=1.351, ppl=2.55, wps=58769.1, ups=15.43, wpb=3809.1, bsz=209.6, num_updates=24100, lr=0.0002037, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.7, wall=1714
2024-01-30 16:02:01 | INFO | train_inner | epoch 017:    603 / 1475 loss=2.955, nll_loss=1.387, ppl=2.61, wps=58241, ups=15.53, wpb=3751, bsz=187.8, num_updates=24200, lr=0.000203279, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.7, wall=1720
2024-01-30 16:02:08 | INFO | train_inner | epoch 017:    703 / 1475 loss=2.93, nll_loss=1.359, ppl=2.57, wps=58119.3, ups=15.46, wpb=3758.5, bsz=206.5, num_updates=24300, lr=0.00020286, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.7, wall=1727
2024-01-30 16:02:14 | INFO | train_inner | epoch 017:    803 / 1475 loss=2.955, nll_loss=1.386, ppl=2.61, wps=58646.6, ups=15.55, wpb=3770.4, bsz=198, num_updates=24400, lr=0.000202444, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.9, wall=1733
2024-01-30 16:02:20 | INFO | train_inner | epoch 017:    903 / 1475 loss=2.953, nll_loss=1.385, ppl=2.61, wps=58249.5, ups=15.49, wpb=3760.4, bsz=197, num_updates=24500, lr=0.000202031, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.8, wall=1740
2024-01-30 16:02:27 | INFO | train_inner | epoch 017:   1003 / 1475 loss=2.977, nll_loss=1.413, ppl=2.66, wps=58525.8, ups=15.59, wpb=3754.7, bsz=194.6, num_updates=24600, lr=0.000201619, gnorm=0.938, loss_scale=32, train_wall=6, gb_free=29.8, wall=1746
2024-01-30 16:02:33 | INFO | train_inner | epoch 017:   1103 / 1475 loss=2.941, nll_loss=1.371, ppl=2.59, wps=56657.1, ups=15.53, wpb=3647.4, bsz=200.3, num_updates=24700, lr=0.000201211, gnorm=0.954, loss_scale=32, train_wall=6, gb_free=29.7, wall=1752
2024-01-30 16:02:40 | INFO | train_inner | epoch 017:   1203 / 1475 loss=2.978, nll_loss=1.414, ppl=2.66, wps=59182, ups=15.53, wpb=3809.8, bsz=193.5, num_updates=24800, lr=0.000200805, gnorm=0.926, loss_scale=32, train_wall=6, gb_free=29.7, wall=1759
2024-01-30 16:02:46 | INFO | train_inner | epoch 017:   1303 / 1475 loss=2.966, nll_loss=1.4, ppl=2.64, wps=58936.8, ups=15.57, wpb=3784.8, bsz=194.6, num_updates=24900, lr=0.000200401, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.7, wall=1765
2024-01-30 16:02:53 | INFO | train_inner | epoch 017:   1403 / 1475 loss=2.984, nll_loss=1.42, ppl=2.68, wps=58731.1, ups=15.63, wpb=3758.5, bsz=192.6, num_updates=25000, lr=0.0002, gnorm=0.941, loss_scale=32, train_wall=6, gb_free=29.7, wall=1772
2024-01-30 16:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:02:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:02:59 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.454 | nll_loss 1.799 | ppl 3.48 | wps 134893 | wpb 3189.1 | bsz 163.4 | num_updates 25072 | best_loss 3.454
2024-01-30 16:02:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25072 updates
2024-01-30 16:02:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint17.pt
2024-01-30 16:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint17.pt
2024-01-30 16:03:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint17.pt (epoch 17 @ 25072 updates, score 3.454) (writing took 5.079196685925126 seconds)
2024-01-30 16:03:05 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-01-30 16:03:05 | INFO | train | epoch 017 | loss 2.952 | nll_loss 1.383 | ppl 2.61 | wps 54285.9 | ups 14.39 | wpb 3771.6 | bsz 195.9 | num_updates 25072 | lr 0.000199713 | gnorm 0.907 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 1784
2024-01-30 16:03:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:03:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:03:05 | INFO | fairseq.trainer | begin training epoch 18
2024-01-30 16:03:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:03:07 | INFO | train_inner | epoch 018:     28 / 1475 loss=2.985, nll_loss=1.421, ppl=2.68, wps=26925.6, ups=7.15, wpb=3766.4, bsz=185.3, num_updates=25100, lr=0.000199601, gnorm=0.915, loss_scale=32, train_wall=6, gb_free=29.7, wall=1786
2024-01-30 16:03:13 | INFO | train_inner | epoch 018:    128 / 1475 loss=2.937, nll_loss=1.365, ppl=2.58, wps=59675.4, ups=15.62, wpb=3820.8, bsz=180.2, num_updates=25200, lr=0.000199205, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.7, wall=1792
2024-01-30 16:03:19 | INFO | train_inner | epoch 018:    228 / 1475 loss=2.874, nll_loss=1.295, ppl=2.45, wps=57510.3, ups=15.58, wpb=3691, bsz=198, num_updates=25300, lr=0.000198811, gnorm=0.877, loss_scale=32, train_wall=6, gb_free=29.7, wall=1799
2024-01-30 16:03:26 | INFO | train_inner | epoch 018:    328 / 1475 loss=2.923, nll_loss=1.35, ppl=2.55, wps=57628.2, ups=15.61, wpb=3691.9, bsz=179.4, num_updates=25400, lr=0.000198419, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.8, wall=1805
2024-01-30 16:03:32 | INFO | train_inner | epoch 018:    428 / 1475 loss=2.91, nll_loss=1.336, ppl=2.52, wps=59071.9, ups=15.49, wpb=3812.8, bsz=192.7, num_updates=25500, lr=0.00019803, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.6, wall=1811
2024-01-30 16:03:39 | INFO | train_inner | epoch 018:    528 / 1475 loss=2.908, nll_loss=1.334, ppl=2.52, wps=59121.5, ups=15.54, wpb=3803.8, bsz=194.6, num_updates=25600, lr=0.000197642, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=1818
2024-01-30 16:03:45 | INFO | train_inner | epoch 018:    628 / 1475 loss=2.9, nll_loss=1.324, ppl=2.5, wps=59785.7, ups=15.54, wpb=3846.2, bsz=200.2, num_updates=25700, lr=0.000197257, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.7, wall=1824
2024-01-30 16:03:52 | INFO | train_inner | epoch 018:    728 / 1475 loss=2.931, nll_loss=1.36, ppl=2.57, wps=58424.8, ups=15.46, wpb=3779.6, bsz=197.6, num_updates=25800, lr=0.000196875, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.7, wall=1831
2024-01-30 16:03:58 | INFO | train_inner | epoch 018:    828 / 1475 loss=2.933, nll_loss=1.363, ppl=2.57, wps=58580, ups=15.5, wpb=3778.9, bsz=194, num_updates=25900, lr=0.000196494, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.9, wall=1837
2024-01-30 16:04:05 | INFO | train_inner | epoch 018:    928 / 1475 loss=2.903, nll_loss=1.327, ppl=2.51, wps=58571.5, ups=15.5, wpb=3778.5, bsz=213.3, num_updates=26000, lr=0.000196116, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.8, wall=1844
2024-01-30 16:04:11 | INFO | train_inner | epoch 018:   1028 / 1475 loss=2.935, nll_loss=1.366, ppl=2.58, wps=58551.8, ups=15.57, wpb=3760.2, bsz=200.1, num_updates=26100, lr=0.00019574, gnorm=0.919, loss_scale=32, train_wall=6, gb_free=29.7, wall=1850
2024-01-30 16:04:17 | INFO | train_inner | epoch 018:   1128 / 1475 loss=2.948, nll_loss=1.38, ppl=2.6, wps=58616.2, ups=15.52, wpb=3776.2, bsz=195.5, num_updates=26200, lr=0.000195366, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.8, wall=1856
2024-01-30 16:04:24 | INFO | train_inner | epoch 018:   1228 / 1475 loss=2.911, nll_loss=1.339, ppl=2.53, wps=58190.8, ups=15.48, wpb=3759.8, bsz=205.9, num_updates=26300, lr=0.000194994, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=1863
2024-01-30 16:04:30 | INFO | train_inner | epoch 018:   1328 / 1475 loss=2.932, nll_loss=1.362, ppl=2.57, wps=58029, ups=15.59, wpb=3722.4, bsz=198.3, num_updates=26400, lr=0.000194625, gnorm=0.935, loss_scale=32, train_wall=6, gb_free=29.6, wall=1869
2024-01-30 16:04:37 | INFO | train_inner | epoch 018:   1428 / 1475 loss=2.936, nll_loss=1.367, ppl=2.58, wps=59092.8, ups=15.55, wpb=3799.6, bsz=199.2, num_updates=26500, lr=0.000194257, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=1876
2024-01-30 16:04:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:04:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:04:42 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.444 | nll_loss 1.79 | ppl 3.46 | wps 133921 | wpb 3189.1 | bsz 163.4 | num_updates 26547 | best_loss 3.444
2024-01-30 16:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26547 updates
2024-01-30 16:04:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint18.pt
2024-01-30 16:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint18.pt
2024-01-30 16:04:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint18.pt (epoch 18 @ 26547 updates, score 3.444) (writing took 4.989670793991536 seconds)
2024-01-30 16:04:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-01-30 16:04:47 | INFO | train | epoch 018 | loss 2.92 | nll_loss 1.348 | ppl 2.55 | wps 54301.9 | ups 14.4 | wpb 3771.6 | bsz 195.9 | num_updates 26547 | lr 0.000194085 | gnorm 0.904 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 1886
2024-01-30 16:04:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:04:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:04:47 | INFO | fairseq.trainer | begin training epoch 19
2024-01-30 16:04:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:04:51 | INFO | train_inner | epoch 019:     53 / 1475 loss=2.907, nll_loss=1.333, ppl=2.52, wps=27143.1, ups=7.17, wpb=3786.2, bsz=189.3, num_updates=26600, lr=0.000193892, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.8, wall=1890
2024-01-30 16:04:57 | INFO | train_inner | epoch 019:    153 / 1475 loss=2.856, nll_loss=1.275, ppl=2.42, wps=59160, ups=15.46, wpb=3826.7, bsz=196.2, num_updates=26700, lr=0.000193528, gnorm=0.85, loss_scale=32, train_wall=6, gb_free=29.7, wall=1896
2024-01-30 16:05:04 | INFO | train_inner | epoch 019:    253 / 1475 loss=2.853, nll_loss=1.272, ppl=2.41, wps=59845, ups=15.58, wpb=3841.1, bsz=197.9, num_updates=26800, lr=0.000193167, gnorm=0.851, loss_scale=32, train_wall=6, gb_free=29.7, wall=1903
2024-01-30 16:05:10 | INFO | train_inner | epoch 019:    353 / 1475 loss=2.85, nll_loss=1.268, ppl=2.41, wps=58028.1, ups=15.56, wpb=3730.2, bsz=204.5, num_updates=26900, lr=0.000192807, gnorm=0.896, loss_scale=32, train_wall=6, gb_free=29.8, wall=1909
2024-01-30 16:05:16 | INFO | train_inner | epoch 019:    453 / 1475 loss=2.86, nll_loss=1.279, ppl=2.43, wps=57825.8, ups=15.46, wpb=3740.7, bsz=207, num_updates=27000, lr=0.00019245, gnorm=0.883, loss_scale=32, train_wall=6, gb_free=29.7, wall=1916
2024-01-30 16:05:23 | INFO | train_inner | epoch 019:    553 / 1475 loss=2.909, nll_loss=1.335, ppl=2.52, wps=58887.6, ups=15.57, wpb=3781.6, bsz=185.4, num_updates=27100, lr=0.000192095, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=1922
2024-01-30 16:05:29 | INFO | train_inner | epoch 019:    653 / 1475 loss=2.89, nll_loss=1.313, ppl=2.49, wps=57663.2, ups=15.64, wpb=3686.9, bsz=194.4, num_updates=27200, lr=0.000191741, gnorm=0.936, loss_scale=32, train_wall=6, gb_free=29.8, wall=1928
2024-01-30 16:05:36 | INFO | train_inner | epoch 019:    753 / 1475 loss=2.93, nll_loss=1.359, ppl=2.56, wps=59450.9, ups=15.54, wpb=3824.7, bsz=187.7, num_updates=27300, lr=0.00019139, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.7, wall=1935
2024-01-30 16:05:42 | INFO | train_inner | epoch 019:    853 / 1475 loss=2.874, nll_loss=1.296, ppl=2.46, wps=58571.4, ups=15.43, wpb=3796.8, bsz=205.2, num_updates=27400, lr=0.00019104, gnorm=0.878, loss_scale=32, train_wall=6, gb_free=29.7, wall=1941
2024-01-30 16:05:49 | INFO | train_inner | epoch 019:    953 / 1475 loss=2.91, nll_loss=1.337, ppl=2.53, wps=58480.9, ups=15.53, wpb=3765, bsz=197.8, num_updates=27500, lr=0.000190693, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=30, wall=1948
2024-01-30 16:05:55 | INFO | train_inner | epoch 019:   1053 / 1475 loss=2.9, nll_loss=1.325, ppl=2.51, wps=58730.9, ups=15.53, wpb=3780.6, bsz=196.4, num_updates=27600, lr=0.000190347, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.8, wall=1954
2024-01-30 16:06:01 | INFO | train_inner | epoch 019:   1153 / 1475 loss=2.937, nll_loss=1.369, ppl=2.58, wps=58164.9, ups=15.61, wpb=3725.4, bsz=185.5, num_updates=27700, lr=0.000190003, gnorm=0.92, loss_scale=32, train_wall=6, gb_free=29.9, wall=1961
2024-01-30 16:06:08 | INFO | train_inner | epoch 019:   1253 / 1475 loss=2.919, nll_loss=1.348, ppl=2.55, wps=57861.8, ups=15.54, wpb=3723.2, bsz=192.7, num_updates=27800, lr=0.000189661, gnorm=0.929, loss_scale=32, train_wall=6, gb_free=29.6, wall=1967
2024-01-30 16:06:14 | INFO | train_inner | epoch 019:   1353 / 1475 loss=2.89, nll_loss=1.315, ppl=2.49, wps=58520.7, ups=15.47, wpb=3782, bsz=203.8, num_updates=27900, lr=0.000189321, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.7, wall=1973
2024-01-30 16:06:21 | INFO | train_inner | epoch 019:   1453 / 1475 loss=2.91, nll_loss=1.337, ppl=2.53, wps=59269.5, ups=15.59, wpb=3802.1, bsz=197.8, num_updates=28000, lr=0.000188982, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.8, wall=1980
2024-01-30 16:06:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:06:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:06:24 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.441 | nll_loss 1.782 | ppl 3.44 | wps 129021 | wpb 3189.1 | bsz 163.4 | num_updates 28022 | best_loss 3.441
2024-01-30 16:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 28022 updates
2024-01-30 16:06:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint19.pt
2024-01-30 16:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint19.pt
2024-01-30 16:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint19.pt (epoch 19 @ 28022 updates, score 3.441) (writing took 5.0578001629328355 seconds)
2024-01-30 16:06:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-01-30 16:06:30 | INFO | train | epoch 019 | loss 2.892 | nll_loss 1.316 | ppl 2.49 | wps 54212.6 | ups 14.37 | wpb 3771.6 | bsz 195.9 | num_updates 28022 | lr 0.000188908 | gnorm 0.897 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 1989
2024-01-30 16:06:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:06:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:06:30 | INFO | fairseq.trainer | begin training epoch 20
2024-01-30 16:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:06:35 | INFO | train_inner | epoch 020:     78 / 1475 loss=2.863, nll_loss=1.283, ppl=2.43, wps=26448.7, ups=7.07, wpb=3741.8, bsz=189.4, num_updates=28100, lr=0.000188646, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.9, wall=1994
2024-01-30 16:06:41 | INFO | train_inner | epoch 020:    178 / 1475 loss=2.845, nll_loss=1.261, ppl=2.4, wps=58838, ups=15.61, wpb=3769.8, bsz=193.1, num_updates=28200, lr=0.000188311, gnorm=0.879, loss_scale=32, train_wall=6, gb_free=29.8, wall=2000
2024-01-30 16:06:48 | INFO | train_inner | epoch 020:    278 / 1475 loss=2.825, nll_loss=1.24, ppl=2.36, wps=59273.4, ups=15.51, wpb=3821.3, bsz=197.8, num_updates=28300, lr=0.000187978, gnorm=0.86, loss_scale=32, train_wall=6, gb_free=29.7, wall=2007
2024-01-30 16:06:54 | INFO | train_inner | epoch 020:    378 / 1475 loss=2.856, nll_loss=1.275, ppl=2.42, wps=59428.8, ups=15.54, wpb=3823.3, bsz=202.7, num_updates=28400, lr=0.000187647, gnorm=0.88, loss_scale=32, train_wall=6, gb_free=29.7, wall=2013
2024-01-30 16:07:01 | INFO | train_inner | epoch 020:    478 / 1475 loss=2.826, nll_loss=1.242, ppl=2.37, wps=59149.7, ups=15.47, wpb=3823.9, bsz=209.3, num_updates=28500, lr=0.000187317, gnorm=0.858, loss_scale=32, train_wall=6, gb_free=29.7, wall=2020
2024-01-30 16:07:07 | INFO | train_inner | epoch 020:    578 / 1475 loss=2.9, nll_loss=1.325, ppl=2.5, wps=58469.4, ups=15.58, wpb=3752.5, bsz=184.4, num_updates=28600, lr=0.000186989, gnorm=0.909, loss_scale=32, train_wall=6, gb_free=29.7, wall=2026
2024-01-30 16:07:13 | INFO | train_inner | epoch 020:    678 / 1475 loss=2.881, nll_loss=1.304, ppl=2.47, wps=58128.5, ups=15.57, wpb=3734.5, bsz=185.8, num_updates=28700, lr=0.000186663, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=2033
2024-01-30 16:07:20 | INFO | train_inner | epoch 020:    778 / 1475 loss=2.878, nll_loss=1.3, ppl=2.46, wps=58764, ups=15.58, wpb=3771.7, bsz=191.4, num_updates=28800, lr=0.000186339, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.7, wall=2039
2024-01-30 16:07:26 | INFO | train_inner | epoch 020:    878 / 1475 loss=2.839, nll_loss=1.257, ppl=2.39, wps=58758.8, ups=15.47, wpb=3799.3, bsz=208.5, num_updates=28900, lr=0.000186016, gnorm=0.872, loss_scale=32, train_wall=6, gb_free=29.7, wall=2045
2024-01-30 16:07:33 | INFO | train_inner | epoch 020:    978 / 1475 loss=2.865, nll_loss=1.286, ppl=2.44, wps=58162.6, ups=15.57, wpb=3736.2, bsz=193.6, num_updates=29000, lr=0.000185695, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.6, wall=2052
2024-01-30 16:07:39 | INFO | train_inner | epoch 020:   1078 / 1475 loss=2.9, nll_loss=1.326, ppl=2.51, wps=58279.1, ups=15.59, wpb=3738.8, bsz=192.5, num_updates=29100, lr=0.000185376, gnorm=0.931, loss_scale=32, train_wall=6, gb_free=29.7, wall=2058
2024-01-30 16:07:46 | INFO | train_inner | epoch 020:   1178 / 1475 loss=2.856, nll_loss=1.277, ppl=2.42, wps=57822.9, ups=15.47, wpb=3736.6, bsz=203.3, num_updates=29200, lr=0.000185058, gnorm=0.912, loss_scale=32, train_wall=6, gb_free=29.7, wall=2065
2024-01-30 16:07:52 | INFO | train_inner | epoch 020:   1278 / 1475 loss=2.844, nll_loss=1.263, ppl=2.4, wps=57852.7, ups=15.42, wpb=3751.5, bsz=202, num_updates=29300, lr=0.000184742, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.7, wall=2071
2024-01-30 16:07:59 | INFO | train_inner | epoch 020:   1378 / 1475 loss=2.9, nll_loss=1.326, ppl=2.51, wps=59306.3, ups=15.64, wpb=3792, bsz=189.4, num_updates=29400, lr=0.000184428, gnorm=0.897, loss_scale=32, train_wall=6, gb_free=29.7, wall=2078
2024-01-30 16:08:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:08:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:08:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.419 | nll_loss 1.773 | ppl 3.42 | wps 133754 | wpb 3189.1 | bsz 163.4 | num_updates 29497 | best_loss 3.419
2024-01-30 16:08:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29497 updates
2024-01-30 16:08:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint20.pt
2024-01-30 16:08:08 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint20.pt
2024-01-30 16:08:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint20.pt (epoch 20 @ 29497 updates, score 3.419) (writing took 5.056382238981314 seconds)
2024-01-30 16:08:12 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-01-30 16:08:12 | INFO | train | epoch 020 | loss 2.865 | nll_loss 1.286 | ppl 2.44 | wps 54257.6 | ups 14.39 | wpb 3771.6 | bsz 195.9 | num_updates 29497 | lr 0.000184124 | gnorm 0.894 | loss_scale 32 | train_wall 92 | gb_free 29.9 | wall 2091
2024-01-30 16:08:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:08:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:08:12 | INFO | fairseq.trainer | begin training epoch 21
2024-01-30 16:08:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:08:13 | INFO | train_inner | epoch 021:      3 / 1475 loss=2.916, nll_loss=1.345, ppl=2.54, wps=26829.2, ups=7.15, wpb=3754.9, bsz=188.3, num_updates=29500, lr=0.000184115, gnorm=0.923, loss_scale=32, train_wall=6, gb_free=29.7, wall=2092
2024-01-30 16:08:19 | INFO | train_inner | epoch 021:    103 / 1475 loss=2.811, nll_loss=1.223, ppl=2.33, wps=57833.6, ups=15.51, wpb=3728.6, bsz=196.7, num_updates=29600, lr=0.000183804, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.8, wall=2098
2024-01-30 16:08:25 | INFO | train_inner | epoch 021:    203 / 1475 loss=2.784, nll_loss=1.192, ppl=2.29, wps=58585.4, ups=15.58, wpb=3759.8, bsz=199.9, num_updates=29700, lr=0.000183494, gnorm=0.869, loss_scale=32, train_wall=6, gb_free=29.8, wall=2105
2024-01-30 16:08:32 | INFO | train_inner | epoch 021:    303 / 1475 loss=2.815, nll_loss=1.228, ppl=2.34, wps=59464.3, ups=15.46, wpb=3845.9, bsz=194.9, num_updates=29800, lr=0.000183186, gnorm=0.865, loss_scale=32, train_wall=6, gb_free=29.7, wall=2111
2024-01-30 16:08:38 | INFO | train_inner | epoch 021:    403 / 1475 loss=2.817, nll_loss=1.232, ppl=2.35, wps=59847.7, ups=15.57, wpb=3844.4, bsz=197.2, num_updates=29900, lr=0.000182879, gnorm=0.874, loss_scale=32, train_wall=6, gb_free=29.7, wall=2117
2024-01-30 16:08:45 | INFO | train_inner | epoch 021:    503 / 1475 loss=2.862, nll_loss=1.282, ppl=2.43, wps=58855.7, ups=15.56, wpb=3782.6, bsz=190.3, num_updates=30000, lr=0.000182574, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.7, wall=2124
2024-01-30 16:08:51 | INFO | train_inner | epoch 021:    603 / 1475 loss=2.857, nll_loss=1.277, ppl=2.42, wps=57737.1, ups=15.56, wpb=3710, bsz=188.6, num_updates=30100, lr=0.000182271, gnorm=0.911, loss_scale=32, train_wall=6, gb_free=29.7, wall=2130
2024-01-30 16:08:58 | INFO | train_inner | epoch 021:    703 / 1475 loss=2.834, nll_loss=1.251, ppl=2.38, wps=57226.3, ups=15.45, wpb=3703.1, bsz=192.1, num_updates=30200, lr=0.000181969, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.8, wall=2137
2024-01-30 16:09:04 | INFO | train_inner | epoch 021:    803 / 1475 loss=2.838, nll_loss=1.256, ppl=2.39, wps=59264.4, ups=15.51, wpb=3820.6, bsz=200.8, num_updates=30300, lr=0.000181668, gnorm=0.886, loss_scale=32, train_wall=6, gb_free=29.7, wall=2143
2024-01-30 16:09:11 | INFO | train_inner | epoch 021:    903 / 1475 loss=2.846, nll_loss=1.265, ppl=2.4, wps=58225.1, ups=15.57, wpb=3739.6, bsz=198.7, num_updates=30400, lr=0.000181369, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=2150
2024-01-30 16:09:17 | INFO | train_inner | epoch 021:   1003 / 1475 loss=2.849, nll_loss=1.269, ppl=2.41, wps=58563.1, ups=15.55, wpb=3765.6, bsz=199, num_updates=30500, lr=0.000181071, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.7, wall=2156
2024-01-30 16:09:23 | INFO | train_inner | epoch 021:   1103 / 1475 loss=2.86, nll_loss=1.281, ppl=2.43, wps=59430.3, ups=15.61, wpb=3806.4, bsz=196.6, num_updates=30600, lr=0.000180775, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.8, wall=2162
2024-01-30 16:09:30 | INFO | train_inner | epoch 021:   1203 / 1475 loss=2.851, nll_loss=1.272, ppl=2.41, wps=57645.6, ups=15.45, wpb=3730.2, bsz=196.6, num_updates=30700, lr=0.000180481, gnorm=0.908, loss_scale=32, train_wall=6, gb_free=29.7, wall=2169
2024-01-30 16:09:36 | INFO | train_inner | epoch 021:   1303 / 1475 loss=2.844, nll_loss=1.265, ppl=2.4, wps=58126.7, ups=15.48, wpb=3755.8, bsz=199.2, num_updates=30800, lr=0.000180187, gnorm=0.921, loss_scale=32, train_wall=6, gb_free=29.7, wall=2175
2024-01-30 16:09:43 | INFO | train_inner | epoch 021:   1403 / 1475 loss=2.842, nll_loss=1.261, ppl=2.4, wps=58680.3, ups=15.51, wpb=3782.7, bsz=207.3, num_updates=30900, lr=0.000179896, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.8, wall=2182
2024-01-30 16:09:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:09:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:09:50 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.418 | nll_loss 1.765 | ppl 3.4 | wps 134363 | wpb 3189.1 | bsz 163.4 | num_updates 30972 | best_loss 3.418
2024-01-30 16:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30972 updates
2024-01-30 16:09:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint21.pt
2024-01-30 16:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint21.pt
2024-01-30 16:09:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint21.pt (epoch 21 @ 30972 updates, score 3.418) (writing took 5.066498497035354 seconds)
2024-01-30 16:09:55 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-01-30 16:09:55 | INFO | train | epoch 021 | loss 2.84 | nll_loss 1.258 | ppl 2.39 | wps 54252.1 | ups 14.38 | wpb 3771.6 | bsz 195.9 | num_updates 30972 | lr 0.000179686 | gnorm 0.896 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 2194
2024-01-30 16:09:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:09:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:09:55 | INFO | fairseq.trainer | begin training epoch 22
2024-01-30 16:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:09:57 | INFO | train_inner | epoch 022:     28 / 1475 loss=2.869, nll_loss=1.291, ppl=2.45, wps=27094.2, ups=7.14, wpb=3796.5, bsz=183.7, num_updates=31000, lr=0.000179605, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=2196
2024-01-30 16:10:03 | INFO | train_inner | epoch 022:    128 / 1475 loss=2.821, nll_loss=1.236, ppl=2.36, wps=59064.3, ups=15.6, wpb=3785.8, bsz=186.2, num_updates=31100, lr=0.000179316, gnorm=0.887, loss_scale=32, train_wall=6, gb_free=29.7, wall=2202
2024-01-30 16:10:10 | INFO | train_inner | epoch 022:    228 / 1475 loss=2.781, nll_loss=1.189, ppl=2.28, wps=58886.5, ups=15.58, wpb=3779.1, bsz=200.9, num_updates=31200, lr=0.000179029, gnorm=0.889, loss_scale=32, train_wall=6, gb_free=29.7, wall=2209
2024-01-30 16:10:16 | INFO | train_inner | epoch 022:    328 / 1475 loss=2.788, nll_loss=1.198, ppl=2.29, wps=59376.7, ups=15.56, wpb=3815.8, bsz=199.4, num_updates=31300, lr=0.000178743, gnorm=0.862, loss_scale=32, train_wall=6, gb_free=29.7, wall=2215
2024-01-30 16:10:22 | INFO | train_inner | epoch 022:    428 / 1475 loss=2.782, nll_loss=1.192, ppl=2.29, wps=59100.2, ups=15.46, wpb=3823.6, bsz=202.2, num_updates=31400, lr=0.000178458, gnorm=0.857, loss_scale=32, train_wall=6, gb_free=29.7, wall=2222
2024-01-30 16:10:29 | INFO | train_inner | epoch 022:    528 / 1475 loss=2.826, nll_loss=1.242, ppl=2.36, wps=57525.8, ups=15.52, wpb=3707.3, bsz=187.3, num_updates=31500, lr=0.000178174, gnorm=0.91, loss_scale=32, train_wall=6, gb_free=29.7, wall=2228
2024-01-30 16:10:35 | INFO | train_inner | epoch 022:    628 / 1475 loss=2.811, nll_loss=1.225, ppl=2.34, wps=58157.3, ups=15.54, wpb=3743.4, bsz=202.2, num_updates=31600, lr=0.000177892, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.6, wall=2234
2024-01-30 16:10:42 | INFO | train_inner | epoch 022:    728 / 1475 loss=2.846, nll_loss=1.264, ppl=2.4, wps=58883.1, ups=15.61, wpb=3772.8, bsz=186.7, num_updates=31700, lr=0.000177611, gnorm=0.922, loss_scale=32, train_wall=6, gb_free=29.7, wall=2241
2024-01-30 16:10:48 | INFO | train_inner | epoch 022:    828 / 1475 loss=2.812, nll_loss=1.227, ppl=2.34, wps=58919.7, ups=15.55, wpb=3789.8, bsz=201.1, num_updates=31800, lr=0.000177332, gnorm=0.867, loss_scale=32, train_wall=6, gb_free=29.8, wall=2247
2024-01-30 16:10:55 | INFO | train_inner | epoch 022:    928 / 1475 loss=2.821, nll_loss=1.238, ppl=2.36, wps=58279.9, ups=15.51, wpb=3757.3, bsz=196.8, num_updates=31900, lr=0.000177054, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=2254
2024-01-30 16:11:01 | INFO | train_inner | epoch 022:   1028 / 1475 loss=2.831, nll_loss=1.249, ppl=2.38, wps=57405.9, ups=15.57, wpb=3688.1, bsz=189.8, num_updates=32000, lr=0.000176777, gnorm=0.91, loss_scale=32, train_wall=6, gb_free=29.7, wall=2260
2024-01-30 16:11:07 | INFO | train_inner | epoch 022:   1128 / 1475 loss=2.848, nll_loss=1.267, ppl=2.41, wps=59175.4, ups=15.51, wpb=3815.8, bsz=193.9, num_updates=32100, lr=0.000176501, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.8, wall=2267
2024-01-30 16:11:14 | INFO | train_inner | epoch 022:   1228 / 1475 loss=2.831, nll_loss=1.249, ppl=2.38, wps=58772.2, ups=15.56, wpb=3778, bsz=196.5, num_updates=32200, lr=0.000176227, gnorm=0.878, loss_scale=32, train_wall=6, gb_free=29.7, wall=2273
2024-01-30 16:11:20 | INFO | train_inner | epoch 022:   1328 / 1475 loss=2.837, nll_loss=1.256, ppl=2.39, wps=58787.7, ups=15.57, wpb=3774.7, bsz=189.7, num_updates=32300, lr=0.000175954, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.7, wall=2279
2024-01-30 16:11:27 | INFO | train_inner | epoch 022:   1428 / 1475 loss=2.822, nll_loss=1.239, ppl=2.36, wps=58036.8, ups=15.57, wpb=3728.6, bsz=201.4, num_updates=32400, lr=0.000175682, gnorm=0.925, loss_scale=32, train_wall=6, gb_free=29.7, wall=2286
2024-01-30 16:11:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:11:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:11:32 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.419 | nll_loss 1.764 | ppl 3.4 | wps 134025 | wpb 3189.1 | bsz 163.4 | num_updates 32447 | best_loss 3.418
2024-01-30 16:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32447 updates
2024-01-30 16:11:32 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint22.pt
2024-01-30 16:11:33 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint22.pt
2024-01-30 16:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint22.pt (epoch 22 @ 32447 updates, score 3.419) (writing took 3.7498292069649324 seconds)
2024-01-30 16:11:36 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-01-30 16:11:36 | INFO | train | epoch 022 | loss 2.817 | nll_loss 1.233 | ppl 2.35 | wps 54979.4 | ups 14.58 | wpb 3771.6 | bsz 195.9 | num_updates 32447 | lr 0.000175555 | gnorm 0.893 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 2295
2024-01-30 16:11:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:11:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:11:36 | INFO | fairseq.trainer | begin training epoch 23
2024-01-30 16:11:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:11:40 | INFO | train_inner | epoch 023:     53 / 1475 loss=2.795, nll_loss=1.208, ppl=2.31, wps=29920, ups=7.84, wpb=3815.4, bsz=198.8, num_updates=32500, lr=0.000175412, gnorm=0.872, loss_scale=32, train_wall=6, gb_free=29.7, wall=2299
2024-01-30 16:11:46 | INFO | train_inner | epoch 023:    153 / 1475 loss=2.755, nll_loss=1.159, ppl=2.23, wps=59644.6, ups=15.5, wpb=3847.3, bsz=203.8, num_updates=32600, lr=0.000175142, gnorm=0.854, loss_scale=32, train_wall=6, gb_free=29.7, wall=2305
2024-01-30 16:11:53 | INFO | train_inner | epoch 023:    253 / 1475 loss=2.778, nll_loss=1.187, ppl=2.28, wps=59443.6, ups=15.47, wpb=3842.8, bsz=201.7, num_updates=32700, lr=0.000174874, gnorm=0.864, loss_scale=32, train_wall=6, gb_free=29.7, wall=2312
2024-01-30 16:11:59 | INFO | train_inner | epoch 023:    353 / 1475 loss=2.79, nll_loss=1.2, ppl=2.3, wps=59911.3, ups=15.61, wpb=3839, bsz=193.8, num_updates=32800, lr=0.000174608, gnorm=0.866, loss_scale=32, train_wall=6, gb_free=29.7, wall=2318
2024-01-30 16:12:06 | INFO | train_inner | epoch 023:    453 / 1475 loss=2.794, nll_loss=1.205, ppl=2.31, wps=58549.7, ups=15.54, wpb=3766.8, bsz=189.8, num_updates=32900, lr=0.000174342, gnorm=0.878, loss_scale=32, train_wall=6, gb_free=29.8, wall=2325
2024-01-30 16:12:12 | INFO | train_inner | epoch 023:    553 / 1475 loss=2.785, nll_loss=1.196, ppl=2.29, wps=57316.8, ups=15.49, wpb=3699.1, bsz=202.3, num_updates=33000, lr=0.000174078, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=2331
2024-01-30 16:12:19 | INFO | train_inner | epoch 023:    653 / 1475 loss=2.798, nll_loss=1.211, ppl=2.31, wps=57921.9, ups=15.55, wpb=3724.7, bsz=191.8, num_updates=33100, lr=0.000173814, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=2338
2024-01-30 16:12:25 | INFO | train_inner | epoch 023:    753 / 1475 loss=2.807, nll_loss=1.22, ppl=2.33, wps=59039.4, ups=15.53, wpb=3802.1, bsz=194, num_updates=33200, lr=0.000173553, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.8, wall=2344
2024-01-30 16:12:31 | INFO | train_inner | epoch 023:    853 / 1475 loss=2.81, nll_loss=1.225, ppl=2.34, wps=57412.3, ups=15.45, wpb=3715.5, bsz=197.1, num_updates=33300, lr=0.000173292, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.8, wall=2351
2024-01-30 16:12:38 | INFO | train_inner | epoch 023:    953 / 1475 loss=2.816, nll_loss=1.233, ppl=2.35, wps=58846.7, ups=15.55, wpb=3785.5, bsz=194.9, num_updates=33400, lr=0.000173032, gnorm=0.926, loss_scale=32, train_wall=6, gb_free=29.8, wall=2357
2024-01-30 16:12:44 | INFO | train_inner | epoch 023:   1053 / 1475 loss=2.831, nll_loss=1.248, ppl=2.38, wps=58742.6, ups=15.59, wpb=3769.2, bsz=190.5, num_updates=33500, lr=0.000172774, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.7, wall=2363
2024-01-30 16:12:51 | INFO | train_inner | epoch 023:   1153 / 1475 loss=2.786, nll_loss=1.199, ppl=2.3, wps=57889.6, ups=15.47, wpb=3742.2, bsz=203.3, num_updates=33600, lr=0.000172516, gnorm=0.882, loss_scale=32, train_wall=6, gb_free=29.7, wall=2370
2024-01-30 16:12:57 | INFO | train_inner | epoch 023:   1253 / 1475 loss=2.833, nll_loss=1.251, ppl=2.38, wps=58500.6, ups=15.59, wpb=3751.5, bsz=183.2, num_updates=33700, lr=0.00017226, gnorm=0.912, loss_scale=64, train_wall=6, gb_free=29.7, wall=2376
2024-01-30 16:13:04 | INFO | train_inner | epoch 023:   1353 / 1475 loss=2.772, nll_loss=1.183, ppl=2.27, wps=58796.9, ups=15.46, wpb=3803.1, bsz=207.2, num_updates=33800, lr=0.000172005, gnorm=0.871, loss_scale=64, train_wall=6, gb_free=29.8, wall=2383
2024-01-30 16:13:10 | INFO | train_inner | epoch 023:   1453 / 1475 loss=2.818, nll_loss=1.234, ppl=2.35, wps=58155.5, ups=15.59, wpb=3730.4, bsz=193, num_updates=33900, lr=0.000171751, gnorm=0.921, loss_scale=64, train_wall=6, gb_free=29.6, wall=2389
2024-01-30 16:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:13:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:13:14 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.409 | nll_loss 1.753 | ppl 3.37 | wps 134252 | wpb 3189.1 | bsz 163.4 | num_updates 33922 | best_loss 3.409
2024-01-30 16:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33922 updates
2024-01-30 16:13:14 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint23.pt
2024-01-30 16:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint23.pt
2024-01-30 16:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint23.pt (epoch 23 @ 33922 updates, score 3.409) (writing took 5.047924326034263 seconds)
2024-01-30 16:13:19 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-01-30 16:13:19 | INFO | train | epoch 023 | loss 2.796 | nll_loss 1.209 | ppl 2.31 | wps 54057.5 | ups 14.33 | wpb 3771.6 | bsz 195.9 | num_updates 33922 | lr 0.000171696 | gnorm 0.892 | loss_scale 64 | train_wall 92 | gb_free 29.7 | wall 2398
2024-01-30 16:13:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:13:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:13:19 | INFO | fairseq.trainer | begin training epoch 24
2024-01-30 16:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:13:24 | INFO | train_inner | epoch 024:     78 / 1475 loss=2.728, nll_loss=1.131, ppl=2.19, wps=26720.7, ups=7.11, wpb=3757.7, bsz=207.3, num_updates=34000, lr=0.000171499, gnorm=0.846, loss_scale=64, train_wall=6, gb_free=29.7, wall=2403
2024-01-30 16:13:31 | INFO | train_inner | epoch 024:    178 / 1475 loss=2.75, nll_loss=1.156, ppl=2.23, wps=57703.8, ups=15.6, wpb=3698.3, bsz=188.3, num_updates=34100, lr=0.000171247, gnorm=0.885, loss_scale=64, train_wall=6, gb_free=29.7, wall=2410
2024-01-30 16:13:37 | INFO | train_inner | epoch 024:    278 / 1475 loss=2.768, nll_loss=1.176, ppl=2.26, wps=58989.6, ups=15.6, wpb=3781.2, bsz=196.2, num_updates=34200, lr=0.000170996, gnorm=0.878, loss_scale=64, train_wall=6, gb_free=29.7, wall=2416
2024-01-30 16:13:43 | INFO | train_inner | epoch 024:    378 / 1475 loss=2.771, nll_loss=1.18, ppl=2.27, wps=58662.5, ups=15.57, wpb=3768.7, bsz=188.6, num_updates=34300, lr=0.000170747, gnorm=0.888, loss_scale=64, train_wall=6, gb_free=29.7, wall=2422
2024-01-30 16:13:50 | INFO | train_inner | epoch 024:    478 / 1475 loss=2.777, nll_loss=1.187, ppl=2.28, wps=58593.5, ups=15.55, wpb=3768.2, bsz=196, num_updates=34400, lr=0.000170499, gnorm=0.887, loss_scale=64, train_wall=6, gb_free=29.8, wall=2429
2024-01-30 16:13:56 | INFO | train_inner | epoch 024:    578 / 1475 loss=2.771, nll_loss=1.181, ppl=2.27, wps=58361.6, ups=15.46, wpb=3774.3, bsz=193.1, num_updates=34500, lr=0.000170251, gnorm=0.891, loss_scale=64, train_wall=6, gb_free=29.7, wall=2435
2024-01-30 16:14:03 | INFO | train_inner | epoch 024:    678 / 1475 loss=2.775, nll_loss=1.186, ppl=2.28, wps=58443.6, ups=15.48, wpb=3774.3, bsz=198.8, num_updates=34600, lr=0.000170005, gnorm=0.893, loss_scale=64, train_wall=6, gb_free=29.7, wall=2442
2024-01-30 16:14:09 | INFO | train_inner | epoch 024:    778 / 1475 loss=2.771, nll_loss=1.181, ppl=2.27, wps=57913.6, ups=15.56, wpb=3720.9, bsz=195.7, num_updates=34700, lr=0.00016976, gnorm=0.907, loss_scale=64, train_wall=6, gb_free=29.7, wall=2448
2024-01-30 16:14:16 | INFO | train_inner | epoch 024:    878 / 1475 loss=2.796, nll_loss=1.209, ppl=2.31, wps=58680.8, ups=15.59, wpb=3763.4, bsz=189.6, num_updates=34800, lr=0.000169516, gnorm=0.902, loss_scale=64, train_wall=6, gb_free=29.7, wall=2455
2024-01-30 16:14:22 | INFO | train_inner | epoch 024:    978 / 1475 loss=2.777, nll_loss=1.187, ppl=2.28, wps=58688.5, ups=15.48, wpb=3790.6, bsz=198.8, num_updates=34900, lr=0.000169273, gnorm=0.894, loss_scale=64, train_wall=6, gb_free=29.7, wall=2461
2024-01-30 16:14:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2024-01-30 16:14:28 | INFO | train_inner | epoch 024:   1079 / 1475 loss=2.804, nll_loss=1.219, ppl=2.33, wps=58316.6, ups=15.44, wpb=3776.6, bsz=186.2, num_updates=35000, lr=0.000169031, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.7, wall=2468
2024-01-30 16:14:35 | INFO | train_inner | epoch 024:   1179 / 1475 loss=2.796, nll_loss=1.209, ppl=2.31, wps=58639.9, ups=15.52, wpb=3778.1, bsz=192.2, num_updates=35100, lr=0.00016879, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.6, wall=2474
2024-01-30 16:14:41 | INFO | train_inner | epoch 024:   1279 / 1475 loss=2.773, nll_loss=1.184, ppl=2.27, wps=58688.8, ups=15.51, wpb=3783.4, bsz=207.8, num_updates=35200, lr=0.00016855, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.8, wall=2480
2024-01-30 16:14:48 | INFO | train_inner | epoch 024:   1379 / 1475 loss=2.781, nll_loss=1.193, ppl=2.29, wps=58927.5, ups=15.49, wpb=3804.6, bsz=204.3, num_updates=35300, lr=0.000168311, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.7, wall=2487
2024-01-30 16:14:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:14:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:14:56 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.404 | nll_loss 1.755 | ppl 3.38 | wps 134748 | wpb 3189.1 | bsz 163.4 | num_updates 35396 | best_loss 3.404
2024-01-30 16:14:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35396 updates
2024-01-30 16:14:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint24.pt
2024-01-30 16:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint24.pt
2024-01-30 16:15:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint24.pt (epoch 24 @ 35396 updates, score 3.404) (writing took 5.048243690980598 seconds)
2024-01-30 16:15:01 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-01-30 16:15:01 | INFO | train | epoch 024 | loss 2.777 | nll_loss 1.187 | ppl 2.28 | wps 54216.6 | ups 14.38 | wpb 3771.4 | bsz 195.9 | num_updates 35396 | lr 0.000168083 | gnorm 0.893 | loss_scale 32 | train_wall 92 | gb_free 29.7 | wall 2501
2024-01-30 16:15:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:15:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:15:01 | INFO | fairseq.trainer | begin training epoch 25
2024-01-30 16:15:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:15:02 | INFO | train_inner | epoch 025:      4 / 1475 loss=2.807, nll_loss=1.222, ppl=2.33, wps=27263.7, ups=7.15, wpb=3811.1, bsz=196.1, num_updates=35400, lr=0.000168073, gnorm=0.887, loss_scale=32, train_wall=6, gb_free=29.8, wall=2501
2024-01-30 16:15:08 | INFO | train_inner | epoch 025:    104 / 1475 loss=2.735, nll_loss=1.139, ppl=2.2, wps=57856.5, ups=15.53, wpb=3726.1, bsz=190.2, num_updates=35500, lr=0.000167836, gnorm=0.885, loss_scale=32, train_wall=6, gb_free=29.8, wall=2507
2024-01-30 16:15:15 | INFO | train_inner | epoch 025:    204 / 1475 loss=2.772, nll_loss=1.181, ppl=2.27, wps=59141.9, ups=15.63, wpb=3782.7, bsz=184.8, num_updates=35600, lr=0.0001676, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.8, wall=2514
2024-01-30 16:15:21 | INFO | train_inner | epoch 025:    304 / 1475 loss=2.736, nll_loss=1.139, ppl=2.2, wps=58706.7, ups=15.54, wpb=3777.6, bsz=195.9, num_updates=35700, lr=0.000167365, gnorm=0.882, loss_scale=32, train_wall=6, gb_free=30, wall=2520
2024-01-30 16:15:28 | INFO | train_inner | epoch 025:    404 / 1475 loss=2.731, nll_loss=1.135, ppl=2.2, wps=58758.5, ups=15.5, wpb=3790.6, bsz=200.1, num_updates=35800, lr=0.000167132, gnorm=0.882, loss_scale=32, train_wall=6, gb_free=29.7, wall=2527
2024-01-30 16:15:34 | INFO | train_inner | epoch 025:    504 / 1475 loss=2.729, nll_loss=1.133, ppl=2.19, wps=58440.2, ups=15.26, wpb=3828.6, bsz=198.2, num_updates=35900, lr=0.000166899, gnorm=0.87, loss_scale=32, train_wall=6, gb_free=29.7, wall=2533
2024-01-30 16:15:40 | INFO | train_inner | epoch 025:    604 / 1475 loss=2.768, nll_loss=1.176, ppl=2.26, wps=59406.4, ups=15.66, wpb=3794.7, bsz=192.2, num_updates=36000, lr=0.000166667, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.6, wall=2540
2024-01-30 16:15:47 | INFO | train_inner | epoch 025:    704 / 1475 loss=2.763, nll_loss=1.172, ppl=2.25, wps=58721.9, ups=15.56, wpb=3774.8, bsz=192.2, num_updates=36100, lr=0.000166436, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.7, wall=2546
2024-01-30 16:15:53 | INFO | train_inner | epoch 025:    804 / 1475 loss=2.777, nll_loss=1.189, ppl=2.28, wps=59132, ups=15.54, wpb=3805.9, bsz=190, num_updates=36200, lr=0.000166206, gnorm=0.877, loss_scale=32, train_wall=6, gb_free=29.6, wall=2552
2024-01-30 16:16:00 | INFO | train_inner | epoch 025:    904 / 1475 loss=2.762, nll_loss=1.172, ppl=2.25, wps=58592, ups=15.62, wpb=3751.4, bsz=196.2, num_updates=36300, lr=0.000165977, gnorm=0.898, loss_scale=32, train_wall=6, gb_free=29.7, wall=2559
2024-01-30 16:16:06 | INFO | train_inner | epoch 025:   1004 / 1475 loss=2.738, nll_loss=1.143, ppl=2.21, wps=57910, ups=15.49, wpb=3739.6, bsz=201.8, num_updates=36400, lr=0.000165748, gnorm=0.911, loss_scale=32, train_wall=6, gb_free=29.6, wall=2565
2024-01-30 16:16:13 | INFO | train_inner | epoch 025:   1104 / 1475 loss=2.774, nll_loss=1.185, ppl=2.27, wps=58583.8, ups=15.47, wpb=3787.7, bsz=198.2, num_updates=36500, lr=0.000165521, gnorm=0.898, loss_scale=32, train_wall=6, gb_free=29.7, wall=2572
2024-01-30 16:16:19 | INFO | train_inner | epoch 025:   1204 / 1475 loss=2.788, nll_loss=1.2, ppl=2.3, wps=58595.3, ups=15.69, wpb=3734.4, bsz=194.2, num_updates=36600, lr=0.000165295, gnorm=0.915, loss_scale=32, train_wall=6, gb_free=29.7, wall=2578
2024-01-30 16:16:25 | INFO | train_inner | epoch 025:   1304 / 1475 loss=2.764, nll_loss=1.174, ppl=2.26, wps=58005.9, ups=15.51, wpb=3739.9, bsz=204.1, num_updates=36700, lr=0.00016507, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.9, wall=2585
2024-01-30 16:16:32 | INFO | train_inner | epoch 025:   1404 / 1475 loss=2.758, nll_loss=1.167, ppl=2.25, wps=58034.4, ups=15.44, wpb=3758.1, bsz=212, num_updates=36800, lr=0.000164845, gnorm=0.908, loss_scale=32, train_wall=6, gb_free=29.7, wall=2591
2024-01-30 16:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:16:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:16:39 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.405 | nll_loss 1.751 | ppl 3.37 | wps 135125 | wpb 3189.1 | bsz 163.4 | num_updates 36871 | best_loss 3.404
2024-01-30 16:16:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36871 updates
2024-01-30 16:16:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint25.pt
2024-01-30 16:16:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint25.pt
2024-01-30 16:16:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint25.pt (epoch 25 @ 36871 updates, score 3.405) (writing took 3.7085632529342547 seconds)
2024-01-30 16:16:43 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-01-30 16:16:43 | INFO | train | epoch 025 | loss 2.758 | nll_loss 1.166 | ppl 2.24 | wps 54982.6 | ups 14.58 | wpb 3771.6 | bsz 195.9 | num_updates 36871 | lr 0.000164686 | gnorm 0.896 | loss_scale 32 | train_wall 92 | gb_free 29.6 | wall 2602
2024-01-30 16:16:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:16:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:16:43 | INFO | fairseq.trainer | begin training epoch 26
2024-01-30 16:16:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:16:45 | INFO | train_inner | epoch 026:     29 / 1475 loss=2.76, nll_loss=1.168, ppl=2.25, wps=29951.1, ups=7.9, wpb=3792.8, bsz=197.2, num_updates=36900, lr=0.000164622, gnorm=0.895, loss_scale=32, train_wall=6, gb_free=29.7, wall=2604
2024-01-30 16:16:51 | INFO | train_inner | epoch 026:    129 / 1475 loss=2.693, nll_loss=1.091, ppl=2.13, wps=58634.6, ups=15.53, wpb=3775.3, bsz=201.8, num_updates=37000, lr=0.000164399, gnorm=0.855, loss_scale=32, train_wall=6, gb_free=29.7, wall=2610
2024-01-30 16:16:58 | INFO | train_inner | epoch 026:    229 / 1475 loss=2.726, nll_loss=1.129, ppl=2.19, wps=58941.6, ups=15.49, wpb=3804.2, bsz=192.2, num_updates=37100, lr=0.000164177, gnorm=0.861, loss_scale=32, train_wall=6, gb_free=29.7, wall=2617
2024-01-30 16:17:04 | INFO | train_inner | epoch 026:    329 / 1475 loss=2.72, nll_loss=1.123, ppl=2.18, wps=58513.8, ups=15.6, wpb=3751.3, bsz=193.3, num_updates=37200, lr=0.000163956, gnorm=0.892, loss_scale=32, train_wall=6, gb_free=29.7, wall=2623
2024-01-30 16:17:10 | INFO | train_inner | epoch 026:    429 / 1475 loss=2.735, nll_loss=1.139, ppl=2.2, wps=59022, ups=15.6, wpb=3783.5, bsz=191.9, num_updates=37300, lr=0.000163737, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.7, wall=2629
2024-01-30 16:17:17 | INFO | train_inner | epoch 026:    529 / 1475 loss=2.723, nll_loss=1.126, ppl=2.18, wps=57862.9, ups=15.54, wpb=3724.5, bsz=199.6, num_updates=37400, lr=0.000163517, gnorm=0.916, loss_scale=32, train_wall=6, gb_free=29.7, wall=2636
2024-01-30 16:17:23 | INFO | train_inner | epoch 026:    629 / 1475 loss=2.749, nll_loss=1.155, ppl=2.23, wps=59574.8, ups=15.58, wpb=3823.7, bsz=193.7, num_updates=37500, lr=0.000163299, gnorm=0.87, loss_scale=32, train_wall=6, gb_free=29.8, wall=2642
2024-01-30 16:17:30 | INFO | train_inner | epoch 026:    729 / 1475 loss=2.735, nll_loss=1.14, ppl=2.2, wps=58778.4, ups=15.56, wpb=3778.6, bsz=195.7, num_updates=37600, lr=0.000163082, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.7, wall=2649
2024-01-30 16:17:36 | INFO | train_inner | epoch 026:    829 / 1475 loss=2.75, nll_loss=1.157, ppl=2.23, wps=58153.3, ups=15.5, wpb=3751.9, bsz=196, num_updates=37700, lr=0.000162866, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.7, wall=2655
2024-01-30 16:17:43 | INFO | train_inner | epoch 026:    929 / 1475 loss=2.754, nll_loss=1.162, ppl=2.24, wps=58844, ups=15.52, wpb=3791.8, bsz=193, num_updates=37800, lr=0.00016265, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.8, wall=2662
2024-01-30 16:17:49 | INFO | train_inner | epoch 026:   1029 / 1475 loss=2.739, nll_loss=1.147, ppl=2.21, wps=59235.4, ups=15.47, wpb=3829.6, bsz=202.5, num_updates=37900, lr=0.000162435, gnorm=0.874, loss_scale=32, train_wall=6, gb_free=29.7, wall=2668
2024-01-30 16:17:55 | INFO | train_inner | epoch 026:   1129 / 1475 loss=2.736, nll_loss=1.142, ppl=2.21, wps=57443.7, ups=15.53, wpb=3699.1, bsz=196, num_updates=38000, lr=0.000162221, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.7, wall=2675
2024-01-30 16:18:02 | INFO | train_inner | epoch 026:   1229 / 1475 loss=2.768, nll_loss=1.178, ppl=2.26, wps=59212.5, ups=15.52, wpb=3815, bsz=193.7, num_updates=38100, lr=0.000162008, gnorm=0.893, loss_scale=32, train_wall=6, gb_free=29.7, wall=2681
2024-01-30 16:18:08 | INFO | train_inner | epoch 026:   1329 / 1475 loss=2.77, nll_loss=1.18, ppl=2.27, wps=58263.2, ups=15.58, wpb=3740.2, bsz=194.6, num_updates=38200, lr=0.000161796, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.8, wall=2687
2024-01-30 16:18:15 | INFO | train_inner | epoch 026:   1429 / 1475 loss=2.748, nll_loss=1.156, ppl=2.23, wps=58507.1, ups=15.54, wpb=3763.8, bsz=199.2, num_updates=38300, lr=0.000161585, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.7, wall=2694
2024-01-30 16:18:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:18:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:18:20 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.397 | nll_loss 1.752 | ppl 3.37 | wps 134468 | wpb 3189.1 | bsz 163.4 | num_updates 38346 | best_loss 3.397
2024-01-30 16:18:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38346 updates
2024-01-30 16:18:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint26.pt
2024-01-30 16:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint26.pt
2024-01-30 16:18:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint26.pt (epoch 26 @ 38346 updates, score 3.397) (writing took 5.189231341006234 seconds)
2024-01-30 16:18:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-01-30 16:18:25 | INFO | train | epoch 026 | loss 2.74 | nll_loss 1.146 | ppl 2.21 | wps 54217.3 | ups 14.38 | wpb 3771.6 | bsz 195.9 | num_updates 38346 | lr 0.000161488 | gnorm 0.892 | loss_scale 32 | train_wall 92 | gb_free 29.9 | wall 2704
2024-01-30 16:18:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:18:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:18:25 | INFO | fairseq.trainer | begin training epoch 27
2024-01-30 16:18:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:18:29 | INFO | train_inner | epoch 027:     54 / 1475 loss=2.75, nll_loss=1.158, ppl=2.23, wps=26354.8, ups=7.09, wpb=3717.9, bsz=182.6, num_updates=38400, lr=0.000161374, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.7, wall=2708
2024-01-30 16:18:35 | INFO | train_inner | epoch 027:    154 / 1475 loss=2.703, nll_loss=1.102, ppl=2.15, wps=59042.3, ups=15.59, wpb=3786.8, bsz=194.1, num_updates=38500, lr=0.000161165, gnorm=0.876, loss_scale=32, train_wall=6, gb_free=29.6, wall=2714
2024-01-30 16:18:42 | INFO | train_inner | epoch 027:    254 / 1475 loss=2.686, nll_loss=1.084, ppl=2.12, wps=58126.4, ups=15.54, wpb=3740.6, bsz=196.9, num_updates=38600, lr=0.000160956, gnorm=0.879, loss_scale=32, train_wall=6, gb_free=29.7, wall=2721
2024-01-30 16:18:48 | INFO | train_inner | epoch 027:    354 / 1475 loss=2.717, nll_loss=1.12, ppl=2.17, wps=59077.8, ups=15.49, wpb=3813.5, bsz=189.8, num_updates=38700, lr=0.000160748, gnorm=0.868, loss_scale=32, train_wall=6, gb_free=29.8, wall=2727
2024-01-30 16:18:55 | INFO | train_inner | epoch 027:    454 / 1475 loss=2.712, nll_loss=1.113, ppl=2.16, wps=58332.7, ups=15.51, wpb=3762.1, bsz=196.7, num_updates=38800, lr=0.00016054, gnorm=0.88, loss_scale=32, train_wall=6, gb_free=29.7, wall=2734
2024-01-30 16:19:01 | INFO | train_inner | epoch 027:    554 / 1475 loss=2.697, nll_loss=1.096, ppl=2.14, wps=58465.3, ups=15.56, wpb=3756.8, bsz=196.8, num_updates=38900, lr=0.000160334, gnorm=0.892, loss_scale=32, train_wall=6, gb_free=29.9, wall=2740
2024-01-30 16:19:07 | INFO | train_inner | epoch 027:    654 / 1475 loss=2.716, nll_loss=1.118, ppl=2.17, wps=58817.7, ups=15.57, wpb=3776.5, bsz=200.3, num_updates=39000, lr=0.000160128, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.8, wall=2747
2024-01-30 16:19:14 | INFO | train_inner | epoch 027:    754 / 1475 loss=2.741, nll_loss=1.148, ppl=2.22, wps=58898.6, ups=15.63, wpb=3767.9, bsz=187.4, num_updates=39100, lr=0.000159923, gnorm=0.909, loss_scale=32, train_wall=6, gb_free=29.8, wall=2753
2024-01-30 16:19:20 | INFO | train_inner | epoch 027:    854 / 1475 loss=2.732, nll_loss=1.136, ppl=2.2, wps=58505.8, ups=15.59, wpb=3753.8, bsz=189, num_updates=39200, lr=0.000159719, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=2759
2024-01-30 16:19:27 | INFO | train_inner | epoch 027:    954 / 1475 loss=2.735, nll_loss=1.141, ppl=2.2, wps=57656.4, ups=15.53, wpb=3711.6, bsz=194.5, num_updates=39300, lr=0.000159516, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.8, wall=2766
2024-01-30 16:19:33 | INFO | train_inner | epoch 027:   1054 / 1475 loss=2.71, nll_loss=1.112, ppl=2.16, wps=58984.7, ups=15.51, wpb=3803.2, bsz=207.2, num_updates=39400, lr=0.000159313, gnorm=0.877, loss_scale=32, train_wall=6, gb_free=29.9, wall=2772
2024-01-30 16:19:40 | INFO | train_inner | epoch 027:   1154 / 1475 loss=2.744, nll_loss=1.152, ppl=2.22, wps=58814.3, ups=15.45, wpb=3806.5, bsz=202.6, num_updates=39500, lr=0.000159111, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.7, wall=2779
2024-01-30 16:19:46 | INFO | train_inner | epoch 027:   1254 / 1475 loss=2.729, nll_loss=1.135, ppl=2.2, wps=59602.8, ups=15.5, wpb=3845.6, bsz=206.6, num_updates=39600, lr=0.00015891, gnorm=0.875, loss_scale=32, train_wall=6, gb_free=29.9, wall=2785
2024-01-30 16:19:52 | INFO | train_inner | epoch 027:   1354 / 1475 loss=2.779, nll_loss=1.192, ppl=2.28, wps=58430.4, ups=15.58, wpb=3749.8, bsz=184.2, num_updates=39700, lr=0.00015871, gnorm=0.914, loss_scale=32, train_wall=6, gb_free=29.7, wall=2792
2024-01-30 16:19:59 | INFO | train_inner | epoch 027:   1454 / 1475 loss=2.741, nll_loss=1.148, ppl=2.22, wps=58400, ups=15.58, wpb=3748.3, bsz=201.7, num_updates=39800, lr=0.000158511, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.7, wall=2798
2024-01-30 16:20:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:20:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:20:02 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.397 | nll_loss 1.748 | ppl 3.36 | wps 134648 | wpb 3189.1 | bsz 163.4 | num_updates 39821 | best_loss 3.397
2024-01-30 16:20:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39821 updates
2024-01-30 16:20:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint27.pt
2024-01-30 16:20:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint27.pt
2024-01-30 16:20:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint27.pt (epoch 27 @ 39821 updates, score 3.397) (writing took 5.060768882045522 seconds)
2024-01-30 16:20:08 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-01-30 16:20:08 | INFO | train | epoch 027 | loss 2.724 | nll_loss 1.127 | ppl 2.18 | wps 54293.7 | ups 14.4 | wpb 3771.6 | bsz 195.9 | num_updates 39821 | lr 0.000158469 | gnorm 0.893 | loss_scale 32 | train_wall 92 | gb_free 29.8 | wall 2807
2024-01-30 16:20:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:20:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:20:08 | INFO | fairseq.trainer | begin training epoch 28
2024-01-30 16:20:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:20:13 | INFO | train_inner | epoch 028:     79 / 1475 loss=2.684, nll_loss=1.082, ppl=2.12, wps=27345.9, ups=7.13, wpb=3837.2, bsz=202.2, num_updates=39900, lr=0.000158312, gnorm=0.868, loss_scale=32, train_wall=6, gb_free=29.7, wall=2812
2024-01-30 16:20:19 | INFO | train_inner | epoch 028:    179 / 1475 loss=2.704, nll_loss=1.104, ppl=2.15, wps=59319.5, ups=15.57, wpb=3810.3, bsz=187, num_updates=40000, lr=0.000158114, gnorm=0.884, loss_scale=32, train_wall=6, gb_free=29.7, wall=2818
2024-01-30 16:20:26 | INFO | train_inner | epoch 028:    279 / 1475 loss=2.695, nll_loss=1.094, ppl=2.14, wps=58080.1, ups=15.54, wpb=3737.4, bsz=191.2, num_updates=40100, lr=0.000157917, gnorm=0.884, loss_scale=32, train_wall=6, gb_free=29.7, wall=2825
2024-01-30 16:20:32 | INFO | train_inner | epoch 028:    379 / 1475 loss=2.695, nll_loss=1.094, ppl=2.14, wps=58235.8, ups=15.54, wpb=3747.3, bsz=200.5, num_updates=40200, lr=0.00015772, gnorm=0.887, loss_scale=32, train_wall=6, gb_free=29.7, wall=2831
2024-01-30 16:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 16:20:39 | INFO | train_inner | epoch 028:    480 / 1475 loss=2.675, nll_loss=1.072, ppl=2.1, wps=56395.1, ups=15.41, wpb=3660, bsz=196.8, num_updates=40300, lr=0.000157524, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=2838
2024-01-30 16:20:45 | INFO | train_inner | epoch 028:    580 / 1475 loss=2.717, nll_loss=1.12, ppl=2.17, wps=58586.6, ups=15.65, wpb=3743, bsz=190.6, num_updates=40400, lr=0.000157329, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=2844
2024-01-30 16:20:52 | INFO | train_inner | epoch 028:    680 / 1475 loss=2.729, nll_loss=1.133, ppl=2.19, wps=59201.7, ups=15.51, wpb=3817.3, bsz=193.2, num_updates=40500, lr=0.000157135, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.7, wall=2851
2024-01-30 16:20:58 | INFO | train_inner | epoch 028:    780 / 1475 loss=2.713, nll_loss=1.116, ppl=2.17, wps=58754.6, ups=15.55, wpb=3777.9, bsz=191.1, num_updates=40600, lr=0.000156941, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=2857
2024-01-30 16:21:04 | INFO | train_inner | epoch 028:    880 / 1475 loss=2.739, nll_loss=1.145, ppl=2.21, wps=57569, ups=15.6, wpb=3691.1, bsz=183.9, num_updates=40700, lr=0.000156748, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=2863
2024-01-30 16:21:11 | INFO | train_inner | epoch 028:    980 / 1475 loss=2.703, nll_loss=1.105, ppl=2.15, wps=60535.9, ups=15.57, wpb=3887.1, bsz=204.5, num_updates=40800, lr=0.000156556, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.7, wall=2870
2024-01-30 16:21:17 | INFO | train_inner | epoch 028:   1080 / 1475 loss=2.708, nll_loss=1.111, ppl=2.16, wps=58438.5, ups=15.52, wpb=3765.2, bsz=198.2, num_updates=40900, lr=0.000156365, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.8, wall=2876
2024-01-30 16:21:24 | INFO | train_inner | epoch 028:   1180 / 1475 loss=2.712, nll_loss=1.116, ppl=2.17, wps=58498.4, ups=15.55, wpb=3761.8, bsz=196.8, num_updates=41000, lr=0.000156174, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.7, wall=2883
2024-01-30 16:21:30 | INFO | train_inner | epoch 028:   1280 / 1475 loss=2.708, nll_loss=1.111, ppl=2.16, wps=58810.6, ups=15.51, wpb=3791.8, bsz=205.8, num_updates=41100, lr=0.000155984, gnorm=0.886, loss_scale=16, train_wall=6, gb_free=29.7, wall=2889
2024-01-30 16:21:37 | INFO | train_inner | epoch 028:   1380 / 1475 loss=2.724, nll_loss=1.129, ppl=2.19, wps=59076.2, ups=15.57, wpb=3794.3, bsz=198.6, num_updates=41200, lr=0.000155794, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.6, wall=2896
2024-01-30 16:21:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:21:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:21:45 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.401 | nll_loss 1.748 | ppl 3.36 | wps 134315 | wpb 3189.1 | bsz 163.4 | num_updates 41295 | best_loss 3.397
2024-01-30 16:21:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41295 updates
2024-01-30 16:21:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint28.pt
2024-01-30 16:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint28.pt
2024-01-30 16:21:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint28.pt (epoch 28 @ 41295 updates, score 3.401) (writing took 3.7443388130050153 seconds)
2024-01-30 16:21:49 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-01-30 16:21:49 | INFO | train | epoch 028 | loss 2.708 | nll_loss 1.11 | ppl 2.16 | wps 54968 | ups 14.57 | wpb 3771.4 | bsz 195.9 | num_updates 41295 | lr 0.000155615 | gnorm 0.894 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 2908
2024-01-30 16:21:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:21:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:21:49 | INFO | fairseq.trainer | begin training epoch 29
2024-01-30 16:21:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:21:49 | INFO | train_inner | epoch 029:      5 / 1475 loss=2.71, nll_loss=1.114, ppl=2.16, wps=29438.2, ups=7.87, wpb=3742.2, bsz=202.3, num_updates=41300, lr=0.000155606, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=2908
2024-01-30 16:21:56 | INFO | train_inner | epoch 029:    105 / 1475 loss=2.697, nll_loss=1.096, ppl=2.14, wps=57829.3, ups=15.63, wpb=3699.2, bsz=180.2, num_updates=41400, lr=0.000155417, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=2915
2024-01-30 16:22:02 | INFO | train_inner | epoch 029:    205 / 1475 loss=2.68, nll_loss=1.076, ppl=2.11, wps=59290.6, ups=15.55, wpb=3812.8, bsz=197.2, num_updates=41500, lr=0.00015523, gnorm=0.866, loss_scale=16, train_wall=6, gb_free=29.7, wall=2921
2024-01-30 16:22:09 | INFO | train_inner | epoch 029:    305 / 1475 loss=2.663, nll_loss=1.058, ppl=2.08, wps=59995.3, ups=15.57, wpb=3853.7, bsz=210.8, num_updates=41600, lr=0.000155043, gnorm=0.857, loss_scale=16, train_wall=6, gb_free=29.7, wall=2928
2024-01-30 16:22:15 | INFO | train_inner | epoch 029:    405 / 1475 loss=2.698, nll_loss=1.098, ppl=2.14, wps=59444.8, ups=15.54, wpb=3825.7, bsz=192.6, num_updates=41700, lr=0.000154857, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=2934
2024-01-30 16:22:21 | INFO | train_inner | epoch 029:    505 / 1475 loss=2.69, nll_loss=1.089, ppl=2.13, wps=59587.6, ups=15.58, wpb=3823.8, bsz=186.9, num_updates=41800, lr=0.000154672, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.7, wall=2940
2024-01-30 16:22:28 | INFO | train_inner | epoch 029:    605 / 1475 loss=2.702, nll_loss=1.103, ppl=2.15, wps=58633.9, ups=15.63, wpb=3750.2, bsz=193.4, num_updates=41900, lr=0.000154487, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=2947
2024-01-30 16:22:34 | INFO | train_inner | epoch 029:    705 / 1475 loss=2.688, nll_loss=1.088, ppl=2.13, wps=58646.1, ups=15.58, wpb=3764.8, bsz=194.2, num_updates=42000, lr=0.000154303, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.7, wall=2953
2024-01-30 16:22:41 | INFO | train_inner | epoch 029:    805 / 1475 loss=2.683, nll_loss=1.082, ppl=2.12, wps=57627.5, ups=15.57, wpb=3700.1, bsz=201, num_updates=42100, lr=0.00015412, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=2960
2024-01-30 16:22:47 | INFO | train_inner | epoch 029:    905 / 1475 loss=2.688, nll_loss=1.088, ppl=2.13, wps=59385, ups=15.58, wpb=3812.8, bsz=202.1, num_updates=42200, lr=0.000153937, gnorm=0.868, loss_scale=16, train_wall=6, gb_free=29.7, wall=2966
2024-01-30 16:22:53 | INFO | train_inner | epoch 029:   1005 / 1475 loss=2.686, nll_loss=1.085, ppl=2.12, wps=58668.4, ups=15.53, wpb=3776.7, bsz=200.1, num_updates=42300, lr=0.000153755, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=2973
2024-01-30 16:23:00 | INFO | train_inner | epoch 029:   1105 / 1475 loss=2.701, nll_loss=1.102, ppl=2.15, wps=57686.9, ups=15.64, wpb=3688.8, bsz=191.6, num_updates=42400, lr=0.000153574, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.8, wall=2979
2024-01-30 16:23:06 | INFO | train_inner | epoch 029:   1205 / 1475 loss=2.701, nll_loss=1.104, ppl=2.15, wps=58205.5, ups=15.48, wpb=3759, bsz=195.8, num_updates=42500, lr=0.000153393, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=2985
2024-01-30 16:23:13 | INFO | train_inner | epoch 029:   1305 / 1475 loss=2.702, nll_loss=1.104, ppl=2.15, wps=59041.8, ups=15.59, wpb=3786.3, bsz=201.3, num_updates=42600, lr=0.000153213, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=2992
2024-01-30 16:23:19 | INFO | train_inner | epoch 029:   1405 / 1475 loss=2.704, nll_loss=1.106, ppl=2.15, wps=58340.7, ups=15.66, wpb=3726.1, bsz=202.6, num_updates=42700, lr=0.000153033, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.7, wall=2998
2024-01-30 16:23:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:23:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:23:26 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.398 | nll_loss 1.746 | ppl 3.36 | wps 135114 | wpb 3189.1 | bsz 163.4 | num_updates 42770 | best_loss 3.397
2024-01-30 16:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42770 updates
2024-01-30 16:23:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint29.pt
2024-01-30 16:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint29.pt
2024-01-30 16:23:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint29.pt (epoch 29 @ 42770 updates, score 3.398) (writing took 3.7442637119675055 seconds)
2024-01-30 16:23:30 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-01-30 16:23:30 | INFO | train | epoch 029 | loss 2.693 | nll_loss 1.093 | ppl 2.13 | wps 55140.1 | ups 14.62 | wpb 3771.6 | bsz 195.9 | num_updates 42770 | lr 0.000152908 | gnorm 0.893 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3009
2024-01-30 16:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:23:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:23:30 | INFO | fairseq.trainer | begin training epoch 30
2024-01-30 16:23:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:23:32 | INFO | train_inner | epoch 030:     30 / 1475 loss=2.725, nll_loss=1.13, ppl=2.19, wps=30368.9, ups=7.92, wpb=3834.3, bsz=183.4, num_updates=42800, lr=0.000152854, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=3011
2024-01-30 16:23:38 | INFO | train_inner | epoch 030:    130 / 1475 loss=2.632, nll_loss=1.024, ppl=2.03, wps=59489.5, ups=15.57, wpb=3821.2, bsz=201.8, num_updates=42900, lr=0.000152676, gnorm=0.841, loss_scale=16, train_wall=6, gb_free=29.7, wall=3017
2024-01-30 16:23:45 | INFO | train_inner | epoch 030:    230 / 1475 loss=2.667, nll_loss=1.062, ppl=2.09, wps=57987.9, ups=15.59, wpb=3718.4, bsz=194.3, num_updates=43000, lr=0.000152499, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=3024
2024-01-30 16:23:51 | INFO | train_inner | epoch 030:    330 / 1475 loss=2.657, nll_loss=1.052, ppl=2.07, wps=58866.8, ups=15.57, wpb=3780.3, bsz=191.5, num_updates=43100, lr=0.000152322, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.8, wall=3030
2024-01-30 16:23:57 | INFO | train_inner | epoch 030:    430 / 1475 loss=2.635, nll_loss=1.027, ppl=2.04, wps=58227.3, ups=15.59, wpb=3733.7, bsz=205, num_updates=43200, lr=0.000152145, gnorm=0.871, loss_scale=16, train_wall=6, gb_free=29.8, wall=3037
2024-01-30 16:24:04 | INFO | train_inner | epoch 030:    530 / 1475 loss=2.679, nll_loss=1.078, ppl=2.11, wps=58670.7, ups=15.56, wpb=3770.4, bsz=192.2, num_updates=43300, lr=0.000151969, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.8, wall=3043
2024-01-30 16:24:10 | INFO | train_inner | epoch 030:    630 / 1475 loss=2.685, nll_loss=1.083, ppl=2.12, wps=58439.9, ups=15.68, wpb=3727.8, bsz=198.9, num_updates=43400, lr=0.000151794, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.8, wall=3049
2024-01-30 16:24:17 | INFO | train_inner | epoch 030:    730 / 1475 loss=2.65, nll_loss=1.045, ppl=2.06, wps=58914.5, ups=15.56, wpb=3785.2, bsz=209.6, num_updates=43500, lr=0.00015162, gnorm=0.881, loss_scale=16, train_wall=6, gb_free=29.7, wall=3056
2024-01-30 16:24:23 | INFO | train_inner | epoch 030:    830 / 1475 loss=2.679, nll_loss=1.077, ppl=2.11, wps=58298.3, ups=15.63, wpb=3730.6, bsz=197.8, num_updates=43600, lr=0.000151446, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.9, wall=3062
2024-01-30 16:24:29 | INFO | train_inner | epoch 030:    930 / 1475 loss=2.684, nll_loss=1.082, ppl=2.12, wps=58389.1, ups=15.64, wpb=3732.3, bsz=195.8, num_updates=43700, lr=0.000151272, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=3069
2024-01-30 16:24:36 | INFO | train_inner | epoch 030:   1030 / 1475 loss=2.697, nll_loss=1.099, ppl=2.14, wps=59251.8, ups=15.69, wpb=3775.4, bsz=187.1, num_updates=43800, lr=0.000151099, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.8, wall=3075
2024-01-30 16:24:42 | INFO | train_inner | epoch 030:   1130 / 1475 loss=2.709, nll_loss=1.112, ppl=2.16, wps=59934.5, ups=15.67, wpb=3824.9, bsz=192.5, num_updates=43900, lr=0.000150927, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.7, wall=3081
2024-01-30 16:24:49 | INFO | train_inner | epoch 030:   1230 / 1475 loss=2.712, nll_loss=1.115, ppl=2.17, wps=59695.3, ups=15.67, wpb=3809.7, bsz=194.4, num_updates=44000, lr=0.000150756, gnorm=0.882, loss_scale=16, train_wall=6, gb_free=29.7, wall=3088
2024-01-30 16:24:55 | INFO | train_inner | epoch 030:   1330 / 1475 loss=2.725, nll_loss=1.131, ppl=2.19, wps=60055.2, ups=15.52, wpb=3870.3, bsz=188.9, num_updates=44100, lr=0.000150585, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=3094
2024-01-30 16:25:01 | INFO | train_inner | epoch 030:   1430 / 1475 loss=2.669, nll_loss=1.068, ppl=2.1, wps=57303, ups=15.54, wpb=3686.4, bsz=197.7, num_updates=44200, lr=0.000150414, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=3101
2024-01-30 16:25:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:25:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:25:07 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.402 | nll_loss 1.753 | ppl 3.37 | wps 134837 | wpb 3189.1 | bsz 163.4 | num_updates 44245 | best_loss 3.397
2024-01-30 16:25:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44245 updates
2024-01-30 16:25:07 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint30.pt
2024-01-30 16:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint30.pt
2024-01-30 16:25:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint30.pt (epoch 30 @ 44245 updates, score 3.402) (writing took 3.6857464000349864 seconds)
2024-01-30 16:25:10 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-01-30 16:25:10 | INFO | train | epoch 030 | loss 2.679 | nll_loss 1.077 | ppl 2.11 | wps 55241.6 | ups 14.65 | wpb 3771.6 | bsz 195.9 | num_updates 44245 | lr 0.000150338 | gnorm 0.887 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3110
2024-01-30 16:25:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:25:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:25:10 | INFO | fairseq.trainer | begin training epoch 31
2024-01-30 16:25:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:25:14 | INFO | train_inner | epoch 031:     55 / 1475 loss=2.67, nll_loss=1.067, ppl=2.1, wps=29693.7, ups=7.92, wpb=3751.3, bsz=189.4, num_updates=44300, lr=0.000150244, gnorm=0.891, loss_scale=16, train_wall=6, gb_free=29.7, wall=3113
2024-01-30 16:25:20 | INFO | train_inner | epoch 031:    155 / 1475 loss=2.68, nll_loss=1.076, ppl=2.11, wps=59550.1, ups=15.82, wpb=3765.1, bsz=178.6, num_updates=44400, lr=0.000150075, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=3120
2024-01-30 16:25:27 | INFO | train_inner | epoch 031:    255 / 1475 loss=2.639, nll_loss=1.03, ppl=2.04, wps=58328.7, ups=15.62, wpb=3734.5, bsz=198.5, num_updates=44500, lr=0.000149906, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.8, wall=3126
2024-01-30 16:25:33 | INFO | train_inner | epoch 031:    355 / 1475 loss=2.638, nll_loss=1.031, ppl=2.04, wps=59863, ups=15.55, wpb=3849.8, bsz=202.5, num_updates=44600, lr=0.000149738, gnorm=0.857, loss_scale=16, train_wall=6, gb_free=29.6, wall=3132
2024-01-30 16:25:40 | INFO | train_inner | epoch 031:    455 / 1475 loss=2.635, nll_loss=1.028, ppl=2.04, wps=58222.4, ups=15.6, wpb=3731.4, bsz=203.8, num_updates=44700, lr=0.000149571, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.7, wall=3139
2024-01-30 16:25:46 | INFO | train_inner | epoch 031:    555 / 1475 loss=2.646, nll_loss=1.039, ppl=2.06, wps=57291.4, ups=15.61, wpb=3670.4, bsz=198.2, num_updates=44800, lr=0.000149404, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.6, wall=3145
2024-01-30 16:25:52 | INFO | train_inner | epoch 031:    655 / 1475 loss=2.662, nll_loss=1.059, ppl=2.08, wps=58668.2, ups=15.6, wpb=3761.6, bsz=197.3, num_updates=44900, lr=0.000149237, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=3152
2024-01-30 16:25:59 | INFO | train_inner | epoch 031:    755 / 1475 loss=2.68, nll_loss=1.079, ppl=2.11, wps=58778.2, ups=15.58, wpb=3773.4, bsz=193.8, num_updates=45000, lr=0.000149071, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=3158
2024-01-30 16:26:05 | INFO | train_inner | epoch 031:    855 / 1475 loss=2.675, nll_loss=1.074, ppl=2.11, wps=59045.5, ups=15.61, wpb=3782.5, bsz=191.8, num_updates=45100, lr=0.000148906, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.7, wall=3164
2024-01-30 16:26:12 | INFO | train_inner | epoch 031:    955 / 1475 loss=2.654, nll_loss=1.049, ppl=2.07, wps=58368.9, ups=15.55, wpb=3754.1, bsz=200.1, num_updates=45200, lr=0.000148741, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.7, wall=3171
2024-01-30 16:26:18 | INFO | train_inner | epoch 031:   1055 / 1475 loss=2.707, nll_loss=1.11, ppl=2.16, wps=59032.2, ups=15.72, wpb=3756, bsz=184.2, num_updates=45300, lr=0.000148577, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=3177
2024-01-30 16:26:25 | INFO | train_inner | epoch 031:   1155 / 1475 loss=2.671, nll_loss=1.07, ppl=2.1, wps=59667.1, ups=15.53, wpb=3842.5, bsz=206.2, num_updates=45400, lr=0.000148413, gnorm=0.875, loss_scale=16, train_wall=6, gb_free=29.7, wall=3184
2024-01-30 16:26:31 | INFO | train_inner | epoch 031:   1255 / 1475 loss=2.679, nll_loss=1.077, ppl=2.11, wps=58742.5, ups=15.66, wpb=3752.2, bsz=193.6, num_updates=45500, lr=0.00014825, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=3190
2024-01-30 16:26:37 | INFO | train_inner | epoch 031:   1355 / 1475 loss=2.704, nll_loss=1.107, ppl=2.15, wps=58964.9, ups=15.58, wpb=3783.8, bsz=189.1, num_updates=45600, lr=0.000148087, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.9, wall=3196
2024-01-30 16:26:44 | INFO | train_inner | epoch 031:   1455 / 1475 loss=2.657, nll_loss=1.052, ppl=2.07, wps=59730.4, ups=15.56, wpb=3839.5, bsz=210.3, num_updates=45700, lr=0.000147925, gnorm=0.869, loss_scale=16, train_wall=6, gb_free=29.8, wall=3203
2024-01-30 16:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:26:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:26:47 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.385 | nll_loss 1.742 | ppl 3.35 | wps 135317 | wpb 3189.1 | bsz 163.4 | num_updates 45720 | best_loss 3.385
2024-01-30 16:26:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45720 updates
2024-01-30 16:26:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint31.pt
2024-01-30 16:26:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint31.pt
2024-01-30 16:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint31.pt (epoch 31 @ 45720 updates, score 3.385) (writing took 5.036881670006551 seconds)
2024-01-30 16:26:52 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-01-30 16:26:52 | INFO | train | epoch 031 | loss 2.666 | nll_loss 1.063 | ppl 2.09 | wps 54513.6 | ups 14.45 | wpb 3771.6 | bsz 195.9 | num_updates 45720 | lr 0.000147893 | gnorm 0.892 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3212
2024-01-30 16:26:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:26:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:26:53 | INFO | fairseq.trainer | begin training epoch 32
2024-01-30 16:26:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:26:58 | INFO | train_inner | epoch 032:     80 / 1475 loss=2.646, nll_loss=1.04, ppl=2.06, wps=27228.5, ups=7.18, wpb=3792.3, bsz=189.5, num_updates=45800, lr=0.000147764, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.8, wall=3217
2024-01-30 16:27:04 | INFO | train_inner | epoch 032:    180 / 1475 loss=2.632, nll_loss=1.023, ppl=2.03, wps=59434.9, ups=15.62, wpb=3805.7, bsz=197.8, num_updates=45900, lr=0.000147602, gnorm=0.871, loss_scale=16, train_wall=6, gb_free=29.8, wall=3223
2024-01-30 16:27:10 | INFO | train_inner | epoch 032:    280 / 1475 loss=2.628, nll_loss=1.019, ppl=2.03, wps=59135, ups=15.62, wpb=3786.6, bsz=196.2, num_updates=46000, lr=0.000147442, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=3230
2024-01-30 16:27:17 | INFO | train_inner | epoch 032:    380 / 1475 loss=2.628, nll_loss=1.019, ppl=2.03, wps=58564, ups=15.62, wpb=3750.4, bsz=196.1, num_updates=46100, lr=0.000147282, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.8, wall=3236
2024-01-30 16:27:23 | INFO | train_inner | epoch 032:    480 / 1475 loss=2.657, nll_loss=1.052, ppl=2.07, wps=58724.5, ups=15.65, wpb=3753.1, bsz=186, num_updates=46200, lr=0.000147122, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=3242
2024-01-30 16:27:30 | INFO | train_inner | epoch 032:    580 / 1475 loss=2.649, nll_loss=1.044, ppl=2.06, wps=59115.2, ups=15.61, wpb=3786.6, bsz=191, num_updates=46300, lr=0.000146964, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.8, wall=3249
2024-01-30 16:27:36 | INFO | train_inner | epoch 032:    680 / 1475 loss=2.66, nll_loss=1.056, ppl=2.08, wps=60021.2, ups=15.58, wpb=3851.6, bsz=190.6, num_updates=46400, lr=0.000146805, gnorm=0.877, loss_scale=16, train_wall=6, gb_free=29.7, wall=3255
2024-01-30 16:27:43 | INFO | train_inner | epoch 032:    780 / 1475 loss=2.657, nll_loss=1.052, ppl=2.07, wps=58811.3, ups=15.63, wpb=3762.3, bsz=194.9, num_updates=46500, lr=0.000146647, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.9, wall=3262
2024-01-30 16:27:49 | INFO | train_inner | epoch 032:    880 / 1475 loss=2.644, nll_loss=1.039, ppl=2.05, wps=58563.5, ups=15.53, wpb=3770.5, bsz=205.6, num_updates=46600, lr=0.00014649, gnorm=0.875, loss_scale=16, train_wall=6, gb_free=29.8, wall=3268
2024-01-30 16:27:55 | INFO | train_inner | epoch 032:    980 / 1475 loss=2.664, nll_loss=1.061, ppl=2.09, wps=58254.4, ups=15.55, wpb=3745.8, bsz=191, num_updates=46700, lr=0.000146333, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=3274
2024-01-30 16:28:02 | INFO | train_inner | epoch 032:   1080 / 1475 loss=2.678, nll_loss=1.078, ppl=2.11, wps=58637.8, ups=15.65, wpb=3747.7, bsz=180.1, num_updates=46800, lr=0.000146176, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.8, wall=3281
2024-01-30 16:28:08 | INFO | train_inner | epoch 032:   1180 / 1475 loss=2.662, nll_loss=1.06, ppl=2.08, wps=58434.3, ups=15.63, wpb=3738.4, bsz=199.7, num_updates=46900, lr=0.00014602, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=3287
2024-01-30 16:28:15 | INFO | train_inner | epoch 032:   1280 / 1475 loss=2.65, nll_loss=1.046, ppl=2.07, wps=57470.8, ups=15.55, wpb=3694.8, bsz=208.7, num_updates=47000, lr=0.000145865, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=3294
2024-01-30 16:28:21 | INFO | train_inner | epoch 032:   1380 / 1475 loss=2.679, nll_loss=1.078, ppl=2.11, wps=59235.5, ups=15.63, wpb=3790.3, bsz=206.7, num_updates=47100, lr=0.00014571, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.7, wall=3300
2024-01-30 16:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:28:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:28:29 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.392 | nll_loss 1.746 | ppl 3.35 | wps 134301 | wpb 3189.1 | bsz 163.4 | num_updates 47195 | best_loss 3.385
2024-01-30 16:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47195 updates
2024-01-30 16:28:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint32.pt
2024-01-30 16:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint32.pt
2024-01-30 16:28:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint32.pt (epoch 32 @ 47195 updates, score 3.392) (writing took 3.683783842017874 seconds)
2024-01-30 16:28:33 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-01-30 16:28:33 | INFO | train | epoch 032 | loss 2.652 | nll_loss 1.047 | ppl 2.07 | wps 55221.7 | ups 14.64 | wpb 3771.6 | bsz 195.9 | num_updates 47195 | lr 0.000145563 | gnorm 0.893 | loss_scale 16 | train_wall 91 | gb_free 29.9 | wall 3312
2024-01-30 16:28:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:28:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:28:33 | INFO | fairseq.trainer | begin training epoch 33
2024-01-30 16:28:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:28:34 | INFO | train_inner | epoch 033:      5 / 1475 loss=2.654, nll_loss=1.051, ppl=2.07, wps=30123.5, ups=7.91, wpb=3806.5, bsz=205.5, num_updates=47200, lr=0.000145556, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=3313
2024-01-30 16:28:40 | INFO | train_inner | epoch 033:    105 / 1475 loss=2.605, nll_loss=0.993, ppl=1.99, wps=58816.5, ups=15.52, wpb=3788.5, bsz=197.9, num_updates=47300, lr=0.000145402, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.9, wall=3319
2024-01-30 16:28:47 | INFO | train_inner | epoch 033:    205 / 1475 loss=2.605, nll_loss=0.993, ppl=1.99, wps=58552.9, ups=15.52, wpb=3772.4, bsz=206.1, num_updates=47400, lr=0.000145248, gnorm=0.864, loss_scale=16, train_wall=6, gb_free=29.8, wall=3326
2024-01-30 16:28:53 | INFO | train_inner | epoch 033:    305 / 1475 loss=2.606, nll_loss=0.994, ppl=1.99, wps=59361.6, ups=15.58, wpb=3809, bsz=208.1, num_updates=47500, lr=0.000145095, gnorm=0.852, loss_scale=16, train_wall=6, gb_free=29.8, wall=3332
2024-01-30 16:28:59 | INFO | train_inner | epoch 033:    405 / 1475 loss=2.641, nll_loss=1.034, ppl=2.05, wps=59423.8, ups=15.52, wpb=3829.1, bsz=194.2, num_updates=47600, lr=0.000144943, gnorm=0.888, loss_scale=16, train_wall=6, gb_free=29.7, wall=3338
2024-01-30 16:29:06 | INFO | train_inner | epoch 033:    505 / 1475 loss=2.646, nll_loss=1.039, ppl=2.06, wps=58982.7, ups=15.67, wpb=3764.1, bsz=189.4, num_updates=47700, lr=0.000144791, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.8, wall=3345
2024-01-30 16:29:12 | INFO | train_inner | epoch 033:    605 / 1475 loss=2.631, nll_loss=1.024, ppl=2.03, wps=58737, ups=15.57, wpb=3772.9, bsz=197.1, num_updates=47800, lr=0.000144639, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.7, wall=3351
2024-01-30 16:29:19 | INFO | train_inner | epoch 033:    705 / 1475 loss=2.66, nll_loss=1.056, ppl=2.08, wps=58963.7, ups=15.59, wpb=3781.4, bsz=181.4, num_updates=47900, lr=0.000144488, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.7, wall=3358
2024-01-30 16:29:25 | INFO | train_inner | epoch 033:    805 / 1475 loss=2.635, nll_loss=1.029, ppl=2.04, wps=59335.2, ups=15.63, wpb=3797.1, bsz=199.2, num_updates=48000, lr=0.000144338, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=3364
2024-01-30 16:29:31 | INFO | train_inner | epoch 033:    905 / 1475 loss=2.635, nll_loss=1.028, ppl=2.04, wps=59206.6, ups=15.6, wpb=3795, bsz=196.8, num_updates=48100, lr=0.000144187, gnorm=0.897, loss_scale=16, train_wall=6, gb_free=29.7, wall=3371
2024-01-30 16:29:38 | INFO | train_inner | epoch 033:   1005 / 1475 loss=2.682, nll_loss=1.082, ppl=2.12, wps=59631, ups=15.62, wpb=3817.2, bsz=183.5, num_updates=48200, lr=0.000144038, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.8, wall=3377
2024-01-30 16:29:44 | INFO | train_inner | epoch 033:   1105 / 1475 loss=2.642, nll_loss=1.037, ppl=2.05, wps=57466.8, ups=15.59, wpb=3686.5, bsz=199, num_updates=48300, lr=0.000143889, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.7, wall=3383
2024-01-30 16:29:51 | INFO | train_inner | epoch 033:   1205 / 1475 loss=2.649, nll_loss=1.044, ppl=2.06, wps=57582, ups=15.64, wpb=3681.4, bsz=196.2, num_updates=48400, lr=0.00014374, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.8, wall=3390
2024-01-30 16:29:57 | INFO | train_inner | epoch 033:   1305 / 1475 loss=2.669, nll_loss=1.068, ppl=2.1, wps=58688.1, ups=15.58, wpb=3765.7, bsz=191.8, num_updates=48500, lr=0.000143592, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.8, wall=3396
2024-01-30 16:30:03 | INFO | train_inner | epoch 033:   1405 / 1475 loss=2.666, nll_loss=1.063, ppl=2.09, wps=58869.6, ups=15.64, wpb=3763.7, bsz=196.1, num_updates=48600, lr=0.000143444, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.8, wall=3403
2024-01-30 16:30:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:30:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:30:10 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.393 | nll_loss 1.746 | ppl 3.35 | wps 134803 | wpb 3189.1 | bsz 163.4 | num_updates 48670 | best_loss 3.385
2024-01-30 16:30:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48670 updates
2024-01-30 16:30:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint33.pt
2024-01-30 16:30:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint33.pt
2024-01-30 16:30:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint33.pt (epoch 33 @ 48670 updates, score 3.393) (writing took 3.677861531963572 seconds)
2024-01-30 16:30:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-01-30 16:30:14 | INFO | train | epoch 033 | loss 2.641 | nll_loss 1.035 | ppl 2.05 | wps 55213.1 | ups 14.64 | wpb 3771.6 | bsz 195.9 | num_updates 48670 | lr 0.000143341 | gnorm 0.9 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3413
2024-01-30 16:30:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:30:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:30:14 | INFO | fairseq.trainer | begin training epoch 34
2024-01-30 16:30:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:30:16 | INFO | train_inner | epoch 034:     30 / 1475 loss=2.632, nll_loss=1.025, ppl=2.03, wps=29540.4, ups=7.94, wpb=3718.3, bsz=200.2, num_updates=48700, lr=0.000143296, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.9, wall=3415
2024-01-30 16:30:22 | INFO | train_inner | epoch 034:    130 / 1475 loss=2.598, nll_loss=0.984, ppl=1.98, wps=59307.9, ups=15.62, wpb=3796.3, bsz=194.3, num_updates=48800, lr=0.00014315, gnorm=0.859, loss_scale=16, train_wall=6, gb_free=29.6, wall=3422
2024-01-30 16:30:29 | INFO | train_inner | epoch 034:    230 / 1475 loss=2.58, nll_loss=0.965, ppl=1.95, wps=58732, ups=15.52, wpb=3784.7, bsz=206.9, num_updates=48900, lr=0.000143003, gnorm=0.842, loss_scale=16, train_wall=6, gb_free=29.7, wall=3428
2024-01-30 16:30:37 | INFO | train_inner | epoch 034:    330 / 1475 loss=2.599, nll_loss=0.986, ppl=1.98, wps=44818.2, ups=11.62, wpb=3857.7, bsz=207.8, num_updates=49000, lr=0.000142857, gnorm=0.865, loss_scale=16, train_wall=8, gb_free=29.7, wall=3437
2024-01-30 16:30:44 | INFO | train_inner | epoch 034:    430 / 1475 loss=2.605, nll_loss=0.994, ppl=1.99, wps=59488.3, ups=15.59, wpb=3814.6, bsz=196.6, num_updates=49100, lr=0.000142712, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.7, wall=3443
2024-01-30 16:30:50 | INFO | train_inner | epoch 034:    530 / 1475 loss=2.629, nll_loss=1.021, ppl=2.03, wps=58307.9, ups=15.6, wpb=3737.5, bsz=184.6, num_updates=49200, lr=0.000142566, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.7, wall=3449
2024-01-30 16:30:57 | INFO | train_inner | epoch 034:    630 / 1475 loss=2.624, nll_loss=1.015, ppl=2.02, wps=59789.7, ups=15.63, wpb=3826.3, bsz=196.4, num_updates=49300, lr=0.000142422, gnorm=0.873, loss_scale=16, train_wall=6, gb_free=29.9, wall=3456
2024-01-30 16:31:03 | INFO | train_inner | epoch 034:    730 / 1475 loss=2.604, nll_loss=0.994, ppl=1.99, wps=58231.1, ups=15.59, wpb=3735.8, bsz=204, num_updates=49400, lr=0.000142278, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=3462
2024-01-30 16:31:10 | INFO | train_inner | epoch 034:    830 / 1475 loss=2.633, nll_loss=1.027, ppl=2.04, wps=57966.6, ups=15.56, wpb=3724.9, bsz=196.6, num_updates=49500, lr=0.000142134, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.6, wall=3469
2024-01-30 16:31:17 | INFO | train_inner | epoch 034:    930 / 1475 loss=2.665, nll_loss=1.064, ppl=2.09, wps=48423.4, ups=12.95, wpb=3740.7, bsz=183.4, num_updates=49600, lr=0.00014199, gnorm=0.919, loss_scale=16, train_wall=8, gb_free=29.8, wall=3476
2024-01-30 16:31:24 | INFO | train_inner | epoch 034:   1030 / 1475 loss=2.627, nll_loss=1.02, ppl=2.03, wps=58538.6, ups=15.51, wpb=3773.9, bsz=207.7, num_updates=49700, lr=0.000141848, gnorm=0.877, loss_scale=16, train_wall=6, gb_free=29.7, wall=3483
2024-01-30 16:31:30 | INFO | train_inner | epoch 034:   1130 / 1475 loss=2.637, nll_loss=1.03, ppl=2.04, wps=59465, ups=15.65, wpb=3798.8, bsz=201.9, num_updates=49800, lr=0.000141705, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.8, wall=3489
2024-01-30 16:31:37 | INFO | train_inner | epoch 034:   1230 / 1475 loss=2.649, nll_loss=1.045, ppl=2.06, wps=59001, ups=15.56, wpb=3792.8, bsz=195.4, num_updates=49900, lr=0.000141563, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.8, wall=3496
2024-01-30 16:31:43 | INFO | train_inner | epoch 034:   1330 / 1475 loss=2.645, nll_loss=1.041, ppl=2.06, wps=58206, ups=15.59, wpb=3732.7, bsz=192, num_updates=50000, lr=0.000141421, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.8, wall=3502
2024-01-30 16:31:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:31:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:31:45 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.401 | nll_loss 1.754 | ppl 3.37 | wps 134214 | wpb 3189.1 | bsz 163.4 | num_updates 50000 | best_loss 3.385
2024-01-30 16:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2024-01-30 16:31:45 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_34_50000.pt
2024-01-30 16:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_34_50000.pt
2024-01-30 16:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 3.401) (writing took 2.6036661620019004 seconds)
2024-01-30 16:31:54 | INFO | train_inner | epoch 034:   1430 / 1475 loss=2.686, nll_loss=1.087, ppl=2.12, wps=32761.7, ups=8.74, wpb=3749.2, bsz=187.8, num_updates=50100, lr=0.00014128, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.8, wall=3513
2024-01-30 16:31:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:31:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:31:59 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.396 | nll_loss 1.751 | ppl 3.37 | wps 134154 | wpb 3189.1 | bsz 163.4 | num_updates 50145 | best_loss 3.385
2024-01-30 16:31:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50145 updates
2024-01-30 16:31:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint34.pt
2024-01-30 16:32:01 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint34.pt
2024-01-30 16:32:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint34.pt (epoch 34 @ 50145 updates, score 3.396) (writing took 3.7917308979667723 seconds)
2024-01-30 16:32:03 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-01-30 16:32:03 | INFO | train | epoch 034 | loss 2.628 | nll_loss 1.02 | ppl 2.03 | wps 50813.1 | ups 13.47 | wpb 3771.6 | bsz 195.9 | num_updates 50145 | lr 0.000141217 | gnorm 0.891 | loss_scale 16 | train_wall 95 | gb_free 29.7 | wall 3523
2024-01-30 16:32:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:32:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:32:03 | INFO | fairseq.trainer | begin training epoch 35
2024-01-30 16:32:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:32:07 | INFO | train_inner | epoch 035:     55 / 1475 loss=2.612, nll_loss=1.003, ppl=2, wps=29381.2, ups=7.84, wpb=3746.3, bsz=191.2, num_updates=50200, lr=0.000141139, gnorm=0.881, loss_scale=16, train_wall=6, gb_free=29.7, wall=3526
2024-01-30 16:32:14 | INFO | train_inner | epoch 035:    155 / 1475 loss=2.583, nll_loss=0.969, ppl=1.96, wps=58829.5, ups=15.57, wpb=3779.2, bsz=196.3, num_updates=50300, lr=0.000140999, gnorm=0.865, loss_scale=16, train_wall=6, gb_free=29.7, wall=3533
2024-01-30 16:32:20 | INFO | train_inner | epoch 035:    255 / 1475 loss=2.613, nll_loss=1.002, ppl=2, wps=58895.9, ups=15.57, wpb=3782.9, bsz=192.4, num_updates=50400, lr=0.000140859, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.6, wall=3539
2024-01-30 16:32:26 | INFO | train_inner | epoch 035:    355 / 1475 loss=2.623, nll_loss=1.015, ppl=2.02, wps=59343.7, ups=15.58, wpb=3808.9, bsz=191.8, num_updates=50500, lr=0.00014072, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.8, wall=3546
2024-01-30 16:32:33 | INFO | train_inner | epoch 035:    455 / 1475 loss=2.588, nll_loss=0.974, ppl=1.96, wps=58893.2, ups=15.61, wpb=3772.6, bsz=204.6, num_updates=50600, lr=0.00014058, gnorm=0.864, loss_scale=16, train_wall=6, gb_free=29.8, wall=3552
2024-01-30 16:32:39 | INFO | train_inner | epoch 035:    555 / 1475 loss=2.625, nll_loss=1.017, ppl=2.02, wps=59511.3, ups=15.62, wpb=3811, bsz=192.7, num_updates=50700, lr=0.000140442, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.7, wall=3558
2024-01-30 16:32:46 | INFO | train_inner | epoch 035:    655 / 1475 loss=2.583, nll_loss=0.97, ppl=1.96, wps=57953.2, ups=15.49, wpb=3741.6, bsz=209.4, num_updates=50800, lr=0.000140303, gnorm=0.867, loss_scale=16, train_wall=6, gb_free=29.7, wall=3565
2024-01-30 16:32:52 | INFO | train_inner | epoch 035:    755 / 1475 loss=2.614, nll_loss=1.005, ppl=2.01, wps=57497.8, ups=15.64, wpb=3675.9, bsz=189.9, num_updates=50900, lr=0.000140165, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.6, wall=3571
2024-01-30 16:32:58 | INFO | train_inner | epoch 035:    855 / 1475 loss=2.636, nll_loss=1.029, ppl=2.04, wps=59001.7, ups=15.64, wpb=3773.5, bsz=192.1, num_updates=51000, lr=0.000140028, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.7, wall=3578
2024-01-30 16:33:05 | INFO | train_inner | epoch 035:    955 / 1475 loss=2.646, nll_loss=1.04, ppl=2.06, wps=59831.6, ups=15.7, wpb=3811.6, bsz=185.8, num_updates=51100, lr=0.000139891, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.7, wall=3584
2024-01-30 16:33:11 | INFO | train_inner | epoch 035:   1055 / 1475 loss=2.611, nll_loss=1.002, ppl=2, wps=59231.4, ups=15.55, wpb=3809.4, bsz=204.4, num_updates=51200, lr=0.000139754, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.7, wall=3590
2024-01-30 16:33:18 | INFO | train_inner | epoch 035:   1155 / 1475 loss=2.592, nll_loss=0.981, ppl=1.97, wps=58758.6, ups=15.45, wpb=3802.6, bsz=215.8, num_updates=51300, lr=0.000139618, gnorm=0.866, loss_scale=16, train_wall=6, gb_free=29.8, wall=3597
2024-01-30 16:33:24 | INFO | train_inner | epoch 035:   1255 / 1475 loss=2.646, nll_loss=1.042, ppl=2.06, wps=58281.5, ups=15.59, wpb=3737.5, bsz=189.4, num_updates=51400, lr=0.000139482, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=3603
2024-01-30 16:33:31 | INFO | train_inner | epoch 035:   1355 / 1475 loss=2.652, nll_loss=1.048, ppl=2.07, wps=58262.8, ups=15.67, wpb=3717.5, bsz=186.7, num_updates=51500, lr=0.000139347, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=3610
2024-01-30 16:33:37 | INFO | train_inner | epoch 035:   1455 / 1475 loss=2.661, nll_loss=1.057, ppl=2.08, wps=60063.9, ups=15.69, wpb=3828.4, bsz=193.4, num_updates=51600, lr=0.000139212, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=3616
2024-01-30 16:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:33:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:33:40 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.413 | nll_loss 1.762 | ppl 3.39 | wps 135179 | wpb 3189.1 | bsz 163.4 | num_updates 51620 | best_loss 3.385
2024-01-30 16:33:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 51620 updates
2024-01-30 16:33:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint35.pt
2024-01-30 16:33:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint35.pt
2024-01-30 16:33:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint35.pt (epoch 35 @ 51620 updates, score 3.413) (writing took 2.3426056030439213 seconds)
2024-01-30 16:33:43 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-01-30 16:33:43 | INFO | train | epoch 035 | loss 2.618 | nll_loss 1.009 | ppl 2.01 | wps 55970.1 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 51620 | lr 0.000139185 | gnorm 0.891 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3622
2024-01-30 16:33:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:33:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:33:43 | INFO | fairseq.trainer | begin training epoch 36
2024-01-30 16:33:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:33:48 | INFO | train_inner | epoch 036:     80 / 1475 loss=2.595, nll_loss=0.982, ppl=1.97, wps=33148.2, ups=8.92, wpb=3714.3, bsz=186.3, num_updates=51700, lr=0.000139077, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.8, wall=3627
2024-01-30 16:33:55 | INFO | train_inner | epoch 036:    180 / 1475 loss=2.568, nll_loss=0.952, ppl=1.93, wps=59104.8, ups=15.47, wpb=3819.8, bsz=202, num_updates=51800, lr=0.000138943, gnorm=0.848, loss_scale=16, train_wall=6, gb_free=29.7, wall=3634
2024-01-30 16:34:01 | INFO | train_inner | epoch 036:    280 / 1475 loss=2.583, nll_loss=0.968, ppl=1.96, wps=58162.7, ups=15.59, wpb=3730.2, bsz=200.9, num_updates=51900, lr=0.000138809, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.8, wall=3640
2024-01-30 16:34:07 | INFO | train_inner | epoch 036:    380 / 1475 loss=2.632, nll_loss=1.024, ppl=2.03, wps=59401.1, ups=15.67, wpb=3789.8, bsz=180, num_updates=52000, lr=0.000138675, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=3646
2024-01-30 16:34:14 | INFO | train_inner | epoch 036:    480 / 1475 loss=2.601, nll_loss=0.99, ppl=1.99, wps=59008.1, ups=15.53, wpb=3799.7, bsz=192.2, num_updates=52100, lr=0.000138542, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.7, wall=3653
2024-01-30 16:34:20 | INFO | train_inner | epoch 036:    580 / 1475 loss=2.604, nll_loss=0.993, ppl=1.99, wps=59414.8, ups=15.65, wpb=3797.1, bsz=198.7, num_updates=52200, lr=0.000138409, gnorm=0.884, loss_scale=16, train_wall=6, gb_free=29.7, wall=3659
2024-01-30 16:34:27 | INFO | train_inner | epoch 036:    680 / 1475 loss=2.594, nll_loss=0.982, ppl=1.97, wps=57823.5, ups=15.58, wpb=3711.8, bsz=201.6, num_updates=52300, lr=0.000138277, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.7, wall=3666
2024-01-30 16:34:33 | INFO | train_inner | epoch 036:    780 / 1475 loss=2.614, nll_loss=1.005, ppl=2.01, wps=58138.2, ups=15.57, wpb=3733.9, bsz=197.8, num_updates=52400, lr=0.000138145, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.7, wall=3672
2024-01-30 16:34:39 | INFO | train_inner | epoch 036:    880 / 1475 loss=2.608, nll_loss=0.998, ppl=2, wps=58163.2, ups=15.66, wpb=3714.2, bsz=194.3, num_updates=52500, lr=0.000138013, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=3679
2024-01-30 16:34:46 | INFO | train_inner | epoch 036:    980 / 1475 loss=2.618, nll_loss=1.009, ppl=2.01, wps=57669.5, ups=15.65, wpb=3684.1, bsz=188.1, num_updates=52600, lr=0.000137882, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.8, wall=3685
2024-01-30 16:34:52 | INFO | train_inner | epoch 036:   1080 / 1475 loss=2.599, nll_loss=0.988, ppl=1.98, wps=60224.7, ups=15.59, wpb=3863.2, bsz=208.5, num_updates=52700, lr=0.000137751, gnorm=0.874, loss_scale=16, train_wall=6, gb_free=29.7, wall=3691
2024-01-30 16:34:59 | INFO | train_inner | epoch 036:   1180 / 1475 loss=2.644, nll_loss=1.038, ppl=2.05, wps=60066.4, ups=15.66, wpb=3834.7, bsz=185.8, num_updates=52800, lr=0.00013762, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.7, wall=3698
2024-01-30 16:35:05 | INFO | train_inner | epoch 036:   1280 / 1475 loss=2.62, nll_loss=1.012, ppl=2.02, wps=59057.6, ups=15.62, wpb=3780.2, bsz=195.8, num_updates=52900, lr=0.00013749, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.7, wall=3704
2024-01-30 16:35:11 | INFO | train_inner | epoch 036:   1380 / 1475 loss=2.588, nll_loss=0.976, ppl=1.97, wps=59364.6, ups=15.5, wpb=3829.6, bsz=211.3, num_updates=53000, lr=0.000137361, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.7, wall=3711
2024-01-30 16:35:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:35:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:35:20 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.404 | nll_loss 1.758 | ppl 3.38 | wps 134540 | wpb 3189.1 | bsz 163.4 | num_updates 53095 | best_loss 3.385
2024-01-30 16:35:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 53095 updates
2024-01-30 16:35:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint36.pt
2024-01-30 16:35:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint36.pt
2024-01-30 16:35:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint36.pt (epoch 36 @ 53095 updates, score 3.404) (writing took 2.3671814929693937 seconds)
2024-01-30 16:35:22 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-01-30 16:35:22 | INFO | train | epoch 036 | loss 2.606 | nll_loss 0.995 | ppl 1.99 | wps 55957.9 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 53095 | lr 0.000137238 | gnorm 0.895 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 3721
2024-01-30 16:35:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:35:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:35:22 | INFO | fairseq.trainer | begin training epoch 37
2024-01-30 16:35:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:35:23 | INFO | train_inner | epoch 037:      5 / 1475 loss=2.631, nll_loss=1.025, ppl=2.03, wps=33213.7, ups=8.91, wpb=3727.8, bsz=189.8, num_updates=53100, lr=0.000137231, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=3722
2024-01-30 16:35:29 | INFO | train_inner | epoch 037:    105 / 1475 loss=2.55, nll_loss=0.931, ppl=1.91, wps=58098.4, ups=15.52, wpb=3744.5, bsz=196.8, num_updates=53200, lr=0.000137102, gnorm=0.881, loss_scale=16, train_wall=6, gb_free=29.7, wall=3728
2024-01-30 16:35:36 | INFO | train_inner | epoch 037:    205 / 1475 loss=2.554, nll_loss=0.935, ppl=1.91, wps=58596.6, ups=15.57, wpb=3764.3, bsz=203, num_updates=53300, lr=0.000136973, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.8, wall=3735
2024-01-30 16:35:42 | INFO | train_inner | epoch 037:    305 / 1475 loss=2.591, nll_loss=0.979, ppl=1.97, wps=59000.1, ups=15.6, wpb=3782.6, bsz=191.4, num_updates=53400, lr=0.000136845, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.8, wall=3741
2024-01-30 16:35:48 | INFO | train_inner | epoch 037:    405 / 1475 loss=2.629, nll_loss=1.022, ppl=2.03, wps=59252.1, ups=15.77, wpb=3756.6, bsz=171.1, num_updates=53500, lr=0.000136717, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=3747
2024-01-30 16:35:55 | INFO | train_inner | epoch 037:    505 / 1475 loss=2.582, nll_loss=0.968, ppl=1.96, wps=59190.7, ups=15.59, wpb=3796.2, bsz=196.6, num_updates=53600, lr=0.00013659, gnorm=0.882, loss_scale=16, train_wall=6, gb_free=29.7, wall=3754
2024-01-30 16:36:01 | INFO | train_inner | epoch 037:    605 / 1475 loss=2.582, nll_loss=0.969, ppl=1.96, wps=58834.5, ups=15.56, wpb=3781.1, bsz=203.6, num_updates=53700, lr=0.000136462, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.7, wall=3760
2024-01-30 16:36:08 | INFO | train_inner | epoch 037:    705 / 1475 loss=2.603, nll_loss=0.991, ppl=1.99, wps=59883.4, ups=15.67, wpb=3821.3, bsz=196.1, num_updates=53800, lr=0.000136335, gnorm=0.888, loss_scale=16, train_wall=6, gb_free=29.7, wall=3767
2024-01-30 16:36:14 | INFO | train_inner | epoch 037:    805 / 1475 loss=2.569, nll_loss=0.954, ppl=1.94, wps=59103.9, ups=15.54, wpb=3804.2, bsz=202.9, num_updates=53900, lr=0.000136209, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=3773
2024-01-30 16:36:20 | INFO | train_inner | epoch 037:    905 / 1475 loss=2.62, nll_loss=1.011, ppl=2.02, wps=57871.1, ups=15.71, wpb=3683.6, bsz=186.6, num_updates=54000, lr=0.000136083, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.8, wall=3779
2024-01-30 16:36:52 | INFO | train_inner | epoch 037:   1005 / 1475 loss=2.628, nll_loss=1.021, ppl=2.03, wps=11878.1, ups=3.13, wpb=3796.9, bsz=195.7, num_updates=54100, lr=0.000135957, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.8, wall=3811
2024-01-30 16:36:59 | INFO | train_inner | epoch 037:   1105 / 1475 loss=2.595, nll_loss=0.984, ppl=1.98, wps=58596.7, ups=15.54, wpb=3771.6, bsz=201.2, num_updates=54200, lr=0.000135831, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.7, wall=3818
2024-01-30 16:37:05 | INFO | train_inner | epoch 037:   1205 / 1475 loss=2.596, nll_loss=0.985, ppl=1.98, wps=58218.4, ups=15.48, wpb=3760.9, bsz=202.3, num_updates=54300, lr=0.000135706, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.8, wall=3824
2024-01-30 16:37:12 | INFO | train_inner | epoch 037:   1305 / 1475 loss=2.618, nll_loss=1.01, ppl=2.01, wps=58260.9, ups=15.67, wpb=3716.8, bsz=190.5, num_updates=54400, lr=0.000135582, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=3831
2024-01-30 16:37:18 | INFO | train_inner | epoch 037:   1405 / 1475 loss=2.626, nll_loss=1.019, ppl=2.03, wps=59603.7, ups=15.53, wpb=3837.3, bsz=200.4, num_updates=54500, lr=0.000135457, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.8, wall=3837
2024-01-30 16:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:37:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:37:26 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.4 | nll_loss 1.755 | ppl 3.37 | wps 134682 | wpb 3189.1 | bsz 163.4 | num_updates 54570 | best_loss 3.385
2024-01-30 16:37:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 54570 updates
2024-01-30 16:37:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint37.pt
2024-01-30 16:37:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint37.pt
2024-01-30 16:37:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint37.pt (epoch 37 @ 54570 updates, score 3.4) (writing took 2.330131546012126 seconds)
2024-01-30 16:37:28 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-01-30 16:37:28 | INFO | train | epoch 037 | loss 2.596 | nll_loss 0.984 | ppl 1.98 | wps 44270.5 | ups 11.74 | wpb 3771.6 | bsz 195.9 | num_updates 54570 | lr 0.00013537 | gnorm 0.899 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 3847
2024-01-30 16:37:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:37:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:37:28 | INFO | fairseq.trainer | begin training epoch 38
2024-01-30 16:37:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:37:30 | INFO | train_inner | epoch 038:     30 / 1475 loss=2.589, nll_loss=0.976, ppl=1.97, wps=31685.5, ups=8.36, wpb=3789.2, bsz=202, num_updates=54600, lr=0.000135333, gnorm=0.891, loss_scale=16, train_wall=6, gb_free=29.7, wall=3849
2024-01-30 16:37:36 | INFO | train_inner | epoch 038:    130 / 1475 loss=2.543, nll_loss=0.923, ppl=1.9, wps=59055.2, ups=15.55, wpb=3796.8, bsz=203.3, num_updates=54700, lr=0.000135209, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=3856
2024-01-30 16:37:43 | INFO | train_inner | epoch 038:    230 / 1475 loss=2.574, nll_loss=0.959, ppl=1.94, wps=58110.3, ups=15.66, wpb=3710.7, bsz=187.6, num_updates=54800, lr=0.000135086, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.8, wall=3862
2024-01-30 16:37:49 | INFO | train_inner | epoch 038:    330 / 1475 loss=2.574, nll_loss=0.959, ppl=1.94, wps=59737.2, ups=15.56, wpb=3840.2, bsz=199.6, num_updates=54900, lr=0.000134963, gnorm=0.864, loss_scale=16, train_wall=6, gb_free=29.7, wall=3868
2024-01-30 16:37:56 | INFO | train_inner | epoch 038:    430 / 1475 loss=2.55, nll_loss=0.932, ppl=1.91, wps=58687.5, ups=15.47, wpb=3792.9, bsz=205.8, num_updates=55000, lr=0.00013484, gnorm=0.874, loss_scale=16, train_wall=6, gb_free=29.7, wall=3875
2024-01-30 16:38:02 | INFO | train_inner | epoch 038:    530 / 1475 loss=2.571, nll_loss=0.955, ppl=1.94, wps=58401.8, ups=15.53, wpb=3761.1, bsz=198.6, num_updates=55100, lr=0.000134718, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.7, wall=3881
2024-01-30 16:38:09 | INFO | train_inner | epoch 038:    630 / 1475 loss=2.574, nll_loss=0.959, ppl=1.94, wps=59195.9, ups=15.62, wpb=3788.6, bsz=199, num_updates=55200, lr=0.000134595, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=3888
2024-01-30 16:38:15 | INFO | train_inner | epoch 038:    730 / 1475 loss=2.587, nll_loss=0.975, ppl=1.97, wps=58859, ups=15.48, wpb=3803.2, bsz=194.2, num_updates=55300, lr=0.000134474, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.9, wall=3894
2024-01-30 16:38:22 | INFO | train_inner | epoch 038:    830 / 1475 loss=2.603, nll_loss=0.993, ppl=1.99, wps=50890.4, ups=13.48, wpb=3774.1, bsz=195.8, num_updates=55400, lr=0.000134352, gnorm=0.901, loss_scale=16, train_wall=7, gb_free=29.7, wall=3902
2024-01-30 16:38:33 | INFO | train_inner | epoch 038:    930 / 1475 loss=2.586, nll_loss=0.973, ppl=1.96, wps=34455.7, ups=9.09, wpb=3788.4, bsz=200.2, num_updates=55500, lr=0.000134231, gnorm=0.885, loss_scale=16, train_wall=11, gb_free=29.7, wall=3912
2024-01-30 16:38:40 | INFO | train_inner | epoch 038:   1030 / 1475 loss=2.611, nll_loss=1.003, ppl=2, wps=59023, ups=15.6, wpb=3783.4, bsz=192.3, num_updates=55600, lr=0.00013411, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=3919
2024-01-30 16:38:46 | INFO | train_inner | epoch 038:   1130 / 1475 loss=2.618, nll_loss=1.009, ppl=2.01, wps=59160.9, ups=15.64, wpb=3782.4, bsz=188.8, num_updates=55700, lr=0.00013399, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=3925
2024-01-30 16:38:53 | INFO | train_inner | epoch 038:   1230 / 1475 loss=2.597, nll_loss=0.986, ppl=1.98, wps=58253, ups=15.64, wpb=3723.9, bsz=196.7, num_updates=55800, lr=0.00013387, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=3932
2024-01-30 16:38:59 | INFO | train_inner | epoch 038:   1330 / 1475 loss=2.599, nll_loss=0.99, ppl=1.99, wps=58358, ups=15.6, wpb=3741.5, bsz=197.2, num_updates=55900, lr=0.00013375, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.6, wall=3938
2024-01-30 16:39:05 | INFO | train_inner | epoch 038:   1430 / 1475 loss=2.616, nll_loss=1.008, ppl=2.01, wps=57612.4, ups=15.62, wpb=3688.5, bsz=187.4, num_updates=56000, lr=0.000133631, gnorm=0.952, loss_scale=16, train_wall=6, gb_free=29.7, wall=3945
2024-01-30 16:39:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:39:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:39:12 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.4 | nll_loss 1.757 | ppl 3.38 | wps 86517.2 | wpb 3189.1 | bsz 163.4 | num_updates 56045 | best_loss 3.385
2024-01-30 16:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 56045 updates
2024-01-30 16:39:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint38.pt
2024-01-30 16:39:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint38.pt
2024-01-30 16:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint38.pt (epoch 38 @ 56045 updates, score 3.4) (writing took 2.5732620280468836 seconds)
2024-01-30 16:39:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-01-30 16:39:14 | INFO | train | epoch 038 | loss 2.586 | nll_loss 0.974 | ppl 1.96 | wps 52230.4 | ups 13.85 | wpb 3771.6 | bsz 195.9 | num_updates 56045 | lr 0.000133577 | gnorm 0.898 | loss_scale 16 | train_wall 97 | gb_free 29.8 | wall 3954
2024-01-30 16:39:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:39:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:39:14 | INFO | fairseq.trainer | begin training epoch 39
2024-01-30 16:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:39:18 | INFO | train_inner | epoch 039:     55 / 1475 loss=2.585, nll_loss=0.97, ppl=1.96, wps=29981.8, ups=7.89, wpb=3798, bsz=192.3, num_updates=56100, lr=0.000133511, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.8, wall=3957
2024-01-30 16:39:25 | INFO | train_inner | epoch 039:    155 / 1475 loss=2.547, nll_loss=0.928, ppl=1.9, wps=58434.4, ups=15.51, wpb=3768.6, bsz=200.8, num_updates=56200, lr=0.000133393, gnorm=0.869, loss_scale=16, train_wall=6, gb_free=29.6, wall=3964
2024-01-30 16:39:31 | INFO | train_inner | epoch 039:    255 / 1475 loss=2.569, nll_loss=0.953, ppl=1.94, wps=59546.5, ups=15.64, wpb=3807.2, bsz=193.1, num_updates=56300, lr=0.000133274, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.8, wall=3970
2024-01-30 16:39:37 | INFO | train_inner | epoch 039:    355 / 1475 loss=2.553, nll_loss=0.935, ppl=1.91, wps=59152.9, ups=15.62, wpb=3786.5, bsz=194.6, num_updates=56400, lr=0.000133156, gnorm=0.867, loss_scale=16, train_wall=6, gb_free=29.7, wall=3976
2024-01-30 16:39:44 | INFO | train_inner | epoch 039:    455 / 1475 loss=2.566, nll_loss=0.951, ppl=1.93, wps=58147.3, ups=15.55, wpb=3738.7, bsz=202.3, num_updates=56500, lr=0.000133038, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.8, wall=3983
2024-01-30 16:39:50 | INFO | train_inner | epoch 039:    555 / 1475 loss=2.56, nll_loss=0.945, ppl=1.92, wps=59322.8, ups=15.5, wpb=3826.9, bsz=201.6, num_updates=56600, lr=0.00013292, gnorm=0.878, loss_scale=16, train_wall=6, gb_free=29.7, wall=3989
2024-01-30 16:39:57 | INFO | train_inner | epoch 039:    655 / 1475 loss=2.57, nll_loss=0.955, ppl=1.94, wps=58246.6, ups=15.52, wpb=3752.6, bsz=198.4, num_updates=56700, lr=0.000132803, gnorm=0.895, loss_scale=32, train_wall=6, gb_free=29.7, wall=3996
2024-01-30 16:40:03 | INFO | train_inner | epoch 039:    755 / 1475 loss=2.578, nll_loss=0.964, ppl=1.95, wps=58598.1, ups=15.63, wpb=3749.5, bsz=196.2, num_updates=56800, lr=0.000132686, gnorm=0.91, loss_scale=32, train_wall=6, gb_free=29.7, wall=4002
2024-01-30 16:40:09 | INFO | train_inner | epoch 039:    855 / 1475 loss=2.57, nll_loss=0.957, ppl=1.94, wps=58435.2, ups=15.63, wpb=3739.3, bsz=197.2, num_updates=56900, lr=0.00013257, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=4009
2024-01-30 16:40:16 | INFO | train_inner | epoch 039:    955 / 1475 loss=2.579, nll_loss=0.966, ppl=1.95, wps=59075.7, ups=15.62, wpb=3781.7, bsz=196.2, num_updates=57000, lr=0.000132453, gnorm=0.9, loss_scale=32, train_wall=6, gb_free=29.7, wall=4015
2024-01-30 16:40:22 | INFO | train_inner | epoch 039:   1055 / 1475 loss=2.595, nll_loss=0.984, ppl=1.98, wps=59068.4, ups=15.65, wpb=3773.4, bsz=190.3, num_updates=57100, lr=0.000132337, gnorm=0.895, loss_scale=32, train_wall=6, gb_free=29.8, wall=4021
2024-01-30 16:40:29 | INFO | train_inner | epoch 039:   1155 / 1475 loss=2.611, nll_loss=1.002, ppl=2, wps=58447.7, ups=15.68, wpb=3726.4, bsz=186.8, num_updates=57200, lr=0.000132221, gnorm=0.955, loss_scale=32, train_wall=6, gb_free=29.6, wall=4028
2024-01-30 16:40:35 | INFO | train_inner | epoch 039:   1255 / 1475 loss=2.587, nll_loss=0.975, ppl=1.96, wps=59444.4, ups=15.6, wpb=3809.8, bsz=203.8, num_updates=57300, lr=0.000132106, gnorm=0.89, loss_scale=32, train_wall=6, gb_free=29.7, wall=4034
2024-01-30 16:40:41 | INFO | train_inner | epoch 039:   1355 / 1475 loss=2.599, nll_loss=0.989, ppl=1.99, wps=58760.7, ups=15.6, wpb=3767.7, bsz=188.4, num_updates=57400, lr=0.000131991, gnorm=0.922, loss_scale=32, train_wall=6, gb_free=29.7, wall=4041
2024-01-30 16:40:48 | INFO | train_inner | epoch 039:   1455 / 1475 loss=2.586, nll_loss=0.975, ppl=1.97, wps=58808.1, ups=15.62, wpb=3766, bsz=194.2, num_updates=57500, lr=0.000131876, gnorm=0.908, loss_scale=32, train_wall=6, gb_free=29.8, wall=4047
2024-01-30 16:40:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:40:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:40:51 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.404 | nll_loss 1.759 | ppl 3.38 | wps 134945 | wpb 3189.1 | bsz 163.4 | num_updates 57520 | best_loss 3.385
2024-01-30 16:40:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 57520 updates
2024-01-30 16:40:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint39.pt
2024-01-30 16:40:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint39.pt
2024-01-30 16:40:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint39.pt (epoch 39 @ 57520 updates, score 3.404) (writing took 2.4076314399717376 seconds)
2024-01-30 16:40:54 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-01-30 16:40:54 | INFO | train | epoch 039 | loss 2.576 | nll_loss 0.962 | ppl 1.95 | wps 55956.3 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 57520 | lr 0.000131853 | gnorm 0.897 | loss_scale 32 | train_wall 91 | gb_free 29.8 | wall 4053
2024-01-30 16:40:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:40:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:40:54 | INFO | fairseq.trainer | begin training epoch 40
2024-01-30 16:40:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:40:59 | INFO | train_inner | epoch 040:     80 / 1475 loss=2.551, nll_loss=0.933, ppl=1.91, wps=33455.8, ups=8.87, wpb=3772.4, bsz=191.2, num_updates=57600, lr=0.000131762, gnorm=0.882, loss_scale=32, train_wall=6, gb_free=29.6, wall=4058
2024-01-30 16:41:05 | INFO | train_inner | epoch 040:    180 / 1475 loss=2.57, nll_loss=0.954, ppl=1.94, wps=59015.7, ups=15.65, wpb=3770, bsz=181.4, num_updates=57700, lr=0.000131647, gnorm=0.915, loss_scale=32, train_wall=6, gb_free=29.7, wall=4065
2024-01-30 16:41:12 | INFO | train_inner | epoch 040:    280 / 1475 loss=2.558, nll_loss=0.94, ppl=1.92, wps=58316.5, ups=15.67, wpb=3720.6, bsz=195, num_updates=57800, lr=0.000131533, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.7, wall=4071
2024-01-30 16:41:18 | INFO | train_inner | epoch 040:    380 / 1475 loss=2.553, nll_loss=0.936, ppl=1.91, wps=59348.8, ups=15.59, wpb=3807.9, bsz=201.8, num_updates=57900, lr=0.00013142, gnorm=0.88, loss_scale=32, train_wall=6, gb_free=29.7, wall=4077
2024-01-30 16:41:25 | INFO | train_inner | epoch 040:    480 / 1475 loss=2.572, nll_loss=0.957, ppl=1.94, wps=59246.9, ups=15.62, wpb=3793.6, bsz=185.6, num_updates=58000, lr=0.000131306, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.7, wall=4084
2024-01-30 16:41:31 | INFO | train_inner | epoch 040:    580 / 1475 loss=2.552, nll_loss=0.935, ppl=1.91, wps=58327.1, ups=15.58, wpb=3744.9, bsz=201.3, num_updates=58100, lr=0.000131193, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=4090
2024-01-30 16:41:37 | INFO | train_inner | epoch 040:    680 / 1475 loss=2.574, nll_loss=0.96, ppl=1.94, wps=59202.5, ups=15.65, wpb=3781.8, bsz=196.5, num_updates=58200, lr=0.000131081, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.7, wall=4097
2024-01-30 16:41:44 | INFO | train_inner | epoch 040:    780 / 1475 loss=2.57, nll_loss=0.956, ppl=1.94, wps=60318.3, ups=15.64, wpb=3857.8, bsz=195.4, num_updates=58300, lr=0.000130968, gnorm=0.892, loss_scale=32, train_wall=6, gb_free=29.7, wall=4103
2024-01-30 16:41:50 | INFO | train_inner | epoch 040:    880 / 1475 loss=2.569, nll_loss=0.953, ppl=1.94, wps=58318.8, ups=15.62, wpb=3733.7, bsz=201.4, num_updates=58400, lr=0.000130856, gnorm=0.915, loss_scale=32, train_wall=6, gb_free=29.7, wall=4109
2024-01-30 16:41:57 | INFO | train_inner | epoch 040:    980 / 1475 loss=2.582, nll_loss=0.968, ppl=1.96, wps=58569.4, ups=15.59, wpb=3757.4, bsz=190.7, num_updates=58500, lr=0.000130744, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=4116
2024-01-30 16:42:03 | INFO | train_inner | epoch 040:   1080 / 1475 loss=2.559, nll_loss=0.944, ppl=1.92, wps=57698.7, ups=15.6, wpb=3699, bsz=198.2, num_updates=58600, lr=0.000130632, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=4122
2024-01-30 16:42:10 | INFO | train_inner | epoch 040:   1180 / 1475 loss=2.56, nll_loss=0.945, ppl=1.93, wps=58502, ups=15.64, wpb=3741.3, bsz=205.8, num_updates=58700, lr=0.000130521, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.7, wall=4129
2024-01-30 16:42:16 | INFO | train_inner | epoch 040:   1280 / 1475 loss=2.563, nll_loss=0.948, ppl=1.93, wps=58926.3, ups=15.47, wpb=3808.7, bsz=204.1, num_updates=58800, lr=0.00013041, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.7, wall=4135
2024-01-30 16:42:22 | INFO | train_inner | epoch 040:   1380 / 1475 loss=2.595, nll_loss=0.984, ppl=1.98, wps=59921.1, ups=15.61, wpb=3838.6, bsz=200.1, num_updates=58900, lr=0.000130299, gnorm=0.905, loss_scale=32, train_wall=6, gb_free=29.7, wall=4142
2024-01-30 16:42:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:42:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:42:31 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.394 | nll_loss 1.753 | ppl 3.37 | wps 135140 | wpb 3189.1 | bsz 163.4 | num_updates 58995 | best_loss 3.385
2024-01-30 16:42:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 58995 updates
2024-01-30 16:42:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint40.pt
2024-01-30 16:42:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint40.pt
2024-01-30 16:42:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint40.pt (epoch 40 @ 58995 updates, score 3.394) (writing took 3.7013378229457885 seconds)
2024-01-30 16:42:35 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-01-30 16:42:35 | INFO | train | epoch 040 | loss 2.567 | nll_loss 0.952 | ppl 1.93 | wps 55251.9 | ups 14.65 | wpb 3771.6 | bsz 195.9 | num_updates 58995 | lr 0.000130194 | gnorm 0.901 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 4154
2024-01-30 16:42:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:42:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:42:35 | INFO | fairseq.trainer | begin training epoch 41
2024-01-30 16:42:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:42:35 | INFO | train_inner | epoch 041:      5 / 1475 loss=2.588, nll_loss=0.976, ppl=1.97, wps=29321.9, ups=7.95, wpb=3688.1, bsz=189.8, num_updates=59000, lr=0.000130189, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=30, wall=4154
2024-01-30 16:42:41 | INFO | train_inner | epoch 041:    105 / 1475 loss=2.542, nll_loss=0.921, ppl=1.89, wps=58577.4, ups=15.62, wpb=3749, bsz=193.4, num_updates=59100, lr=0.000130079, gnorm=0.91, loss_scale=32, train_wall=6, gb_free=29.7, wall=4160
2024-01-30 16:42:48 | INFO | train_inner | epoch 041:    205 / 1475 loss=2.534, nll_loss=0.914, ppl=1.88, wps=59665.3, ups=15.59, wpb=3827.9, bsz=199.6, num_updates=59200, lr=0.000129969, gnorm=0.867, loss_scale=32, train_wall=6, gb_free=29.7, wall=4167
2024-01-30 16:42:54 | INFO | train_inner | epoch 041:    305 / 1475 loss=2.553, nll_loss=0.936, ppl=1.91, wps=59083.1, ups=15.68, wpb=3768.5, bsz=188.4, num_updates=59300, lr=0.000129859, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.7, wall=4173
2024-01-30 16:43:01 | INFO | train_inner | epoch 041:    405 / 1475 loss=2.551, nll_loss=0.934, ppl=1.91, wps=57999.5, ups=15.65, wpb=3706.4, bsz=193.7, num_updates=59400, lr=0.00012975, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.7, wall=4180
2024-01-30 16:43:07 | INFO | train_inner | epoch 041:    505 / 1475 loss=2.566, nll_loss=0.95, ppl=1.93, wps=58371.4, ups=15.66, wpb=3726.7, bsz=184.8, num_updates=59500, lr=0.000129641, gnorm=0.922, loss_scale=32, train_wall=6, gb_free=29.7, wall=4186
2024-01-30 16:43:13 | INFO | train_inner | epoch 041:    605 / 1475 loss=2.55, nll_loss=0.933, ppl=1.91, wps=59486, ups=15.52, wpb=3832.6, bsz=198.2, num_updates=59600, lr=0.000129532, gnorm=0.878, loss_scale=32, train_wall=6, gb_free=29.7, wall=4192
2024-01-30 16:43:20 | INFO | train_inner | epoch 041:    705 / 1475 loss=2.541, nll_loss=0.922, ppl=1.89, wps=58198.8, ups=15.65, wpb=3717.6, bsz=209, num_updates=59700, lr=0.000129423, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.7, wall=4199
2024-01-30 16:43:26 | INFO | train_inner | epoch 041:    805 / 1475 loss=2.534, nll_loss=0.915, ppl=1.89, wps=57574.1, ups=15.54, wpb=3705, bsz=205.8, num_updates=59800, lr=0.000129315, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.7, wall=4205
2024-01-30 16:43:33 | INFO | train_inner | epoch 041:    905 / 1475 loss=2.543, nll_loss=0.925, ppl=1.9, wps=58709.2, ups=15.47, wpb=3795.7, bsz=205.6, num_updates=59900, lr=0.000129207, gnorm=0.88, loss_scale=32, train_wall=6, gb_free=29.7, wall=4212
2024-01-30 16:43:39 | INFO | train_inner | epoch 041:   1005 / 1475 loss=2.55, nll_loss=0.933, ppl=1.91, wps=59514.9, ups=15.65, wpb=3804, bsz=205.3, num_updates=60000, lr=0.000129099, gnorm=0.881, loss_scale=32, train_wall=6, gb_free=29.7, wall=4218
2024-01-30 16:43:45 | INFO | train_inner | epoch 041:   1105 / 1475 loss=2.596, nll_loss=0.984, ppl=1.98, wps=60001, ups=15.71, wpb=3818.5, bsz=185, num_updates=60100, lr=0.000128992, gnorm=0.919, loss_scale=32, train_wall=6, gb_free=29.7, wall=4225
2024-01-30 16:43:52 | INFO | train_inner | epoch 041:   1205 / 1475 loss=2.585, nll_loss=0.973, ppl=1.96, wps=58872.2, ups=15.51, wpb=3796.1, bsz=197.2, num_updates=60200, lr=0.000128885, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=4231
2024-01-30 16:43:58 | INFO | train_inner | epoch 041:   1305 / 1475 loss=2.579, nll_loss=0.966, ppl=1.95, wps=58704.1, ups=15.6, wpb=3762.5, bsz=186.8, num_updates=60300, lr=0.000128778, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.8, wall=4237
2024-01-30 16:44:05 | INFO | train_inner | epoch 041:   1405 / 1475 loss=2.574, nll_loss=0.961, ppl=1.95, wps=59030.5, ups=15.56, wpb=3794.3, bsz=193.8, num_updates=60400, lr=0.000128671, gnorm=0.91, loss_scale=32, train_wall=6, gb_free=29.7, wall=4244
2024-01-30 16:44:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:44:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:44:11 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.402 | nll_loss 1.76 | ppl 3.39 | wps 133933 | wpb 3189.1 | bsz 163.4 | num_updates 60470 | best_loss 3.385
2024-01-30 16:44:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 60470 updates
2024-01-30 16:44:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint41.pt
2024-01-30 16:44:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint41.pt
2024-01-30 16:44:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint41.pt (epoch 41 @ 60470 updates, score 3.402) (writing took 2.3666583669837564 seconds)
2024-01-30 16:44:14 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-01-30 16:44:14 | INFO | train | epoch 041 | loss 2.558 | nll_loss 0.942 | ppl 1.92 | wps 55974.7 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 60470 | lr 0.000128597 | gnorm 0.903 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 4253
2024-01-30 16:44:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:44:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:44:14 | INFO | fairseq.trainer | begin training epoch 42
2024-01-30 16:44:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:44:16 | INFO | train_inner | epoch 042:     30 / 1475 loss=2.553, nll_loss=0.936, ppl=1.91, wps=33673.4, ups=8.87, wpb=3796.1, bsz=193.3, num_updates=60500, lr=0.000128565, gnorm=0.896, loss_scale=32, train_wall=6, gb_free=29.7, wall=4255
2024-01-30 16:44:22 | INFO | train_inner | epoch 042:    130 / 1475 loss=2.528, nll_loss=0.907, ppl=1.88, wps=59725.3, ups=15.65, wpb=3816.1, bsz=194.3, num_updates=60600, lr=0.000128459, gnorm=0.878, loss_scale=32, train_wall=6, gb_free=29.7, wall=4261
2024-01-30 16:44:29 | INFO | train_inner | epoch 042:    230 / 1475 loss=2.526, nll_loss=0.904, ppl=1.87, wps=59333.9, ups=15.59, wpb=3805, bsz=200.2, num_updates=60700, lr=0.000128353, gnorm=0.873, loss_scale=32, train_wall=6, gb_free=29.7, wall=4268
2024-01-30 16:44:35 | INFO | train_inner | epoch 042:    330 / 1475 loss=2.539, nll_loss=0.921, ppl=1.89, wps=57418.2, ups=15.61, wpb=3679.1, bsz=190.6, num_updates=60800, lr=0.000128247, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.7, wall=4274
2024-01-30 16:44:42 | INFO | train_inner | epoch 042:    430 / 1475 loss=2.554, nll_loss=0.937, ppl=1.91, wps=59402.9, ups=15.68, wpb=3789.3, bsz=189.2, num_updates=60900, lr=0.000128142, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.7, wall=4281
2024-01-30 16:44:48 | INFO | train_inner | epoch 042:    530 / 1475 loss=2.545, nll_loss=0.926, ppl=1.9, wps=58582.5, ups=15.53, wpb=3773.4, bsz=192.4, num_updates=61000, lr=0.000128037, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.7, wall=4287
2024-01-30 16:44:54 | INFO | train_inner | epoch 042:    630 / 1475 loss=2.55, nll_loss=0.933, ppl=1.91, wps=58139.4, ups=15.67, wpb=3711, bsz=199.3, num_updates=61100, lr=0.000127932, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.8, wall=4294
2024-01-30 16:45:01 | INFO | train_inner | epoch 042:    730 / 1475 loss=2.553, nll_loss=0.936, ppl=1.91, wps=59287, ups=15.65, wpb=3787.6, bsz=199.7, num_updates=61200, lr=0.000127827, gnorm=0.892, loss_scale=32, train_wall=6, gb_free=29.7, wall=4300
2024-01-30 16:45:07 | INFO | train_inner | epoch 042:    830 / 1475 loss=2.542, nll_loss=0.925, ppl=1.9, wps=59719.3, ups=15.52, wpb=3847.9, bsz=204.4, num_updates=61300, lr=0.000127723, gnorm=0.88, loss_scale=32, train_wall=6, gb_free=29.7, wall=4306
2024-01-30 16:45:14 | INFO | train_inner | epoch 042:    930 / 1475 loss=2.573, nll_loss=0.959, ppl=1.94, wps=59565.6, ups=15.65, wpb=3806.6, bsz=191.5, num_updates=61400, lr=0.000127619, gnorm=0.909, loss_scale=32, train_wall=6, gb_free=29.7, wall=4313
2024-01-30 16:45:20 | INFO | train_inner | epoch 042:   1030 / 1475 loss=2.556, nll_loss=0.939, ppl=1.92, wps=58561.8, ups=15.59, wpb=3756.8, bsz=199.8, num_updates=61500, lr=0.000127515, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.8, wall=4319
2024-01-30 16:45:26 | INFO | train_inner | epoch 042:   1130 / 1475 loss=2.532, nll_loss=0.914, ppl=1.88, wps=58637.1, ups=15.51, wpb=3779.8, bsz=203.8, num_updates=61600, lr=0.000127412, gnorm=0.886, loss_scale=32, train_wall=6, gb_free=29.7, wall=4326
2024-01-30 16:45:33 | INFO | train_inner | epoch 042:   1230 / 1475 loss=2.564, nll_loss=0.949, ppl=1.93, wps=58209.2, ups=15.62, wpb=3726.8, bsz=188.1, num_updates=61700, lr=0.000127309, gnorm=0.929, loss_scale=32, train_wall=6, gb_free=29.6, wall=4332
2024-01-30 16:45:39 | INFO | train_inner | epoch 042:   1330 / 1475 loss=2.579, nll_loss=0.967, ppl=1.95, wps=58568.8, ups=15.67, wpb=3738.3, bsz=189.4, num_updates=61800, lr=0.000127205, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.7, wall=4338
2024-01-30 16:45:46 | INFO | train_inner | epoch 042:   1430 / 1475 loss=2.592, nll_loss=0.981, ppl=1.97, wps=59298.3, ups=15.72, wpb=3771.5, bsz=188.2, num_updates=61900, lr=0.000127103, gnorm=0.922, loss_scale=32, train_wall=6, gb_free=29.6, wall=4345
2024-01-30 16:45:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:45:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:45:51 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.397 | nll_loss 1.752 | ppl 3.37 | wps 135021 | wpb 3189.1 | bsz 163.4 | num_updates 61945 | best_loss 3.385
2024-01-30 16:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 61945 updates
2024-01-30 16:45:51 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint42.pt
2024-01-30 16:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint42.pt
2024-01-30 16:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint42.pt (epoch 42 @ 61945 updates, score 3.397) (writing took 2.400589616037905 seconds)
2024-01-30 16:45:53 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-01-30 16:45:53 | INFO | train | epoch 042 | loss 2.55 | nll_loss 0.933 | ppl 1.91 | wps 55990.8 | ups 14.85 | wpb 3771.6 | bsz 195.9 | num_updates 61945 | lr 0.000127056 | gnorm 0.898 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 4352
2024-01-30 16:45:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:45:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:45:53 | INFO | fairseq.trainer | begin training epoch 43
2024-01-30 16:45:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:45:57 | INFO | train_inner | epoch 043:     55 / 1475 loss=2.525, nll_loss=0.905, ppl=1.87, wps=33569.4, ups=8.84, wpb=3796.3, bsz=207.5, num_updates=62000, lr=0.000127, gnorm=0.88, loss_scale=32, train_wall=6, gb_free=29.7, wall=4356
2024-01-30 16:46:03 | INFO | train_inner | epoch 043:    155 / 1475 loss=2.524, nll_loss=0.903, ppl=1.87, wps=58430.5, ups=15.65, wpb=3734.1, bsz=185.4, num_updates=62100, lr=0.000126898, gnorm=0.882, loss_scale=32, train_wall=6, gb_free=29.7, wall=4362
2024-01-30 16:46:10 | INFO | train_inner | epoch 043:    255 / 1475 loss=2.545, nll_loss=0.926, ppl=1.9, wps=58546.2, ups=15.76, wpb=3713.9, bsz=182.7, num_updates=62200, lr=0.000126796, gnorm=0.925, loss_scale=32, train_wall=6, gb_free=29.8, wall=4369
2024-01-30 16:46:16 | INFO | train_inner | epoch 043:    355 / 1475 loss=2.525, nll_loss=0.903, ppl=1.87, wps=58582.5, ups=15.63, wpb=3749, bsz=197.9, num_updates=62300, lr=0.000126694, gnorm=0.901, loss_scale=32, train_wall=6, gb_free=29.6, wall=4375
2024-01-30 16:46:23 | INFO | train_inner | epoch 043:    455 / 1475 loss=2.519, nll_loss=0.898, ppl=1.86, wps=59141.1, ups=15.48, wpb=3821.1, bsz=208.5, num_updates=62400, lr=0.000126592, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.7, wall=4382
2024-01-30 16:46:29 | INFO | train_inner | epoch 043:    555 / 1475 loss=2.517, nll_loss=0.897, ppl=1.86, wps=59085.8, ups=15.5, wpb=3812.5, bsz=207, num_updates=62500, lr=0.000126491, gnorm=0.866, loss_scale=32, train_wall=6, gb_free=29.7, wall=4388
2024-01-30 16:46:35 | INFO | train_inner | epoch 043:    655 / 1475 loss=2.547, nll_loss=0.93, ppl=1.91, wps=59078.9, ups=15.67, wpb=3770.3, bsz=194.6, num_updates=62600, lr=0.00012639, gnorm=0.895, loss_scale=32, train_wall=6, gb_free=30.3, wall=4394
2024-01-30 16:46:42 | INFO | train_inner | epoch 043:    755 / 1475 loss=2.531, nll_loss=0.911, ppl=1.88, wps=57972.8, ups=15.57, wpb=3722.6, bsz=191, num_updates=62700, lr=0.000126289, gnorm=0.909, loss_scale=32, train_wall=6, gb_free=29.7, wall=4401
2024-01-30 16:46:48 | INFO | train_inner | epoch 043:    855 / 1475 loss=2.569, nll_loss=0.953, ppl=1.94, wps=59618.4, ups=15.63, wpb=3813.2, bsz=189.5, num_updates=62800, lr=0.000126189, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.7, wall=4407
2024-01-30 16:46:55 | INFO | train_inner | epoch 043:    955 / 1475 loss=2.535, nll_loss=0.915, ppl=1.89, wps=59464, ups=15.62, wpb=3806.4, bsz=200.6, num_updates=62900, lr=0.000126088, gnorm=0.881, loss_scale=32, train_wall=6, gb_free=29.8, wall=4414
2024-01-30 16:47:01 | INFO | train_inner | epoch 043:   1055 / 1475 loss=2.545, nll_loss=0.927, ppl=1.9, wps=58708.8, ups=15.58, wpb=3767.2, bsz=200.7, num_updates=63000, lr=0.000125988, gnorm=0.908, loss_scale=32, train_wall=6, gb_free=29.9, wall=4420
2024-01-30 16:47:07 | INFO | train_inner | epoch 043:   1155 / 1475 loss=2.562, nll_loss=0.947, ppl=1.93, wps=59046.6, ups=15.6, wpb=3784.4, bsz=193, num_updates=63100, lr=0.000125888, gnorm=0.904, loss_scale=32, train_wall=6, gb_free=29.8, wall=4427
2024-01-30 16:47:14 | INFO | train_inner | epoch 043:   1255 / 1475 loss=2.58, nll_loss=0.967, ppl=1.95, wps=58964.9, ups=15.61, wpb=3777.7, bsz=187.5, num_updates=63200, lr=0.000125789, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=4433
2024-01-30 16:47:20 | INFO | train_inner | epoch 043:   1355 / 1475 loss=2.55, nll_loss=0.933, ppl=1.91, wps=59583.2, ups=15.64, wpb=3809.3, bsz=199.3, num_updates=63300, lr=0.000125689, gnorm=0.902, loss_scale=32, train_wall=6, gb_free=29.8, wall=4439
2024-01-30 16:47:27 | INFO | train_inner | epoch 043:   1455 / 1475 loss=2.547, nll_loss=0.93, ppl=1.91, wps=57309.1, ups=15.52, wpb=3692.8, bsz=201.3, num_updates=63400, lr=0.00012559, gnorm=0.922, loss_scale=32, train_wall=6, gb_free=29.8, wall=4446
2024-01-30 16:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:47:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:47:30 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.407 | nll_loss 1.763 | ppl 3.39 | wps 134204 | wpb 3189.1 | bsz 163.4 | num_updates 63420 | best_loss 3.385
2024-01-30 16:47:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 63420 updates
2024-01-30 16:47:30 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint43.pt
2024-01-30 16:47:31 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint43.pt
2024-01-30 16:47:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint43.pt (epoch 43 @ 63420 updates, score 3.407) (writing took 2.3627119350712746 seconds)
2024-01-30 16:47:33 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-01-30 16:47:33 | INFO | train | epoch 043 | loss 2.542 | nll_loss 0.923 | ppl 1.9 | wps 55974.5 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 63420 | lr 0.00012557 | gnorm 0.9 | loss_scale 32 | train_wall 91 | gb_free 29.6 | wall 4452
2024-01-30 16:47:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:47:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:47:33 | INFO | fairseq.trainer | begin training epoch 44
2024-01-30 16:47:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:47:38 | INFO | train_inner | epoch 044:     80 / 1475 loss=2.516, nll_loss=0.893, ppl=1.86, wps=33275.8, ups=8.92, wpb=3730.2, bsz=195, num_updates=63500, lr=0.000125491, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.8, wall=4457
2024-01-30 16:47:44 | INFO | train_inner | epoch 044:    180 / 1475 loss=2.506, nll_loss=0.881, ppl=1.84, wps=59834.3, ups=15.57, wpb=3841.8, bsz=198.5, num_updates=63600, lr=0.000125392, gnorm=0.889, loss_scale=32, train_wall=6, gb_free=29.8, wall=4463
2024-01-30 16:47:51 | INFO | train_inner | epoch 044:    280 / 1475 loss=2.526, nll_loss=0.905, ppl=1.87, wps=59329.4, ups=15.58, wpb=3808.1, bsz=196.6, num_updates=63700, lr=0.000125294, gnorm=0.879, loss_scale=32, train_wall=6, gb_free=29.7, wall=4470
2024-01-30 16:47:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 16:47:57 | INFO | train_inner | epoch 044:    381 / 1475 loss=2.506, nll_loss=0.882, ppl=1.84, wps=57647, ups=15.43, wpb=3736.6, bsz=202.2, num_updates=63800, lr=0.000125196, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.7, wall=4476
2024-01-30 16:48:04 | INFO | train_inner | epoch 044:    481 / 1475 loss=2.517, nll_loss=0.896, ppl=1.86, wps=58141.2, ups=15.63, wpb=3719.9, bsz=202.3, num_updates=63900, lr=0.000125098, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.7, wall=4483
2024-01-30 16:48:10 | INFO | train_inner | epoch 044:    581 / 1475 loss=2.535, nll_loss=0.917, ppl=1.89, wps=58025.8, ups=15.58, wpb=3723.3, bsz=193.3, num_updates=64000, lr=0.000125, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=4489
2024-01-30 16:48:16 | INFO | train_inner | epoch 044:    681 / 1475 loss=2.51, nll_loss=0.886, ppl=1.85, wps=59059.2, ups=15.7, wpb=3762.2, bsz=203.5, num_updates=64100, lr=0.000124902, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.7, wall=4495
2024-01-30 16:48:23 | INFO | train_inner | epoch 044:    781 / 1475 loss=2.521, nll_loss=0.901, ppl=1.87, wps=58787.2, ups=15.55, wpb=3779.6, bsz=207.2, num_updates=64200, lr=0.000124805, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=4502
2024-01-30 16:48:29 | INFO | train_inner | epoch 044:    881 / 1475 loss=2.538, nll_loss=0.92, ppl=1.89, wps=59704.7, ups=15.53, wpb=3844.1, bsz=198.6, num_updates=64300, lr=0.000124708, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=4508
2024-01-30 16:48:36 | INFO | train_inner | epoch 044:    981 / 1475 loss=2.566, nll_loss=0.951, ppl=1.93, wps=58977.6, ups=15.67, wpb=3764.9, bsz=182.2, num_updates=64400, lr=0.000124611, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.8, wall=4515
2024-01-30 16:48:42 | INFO | train_inner | epoch 044:   1081 / 1475 loss=2.525, nll_loss=0.905, ppl=1.87, wps=60072.3, ups=15.48, wpb=3879.7, bsz=206.9, num_updates=64500, lr=0.000124515, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.6, wall=4521
2024-01-30 16:48:48 | INFO | train_inner | epoch 044:   1181 / 1475 loss=2.537, nll_loss=0.919, ppl=1.89, wps=59105.8, ups=15.63, wpb=3781.7, bsz=201.1, num_updates=64600, lr=0.000124418, gnorm=0.889, loss_scale=16, train_wall=6, gb_free=29.7, wall=4528
2024-01-30 16:48:55 | INFO | train_inner | epoch 044:   1281 / 1475 loss=2.572, nll_loss=0.959, ppl=1.94, wps=57926.9, ups=15.64, wpb=3704.5, bsz=176.8, num_updates=64700, lr=0.000124322, gnorm=0.962, loss_scale=16, train_wall=6, gb_free=29.7, wall=4534
2024-01-30 16:49:01 | INFO | train_inner | epoch 044:   1381 / 1475 loss=2.567, nll_loss=0.954, ppl=1.94, wps=58740.4, ups=15.62, wpb=3761.7, bsz=187.5, num_updates=64800, lr=0.000124226, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=4540
2024-01-30 16:49:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:49:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:49:10 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.403 | nll_loss 1.764 | ppl 3.4 | wps 134182 | wpb 3189.1 | bsz 163.4 | num_updates 64894 | best_loss 3.385
2024-01-30 16:49:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 64894 updates
2024-01-30 16:49:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint44.pt
2024-01-30 16:49:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint44.pt
2024-01-30 16:49:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint44.pt (epoch 44 @ 64894 updates, score 3.403) (writing took 2.367349231033586 seconds)
2024-01-30 16:49:12 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-01-30 16:49:12 | INFO | train | epoch 044 | loss 2.533 | nll_loss 0.914 | ppl 1.88 | wps 55968.5 | ups 14.84 | wpb 3771.4 | bsz 196 | num_updates 64894 | lr 0.000124136 | gnorm 0.903 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 4551
2024-01-30 16:49:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:49:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:49:12 | INFO | fairseq.trainer | begin training epoch 45
2024-01-30 16:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:49:12 | INFO | train_inner | epoch 045:      6 / 1475 loss=2.556, nll_loss=0.94, ppl=1.92, wps=33434.7, ups=8.94, wpb=3741.4, bsz=192.3, num_updates=64900, lr=0.00012413, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.6, wall=4552
2024-01-30 16:49:19 | INFO | train_inner | epoch 045:    106 / 1475 loss=2.505, nll_loss=0.881, ppl=1.84, wps=58989.5, ups=15.6, wpb=3781, bsz=195.4, num_updates=65000, lr=0.000124035, gnorm=0.877, loss_scale=16, train_wall=6, gb_free=29.7, wall=4558
2024-01-30 16:49:25 | INFO | train_inner | epoch 045:    206 / 1475 loss=2.483, nll_loss=0.857, ppl=1.81, wps=58114.9, ups=15.61, wpb=3721.9, bsz=207.4, num_updates=65100, lr=0.000123939, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=4564
2024-01-30 16:49:32 | INFO | train_inner | epoch 045:    306 / 1475 loss=2.524, nll_loss=0.903, ppl=1.87, wps=58184.8, ups=15.6, wpb=3728.9, bsz=184.5, num_updates=65200, lr=0.000123844, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.6, wall=4571
2024-01-30 16:49:38 | INFO | train_inner | epoch 045:    406 / 1475 loss=2.511, nll_loss=0.887, ppl=1.85, wps=58722.7, ups=15.62, wpb=3759.8, bsz=196.2, num_updates=65300, lr=0.000123749, gnorm=0.886, loss_scale=16, train_wall=6, gb_free=29.7, wall=4577
2024-01-30 16:49:45 | INFO | train_inner | epoch 045:    506 / 1475 loss=2.52, nll_loss=0.899, ppl=1.86, wps=58851.7, ups=15.61, wpb=3768.9, bsz=199.5, num_updates=65400, lr=0.000123655, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=4584
2024-01-30 16:49:51 | INFO | train_inner | epoch 045:    606 / 1475 loss=2.54, nll_loss=0.921, ppl=1.89, wps=59621.6, ups=15.62, wpb=3817.4, bsz=187.9, num_updates=65500, lr=0.00012356, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.7, wall=4590
2024-01-30 16:49:57 | INFO | train_inner | epoch 045:    706 / 1475 loss=2.528, nll_loss=0.907, ppl=1.88, wps=58638.2, ups=15.56, wpb=3768.5, bsz=193.8, num_updates=65600, lr=0.000123466, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.8, wall=4596
2024-01-30 16:50:04 | INFO | train_inner | epoch 045:    806 / 1475 loss=2.509, nll_loss=0.887, ppl=1.85, wps=59195.1, ups=15.66, wpb=3779.5, bsz=197.8, num_updates=65700, lr=0.000123372, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=4603
2024-01-30 16:50:10 | INFO | train_inner | epoch 045:    906 / 1475 loss=2.53, nll_loss=0.911, ppl=1.88, wps=59344.7, ups=15.59, wpb=3805.9, bsz=192.7, num_updates=65800, lr=0.000123278, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.7, wall=4609
2024-01-30 16:50:17 | INFO | train_inner | epoch 045:   1006 / 1475 loss=2.523, nll_loss=0.903, ppl=1.87, wps=52649.5, ups=13.74, wpb=3831.3, bsz=201.5, num_updates=65900, lr=0.000123185, gnorm=0.877, loss_scale=16, train_wall=7, gb_free=29.8, wall=4617
2024-01-30 16:50:24 | INFO | train_inner | epoch 045:   1106 / 1475 loss=2.538, nll_loss=0.92, ppl=1.89, wps=57941.3, ups=15.63, wpb=3707.3, bsz=200.4, num_updates=66000, lr=0.000123091, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.7, wall=4623
2024-01-30 16:50:30 | INFO | train_inner | epoch 045:   1206 / 1475 loss=2.544, nll_loss=0.927, ppl=1.9, wps=59258, ups=15.54, wpb=3814.2, bsz=197.6, num_updates=66100, lr=0.000122998, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.8, wall=4629
2024-01-30 16:50:37 | INFO | train_inner | epoch 045:   1306 / 1475 loss=2.542, nll_loss=0.925, ppl=1.9, wps=59400.5, ups=15.59, wpb=3809.4, bsz=193.6, num_updates=66200, lr=0.000122905, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=4636
2024-01-30 16:50:43 | INFO | train_inner | epoch 045:   1406 / 1475 loss=2.546, nll_loss=0.929, ppl=1.9, wps=58125.8, ups=15.65, wpb=3713.3, bsz=193, num_updates=66300, lr=0.000122813, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.8, wall=4642
2024-01-30 16:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:50:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:50:50 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.409 | nll_loss 1.767 | ppl 3.4 | wps 134214 | wpb 3189.1 | bsz 163.4 | num_updates 66369 | best_loss 3.385
2024-01-30 16:50:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 66369 updates
2024-01-30 16:50:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint45.pt
2024-01-30 16:50:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint45.pt
2024-01-30 16:50:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint45.pt (epoch 45 @ 66369 updates, score 3.409) (writing took 2.379336647922173 seconds)
2024-01-30 16:50:52 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-01-30 16:50:52 | INFO | train | epoch 045 | loss 2.525 | nll_loss 0.905 | ppl 1.87 | wps 55500.8 | ups 14.72 | wpb 3771.6 | bsz 195.9 | num_updates 66369 | lr 0.000122749 | gnorm 0.901 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 4651
2024-01-30 16:50:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:50:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:50:52 | INFO | fairseq.trainer | begin training epoch 46
2024-01-30 16:50:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:50:54 | INFO | train_inner | epoch 046:     31 / 1475 loss=2.532, nll_loss=0.914, ppl=1.88, wps=33206.3, ups=8.87, wpb=3742.3, bsz=194.2, num_updates=66400, lr=0.00012272, gnorm=0.889, loss_scale=16, train_wall=6, gb_free=29.7, wall=4653
2024-01-30 16:51:01 | INFO | train_inner | epoch 046:    131 / 1475 loss=2.504, nll_loss=0.88, ppl=1.84, wps=58805.3, ups=15.7, wpb=3744.5, bsz=191.3, num_updates=66500, lr=0.000122628, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.7, wall=4660
2024-01-30 16:51:07 | INFO | train_inner | epoch 046:    231 / 1475 loss=2.496, nll_loss=0.87, ppl=1.83, wps=59189.2, ups=15.6, wpb=3793.3, bsz=195.5, num_updates=66600, lr=0.000122536, gnorm=0.874, loss_scale=16, train_wall=6, gb_free=29.7, wall=4666
2024-01-30 16:51:14 | INFO | train_inner | epoch 046:    331 / 1475 loss=2.469, nll_loss=0.841, ppl=1.79, wps=58517.6, ups=15.53, wpb=3768.2, bsz=213.5, num_updates=66700, lr=0.000122444, gnorm=0.866, loss_scale=16, train_wall=6, gb_free=29.8, wall=4673
2024-01-30 16:51:20 | INFO | train_inner | epoch 046:    431 / 1475 loss=2.512, nll_loss=0.891, ppl=1.85, wps=57826.9, ups=15.51, wpb=3729.3, bsz=193, num_updates=66800, lr=0.000122352, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=4679
2024-01-30 16:51:26 | INFO | train_inner | epoch 046:    531 / 1475 loss=2.535, nll_loss=0.916, ppl=1.89, wps=58199, ups=15.62, wpb=3725.6, bsz=185.8, num_updates=66900, lr=0.000122261, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=4686
2024-01-30 16:51:33 | INFO | train_inner | epoch 046:    631 / 1475 loss=2.505, nll_loss=0.881, ppl=1.84, wps=59939.5, ups=15.58, wpb=3847.1, bsz=203.6, num_updates=67000, lr=0.000122169, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.7, wall=4692
2024-01-30 16:51:39 | INFO | train_inner | epoch 046:    731 / 1475 loss=2.528, nll_loss=0.909, ppl=1.88, wps=59646.1, ups=15.55, wpb=3834.8, bsz=190.3, num_updates=67100, lr=0.000122078, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.8, wall=4698
2024-01-30 16:51:46 | INFO | train_inner | epoch 046:    831 / 1475 loss=2.525, nll_loss=0.905, ppl=1.87, wps=58541.5, ups=15.67, wpb=3736.8, bsz=190.3, num_updates=67200, lr=0.000121988, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=4705
2024-01-30 16:51:52 | INFO | train_inner | epoch 046:    931 / 1475 loss=2.535, nll_loss=0.916, ppl=1.89, wps=58914.4, ups=15.65, wpb=3764.5, bsz=186.4, num_updates=67300, lr=0.000121897, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=4711
2024-01-30 16:51:58 | INFO | train_inner | epoch 046:   1031 / 1475 loss=2.52, nll_loss=0.9, ppl=1.87, wps=57352.9, ups=15.45, wpb=3712.6, bsz=207, num_updates=67400, lr=0.000121806, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.8, wall=4718
2024-01-30 16:52:05 | INFO | train_inner | epoch 046:   1131 / 1475 loss=2.523, nll_loss=0.903, ppl=1.87, wps=59399.8, ups=15.53, wpb=3824, bsz=198.4, num_updates=67500, lr=0.000121716, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=4724
2024-01-30 16:52:11 | INFO | train_inner | epoch 046:   1231 / 1475 loss=2.528, nll_loss=0.909, ppl=1.88, wps=59405.7, ups=15.55, wpb=3820.6, bsz=201, num_updates=67600, lr=0.000121626, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=4730
2024-01-30 16:52:18 | INFO | train_inner | epoch 046:   1331 / 1475 loss=2.532, nll_loss=0.913, ppl=1.88, wps=58656.9, ups=15.61, wpb=3757.8, bsz=200.9, num_updates=67700, lr=0.000121536, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.7, wall=4737
2024-01-30 16:52:24 | INFO | train_inner | epoch 046:   1431 / 1475 loss=2.54, nll_loss=0.924, ppl=1.9, wps=58578.1, ups=15.57, wpb=3762.1, bsz=191.7, num_updates=67800, lr=0.000121447, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.8, wall=4743
2024-01-30 16:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:52:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:52:29 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.412 | nll_loss 1.772 | ppl 3.41 | wps 134326 | wpb 3189.1 | bsz 163.4 | num_updates 67844 | best_loss 3.385
2024-01-30 16:52:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 67844 updates
2024-01-30 16:52:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint46.pt
2024-01-30 16:52:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint46.pt
2024-01-30 16:52:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint46.pt (epoch 46 @ 67844 updates, score 3.412) (writing took 2.3779709360096604 seconds)
2024-01-30 16:52:32 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-01-30 16:52:32 | INFO | train | epoch 046 | loss 2.518 | nll_loss 0.897 | ppl 1.86 | wps 55910.2 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 67844 | lr 0.000121407 | gnorm 0.902 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4751
2024-01-30 16:52:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:52:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:52:32 | INFO | fairseq.trainer | begin training epoch 47
2024-01-30 16:52:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:52:35 | INFO | train_inner | epoch 047:     56 / 1475 loss=2.515, nll_loss=0.893, ppl=1.86, wps=33791.6, ups=8.91, wpb=3793.4, bsz=186.5, num_updates=67900, lr=0.000121357, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.8, wall=4755
2024-01-30 16:52:42 | INFO | train_inner | epoch 047:    156 / 1475 loss=2.466, nll_loss=0.838, ppl=1.79, wps=57523.1, ups=15.46, wpb=3720, bsz=201.3, num_updates=68000, lr=0.000121268, gnorm=0.878, loss_scale=16, train_wall=6, gb_free=29.6, wall=4761
2024-01-30 16:52:48 | INFO | train_inner | epoch 047:    256 / 1475 loss=2.532, nll_loss=0.912, ppl=1.88, wps=59544.5, ups=15.66, wpb=3801.9, bsz=181.6, num_updates=68100, lr=0.000121179, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.8, wall=4767
2024-01-30 16:52:55 | INFO | train_inner | epoch 047:    356 / 1475 loss=2.5, nll_loss=0.876, ppl=1.84, wps=59209.1, ups=15.5, wpb=3819.3, bsz=195.3, num_updates=68200, lr=0.00012109, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.7, wall=4774
2024-01-30 16:53:01 | INFO | train_inner | epoch 047:    456 / 1475 loss=2.483, nll_loss=0.857, ppl=1.81, wps=58904.7, ups=15.56, wpb=3786.3, bsz=200.2, num_updates=68300, lr=0.000121001, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=4780
2024-01-30 16:53:08 | INFO | train_inner | epoch 047:    556 / 1475 loss=2.512, nll_loss=0.89, ppl=1.85, wps=59278.4, ups=15.61, wpb=3797.2, bsz=194, num_updates=68400, lr=0.000120913, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.8, wall=4787
2024-01-30 16:53:14 | INFO | train_inner | epoch 047:    656 / 1475 loss=2.494, nll_loss=0.87, ppl=1.83, wps=58797.7, ups=15.52, wpb=3787.6, bsz=205.2, num_updates=68500, lr=0.000120824, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.7, wall=4793
2024-01-30 16:53:20 | INFO | train_inner | epoch 047:    756 / 1475 loss=2.512, nll_loss=0.89, ppl=1.85, wps=58475.3, ups=15.66, wpb=3734.7, bsz=191.3, num_updates=68600, lr=0.000120736, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=4799
2024-01-30 16:53:27 | INFO | train_inner | epoch 047:    856 / 1475 loss=2.504, nll_loss=0.882, ppl=1.84, wps=59105.3, ups=15.56, wpb=3798, bsz=203.5, num_updates=68700, lr=0.000120648, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.7, wall=4806
2024-01-30 16:53:33 | INFO | train_inner | epoch 047:    956 / 1475 loss=2.527, nll_loss=0.907, ppl=1.88, wps=59260.2, ups=15.66, wpb=3784.2, bsz=191.2, num_updates=68800, lr=0.000120561, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=4812
2024-01-30 16:53:40 | INFO | train_inner | epoch 047:   1056 / 1475 loss=2.516, nll_loss=0.895, ppl=1.86, wps=58686.8, ups=15.56, wpb=3771.8, bsz=195, num_updates=68900, lr=0.000120473, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=4819
2024-01-30 16:53:46 | INFO | train_inner | epoch 047:   1156 / 1475 loss=2.522, nll_loss=0.903, ppl=1.87, wps=59095.1, ups=15.6, wpb=3787.2, bsz=201.4, num_updates=69000, lr=0.000120386, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.7, wall=4825
2024-01-30 16:53:52 | INFO | train_inner | epoch 047:   1256 / 1475 loss=2.508, nll_loss=0.887, ppl=1.85, wps=59440, ups=15.52, wpb=3831.1, bsz=205.9, num_updates=69100, lr=0.000120299, gnorm=0.877, loss_scale=16, train_wall=6, gb_free=29.7, wall=4832
2024-01-30 16:53:59 | INFO | train_inner | epoch 047:   1356 / 1475 loss=2.535, nll_loss=0.916, ppl=1.89, wps=57019.1, ups=15.62, wpb=3650, bsz=192.2, num_updates=69200, lr=0.000120212, gnorm=0.967, loss_scale=16, train_wall=6, gb_free=29.7, wall=4838
2024-01-30 16:54:05 | INFO | train_inner | epoch 047:   1456 / 1475 loss=2.539, nll_loss=0.921, ppl=1.89, wps=58600, ups=15.68, wpb=3738.2, bsz=190.6, num_updates=69300, lr=0.000120125, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.8, wall=4844
2024-01-30 16:54:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:54:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:54:09 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.412 | nll_loss 1.768 | ppl 3.4 | wps 129157 | wpb 3189.1 | bsz 163.4 | num_updates 69319 | best_loss 3.385
2024-01-30 16:54:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 69319 updates
2024-01-30 16:54:09 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint47.pt
2024-01-30 16:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint47.pt
2024-01-30 16:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint47.pt (epoch 47 @ 69319 updates, score 3.412) (writing took 2.364533626008779 seconds)
2024-01-30 16:54:11 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-01-30 16:54:11 | INFO | train | epoch 047 | loss 2.511 | nll_loss 0.889 | ppl 1.85 | wps 55880.5 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 69319 | lr 0.000120109 | gnorm 0.905 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 4850
2024-01-30 16:54:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:54:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:54:11 | INFO | fairseq.trainer | begin training epoch 48
2024-01-30 16:54:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:54:17 | INFO | train_inner | epoch 048:     81 / 1475 loss=2.489, nll_loss=0.864, ppl=1.82, wps=32923.2, ups=8.83, wpb=3729.8, bsz=198.2, num_updates=69400, lr=0.000120038, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=4856
2024-01-30 16:54:23 | INFO | train_inner | epoch 048:    181 / 1475 loss=2.479, nll_loss=0.851, ppl=1.8, wps=59143.6, ups=15.59, wpb=3793.3, bsz=203.8, num_updates=69500, lr=0.000119952, gnorm=0.888, loss_scale=16, train_wall=6, gb_free=29.7, wall=4862
2024-01-30 16:54:29 | INFO | train_inner | epoch 048:    281 / 1475 loss=2.481, nll_loss=0.855, ppl=1.81, wps=58329.3, ups=15.59, wpb=3742.2, bsz=191, num_updates=69600, lr=0.000119866, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=4869
2024-01-30 16:54:36 | INFO | train_inner | epoch 048:    381 / 1475 loss=2.462, nll_loss=0.834, ppl=1.78, wps=59287.7, ups=15.49, wpb=3827.9, bsz=212.5, num_updates=69700, lr=0.00011978, gnorm=0.862, loss_scale=16, train_wall=6, gb_free=29.7, wall=4875
2024-01-30 16:54:42 | INFO | train_inner | epoch 048:    481 / 1475 loss=2.511, nll_loss=0.888, ppl=1.85, wps=59434.8, ups=15.51, wpb=3831.1, bsz=195.1, num_updates=69800, lr=0.000119694, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.7, wall=4881
2024-01-30 16:54:49 | INFO | train_inner | epoch 048:    581 / 1475 loss=2.486, nll_loss=0.861, ppl=1.82, wps=57102.9, ups=15.66, wpb=3646, bsz=195.6, num_updates=69900, lr=0.000119608, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=4888
2024-01-30 16:54:55 | INFO | train_inner | epoch 048:    681 / 1475 loss=2.513, nll_loss=0.891, ppl=1.85, wps=60366.6, ups=15.69, wpb=3848.5, bsz=184.9, num_updates=70000, lr=0.000119523, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=4894
2024-01-30 16:55:01 | INFO | train_inner | epoch 048:    781 / 1475 loss=2.505, nll_loss=0.883, ppl=1.84, wps=59174.1, ups=15.59, wpb=3796, bsz=200.6, num_updates=70100, lr=0.000119438, gnorm=0.891, loss_scale=16, train_wall=6, gb_free=29.8, wall=4901
2024-01-30 16:55:08 | INFO | train_inner | epoch 048:    881 / 1475 loss=2.528, nll_loss=0.908, ppl=1.88, wps=59394.3, ups=15.66, wpb=3791.8, bsz=187.4, num_updates=70200, lr=0.000119352, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=4907
2024-01-30 16:55:14 | INFO | train_inner | epoch 048:    981 / 1475 loss=2.521, nll_loss=0.901, ppl=1.87, wps=58484, ups=15.69, wpb=3728.6, bsz=191.3, num_updates=70300, lr=0.000119268, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=4913
2024-01-30 16:55:21 | INFO | train_inner | epoch 048:   1081 / 1475 loss=2.525, nll_loss=0.905, ppl=1.87, wps=59137.5, ups=15.65, wpb=3778, bsz=195.4, num_updates=70400, lr=0.000119183, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=4920
2024-01-30 16:55:27 | INFO | train_inner | epoch 048:   1181 / 1475 loss=2.497, nll_loss=0.875, ppl=1.83, wps=58541.5, ups=15.52, wpb=3771, bsz=198.2, num_updates=70500, lr=0.000119098, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=4926
2024-01-30 16:55:33 | INFO | train_inner | epoch 048:   1281 / 1475 loss=2.504, nll_loss=0.882, ppl=1.84, wps=57780.2, ups=15.58, wpb=3709.4, bsz=196.6, num_updates=70600, lr=0.000119014, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.8, wall=4933
2024-01-30 16:55:40 | INFO | train_inner | epoch 048:   1381 / 1475 loss=2.542, nll_loss=0.925, ppl=1.9, wps=59160.7, ups=15.63, wpb=3785.9, bsz=191.4, num_updates=70700, lr=0.00011893, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=4939
2024-01-30 16:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:55:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:55:48 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.415 | nll_loss 1.775 | ppl 3.42 | wps 134824 | wpb 3189.1 | bsz 163.4 | num_updates 70794 | best_loss 3.385
2024-01-30 16:55:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 70794 updates
2024-01-30 16:55:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint48.pt
2024-01-30 16:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint48.pt
2024-01-30 16:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint48.pt (epoch 48 @ 70794 updates, score 3.415) (writing took 2.3483034919481725 seconds)
2024-01-30 16:55:51 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-01-30 16:55:51 | INFO | train | epoch 048 | loss 2.503 | nll_loss 0.881 | ppl 1.84 | wps 55984.7 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 70794 | lr 0.000118851 | gnorm 0.902 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 4950
2024-01-30 16:55:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:55:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:55:51 | INFO | fairseq.trainer | begin training epoch 49
2024-01-30 16:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:55:51 | INFO | train_inner | epoch 049:      6 / 1475 loss=2.511, nll_loss=0.891, ppl=1.85, wps=33697.3, ups=8.9, wpb=3787.9, bsz=199, num_updates=70800, lr=0.000118846, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=4950
2024-01-30 16:55:58 | INFO | train_inner | epoch 049:    106 / 1475 loss=2.469, nll_loss=0.841, ppl=1.79, wps=58267.7, ups=15.52, wpb=3755.4, bsz=197, num_updates=70900, lr=0.000118762, gnorm=0.874, loss_scale=16, train_wall=6, gb_free=29.6, wall=4957
2024-01-30 16:56:04 | INFO | train_inner | epoch 049:    206 / 1475 loss=2.471, nll_loss=0.844, ppl=1.79, wps=57552.1, ups=15.44, wpb=3726.3, bsz=202.2, num_updates=71000, lr=0.000118678, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.7, wall=4963
2024-01-30 16:56:11 | INFO | train_inner | epoch 049:    306 / 1475 loss=2.475, nll_loss=0.848, ppl=1.8, wps=58798.7, ups=15.52, wpb=3789.2, bsz=202, num_updates=71100, lr=0.000118595, gnorm=0.889, loss_scale=16, train_wall=6, gb_free=29.7, wall=4970
2024-01-30 16:56:17 | INFO | train_inner | epoch 049:    406 / 1475 loss=2.475, nll_loss=0.848, ppl=1.8, wps=59450.1, ups=15.5, wpb=3835.8, bsz=203, num_updates=71200, lr=0.000118511, gnorm=0.873, loss_scale=16, train_wall=6, gb_free=29.7, wall=4976
2024-01-30 16:56:23 | INFO | train_inner | epoch 049:    506 / 1475 loss=2.495, nll_loss=0.87, ppl=1.83, wps=58398.4, ups=15.68, wpb=3723.5, bsz=190.6, num_updates=71300, lr=0.000118428, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=4982
2024-01-30 16:56:30 | INFO | train_inner | epoch 049:    606 / 1475 loss=2.51, nll_loss=0.888, ppl=1.85, wps=58443.3, ups=15.13, wpb=3861.6, bsz=186.2, num_updates=71400, lr=0.000118345, gnorm=0.884, loss_scale=16, train_wall=6, gb_free=29.7, wall=4989
2024-01-30 16:56:36 | INFO | train_inner | epoch 049:    706 / 1475 loss=2.495, nll_loss=0.871, ppl=1.83, wps=59409.6, ups=15.64, wpb=3798.7, bsz=200.9, num_updates=71500, lr=0.000118262, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.7, wall=4995
2024-01-30 16:56:43 | INFO | train_inner | epoch 049:    806 / 1475 loss=2.493, nll_loss=0.869, ppl=1.83, wps=59016.4, ups=15.57, wpb=3790.1, bsz=198.2, num_updates=71600, lr=0.00011818, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.9, wall=5002
2024-01-30 16:56:49 | INFO | train_inner | epoch 049:    906 / 1475 loss=2.504, nll_loss=0.882, ppl=1.84, wps=59097.8, ups=15.72, wpb=3759.6, bsz=189.4, num_updates=71700, lr=0.000118097, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.9, wall=5008
2024-01-30 16:56:56 | INFO | train_inner | epoch 049:   1006 / 1475 loss=2.507, nll_loss=0.884, ppl=1.85, wps=59233.4, ups=15.61, wpb=3794.5, bsz=195.8, num_updates=71800, lr=0.000118015, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=5015
2024-01-30 16:57:02 | INFO | train_inner | epoch 049:   1106 / 1475 loss=2.495, nll_loss=0.872, ppl=1.83, wps=57649.7, ups=15.69, wpb=3673.4, bsz=196.9, num_updates=71900, lr=0.000117933, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=5021
2024-01-30 16:57:08 | INFO | train_inner | epoch 049:   1206 / 1475 loss=2.529, nll_loss=0.911, ppl=1.88, wps=58809, ups=15.68, wpb=3750.9, bsz=185.8, num_updates=72000, lr=0.000117851, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=5027
2024-01-30 16:57:15 | INFO | train_inner | epoch 049:   1306 / 1475 loss=2.505, nll_loss=0.884, ppl=1.85, wps=58839.4, ups=15.59, wpb=3774.4, bsz=200.8, num_updates=72100, lr=0.000117769, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=5034
2024-01-30 16:57:21 | INFO | train_inner | epoch 049:   1406 / 1475 loss=2.518, nll_loss=0.899, ppl=1.86, wps=59172, ups=15.5, wpb=3816.9, bsz=190.6, num_updates=72200, lr=0.000117688, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.8, wall=5040
2024-01-30 16:57:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:57:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:57:28 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.416 | nll_loss 1.777 | ppl 3.43 | wps 135116 | wpb 3189.1 | bsz 163.4 | num_updates 72269 | best_loss 3.385
2024-01-30 16:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 72269 updates
2024-01-30 16:57:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint49.pt
2024-01-30 16:57:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint49.pt
2024-01-30 16:57:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint49.pt (epoch 49 @ 72269 updates, score 3.416) (writing took 2.766378549975343 seconds)
2024-01-30 16:57:31 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-01-30 16:57:31 | INFO | train | epoch 049 | loss 2.496 | nll_loss 0.872 | ppl 1.83 | wps 55489.8 | ups 14.71 | wpb 3771.6 | bsz 195.9 | num_updates 72269 | lr 0.000117632 | gnorm 0.902 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 5050
2024-01-30 16:57:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:57:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:57:31 | INFO | fairseq.trainer | begin training epoch 50
2024-01-30 16:57:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:57:34 | INFO | train_inner | epoch 050:     31 / 1475 loss=2.496, nll_loss=0.872, ppl=1.83, wps=29896.7, ups=8.05, wpb=3712.1, bsz=197.8, num_updates=72300, lr=0.000117606, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.8, wall=5053
2024-01-30 16:57:41 | INFO | train_inner | epoch 050:    131 / 1475 loss=2.468, nll_loss=0.84, ppl=1.79, wps=47796.8, ups=12.63, wpb=3784.4, bsz=200.2, num_updates=72400, lr=0.000117525, gnorm=0.876, loss_scale=16, train_wall=8, gb_free=29.7, wall=5061
2024-01-30 16:57:48 | INFO | train_inner | epoch 050:    231 / 1475 loss=2.464, nll_loss=0.836, ppl=1.78, wps=58118.4, ups=15.62, wpb=3721.4, bsz=200.7, num_updates=72500, lr=0.000117444, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=5067
2024-01-30 16:57:54 | INFO | train_inner | epoch 050:    331 / 1475 loss=2.472, nll_loss=0.846, ppl=1.8, wps=58896.5, ups=15.57, wpb=3782.8, bsz=195, num_updates=72600, lr=0.000117363, gnorm=0.875, loss_scale=16, train_wall=6, gb_free=29.7, wall=5073
2024-01-30 16:58:01 | INFO | train_inner | epoch 050:    431 / 1475 loss=2.484, nll_loss=0.858, ppl=1.81, wps=58952, ups=15.59, wpb=3780.6, bsz=195.8, num_updates=72700, lr=0.000117282, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.7, wall=5080
2024-01-30 16:58:07 | INFO | train_inner | epoch 050:    531 / 1475 loss=2.487, nll_loss=0.862, ppl=1.82, wps=58614.3, ups=15.65, wpb=3744.3, bsz=186.9, num_updates=72800, lr=0.000117202, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.7, wall=5086
2024-01-30 16:58:14 | INFO | train_inner | epoch 050:    631 / 1475 loss=2.5, nll_loss=0.876, ppl=1.84, wps=59411.8, ups=15.54, wpb=3824.3, bsz=192.6, num_updates=72900, lr=0.000117121, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.8, wall=5093
2024-01-30 16:58:20 | INFO | train_inner | epoch 050:    731 / 1475 loss=2.483, nll_loss=0.858, ppl=1.81, wps=57253.3, ups=15.73, wpb=3640.6, bsz=194.4, num_updates=73000, lr=0.000117041, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.8, wall=5099
2024-01-30 16:58:26 | INFO | train_inner | epoch 050:    831 / 1475 loss=2.515, nll_loss=0.894, ppl=1.86, wps=59029.9, ups=15.63, wpb=3776.9, bsz=188.5, num_updates=73100, lr=0.000116961, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=5105
2024-01-30 16:58:33 | INFO | train_inner | epoch 050:    931 / 1475 loss=2.479, nll_loss=0.854, ppl=1.81, wps=58826.3, ups=15.53, wpb=3787.4, bsz=200, num_updates=73200, lr=0.000116881, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.7, wall=5112
2024-01-30 16:58:39 | INFO | train_inner | epoch 050:   1031 / 1475 loss=2.483, nll_loss=0.859, ppl=1.81, wps=59301.5, ups=15.54, wpb=3815.7, bsz=202.6, num_updates=73300, lr=0.000116801, gnorm=0.884, loss_scale=16, train_wall=6, gb_free=29.7, wall=5118
2024-01-30 16:58:46 | INFO | train_inner | epoch 050:   1131 / 1475 loss=2.489, nll_loss=0.865, ppl=1.82, wps=58663.8, ups=15.56, wpb=3770.2, bsz=202.6, num_updates=73400, lr=0.000116722, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.8, wall=5125
2024-01-30 16:58:52 | INFO | train_inner | epoch 050:   1231 / 1475 loss=2.495, nll_loss=0.872, ppl=1.83, wps=59158.2, ups=15.59, wpb=3794.3, bsz=198.1, num_updates=73500, lr=0.000116642, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.6, wall=5131
2024-01-30 16:58:59 | INFO | train_inner | epoch 050:   1331 / 1475 loss=2.508, nll_loss=0.888, ppl=1.85, wps=59712, ups=15.53, wpb=3844.3, bsz=199.3, num_updates=73600, lr=0.000116563, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.8, wall=5138
2024-01-30 16:59:05 | INFO | train_inner | epoch 050:   1431 / 1475 loss=2.517, nll_loss=0.897, ppl=1.86, wps=58489.1, ups=15.59, wpb=3751.8, bsz=189.4, num_updates=73700, lr=0.000116484, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=5144
2024-01-30 16:59:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 16:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:59:10 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.413 | nll_loss 1.772 | ppl 3.41 | wps 135356 | wpb 3189.1 | bsz 163.4 | num_updates 73744 | best_loss 3.385
2024-01-30 16:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 73744 updates
2024-01-30 16:59:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint50.pt
2024-01-30 16:59:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint50.pt
2024-01-30 16:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint50.pt (epoch 50 @ 73744 updates, score 3.413) (writing took 2.5299056499497965 seconds)
2024-01-30 16:59:13 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-01-30 16:59:13 | INFO | train | epoch 050 | loss 2.49 | nll_loss 0.866 | ppl 1.82 | wps 54616.8 | ups 14.48 | wpb 3771.6 | bsz 195.9 | num_updates 73744 | lr 0.000116449 | gnorm 0.908 | loss_scale 16 | train_wall 93 | gb_free 29.7 | wall 5152
2024-01-30 16:59:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 16:59:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 16:59:13 | INFO | fairseq.trainer | begin training epoch 51
2024-01-30 16:59:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 16:59:16 | INFO | train_inner | epoch 051:     56 / 1475 loss=2.479, nll_loss=0.853, ppl=1.81, wps=32910.1, ups=8.66, wpb=3798.2, bsz=201.7, num_updates=73800, lr=0.000116405, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=5156
2024-01-30 16:59:23 | INFO | train_inner | epoch 051:    156 / 1475 loss=2.463, nll_loss=0.834, ppl=1.78, wps=58369.5, ups=15.53, wpb=3758, bsz=197.9, num_updates=73900, lr=0.000116326, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.7, wall=5162
2024-01-30 16:59:29 | INFO | train_inner | epoch 051:    256 / 1475 loss=2.437, nll_loss=0.807, ppl=1.75, wps=56566.9, ups=15.48, wpb=3654.1, bsz=206.1, num_updates=74000, lr=0.000116248, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.6, wall=5168
2024-01-30 16:59:36 | INFO | train_inner | epoch 051:    356 / 1475 loss=2.479, nll_loss=0.852, ppl=1.81, wps=58901.8, ups=15.64, wpb=3766.8, bsz=186.2, num_updates=74100, lr=0.000116169, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.8, wall=5175
2024-01-30 16:59:42 | INFO | train_inner | epoch 051:    456 / 1475 loss=2.468, nll_loss=0.841, ppl=1.79, wps=59591.7, ups=15.55, wpb=3832.8, bsz=197.4, num_updates=74200, lr=0.000116091, gnorm=0.886, loss_scale=16, train_wall=6, gb_free=29.7, wall=5181
2024-01-30 16:59:49 | INFO | train_inner | epoch 051:    556 / 1475 loss=2.455, nll_loss=0.826, ppl=1.77, wps=58970.8, ups=15.55, wpb=3791.2, bsz=210.1, num_updates=74300, lr=0.000116013, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=5188
2024-01-30 16:59:55 | INFO | train_inner | epoch 051:    656 / 1475 loss=2.492, nll_loss=0.867, ppl=1.82, wps=58485.7, ups=15.65, wpb=3738, bsz=188.7, num_updates=74400, lr=0.000115935, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.8, wall=5194
2024-01-30 17:00:01 | INFO | train_inner | epoch 051:    756 / 1475 loss=2.484, nll_loss=0.859, ppl=1.81, wps=59311.3, ups=15.63, wpb=3795.4, bsz=199.1, num_updates=74500, lr=0.000115857, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.7, wall=5201
2024-01-30 17:00:08 | INFO | train_inner | epoch 051:    856 / 1475 loss=2.507, nll_loss=0.884, ppl=1.85, wps=59211.7, ups=15.77, wpb=3755.4, bsz=180.3, num_updates=74600, lr=0.000115779, gnorm=0.935, loss_scale=16, train_wall=6, gb_free=29.7, wall=5207
2024-01-30 17:00:14 | INFO | train_inner | epoch 051:    956 / 1475 loss=2.509, nll_loss=0.888, ppl=1.85, wps=58878.8, ups=15.56, wpb=3785.1, bsz=184.6, num_updates=74700, lr=0.000115702, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.8, wall=5213
2024-01-30 17:00:21 | INFO | train_inner | epoch 051:   1056 / 1475 loss=2.491, nll_loss=0.867, ppl=1.82, wps=58949.3, ups=15.52, wpb=3798.7, bsz=202, num_updates=74800, lr=0.000115624, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.7, wall=5220
2024-01-30 17:00:27 | INFO | train_inner | epoch 051:   1156 / 1475 loss=2.497, nll_loss=0.874, ppl=1.83, wps=58496.1, ups=15.56, wpb=3760.2, bsz=191.9, num_updates=74900, lr=0.000115547, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.7, wall=5226
2024-01-30 17:00:33 | INFO | train_inner | epoch 051:   1256 / 1475 loss=2.491, nll_loss=0.868, ppl=1.82, wps=58814.3, ups=15.57, wpb=3778.3, bsz=199.8, num_updates=75000, lr=0.00011547, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=5233
2024-01-30 17:00:40 | INFO | train_inner | epoch 051:   1356 / 1475 loss=2.491, nll_loss=0.867, ppl=1.82, wps=58489.7, ups=15.47, wpb=3782.1, bsz=204.6, num_updates=75100, lr=0.000115393, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=5239
2024-01-30 17:00:46 | INFO | train_inner | epoch 051:   1456 / 1475 loss=2.516, nll_loss=0.896, ppl=1.86, wps=57640.7, ups=15.43, wpb=3735.6, bsz=187.1, num_updates=75200, lr=0.000115316, gnorm=0.945, loss_scale=16, train_wall=6, gb_free=29.8, wall=5246
2024-01-30 17:00:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:00:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:00:50 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.41 | nll_loss 1.773 | ppl 3.42 | wps 131760 | wpb 3189.1 | bsz 163.4 | num_updates 75219 | best_loss 3.385
2024-01-30 17:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 75219 updates
2024-01-30 17:00:50 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint51.pt
2024-01-30 17:00:51 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint51.pt
2024-01-30 17:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint51.pt (epoch 51 @ 75219 updates, score 3.41) (writing took 2.5280258710263297 seconds)
2024-01-30 17:00:53 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-01-30 17:00:53 | INFO | train | epoch 051 | loss 2.483 | nll_loss 0.858 | ppl 1.81 | wps 55713.3 | ups 14.77 | wpb 3771.6 | bsz 195.9 | num_updates 75219 | lr 0.000115302 | gnorm 0.906 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 5252
2024-01-30 17:00:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:00:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:00:53 | INFO | fairseq.trainer | begin training epoch 52
2024-01-30 17:00:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:00:58 | INFO | train_inner | epoch 052:     81 / 1475 loss=2.44, nll_loss=0.809, ppl=1.75, wps=33120.8, ups=8.65, wpb=3830.7, bsz=205.6, num_updates=75300, lr=0.00011524, gnorm=0.86, loss_scale=16, train_wall=6, gb_free=29.7, wall=5257
2024-01-30 17:01:04 | INFO | train_inner | epoch 052:    181 / 1475 loss=2.449, nll_loss=0.819, ppl=1.76, wps=57574.7, ups=15.42, wpb=3734.1, bsz=201.9, num_updates=75400, lr=0.000115163, gnorm=0.886, loss_scale=16, train_wall=6, gb_free=29.6, wall=5264
2024-01-30 17:01:11 | INFO | train_inner | epoch 052:    281 / 1475 loss=2.468, nll_loss=0.841, ppl=1.79, wps=58406.2, ups=15.4, wpb=3793.1, bsz=190.2, num_updates=75500, lr=0.000115087, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=5270
2024-01-30 17:01:18 | INFO | train_inner | epoch 052:    381 / 1475 loss=2.487, nll_loss=0.861, ppl=1.82, wps=56997.4, ups=15.28, wpb=3729.6, bsz=182.6, num_updates=75600, lr=0.000115011, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.8, wall=5277
2024-01-30 17:01:24 | INFO | train_inner | epoch 052:    481 / 1475 loss=2.474, nll_loss=0.847, ppl=1.8, wps=58686.9, ups=15.38, wpb=3815.7, bsz=196.1, num_updates=75700, lr=0.000114935, gnorm=0.897, loss_scale=16, train_wall=6, gb_free=29.7, wall=5283
2024-01-30 17:01:31 | INFO | train_inner | epoch 052:    581 / 1475 loss=2.503, nll_loss=0.879, ppl=1.84, wps=58379.3, ups=15.38, wpb=3795.4, bsz=182.5, num_updates=75800, lr=0.000114859, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=5290
2024-01-30 17:01:37 | INFO | train_inner | epoch 052:    681 / 1475 loss=2.501, nll_loss=0.878, ppl=1.84, wps=57040.6, ups=15.5, wpb=3680.2, bsz=176.6, num_updates=75900, lr=0.000114783, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=5296
2024-01-30 17:01:44 | INFO | train_inner | epoch 052:    781 / 1475 loss=2.467, nll_loss=0.839, ppl=1.79, wps=57864.2, ups=15.13, wpb=3824.6, bsz=205.7, num_updates=76000, lr=0.000114708, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.7, wall=5303
2024-01-30 17:01:50 | INFO | train_inner | epoch 052:    881 / 1475 loss=2.486, nll_loss=0.861, ppl=1.82, wps=56405.5, ups=14.91, wpb=3784.2, bsz=199.1, num_updates=76100, lr=0.000114632, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.7, wall=5309
2024-01-30 17:01:57 | INFO | train_inner | epoch 052:    981 / 1475 loss=2.47, nll_loss=0.843, ppl=1.79, wps=57779, ups=15.24, wpb=3791.8, bsz=204.9, num_updates=76200, lr=0.000114557, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.7, wall=5316
2024-01-30 17:02:03 | INFO | train_inner | epoch 052:   1081 / 1475 loss=2.502, nll_loss=0.88, ppl=1.84, wps=57701.1, ups=15.38, wpb=3751, bsz=189.7, num_updates=76300, lr=0.000114482, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.8, wall=5322
2024-01-30 17:02:10 | INFO | train_inner | epoch 052:   1181 / 1475 loss=2.478, nll_loss=0.853, ppl=1.81, wps=57026.6, ups=15.31, wpb=3724.7, bsz=197.7, num_updates=76400, lr=0.000114407, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=5329
2024-01-30 17:02:16 | INFO | train_inner | epoch 052:   1281 / 1475 loss=2.476, nll_loss=0.851, ppl=1.8, wps=57452.3, ups=15.33, wpb=3748.1, bsz=195.9, num_updates=76500, lr=0.000114332, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.6, wall=5336
2024-01-30 17:02:23 | INFO | train_inner | epoch 052:   1381 / 1475 loss=2.473, nll_loss=0.848, ppl=1.8, wps=59557.9, ups=15.42, wpb=3862.1, bsz=215.8, num_updates=76600, lr=0.000114258, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=5342
2024-01-30 17:02:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:02:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:02:31 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.41 | nll_loss 1.775 | ppl 3.42 | wps 134095 | wpb 3189.1 | bsz 163.4 | num_updates 76694 | best_loss 3.385
2024-01-30 17:02:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 76694 updates
2024-01-30 17:02:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint52.pt
2024-01-30 17:02:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint52.pt
2024-01-30 17:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint52.pt (epoch 52 @ 76694 updates, score 3.41) (writing took 2.3686608960852027 seconds)
2024-01-30 17:02:34 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-01-30 17:02:34 | INFO | train | epoch 052 | loss 2.478 | nll_loss 0.852 | ppl 1.8 | wps 54988.5 | ups 14.58 | wpb 3771.6 | bsz 195.9 | num_updates 76694 | lr 0.000114188 | gnorm 0.907 | loss_scale 16 | train_wall 93 | gb_free 29.8 | wall 5353
2024-01-30 17:02:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:02:34 | INFO | fairseq.trainer | begin training epoch 53
2024-01-30 17:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:02:34 | INFO | train_inner | epoch 053:      6 / 1475 loss=2.493, nll_loss=0.871, ppl=1.83, wps=32900.2, ups=8.8, wpb=3739.3, bsz=197.2, num_updates=76700, lr=0.000114183, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.8, wall=5353
2024-01-30 17:02:41 | INFO | train_inner | epoch 053:    106 / 1475 loss=2.455, nll_loss=0.824, ppl=1.77, wps=59368.2, ups=15.44, wpb=3845.3, bsz=191.2, num_updates=76800, lr=0.000114109, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.7, wall=5360
2024-01-30 17:02:47 | INFO | train_inner | epoch 053:    206 / 1475 loss=2.45, nll_loss=0.82, ppl=1.77, wps=58964.8, ups=15.57, wpb=3787.6, bsz=200, num_updates=76900, lr=0.000114035, gnorm=0.889, loss_scale=16, train_wall=6, gb_free=29.8, wall=5366
2024-01-30 17:02:54 | INFO | train_inner | epoch 053:    306 / 1475 loss=2.474, nll_loss=0.848, ppl=1.8, wps=58104.6, ups=15.64, wpb=3714.5, bsz=178.4, num_updates=77000, lr=0.000113961, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=5373
2024-01-30 17:03:00 | INFO | train_inner | epoch 053:    406 / 1475 loss=2.47, nll_loss=0.843, ppl=1.79, wps=59482.7, ups=15.5, wpb=3837.4, bsz=188.2, num_updates=77100, lr=0.000113887, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.8, wall=5379
2024-01-30 17:03:06 | INFO | train_inner | epoch 053:    506 / 1475 loss=2.473, nll_loss=0.846, ppl=1.8, wps=57752.1, ups=15.7, wpb=3679.4, bsz=186.2, num_updates=77200, lr=0.000113813, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.8, wall=5386
2024-01-30 17:03:13 | INFO | train_inner | epoch 053:    606 / 1475 loss=2.473, nll_loss=0.846, ppl=1.8, wps=57982.4, ups=15.57, wpb=3725.2, bsz=193.2, num_updates=77300, lr=0.000113739, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=5392
2024-01-30 17:03:19 | INFO | train_inner | epoch 053:    706 / 1475 loss=2.462, nll_loss=0.834, ppl=1.78, wps=58685, ups=15.49, wpb=3788.9, bsz=200.2, num_updates=77400, lr=0.000113666, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.7, wall=5398
2024-01-30 17:03:26 | INFO | train_inner | epoch 053:    806 / 1475 loss=2.48, nll_loss=0.854, ppl=1.81, wps=58037.9, ups=15.46, wpb=3753.8, bsz=192.2, num_updates=77500, lr=0.000113592, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=5405
2024-01-30 17:03:32 | INFO | train_inner | epoch 053:    906 / 1475 loss=2.469, nll_loss=0.844, ppl=1.79, wps=58416.3, ups=15.42, wpb=3788.6, bsz=202, num_updates=77600, lr=0.000113519, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.7, wall=5411
2024-01-30 17:03:39 | INFO | train_inner | epoch 053:   1006 / 1475 loss=2.484, nll_loss=0.861, ppl=1.82, wps=59274, ups=15.5, wpb=3824.9, bsz=202.3, num_updates=77700, lr=0.000113446, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=5418
2024-01-30 17:03:45 | INFO | train_inner | epoch 053:   1106 / 1475 loss=2.462, nll_loss=0.835, ppl=1.78, wps=57371, ups=15.5, wpb=3700.8, bsz=207.2, num_updates=77800, lr=0.000113373, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=5424
2024-01-30 17:03:52 | INFO | train_inner | epoch 053:   1206 / 1475 loss=2.474, nll_loss=0.848, ppl=1.8, wps=58732.5, ups=15.58, wpb=3769.5, bsz=200.4, num_updates=77900, lr=0.0001133, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.8, wall=5431
2024-01-30 17:03:58 | INFO | train_inner | epoch 053:   1306 / 1475 loss=2.492, nll_loss=0.869, ppl=1.83, wps=59634.2, ups=15.61, wpb=3819.1, bsz=195.8, num_updates=78000, lr=0.000113228, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=5437
2024-01-30 17:04:04 | INFO | train_inner | epoch 053:   1406 / 1475 loss=2.481, nll_loss=0.856, ppl=1.81, wps=58982.4, ups=15.58, wpb=3785.8, bsz=194.6, num_updates=78100, lr=0.000113155, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.7, wall=5443
2024-01-30 17:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:04:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:04:11 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.412 | nll_loss 1.777 | ppl 3.43 | wps 134419 | wpb 3189.1 | bsz 163.4 | num_updates 78169 | best_loss 3.385
2024-01-30 17:04:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 78169 updates
2024-01-30 17:04:11 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint53.pt
2024-01-30 17:04:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint53.pt
2024-01-30 17:04:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint53.pt (epoch 53 @ 78169 updates, score 3.412) (writing took 2.482333516003564 seconds)
2024-01-30 17:04:14 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-01-30 17:04:14 | INFO | train | epoch 053 | loss 2.471 | nll_loss 0.845 | ppl 1.8 | wps 55722.1 | ups 14.77 | wpb 3771.6 | bsz 195.9 | num_updates 78169 | lr 0.000113105 | gnorm 0.909 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 5453
2024-01-30 17:04:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:04:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:04:14 | INFO | fairseq.trainer | begin training epoch 54
2024-01-30 17:04:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:04:16 | INFO | train_inner | epoch 054:     31 / 1475 loss=2.463, nll_loss=0.837, ppl=1.79, wps=33001.4, ups=8.78, wpb=3758.7, bsz=205.7, num_updates=78200, lr=0.000113083, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=5455
2024-01-30 17:04:22 | INFO | train_inner | epoch 054:    131 / 1475 loss=2.438, nll_loss=0.807, ppl=1.75, wps=57390.5, ups=14.87, wpb=3859.7, bsz=200.8, num_updates=78300, lr=0.000113011, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.7, wall=5462
2024-01-30 17:04:29 | INFO | train_inner | epoch 054:    231 / 1475 loss=2.447, nll_loss=0.816, ppl=1.76, wps=52659.7, ups=14.49, wpb=3634.3, bsz=188.5, num_updates=78400, lr=0.000112938, gnorm=0.952, loss_scale=16, train_wall=7, gb_free=29.8, wall=5469
2024-01-30 17:04:36 | INFO | train_inner | epoch 054:    331 / 1475 loss=2.464, nll_loss=0.836, ppl=1.79, wps=55831.3, ups=14.3, wpb=3904.1, bsz=193.9, num_updates=78500, lr=0.000112867, gnorm=0.874, loss_scale=16, train_wall=7, gb_free=29.7, wall=5475
2024-01-30 17:04:43 | INFO | train_inner | epoch 054:    431 / 1475 loss=2.47, nll_loss=0.843, ppl=1.79, wps=53678.8, ups=14.29, wpb=3756.5, bsz=186.2, num_updates=78600, lr=0.000112795, gnorm=0.93, loss_scale=16, train_wall=7, gb_free=29.7, wall=5482
2024-01-30 17:04:50 | INFO | train_inner | epoch 054:    531 / 1475 loss=2.452, nll_loss=0.823, ppl=1.77, wps=53048.9, ups=14.25, wpb=3723.3, bsz=199.4, num_updates=78700, lr=0.000112723, gnorm=0.902, loss_scale=16, train_wall=7, gb_free=29.7, wall=5490
2024-01-30 17:04:57 | INFO | train_inner | epoch 054:    631 / 1475 loss=2.465, nll_loss=0.837, ppl=1.79, wps=54547.5, ups=14.47, wpb=3770.8, bsz=197, num_updates=78800, lr=0.000112651, gnorm=0.899, loss_scale=16, train_wall=7, gb_free=29.8, wall=5496
2024-01-30 17:05:04 | INFO | train_inner | epoch 054:    731 / 1475 loss=2.467, nll_loss=0.84, ppl=1.79, wps=53647.9, ups=14.48, wpb=3705.1, bsz=194.8, num_updates=78900, lr=0.00011258, gnorm=0.919, loss_scale=16, train_wall=7, gb_free=29.8, wall=5503
2024-01-30 17:05:11 | INFO | train_inner | epoch 054:    831 / 1475 loss=2.446, nll_loss=0.817, ppl=1.76, wps=53617, ups=14.06, wpb=3814.6, bsz=212.2, num_updates=79000, lr=0.000112509, gnorm=0.891, loss_scale=16, train_wall=7, gb_free=29.7, wall=5510
2024-01-30 17:05:18 | INFO | train_inner | epoch 054:    931 / 1475 loss=2.473, nll_loss=0.847, ppl=1.8, wps=59488.2, ups=15.54, wpb=3828, bsz=194.2, num_updates=79100, lr=0.000112438, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=5517
2024-01-30 17:05:24 | INFO | train_inner | epoch 054:   1031 / 1475 loss=2.462, nll_loss=0.834, ppl=1.78, wps=58869.3, ups=15.57, wpb=3780.8, bsz=202.1, num_updates=79200, lr=0.000112367, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=5523
2024-01-30 17:05:31 | INFO | train_inner | epoch 054:   1131 / 1475 loss=2.476, nll_loss=0.851, ppl=1.8, wps=58551, ups=15.56, wpb=3762, bsz=194.2, num_updates=79300, lr=0.000112296, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.8, wall=5530
2024-01-30 17:05:37 | INFO | train_inner | epoch 054:   1231 / 1475 loss=2.502, nll_loss=0.882, ppl=1.84, wps=58004, ups=15.67, wpb=3701.7, bsz=179.8, num_updates=79400, lr=0.000112225, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.8, wall=5536
2024-01-30 17:05:43 | INFO | train_inner | epoch 054:   1331 / 1475 loss=2.484, nll_loss=0.86, ppl=1.82, wps=57910.7, ups=15.55, wpb=3724.1, bsz=191.5, num_updates=79500, lr=0.000112154, gnorm=0.945, loss_scale=16, train_wall=6, gb_free=29.9, wall=5543
2024-01-30 17:05:50 | INFO | train_inner | epoch 054:   1431 / 1475 loss=2.47, nll_loss=0.845, ppl=1.8, wps=57696.3, ups=15.13, wpb=3814.1, bsz=205.5, num_updates=79600, lr=0.000112084, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=5549
2024-01-30 17:05:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:05:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:05:56 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.42 | nll_loss 1.788 | ppl 3.45 | wps 107949 | wpb 3189.1 | bsz 163.4 | num_updates 79644 | best_loss 3.385
2024-01-30 17:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 79644 updates
2024-01-30 17:05:56 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint54.pt
2024-01-30 17:05:57 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint54.pt
2024-01-30 17:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint54.pt (epoch 54 @ 79644 updates, score 3.42) (writing took 2.634507045033388 seconds)
2024-01-30 17:05:59 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-01-30 17:05:59 | INFO | train | epoch 054 | loss 2.465 | nll_loss 0.838 | ppl 1.79 | wps 52945.1 | ups 14.04 | wpb 3771.6 | bsz 195.9 | num_updates 79644 | lr 0.000112053 | gnorm 0.911 | loss_scale 16 | train_wall 96 | gb_free 29.6 | wall 5558
2024-01-30 17:05:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:05:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:05:59 | INFO | fairseq.trainer | begin training epoch 55
2024-01-30 17:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:06:03 | INFO | train_inner | epoch 055:     56 / 1475 loss=2.431, nll_loss=0.8, ppl=1.74, wps=30084.9, ups=7.86, wpb=3826.2, bsz=213.4, num_updates=79700, lr=0.000112014, gnorm=0.868, loss_scale=16, train_wall=7, gb_free=29.7, wall=5562
2024-01-30 17:06:10 | INFO | train_inner | epoch 055:    156 / 1475 loss=2.412, nll_loss=0.778, ppl=1.71, wps=52555.8, ups=14.13, wpb=3718.8, bsz=205.8, num_updates=79800, lr=0.000111943, gnorm=0.877, loss_scale=16, train_wall=7, gb_free=29.7, wall=5569
2024-01-30 17:06:17 | INFO | train_inner | epoch 055:    256 / 1475 loss=2.424, nll_loss=0.791, ppl=1.73, wps=53692.3, ups=14.08, wpb=3813.6, bsz=208.3, num_updates=79900, lr=0.000111873, gnorm=0.86, loss_scale=16, train_wall=7, gb_free=29.8, wall=5576
2024-01-30 17:06:24 | INFO | train_inner | epoch 055:    356 / 1475 loss=2.435, nll_loss=0.803, ppl=1.75, wps=52715.9, ups=14.06, wpb=3750.1, bsz=200.2, num_updates=80000, lr=0.000111803, gnorm=0.895, loss_scale=16, train_wall=7, gb_free=29.7, wall=5583
2024-01-30 17:06:31 | INFO | train_inner | epoch 055:    456 / 1475 loss=2.454, nll_loss=0.824, ppl=1.77, wps=54741.8, ups=14.56, wpb=3758.8, bsz=187.3, num_updates=80100, lr=0.000111734, gnorm=0.911, loss_scale=16, train_wall=7, gb_free=29.7, wall=5590
2024-01-30 17:06:38 | INFO | train_inner | epoch 055:    556 / 1475 loss=2.454, nll_loss=0.826, ppl=1.77, wps=54612.9, ups=14.69, wpb=3717.9, bsz=192.3, num_updates=80200, lr=0.000111664, gnorm=0.918, loss_scale=32, train_wall=7, gb_free=29.7, wall=5597
2024-01-30 17:06:44 | INFO | train_inner | epoch 055:    656 / 1475 loss=2.471, nll_loss=0.845, ppl=1.8, wps=57829.1, ups=15.64, wpb=3698.4, bsz=186.2, num_updates=80300, lr=0.000111594, gnorm=0.937, loss_scale=32, train_wall=6, gb_free=29.6, wall=5603
2024-01-30 17:06:51 | INFO | train_inner | epoch 055:    756 / 1475 loss=2.467, nll_loss=0.84, ppl=1.79, wps=59261.3, ups=15.62, wpb=3793.5, bsz=199, num_updates=80400, lr=0.000111525, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.7, wall=5610
2024-01-30 17:06:57 | INFO | train_inner | epoch 055:    856 / 1475 loss=2.475, nll_loss=0.85, ppl=1.8, wps=58273, ups=15.53, wpb=3752.1, bsz=189, num_updates=80500, lr=0.000111456, gnorm=0.933, loss_scale=32, train_wall=6, gb_free=29.7, wall=5616
2024-01-30 17:07:04 | INFO | train_inner | epoch 055:    956 / 1475 loss=2.471, nll_loss=0.846, ppl=1.8, wps=57366.5, ups=15.03, wpb=3816.2, bsz=198, num_updates=80600, lr=0.000111386, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.6, wall=5623
2024-01-30 17:07:10 | INFO | train_inner | epoch 055:   1056 / 1475 loss=2.479, nll_loss=0.853, ppl=1.81, wps=57725.4, ups=15.26, wpb=3782.3, bsz=188.2, num_updates=80700, lr=0.000111317, gnorm=0.909, loss_scale=32, train_wall=6, gb_free=29.7, wall=5629
2024-01-30 17:07:17 | INFO | train_inner | epoch 055:   1156 / 1475 loss=2.479, nll_loss=0.854, ppl=1.81, wps=56496.6, ups=14.82, wpb=3813.4, bsz=194.1, num_updates=80800, lr=0.000111249, gnorm=0.904, loss_scale=32, train_wall=7, gb_free=29.7, wall=5636
2024-01-30 17:07:24 | INFO | train_inner | epoch 055:   1256 / 1475 loss=2.464, nll_loss=0.838, ppl=1.79, wps=52696.8, ups=14.01, wpb=3760.5, bsz=196.8, num_updates=80900, lr=0.00011118, gnorm=0.92, loss_scale=32, train_wall=7, gb_free=29.7, wall=5643
2024-01-30 17:07:31 | INFO | train_inner | epoch 055:   1356 / 1475 loss=2.474, nll_loss=0.849, ppl=1.8, wps=52081.5, ups=13.79, wpb=3775.5, bsz=193.6, num_updates=81000, lr=0.000111111, gnorm=0.919, loss_scale=32, train_wall=7, gb_free=29.8, wall=5650
2024-01-30 17:07:38 | INFO | train_inner | epoch 055:   1456 / 1475 loss=2.49, nll_loss=0.866, ppl=1.82, wps=53214, ups=13.95, wpb=3813.8, bsz=192.8, num_updates=81100, lr=0.000111043, gnorm=0.915, loss_scale=32, train_wall=7, gb_free=29.7, wall=5658
2024-01-30 17:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:07:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:07:43 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.413 | nll_loss 1.777 | ppl 3.43 | wps 102778 | wpb 3189.1 | bsz 163.4 | num_updates 81119 | best_loss 3.385
2024-01-30 17:07:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 81119 updates
2024-01-30 17:07:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint55.pt
2024-01-30 17:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint55.pt
2024-01-30 17:07:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint55.pt (epoch 55 @ 81119 updates, score 3.413) (writing took 2.6052846589591354 seconds)
2024-01-30 17:07:45 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-01-30 17:07:45 | INFO | train | epoch 055 | loss 2.459 | nll_loss 0.831 | ppl 1.78 | wps 52090.8 | ups 13.81 | wpb 3771.6 | bsz 195.9 | num_updates 81119 | lr 0.00011103 | gnorm 0.906 | loss_scale 32 | train_wall 97 | gb_free 29.7 | wall 5665
2024-01-30 17:07:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:07:46 | INFO | fairseq.trainer | begin training epoch 56
2024-01-30 17:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:07:51 | INFO | train_inner | epoch 056:     81 / 1475 loss=2.42, nll_loss=0.787, ppl=1.73, wps=30103.6, ups=7.95, wpb=3788.5, bsz=211.6, num_updates=81200, lr=0.000110974, gnorm=0.875, loss_scale=32, train_wall=7, gb_free=29.7, wall=5670
2024-01-30 17:07:57 | INFO | train_inner | epoch 056:    181 / 1475 loss=2.437, nll_loss=0.807, ppl=1.75, wps=59342.9, ups=15.55, wpb=3815, bsz=190.2, num_updates=81300, lr=0.000110906, gnorm=0.891, loss_scale=32, train_wall=6, gb_free=29.6, wall=5677
2024-01-30 17:08:04 | INFO | train_inner | epoch 056:    281 / 1475 loss=2.447, nll_loss=0.817, ppl=1.76, wps=59410.8, ups=15.59, wpb=3811.6, bsz=194.7, num_updates=81400, lr=0.000110838, gnorm=0.894, loss_scale=32, train_wall=6, gb_free=29.7, wall=5683
2024-01-30 17:08:10 | INFO | train_inner | epoch 056:    381 / 1475 loss=2.422, nll_loss=0.788, ppl=1.73, wps=58923.7, ups=15.54, wpb=3792.2, bsz=208.6, num_updates=81500, lr=0.00011077, gnorm=0.895, loss_scale=32, train_wall=6, gb_free=29.7, wall=5689
2024-01-30 17:08:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 17:08:17 | INFO | train_inner | epoch 056:    482 / 1475 loss=2.428, nll_loss=0.796, ppl=1.74, wps=57834.7, ups=15.43, wpb=3749.3, bsz=202.7, num_updates=81600, lr=0.000110702, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.7, wall=5696
2024-01-30 17:08:23 | INFO | train_inner | epoch 056:    582 / 1475 loss=2.454, nll_loss=0.825, ppl=1.77, wps=58345, ups=15.61, wpb=3736.7, bsz=191.2, num_updates=81700, lr=0.000110634, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=5702
2024-01-30 17:08:30 | INFO | train_inner | epoch 056:    682 / 1475 loss=2.466, nll_loss=0.839, ppl=1.79, wps=58264.3, ups=15.63, wpb=3727.6, bsz=186.9, num_updates=81800, lr=0.000110566, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.6, wall=5709
2024-01-30 17:08:36 | INFO | train_inner | epoch 056:    782 / 1475 loss=2.484, nll_loss=0.859, ppl=1.81, wps=58973.2, ups=15.63, wpb=3773.2, bsz=182.4, num_updates=81900, lr=0.000110499, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.7, wall=5715
2024-01-30 17:08:42 | INFO | train_inner | epoch 056:    882 / 1475 loss=2.443, nll_loss=0.814, ppl=1.76, wps=58404.7, ups=15.55, wpb=3756.4, bsz=197, num_updates=82000, lr=0.000110432, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.7, wall=5722
2024-01-30 17:08:49 | INFO | train_inner | epoch 056:    982 / 1475 loss=2.469, nll_loss=0.844, ppl=1.79, wps=59161.9, ups=15.62, wpb=3787.2, bsz=199.9, num_updates=82100, lr=0.000110364, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.7, wall=5728
2024-01-30 17:08:55 | INFO | train_inner | epoch 056:   1082 / 1475 loss=2.435, nll_loss=0.805, ppl=1.75, wps=59106.7, ups=15.41, wpb=3836.3, bsz=217.7, num_updates=82200, lr=0.000110297, gnorm=0.87, loss_scale=16, train_wall=6, gb_free=29.8, wall=5734
2024-01-30 17:09:02 | INFO | train_inner | epoch 056:   1182 / 1475 loss=2.481, nll_loss=0.857, ppl=1.81, wps=58974.7, ups=15.58, wpb=3786.4, bsz=184.5, num_updates=82300, lr=0.00011023, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.6, wall=5741
2024-01-30 17:09:08 | INFO | train_inner | epoch 056:   1282 / 1475 loss=2.477, nll_loss=0.853, ppl=1.81, wps=58414.7, ups=15.67, wpb=3727, bsz=189, num_updates=82400, lr=0.000110163, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=5747
2024-01-30 17:09:15 | INFO | train_inner | epoch 056:   1382 / 1475 loss=2.467, nll_loss=0.841, ppl=1.79, wps=58531.2, ups=15.66, wpb=3737.3, bsz=192.3, num_updates=82500, lr=0.000110096, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=5754
2024-01-30 17:09:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:09:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:09:23 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.421 | nll_loss 1.784 | ppl 3.44 | wps 134250 | wpb 3189.1 | bsz 163.4 | num_updates 82593 | best_loss 3.385
2024-01-30 17:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 82593 updates
2024-01-30 17:09:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint56.pt
2024-01-30 17:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint56.pt
2024-01-30 17:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint56.pt (epoch 56 @ 82593 updates, score 3.421) (writing took 2.4098057929659262 seconds)
2024-01-30 17:09:25 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-01-30 17:09:25 | INFO | train | epoch 056 | loss 2.453 | nll_loss 0.825 | ppl 1.77 | wps 55745.1 | ups 14.78 | wpb 3771.4 | bsz 196 | num_updates 82593 | lr 0.000110034 | gnorm 0.913 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 5764
2024-01-30 17:09:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:09:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:09:25 | INFO | fairseq.trainer | begin training epoch 57
2024-01-30 17:09:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:09:26 | INFO | train_inner | epoch 057:      7 / 1475 loss=2.471, nll_loss=0.844, ppl=1.8, wps=33296.9, ups=8.89, wpb=3745.4, bsz=191.4, num_updates=82600, lr=0.00011003, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=5765
2024-01-30 17:09:32 | INFO | train_inner | epoch 057:    107 / 1475 loss=2.408, nll_loss=0.773, ppl=1.71, wps=59201.6, ups=15.51, wpb=3816, bsz=202.3, num_updates=82700, lr=0.000109963, gnorm=0.862, loss_scale=16, train_wall=6, gb_free=29.8, wall=5771
2024-01-30 17:09:39 | INFO | train_inner | epoch 057:    207 / 1475 loss=2.431, nll_loss=0.799, ppl=1.74, wps=59019.8, ups=15.56, wpb=3792.5, bsz=189.4, num_updates=82800, lr=0.000109897, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.7, wall=5778
2024-01-30 17:09:45 | INFO | train_inner | epoch 057:    307 / 1475 loss=2.417, nll_loss=0.784, ppl=1.72, wps=58382, ups=15.54, wpb=3756.6, bsz=205.4, num_updates=82900, lr=0.00010983, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=5784
2024-01-30 17:09:51 | INFO | train_inner | epoch 057:    407 / 1475 loss=2.445, nll_loss=0.815, ppl=1.76, wps=59584.3, ups=15.6, wpb=3819.9, bsz=196.2, num_updates=83000, lr=0.000109764, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.6, wall=5791
2024-01-30 17:09:58 | INFO | train_inner | epoch 057:    507 / 1475 loss=2.438, nll_loss=0.807, ppl=1.75, wps=59657, ups=15.58, wpb=3830, bsz=196.4, num_updates=83100, lr=0.000109698, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.8, wall=5797
2024-01-30 17:10:04 | INFO | train_inner | epoch 057:    607 / 1475 loss=2.461, nll_loss=0.834, ppl=1.78, wps=57962.6, ups=15.67, wpb=3698.2, bsz=184.2, num_updates=83200, lr=0.000109632, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=5803
2024-01-30 17:10:11 | INFO | train_inner | epoch 057:    707 / 1475 loss=2.441, nll_loss=0.812, ppl=1.76, wps=58013.5, ups=15.56, wpb=3727.2, bsz=193.6, num_updates=83300, lr=0.000109566, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.6, wall=5810
2024-01-30 17:10:17 | INFO | train_inner | epoch 057:    807 / 1475 loss=2.445, nll_loss=0.816, ppl=1.76, wps=59178.8, ups=15.57, wpb=3801, bsz=203.7, num_updates=83400, lr=0.000109501, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=5816
2024-01-30 17:10:24 | INFO | train_inner | epoch 057:    907 / 1475 loss=2.454, nll_loss=0.825, ppl=1.77, wps=57697.3, ups=15.53, wpb=3715.7, bsz=187, num_updates=83500, lr=0.000109435, gnorm=0.949, loss_scale=16, train_wall=6, gb_free=29.7, wall=5823
2024-01-30 17:10:30 | INFO | train_inner | epoch 057:   1007 / 1475 loss=2.431, nll_loss=0.8, ppl=1.74, wps=57983.1, ups=15.49, wpb=3744.2, bsz=210.4, num_updates=83600, lr=0.00010937, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.9, wall=5829
2024-01-30 17:10:36 | INFO | train_inner | epoch 057:   1107 / 1475 loss=2.476, nll_loss=0.85, ppl=1.8, wps=58992.5, ups=15.76, wpb=3743.5, bsz=186.6, num_updates=83700, lr=0.000109304, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.7, wall=5836
2024-01-30 17:10:43 | INFO | train_inner | epoch 057:   1207 / 1475 loss=2.453, nll_loss=0.825, ppl=1.77, wps=58752.5, ups=15.54, wpb=3780.3, bsz=205.5, num_updates=83800, lr=0.000109239, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.8, wall=5842
2024-01-30 17:10:49 | INFO | train_inner | epoch 057:   1307 / 1475 loss=2.48, nll_loss=0.856, ppl=1.81, wps=59330.8, ups=15.56, wpb=3813.7, bsz=188.2, num_updates=83900, lr=0.000109174, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.6, wall=5848
2024-01-30 17:10:56 | INFO | train_inner | epoch 057:   1407 / 1475 loss=2.476, nll_loss=0.851, ppl=1.8, wps=58965.2, ups=15.66, wpb=3766.1, bsz=191.4, num_updates=84000, lr=0.000109109, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.8, wall=5855
2024-01-30 17:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:11:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:11:02 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.416 | nll_loss 1.782 | ppl 3.44 | wps 134360 | wpb 3189.1 | bsz 163.4 | num_updates 84068 | best_loss 3.385
2024-01-30 17:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 84068 updates
2024-01-30 17:11:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint57.pt
2024-01-30 17:11:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint57.pt
2024-01-30 17:11:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint57.pt (epoch 57 @ 84068 updates, score 3.416) (writing took 2.3303487800294533 seconds)
2024-01-30 17:11:05 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-01-30 17:11:05 | INFO | train | epoch 057 | loss 2.448 | nll_loss 0.819 | ppl 1.76 | wps 55936.2 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 84068 | lr 0.000109065 | gnorm 0.914 | loss_scale 16 | train_wall 91 | gb_free 29.6 | wall 5864
2024-01-30 17:11:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:11:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:11:05 | INFO | fairseq.trainer | begin training epoch 58
2024-01-30 17:11:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:11:07 | INFO | train_inner | epoch 058:     32 / 1475 loss=2.464, nll_loss=0.837, ppl=1.79, wps=33809.6, ups=8.94, wpb=3783.9, bsz=196.2, num_updates=84100, lr=0.000109044, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=5866
2024-01-30 17:11:13 | INFO | train_inner | epoch 058:    132 / 1475 loss=2.421, nll_loss=0.788, ppl=1.73, wps=59191.8, ups=15.6, wpb=3794, bsz=197.7, num_updates=84200, lr=0.000108979, gnorm=0.879, loss_scale=16, train_wall=6, gb_free=29.7, wall=5872
2024-01-30 17:11:20 | INFO | train_inner | epoch 058:    232 / 1475 loss=2.415, nll_loss=0.783, ppl=1.72, wps=57867.1, ups=15.48, wpb=3737.3, bsz=201.3, num_updates=84300, lr=0.000108915, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.7, wall=5879
2024-01-30 17:11:26 | INFO | train_inner | epoch 058:    332 / 1475 loss=2.416, nll_loss=0.782, ppl=1.72, wps=58785.5, ups=15.51, wpb=3789, bsz=203.2, num_updates=84400, lr=0.00010885, gnorm=0.873, loss_scale=16, train_wall=6, gb_free=29.7, wall=5885
2024-01-30 17:11:32 | INFO | train_inner | epoch 058:    432 / 1475 loss=2.45, nll_loss=0.82, ppl=1.77, wps=58754.5, ups=15.74, wpb=3733, bsz=188.2, num_updates=84500, lr=0.000108786, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.8, wall=5892
2024-01-30 17:11:39 | INFO | train_inner | epoch 058:    532 / 1475 loss=2.441, nll_loss=0.81, ppl=1.75, wps=58468.5, ups=15.66, wpb=3734.4, bsz=189, num_updates=84600, lr=0.000108721, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=5898
2024-01-30 17:11:45 | INFO | train_inner | epoch 058:    632 / 1475 loss=2.433, nll_loss=0.801, ppl=1.74, wps=58875.2, ups=15.59, wpb=3776, bsz=192.6, num_updates=84700, lr=0.000108657, gnorm=0.943, loss_scale=16, train_wall=6, gb_free=29.7, wall=5904
2024-01-30 17:11:52 | INFO | train_inner | epoch 058:    732 / 1475 loss=2.442, nll_loss=0.811, ppl=1.75, wps=59531.4, ups=15.61, wpb=3812.6, bsz=197.5, num_updates=84800, lr=0.000108593, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=5911
2024-01-30 17:11:58 | INFO | train_inner | epoch 058:    832 / 1475 loss=2.456, nll_loss=0.828, ppl=1.78, wps=58419.8, ups=15.58, wpb=3750.3, bsz=196.2, num_updates=84900, lr=0.000108529, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.7, wall=5917
2024-01-30 17:12:05 | INFO | train_inner | epoch 058:    932 / 1475 loss=2.432, nll_loss=0.801, ppl=1.74, wps=57986.3, ups=15.58, wpb=3721.3, bsz=198.1, num_updates=85000, lr=0.000108465, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=5924
2024-01-30 17:12:11 | INFO | train_inner | epoch 058:   1032 / 1475 loss=2.461, nll_loss=0.836, ppl=1.78, wps=58982, ups=15.51, wpb=3802, bsz=190.4, num_updates=85100, lr=0.000108401, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=5930
2024-01-30 17:12:17 | INFO | train_inner | epoch 058:   1132 / 1475 loss=2.455, nll_loss=0.827, ppl=1.77, wps=58810.1, ups=15.57, wpb=3777.1, bsz=197, num_updates=85200, lr=0.000108338, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.6, wall=5937
2024-01-30 17:12:24 | INFO | train_inner | epoch 058:   1232 / 1475 loss=2.46, nll_loss=0.833, ppl=1.78, wps=58216.7, ups=15.49, wpb=3758.4, bsz=195, num_updates=85300, lr=0.000108274, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=5943
2024-01-30 17:12:30 | INFO | train_inner | epoch 058:   1332 / 1475 loss=2.465, nll_loss=0.839, ppl=1.79, wps=59680.4, ups=15.58, wpb=3830, bsz=194.9, num_updates=85400, lr=0.000108211, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=5949
2024-01-30 17:12:37 | INFO | train_inner | epoch 058:   1432 / 1475 loss=2.45, nll_loss=0.822, ppl=1.77, wps=58450.6, ups=15.6, wpb=3746, bsz=200.5, num_updates=85500, lr=0.000108148, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=5956
2024-01-30 17:12:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:12:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:12:42 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.416 | nll_loss 1.782 | ppl 3.44 | wps 134319 | wpb 3189.1 | bsz 163.4 | num_updates 85543 | best_loss 3.385
2024-01-30 17:12:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 85543 updates
2024-01-30 17:12:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint58.pt
2024-01-30 17:12:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint58.pt
2024-01-30 17:12:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint58.pt (epoch 58 @ 85543 updates, score 3.416) (writing took 2.342580024036579 seconds)
2024-01-30 17:12:44 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-01-30 17:12:44 | INFO | train | epoch 058 | loss 2.442 | nll_loss 0.813 | ppl 1.76 | wps 55922.2 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 85543 | lr 0.00010812 | gnorm 0.916 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 5963
2024-01-30 17:12:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:12:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:12:44 | INFO | fairseq.trainer | begin training epoch 59
2024-01-30 17:12:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:12:48 | INFO | train_inner | epoch 059:     57 / 1475 loss=2.43, nll_loss=0.798, ppl=1.74, wps=33648.3, ups=8.92, wpb=3772.9, bsz=196.9, num_updates=85600, lr=0.000108084, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.8, wall=5967
2024-01-30 17:12:54 | INFO | train_inner | epoch 059:    157 / 1475 loss=2.417, nll_loss=0.783, ppl=1.72, wps=58854.6, ups=15.64, wpb=3762.3, bsz=191.4, num_updates=85700, lr=0.000108021, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.7, wall=5973
2024-01-30 17:13:01 | INFO | train_inner | epoch 059:    257 / 1475 loss=2.401, nll_loss=0.765, ppl=1.7, wps=59197.6, ups=15.62, wpb=3789.9, bsz=207.4, num_updates=85800, lr=0.000107958, gnorm=0.869, loss_scale=16, train_wall=6, gb_free=29.7, wall=5980
2024-01-30 17:13:07 | INFO | train_inner | epoch 059:    357 / 1475 loss=2.438, nll_loss=0.807, ppl=1.75, wps=59607.4, ups=15.54, wpb=3836.8, bsz=193.9, num_updates=85900, lr=0.000107896, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=5986
2024-01-30 17:13:14 | INFO | train_inner | epoch 059:    457 / 1475 loss=2.424, nll_loss=0.792, ppl=1.73, wps=59076.2, ups=15.53, wpb=3803.4, bsz=198.4, num_updates=86000, lr=0.000107833, gnorm=0.886, loss_scale=16, train_wall=6, gb_free=29.7, wall=5993
2024-01-30 17:13:20 | INFO | train_inner | epoch 059:    557 / 1475 loss=2.439, nll_loss=0.808, ppl=1.75, wps=59953.2, ups=15.65, wpb=3831.6, bsz=190, num_updates=86100, lr=0.00010777, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=5999
2024-01-30 17:13:26 | INFO | train_inner | epoch 059:    657 / 1475 loss=2.433, nll_loss=0.803, ppl=1.74, wps=58362.2, ups=15.57, wpb=3748.1, bsz=195.5, num_updates=86200, lr=0.000107708, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=6006
2024-01-30 17:13:33 | INFO | train_inner | epoch 059:    757 / 1475 loss=2.458, nll_loss=0.83, ppl=1.78, wps=58646.5, ups=15.61, wpb=3757.8, bsz=187.1, num_updates=86300, lr=0.000107645, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.8, wall=6012
2024-01-30 17:13:39 | INFO | train_inner | epoch 059:    857 / 1475 loss=2.461, nll_loss=0.833, ppl=1.78, wps=58612.3, ups=15.74, wpb=3724.1, bsz=189.8, num_updates=86400, lr=0.000107583, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=6018
2024-01-30 17:13:46 | INFO | train_inner | epoch 059:    957 / 1475 loss=2.438, nll_loss=0.809, ppl=1.75, wps=58810.1, ups=15.56, wpb=3779.8, bsz=195.4, num_updates=86500, lr=0.000107521, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=6025
2024-01-30 17:13:52 | INFO | train_inner | epoch 059:   1057 / 1475 loss=2.442, nll_loss=0.812, ppl=1.76, wps=59010.4, ups=15.61, wpb=3779.2, bsz=200.2, num_updates=86600, lr=0.000107459, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=6031
2024-01-30 17:13:58 | INFO | train_inner | epoch 059:   1157 / 1475 loss=2.442, nll_loss=0.813, ppl=1.76, wps=58270.8, ups=15.57, wpb=3742.8, bsz=193.2, num_updates=86700, lr=0.000107397, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=6038
2024-01-30 17:14:05 | INFO | train_inner | epoch 059:   1257 / 1475 loss=2.427, nll_loss=0.797, ppl=1.74, wps=58245.1, ups=15.55, wpb=3746.1, bsz=205.7, num_updates=86800, lr=0.000107335, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=6044
2024-01-30 17:14:11 | INFO | train_inner | epoch 059:   1357 / 1475 loss=2.443, nll_loss=0.813, ppl=1.76, wps=58350.1, ups=15.57, wpb=3746.5, bsz=200.7, num_updates=86900, lr=0.000107273, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.7, wall=6050
2024-01-30 17:14:18 | INFO | train_inner | epoch 059:   1457 / 1475 loss=2.46, nll_loss=0.834, ppl=1.78, wps=58648.9, ups=15.57, wpb=3766.3, bsz=192.6, num_updates=87000, lr=0.000107211, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=6057
2024-01-30 17:14:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:14:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:14:21 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.425 | nll_loss 1.79 | ppl 3.46 | wps 135151 | wpb 3189.1 | bsz 163.4 | num_updates 87018 | best_loss 3.385
2024-01-30 17:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 87018 updates
2024-01-30 17:14:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint59.pt
2024-01-30 17:14:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint59.pt
2024-01-30 17:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint59.pt (epoch 59 @ 87018 updates, score 3.425) (writing took 2.3352347490144894 seconds)
2024-01-30 17:14:23 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-01-30 17:14:23 | INFO | train | epoch 059 | loss 2.436 | nll_loss 0.806 | ppl 1.75 | wps 55983.2 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 87018 | lr 0.0001072 | gnorm 0.909 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 6063
2024-01-30 17:14:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:14:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:14:24 | INFO | fairseq.trainer | begin training epoch 60
2024-01-30 17:14:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:14:29 | INFO | train_inner | epoch 060:     82 / 1475 loss=2.405, nll_loss=0.769, ppl=1.7, wps=33728.7, ups=8.9, wpb=3791.8, bsz=199.8, num_updates=87100, lr=0.00010715, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=6068
2024-01-30 17:14:35 | INFO | train_inner | epoch 060:    182 / 1475 loss=2.411, nll_loss=0.777, ppl=1.71, wps=59991.6, ups=15.5, wpb=3870.9, bsz=202.5, num_updates=87200, lr=0.000107088, gnorm=0.886, loss_scale=16, train_wall=6, gb_free=29.8, wall=6074
2024-01-30 17:14:42 | INFO | train_inner | epoch 060:    282 / 1475 loss=2.409, nll_loss=0.774, ppl=1.71, wps=57814.7, ups=15.57, wpb=3712.4, bsz=203.2, num_updates=87300, lr=0.000107027, gnorm=0.893, loss_scale=16, train_wall=6, gb_free=29.6, wall=6081
2024-01-30 17:14:48 | INFO | train_inner | epoch 060:    382 / 1475 loss=2.412, nll_loss=0.779, ppl=1.72, wps=57939.2, ups=15.55, wpb=3726.7, bsz=197.9, num_updates=87400, lr=0.000106966, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=6087
2024-01-30 17:14:55 | INFO | train_inner | epoch 060:    482 / 1475 loss=2.417, nll_loss=0.784, ppl=1.72, wps=57615.4, ups=15.61, wpb=3691.7, bsz=194.6, num_updates=87500, lr=0.000106904, gnorm=0.895, loss_scale=16, train_wall=6, gb_free=29.7, wall=6094
2024-01-30 17:15:01 | INFO | train_inner | epoch 060:    582 / 1475 loss=2.427, nll_loss=0.796, ppl=1.74, wps=58705.1, ups=15.53, wpb=3780.5, bsz=199.6, num_updates=87600, lr=0.000106843, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.8, wall=6100
2024-01-30 17:15:07 | INFO | train_inner | epoch 060:    682 / 1475 loss=2.434, nll_loss=0.803, ppl=1.74, wps=58661.1, ups=15.6, wpb=3760.8, bsz=195.7, num_updates=87700, lr=0.000106783, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.8, wall=6107
2024-01-30 17:15:14 | INFO | train_inner | epoch 060:    782 / 1475 loss=2.45, nll_loss=0.822, ppl=1.77, wps=58190.8, ups=15.67, wpb=3714.4, bsz=179.1, num_updates=87800, lr=0.000106722, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.6, wall=6113
2024-01-30 17:15:20 | INFO | train_inner | epoch 060:    882 / 1475 loss=2.434, nll_loss=0.803, ppl=1.75, wps=59051.2, ups=15.55, wpb=3797.1, bsz=196.8, num_updates=87900, lr=0.000106661, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=6119
2024-01-30 17:15:27 | INFO | train_inner | epoch 060:    982 / 1475 loss=2.457, nll_loss=0.829, ppl=1.78, wps=58392.6, ups=15.67, wpb=3725.9, bsz=186.8, num_updates=88000, lr=0.0001066, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.8, wall=6126
2024-01-30 17:15:33 | INFO | train_inner | epoch 060:   1082 / 1475 loss=2.439, nll_loss=0.809, ppl=1.75, wps=58975.1, ups=15.57, wpb=3786.8, bsz=199.6, num_updates=88100, lr=0.00010654, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=6132
2024-01-30 17:15:40 | INFO | train_inner | epoch 060:   1182 / 1475 loss=2.435, nll_loss=0.804, ppl=1.75, wps=59698.9, ups=15.58, wpb=3830.8, bsz=203.2, num_updates=88200, lr=0.000106479, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.8, wall=6139
2024-01-30 17:15:46 | INFO | train_inner | epoch 060:   1282 / 1475 loss=2.439, nll_loss=0.809, ppl=1.75, wps=59526, ups=15.59, wpb=3817.9, bsz=203.2, num_updates=88300, lr=0.000106419, gnorm=0.892, loss_scale=16, train_wall=6, gb_free=29.7, wall=6145
2024-01-30 17:15:52 | INFO | train_inner | epoch 060:   1382 / 1475 loss=2.457, nll_loss=0.829, ppl=1.78, wps=58663.3, ups=15.63, wpb=3753.9, bsz=191.1, num_updates=88400, lr=0.000106359, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=6151
2024-01-30 17:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:15:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:16:01 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.432 | nll_loss 1.797 | ppl 3.48 | wps 134169 | wpb 3189.1 | bsz 163.4 | num_updates 88493 | best_loss 3.385
2024-01-30 17:16:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 88493 updates
2024-01-30 17:16:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint60.pt
2024-01-30 17:16:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint60.pt
2024-01-30 17:16:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint60.pt (epoch 60 @ 88493 updates, score 3.432) (writing took 2.436653947108425 seconds)
2024-01-30 17:16:03 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-01-30 17:16:03 | INFO | train | epoch 060 | loss 2.432 | nll_loss 0.801 | ppl 1.74 | wps 55860.4 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 88493 | lr 0.000106303 | gnorm 0.914 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 6162
2024-01-30 17:16:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:16:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:16:03 | INFO | fairseq.trainer | begin training epoch 61
2024-01-30 17:16:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:16:04 | INFO | train_inner | epoch 061:      7 / 1475 loss=2.449, nll_loss=0.821, ppl=1.77, wps=33714.2, ups=8.82, wpb=3822.1, bsz=191.8, num_updates=88500, lr=0.000106299, gnorm=0.903, loss_scale=16, train_wall=6, gb_free=29.8, wall=6163
2024-01-30 17:16:10 | INFO | train_inner | epoch 061:    107 / 1475 loss=2.396, nll_loss=0.76, ppl=1.69, wps=59171, ups=15.57, wpb=3801.4, bsz=203.3, num_updates=88600, lr=0.000106239, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.7, wall=6169
2024-01-30 17:16:17 | INFO | train_inner | epoch 061:    207 / 1475 loss=2.384, nll_loss=0.747, ppl=1.68, wps=59025.6, ups=15.56, wpb=3793.6, bsz=204.6, num_updates=88700, lr=0.000106179, gnorm=0.869, loss_scale=16, train_wall=6, gb_free=29.7, wall=6176
2024-01-30 17:16:23 | INFO | train_inner | epoch 061:    307 / 1475 loss=2.398, nll_loss=0.763, ppl=1.7, wps=58275.2, ups=15.55, wpb=3747.1, bsz=201.1, num_updates=88800, lr=0.000106119, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.8, wall=6182
2024-01-30 17:16:29 | INFO | train_inner | epoch 061:    407 / 1475 loss=2.413, nll_loss=0.779, ppl=1.72, wps=58580.5, ups=15.51, wpb=3777.2, bsz=199.4, num_updates=88900, lr=0.000106059, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.9, wall=6189
2024-01-30 17:16:36 | INFO | train_inner | epoch 061:    507 / 1475 loss=2.439, nll_loss=0.809, ppl=1.75, wps=59860.8, ups=15.65, wpb=3824.7, bsz=188, num_updates=89000, lr=0.000106, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=6195
2024-01-30 17:16:42 | INFO | train_inner | epoch 061:    607 / 1475 loss=2.425, nll_loss=0.794, ppl=1.73, wps=58610, ups=15.56, wpb=3765.9, bsz=196.6, num_updates=89100, lr=0.00010594, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.8, wall=6201
2024-01-30 17:16:49 | INFO | train_inner | epoch 061:    707 / 1475 loss=2.423, nll_loss=0.792, ppl=1.73, wps=59008.2, ups=15.58, wpb=3786.3, bsz=196.7, num_updates=89200, lr=0.000105881, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.7, wall=6208
2024-01-30 17:16:55 | INFO | train_inner | epoch 061:    807 / 1475 loss=2.434, nll_loss=0.804, ppl=1.75, wps=59299.2, ups=15.55, wpb=3814.4, bsz=191.7, num_updates=89300, lr=0.000105822, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.6, wall=6214
2024-01-30 17:17:01 | INFO | train_inner | epoch 061:    907 / 1475 loss=2.439, nll_loss=0.809, ppl=1.75, wps=59473.3, ups=15.64, wpb=3803.6, bsz=188.4, num_updates=89400, lr=0.000105762, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.8, wall=6221
2024-01-30 17:17:08 | INFO | train_inner | epoch 061:   1007 / 1475 loss=2.443, nll_loss=0.814, ppl=1.76, wps=57983.2, ups=15.59, wpb=3720.3, bsz=189.8, num_updates=89500, lr=0.000105703, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=6227
2024-01-30 17:17:14 | INFO | train_inner | epoch 061:   1107 / 1475 loss=2.439, nll_loss=0.81, ppl=1.75, wps=59351.9, ups=15.54, wpb=3819.9, bsz=199, num_updates=89600, lr=0.000105644, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=6233
2024-01-30 17:17:21 | INFO | train_inner | epoch 061:   1207 / 1475 loss=2.425, nll_loss=0.793, ppl=1.73, wps=58053.7, ups=15.66, wpb=3706.1, bsz=192.8, num_updates=89700, lr=0.000105585, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=6240
2024-01-30 17:17:27 | INFO | train_inner | epoch 061:   1307 / 1475 loss=2.447, nll_loss=0.819, ppl=1.76, wps=59008.1, ups=15.57, wpb=3789.9, bsz=199.6, num_updates=89800, lr=0.000105527, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=6246
2024-01-30 17:17:34 | INFO | train_inner | epoch 061:   1407 / 1475 loss=2.463, nll_loss=0.836, ppl=1.79, wps=58605.6, ups=15.64, wpb=3746.3, bsz=188.3, num_updates=89900, lr=0.000105468, gnorm=0.961, loss_scale=16, train_wall=6, gb_free=29.8, wall=6253
2024-01-30 17:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:17:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:17:40 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.425 | nll_loss 1.79 | ppl 3.46 | wps 135442 | wpb 3189.1 | bsz 163.4 | num_updates 89968 | best_loss 3.385
2024-01-30 17:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 89968 updates
2024-01-30 17:17:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint61.pt
2024-01-30 17:17:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint61.pt
2024-01-30 17:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint61.pt (epoch 61 @ 89968 updates, score 3.425) (writing took 2.382956227986142 seconds)
2024-01-30 17:17:43 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-01-30 17:17:43 | INFO | train | epoch 061 | loss 2.427 | nll_loss 0.795 | ppl 1.74 | wps 55908.6 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 89968 | lr 0.000105428 | gnorm 0.916 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 6262
2024-01-30 17:17:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:17:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:17:43 | INFO | fairseq.trainer | begin training epoch 62
2024-01-30 17:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:17:45 | INFO | train_inner | epoch 062:     32 / 1475 loss=2.434, nll_loss=0.805, ppl=1.75, wps=32696.9, ups=8.88, wpb=3680.3, bsz=193.8, num_updates=90000, lr=0.000105409, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=6264
2024-01-30 17:17:51 | INFO | train_inner | epoch 062:    132 / 1475 loss=2.395, nll_loss=0.758, ppl=1.69, wps=58984.2, ups=15.63, wpb=3772.9, bsz=195.8, num_updates=90100, lr=0.000105351, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.9, wall=6270
2024-01-30 17:17:58 | INFO | train_inner | epoch 062:    232 / 1475 loss=2.39, nll_loss=0.753, ppl=1.69, wps=58092.7, ups=15.52, wpb=3743.8, bsz=209.2, num_updates=90200, lr=0.000105292, gnorm=0.904, loss_scale=16, train_wall=6, gb_free=29.7, wall=6277
2024-01-30 17:18:04 | INFO | train_inner | epoch 062:    332 / 1475 loss=2.404, nll_loss=0.77, ppl=1.71, wps=59350.5, ups=15.53, wpb=3822.1, bsz=202.8, num_updates=90300, lr=0.000105234, gnorm=0.883, loss_scale=16, train_wall=6, gb_free=29.7, wall=6283
2024-01-30 17:18:10 | INFO | train_inner | epoch 062:    432 / 1475 loss=2.403, nll_loss=0.768, ppl=1.7, wps=58848.6, ups=15.57, wpb=3778.9, bsz=199, num_updates=90400, lr=0.000105176, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=6290
2024-01-30 17:18:17 | INFO | train_inner | epoch 062:    532 / 1475 loss=2.421, nll_loss=0.789, ppl=1.73, wps=59838.8, ups=15.52, wpb=3855.1, bsz=197.3, num_updates=90500, lr=0.000105118, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.6, wall=6296
2024-01-30 17:18:23 | INFO | train_inner | epoch 062:    632 / 1475 loss=2.422, nll_loss=0.79, ppl=1.73, wps=58613.3, ups=15.56, wpb=3766.2, bsz=186.3, num_updates=90600, lr=0.00010506, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=6302
2024-01-30 17:18:30 | INFO | train_inner | epoch 062:    732 / 1475 loss=2.402, nll_loss=0.768, ppl=1.7, wps=58574.8, ups=15.48, wpb=3784.7, bsz=210.2, num_updates=90700, lr=0.000105002, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.7, wall=6309
2024-01-30 17:18:36 | INFO | train_inner | epoch 062:    832 / 1475 loss=2.434, nll_loss=0.802, ppl=1.74, wps=58692.2, ups=15.72, wpb=3732.7, bsz=183.5, num_updates=90800, lr=0.000104944, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.7, wall=6315
2024-01-30 17:18:43 | INFO | train_inner | epoch 062:    932 / 1475 loss=2.428, nll_loss=0.796, ppl=1.74, wps=58871.5, ups=15.61, wpb=3771.2, bsz=197, num_updates=90900, lr=0.000104886, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=6322
2024-01-30 17:18:49 | INFO | train_inner | epoch 062:   1032 / 1475 loss=2.433, nll_loss=0.803, ppl=1.74, wps=58353.4, ups=15.6, wpb=3740.4, bsz=189, num_updates=91000, lr=0.000104828, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=30.1, wall=6328
2024-01-30 17:18:55 | INFO | train_inner | epoch 062:   1132 / 1475 loss=2.442, nll_loss=0.813, ppl=1.76, wps=59698.4, ups=15.6, wpb=3826.5, bsz=195.8, num_updates=91100, lr=0.000104771, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.7, wall=6335
2024-01-30 17:19:02 | INFO | train_inner | epoch 062:   1232 / 1475 loss=2.433, nll_loss=0.803, ppl=1.74, wps=57672.3, ups=15.55, wpb=3707.9, bsz=195.4, num_updates=91200, lr=0.000104713, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.6, wall=6341
2024-01-30 17:19:08 | INFO | train_inner | epoch 062:   1332 / 1475 loss=2.458, nll_loss=0.831, ppl=1.78, wps=58827.8, ups=15.6, wpb=3771.1, bsz=182.2, num_updates=91300, lr=0.000104656, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.7, wall=6347
2024-01-30 17:19:15 | INFO | train_inner | epoch 062:   1432 / 1475 loss=2.426, nll_loss=0.794, ppl=1.73, wps=58844.3, ups=15.5, wpb=3795.7, bsz=210.4, num_updates=91400, lr=0.000104599, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.7, wall=6354
2024-01-30 17:19:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:19:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:19:20 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.426 | nll_loss 1.795 | ppl 3.47 | wps 135073 | wpb 3189.1 | bsz 163.4 | num_updates 91443 | best_loss 3.385
2024-01-30 17:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 91443 updates
2024-01-30 17:19:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint62.pt
2024-01-30 17:19:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint62.pt
2024-01-30 17:19:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint62.pt (epoch 62 @ 91443 updates, score 3.426) (writing took 2.3434612499549985 seconds)
2024-01-30 17:19:22 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-01-30 17:19:22 | INFO | train | epoch 062 | loss 2.422 | nll_loss 0.79 | ppl 1.73 | wps 55922.7 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 91443 | lr 0.000104574 | gnorm 0.919 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 6361
2024-01-30 17:19:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:19:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:19:22 | INFO | fairseq.trainer | begin training epoch 63
2024-01-30 17:19:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:19:26 | INFO | train_inner | epoch 063:     57 / 1475 loss=2.443, nll_loss=0.812, ppl=1.76, wps=33358.4, ups=8.99, wpb=3712.1, bsz=179.2, num_updates=91500, lr=0.000104542, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.7, wall=6365
2024-01-30 17:19:32 | INFO | train_inner | epoch 063:    157 / 1475 loss=2.384, nll_loss=0.746, ppl=1.68, wps=57799, ups=15.53, wpb=3722.7, bsz=196.1, num_updates=91600, lr=0.000104485, gnorm=0.891, loss_scale=16, train_wall=6, gb_free=29.7, wall=6371
2024-01-30 17:19:39 | INFO | train_inner | epoch 063:    257 / 1475 loss=2.406, nll_loss=0.772, ppl=1.71, wps=58746.3, ups=15.61, wpb=3763.4, bsz=193.7, num_updates=91700, lr=0.000104428, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=6378
2024-01-30 17:19:45 | INFO | train_inner | epoch 063:    357 / 1475 loss=2.407, nll_loss=0.773, ppl=1.71, wps=58170.7, ups=15.65, wpb=3717.4, bsz=186.6, num_updates=91800, lr=0.000104371, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=6384
2024-01-30 17:19:51 | INFO | train_inner | epoch 063:    457 / 1475 loss=2.414, nll_loss=0.78, ppl=1.72, wps=59515.1, ups=15.57, wpb=3822.7, bsz=192.3, num_updates=91900, lr=0.000104314, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=6391
2024-01-30 17:19:58 | INFO | train_inner | epoch 063:    557 / 1475 loss=2.418, nll_loss=0.785, ppl=1.72, wps=58790.3, ups=15.65, wpb=3756.1, bsz=190, num_updates=92000, lr=0.000104257, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=6397
2024-01-30 17:20:04 | INFO | train_inner | epoch 063:    657 / 1475 loss=2.403, nll_loss=0.77, ppl=1.7, wps=58479.4, ups=15.55, wpb=3760.1, bsz=197.4, num_updates=92100, lr=0.000104201, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=6403
2024-01-30 17:20:11 | INFO | train_inner | epoch 063:    757 / 1475 loss=2.42, nll_loss=0.788, ppl=1.73, wps=59448.2, ups=15.62, wpb=3805.6, bsz=202.9, num_updates=92200, lr=0.000104144, gnorm=0.903, loss_scale=16, train_wall=6, gb_free=29.9, wall=6410
2024-01-30 17:20:17 | INFO | train_inner | epoch 063:    857 / 1475 loss=2.43, nll_loss=0.8, ppl=1.74, wps=60099.6, ups=15.58, wpb=3857.8, bsz=192.5, num_updates=92300, lr=0.000104088, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.8, wall=6416
2024-01-30 17:20:24 | INFO | train_inner | epoch 063:    957 / 1475 loss=2.415, nll_loss=0.783, ppl=1.72, wps=59579.6, ups=15.49, wpb=3845.5, bsz=203, num_updates=92400, lr=0.000104031, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.7, wall=6423
2024-01-30 17:20:30 | INFO | train_inner | epoch 063:   1057 / 1475 loss=2.415, nll_loss=0.783, ppl=1.72, wps=58907.9, ups=15.5, wpb=3800.2, bsz=210.6, num_updates=92500, lr=0.000103975, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=6429
2024-01-30 17:20:36 | INFO | train_inner | epoch 063:   1157 / 1475 loss=2.417, nll_loss=0.785, ppl=1.72, wps=57284.6, ups=15.61, wpb=3669.7, bsz=193.2, num_updates=92600, lr=0.000103919, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.7, wall=6436
2024-01-30 17:20:43 | INFO | train_inner | epoch 063:   1257 / 1475 loss=2.42, nll_loss=0.788, ppl=1.73, wps=58875.2, ups=15.5, wpb=3798.8, bsz=207, num_updates=92700, lr=0.000103863, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.6, wall=6442
2024-01-30 17:20:49 | INFO | train_inner | epoch 063:   1357 / 1475 loss=2.425, nll_loss=0.793, ppl=1.73, wps=57431.6, ups=15.56, wpb=3689.9, bsz=201.8, num_updates=92800, lr=0.000103807, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.8, wall=6448
2024-01-30 17:20:56 | INFO | train_inner | epoch 063:   1457 / 1475 loss=2.456, nll_loss=0.83, ppl=1.78, wps=59473.1, ups=15.69, wpb=3789.8, bsz=183.6, num_updates=92900, lr=0.000103751, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.7, wall=6455
2024-01-30 17:20:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:20:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:20:59 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.438 | nll_loss 1.81 | ppl 3.51 | wps 134360 | wpb 3189.1 | bsz 163.4 | num_updates 92918 | best_loss 3.385
2024-01-30 17:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 92918 updates
2024-01-30 17:20:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint63.pt
2024-01-30 17:21:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint63.pt
2024-01-30 17:21:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint63.pt (epoch 63 @ 92918 updates, score 3.438) (writing took 2.362724678008817 seconds)
2024-01-30 17:21:02 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-01-30 17:21:02 | INFO | train | epoch 063 | loss 2.417 | nll_loss 0.784 | ppl 1.72 | wps 55918.3 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 92918 | lr 0.000103741 | gnorm 0.915 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 6461
2024-01-30 17:21:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:21:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:21:02 | INFO | fairseq.trainer | begin training epoch 64
2024-01-30 17:21:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:21:07 | INFO | train_inner | epoch 064:     82 / 1475 loss=2.407, nll_loss=0.772, ppl=1.71, wps=33981.5, ups=8.89, wpb=3821.1, bsz=194.8, num_updates=93000, lr=0.000103695, gnorm=0.891, loss_scale=16, train_wall=6, gb_free=29.7, wall=6466
2024-01-30 17:21:13 | INFO | train_inner | epoch 064:    182 / 1475 loss=2.386, nll_loss=0.748, ppl=1.68, wps=58677.5, ups=15.58, wpb=3766, bsz=198, num_updates=93100, lr=0.000103639, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=29.7, wall=6472
2024-01-30 17:21:20 | INFO | train_inner | epoch 064:    282 / 1475 loss=2.401, nll_loss=0.765, ppl=1.7, wps=59029.2, ups=15.66, wpb=3769, bsz=190.6, num_updates=93200, lr=0.000103584, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.8, wall=6479
2024-01-30 17:21:26 | INFO | train_inner | epoch 064:    382 / 1475 loss=2.382, nll_loss=0.744, ppl=1.68, wps=57291.1, ups=15.53, wpb=3689.3, bsz=207.1, num_updates=93300, lr=0.000103528, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=6485
2024-01-30 17:21:33 | INFO | train_inner | epoch 064:    482 / 1475 loss=2.421, nll_loss=0.789, ppl=1.73, wps=59647.2, ups=15.63, wpb=3817.1, bsz=189.3, num_updates=93400, lr=0.000103473, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.8, wall=6492
2024-01-30 17:21:39 | INFO | train_inner | epoch 064:    582 / 1475 loss=2.405, nll_loss=0.77, ppl=1.71, wps=57905.2, ups=15.67, wpb=3696.3, bsz=193.7, num_updates=93500, lr=0.000103418, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=6498
2024-01-30 17:21:45 | INFO | train_inner | epoch 064:    682 / 1475 loss=2.41, nll_loss=0.776, ppl=1.71, wps=59637.5, ups=15.56, wpb=3832.8, bsz=202.8, num_updates=93600, lr=0.000103362, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=6504
2024-01-30 17:21:52 | INFO | train_inner | epoch 064:    782 / 1475 loss=2.412, nll_loss=0.779, ppl=1.72, wps=58780.2, ups=15.52, wpb=3786.5, bsz=200.1, num_updates=93700, lr=0.000103307, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=6511
2024-01-30 17:21:58 | INFO | train_inner | epoch 064:    882 / 1475 loss=2.432, nll_loss=0.801, ppl=1.74, wps=59766.9, ups=15.63, wpb=3823.6, bsz=189.5, num_updates=93800, lr=0.000103252, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=6517
2024-01-30 17:22:05 | INFO | train_inner | epoch 064:    982 / 1475 loss=2.419, nll_loss=0.788, ppl=1.73, wps=58808.4, ups=15.56, wpb=3779.8, bsz=197.5, num_updates=93900, lr=0.000103197, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=6524
2024-01-30 17:22:11 | INFO | train_inner | epoch 064:   1082 / 1475 loss=2.426, nll_loss=0.794, ppl=1.73, wps=58977.8, ups=15.64, wpb=3770.1, bsz=193.2, num_updates=94000, lr=0.000103142, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=6530
2024-01-30 17:22:17 | INFO | train_inner | epoch 064:   1182 / 1475 loss=2.426, nll_loss=0.795, ppl=1.74, wps=59379.6, ups=15.57, wpb=3814.7, bsz=193.4, num_updates=94100, lr=0.000103087, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.7, wall=6537
2024-01-30 17:22:24 | INFO | train_inner | epoch 064:   1282 / 1475 loss=2.422, nll_loss=0.791, ppl=1.73, wps=57464, ups=15.57, wpb=3690.8, bsz=192.8, num_updates=94200, lr=0.000103033, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.9, wall=6543
2024-01-30 17:22:30 | INFO | train_inner | epoch 064:   1382 / 1475 loss=2.435, nll_loss=0.806, ppl=1.75, wps=58390.3, ups=15.5, wpb=3767.2, bsz=199, num_updates=94300, lr=0.000102978, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.7, wall=6549
2024-01-30 17:22:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:22:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:22:39 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.434 | nll_loss 1.804 | ppl 3.49 | wps 134970 | wpb 3189.1 | bsz 163.4 | num_updates 94393 | best_loss 3.385
2024-01-30 17:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 94393 updates
2024-01-30 17:22:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint64.pt
2024-01-30 17:22:39 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint64.pt
2024-01-30 17:22:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint64.pt (epoch 64 @ 94393 updates, score 3.434) (writing took 2.3327678170753643 seconds)
2024-01-30 17:22:41 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-01-30 17:22:41 | INFO | train | epoch 064 | loss 2.413 | nll_loss 0.78 | ppl 1.72 | wps 55948.1 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 94393 | lr 0.000102927 | gnorm 0.917 | loss_scale 16 | train_wall 91 | gb_free 29.9 | wall 6560
2024-01-30 17:22:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:22:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:22:41 | INFO | fairseq.trainer | begin training epoch 65
2024-01-30 17:22:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:22:42 | INFO | train_inner | epoch 065:      7 / 1475 loss=2.412, nll_loss=0.779, ppl=1.72, wps=33385.4, ups=8.91, wpb=3748.3, bsz=198.8, num_updates=94400, lr=0.000102923, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.8, wall=6561
2024-01-30 17:22:48 | INFO | train_inner | epoch 065:    107 / 1475 loss=2.383, nll_loss=0.746, ppl=1.68, wps=59706.5, ups=15.56, wpb=3838.3, bsz=195.2, num_updates=94500, lr=0.000102869, gnorm=0.88, loss_scale=16, train_wall=6, gb_free=29.7, wall=6567
2024-01-30 17:22:54 | INFO | train_inner | epoch 065:    207 / 1475 loss=2.378, nll_loss=0.74, ppl=1.67, wps=60022.1, ups=15.6, wpb=3848.5, bsz=203.4, num_updates=94600, lr=0.000102815, gnorm=0.876, loss_scale=16, train_wall=6, gb_free=29.7, wall=6574
2024-01-30 17:23:01 | INFO | train_inner | epoch 065:    307 / 1475 loss=2.389, nll_loss=0.753, ppl=1.69, wps=58581.1, ups=15.54, wpb=3770.3, bsz=201.4, num_updates=94700, lr=0.00010276, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.7, wall=6580
2024-01-30 17:23:07 | INFO | train_inner | epoch 065:    407 / 1475 loss=2.415, nll_loss=0.781, ppl=1.72, wps=59268.5, ups=15.64, wpb=3790.7, bsz=185.4, num_updates=94800, lr=0.000102706, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=6586
2024-01-30 17:23:14 | INFO | train_inner | epoch 065:    507 / 1475 loss=2.4, nll_loss=0.764, ppl=1.7, wps=59264.5, ups=15.54, wpb=3813.7, bsz=194.2, num_updates=94900, lr=0.000102652, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.7, wall=6593
2024-01-30 17:23:20 | INFO | train_inner | epoch 065:    607 / 1475 loss=2.386, nll_loss=0.75, ppl=1.68, wps=57017.6, ups=15.52, wpb=3673.3, bsz=203.9, num_updates=95000, lr=0.000102598, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=6599
2024-01-30 17:23:27 | INFO | train_inner | epoch 065:    707 / 1475 loss=2.401, nll_loss=0.767, ppl=1.7, wps=58202.1, ups=15.46, wpb=3765.7, bsz=198.8, num_updates=95100, lr=0.000102544, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=6606
2024-01-30 17:23:33 | INFO | train_inner | epoch 065:    807 / 1475 loss=2.404, nll_loss=0.77, ppl=1.7, wps=58985, ups=15.58, wpb=3786.3, bsz=202.3, num_updates=95200, lr=0.00010249, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=6612
2024-01-30 17:23:39 | INFO | train_inner | epoch 065:    907 / 1475 loss=2.398, nll_loss=0.764, ppl=1.7, wps=58048.4, ups=15.57, wpb=3727.5, bsz=207.1, num_updates=95300, lr=0.000102436, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.7, wall=6619
2024-01-30 17:23:46 | INFO | train_inner | epoch 065:   1007 / 1475 loss=2.411, nll_loss=0.778, ppl=1.72, wps=58732.8, ups=15.66, wpb=3750.7, bsz=199, num_updates=95400, lr=0.000102383, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.8, wall=6625
2024-01-30 17:23:52 | INFO | train_inner | epoch 065:   1107 / 1475 loss=2.427, nll_loss=0.797, ppl=1.74, wps=58235.8, ups=15.65, wpb=3720.3, bsz=189.4, num_updates=95500, lr=0.000102329, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.6, wall=6631
2024-01-30 17:23:59 | INFO | train_inner | epoch 065:   1207 / 1475 loss=2.437, nll_loss=0.808, ppl=1.75, wps=60076.6, ups=15.58, wpb=3855.4, bsz=187.4, num_updates=95600, lr=0.000102275, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.6, wall=6638
2024-01-30 17:24:05 | INFO | train_inner | epoch 065:   1307 / 1475 loss=2.437, nll_loss=0.807, ppl=1.75, wps=59522, ups=15.59, wpb=3818.5, bsz=192.6, num_updates=95700, lr=0.000102222, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.7, wall=6644
2024-01-30 17:24:11 | INFO | train_inner | epoch 065:   1407 / 1475 loss=2.43, nll_loss=0.8, ppl=1.74, wps=56907.1, ups=15.65, wpb=3636.9, bsz=185.9, num_updates=95800, lr=0.000102169, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.7, wall=6651
2024-01-30 17:24:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:24:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:24:18 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.439 | nll_loss 1.808 | ppl 3.5 | wps 134573 | wpb 3189.1 | bsz 163.4 | num_updates 95868 | best_loss 3.385
2024-01-30 17:24:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 95868 updates
2024-01-30 17:24:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint65.pt
2024-01-30 17:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint65.pt
2024-01-30 17:24:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint65.pt (epoch 65 @ 95868 updates, score 3.439) (writing took 2.329324741032906 seconds)
2024-01-30 17:24:20 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-01-30 17:24:20 | INFO | train | epoch 065 | loss 2.408 | nll_loss 0.774 | ppl 1.71 | wps 55932.3 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 95868 | lr 0.000102132 | gnorm 0.918 | loss_scale 16 | train_wall 91 | gb_free 29.9 | wall 6660
2024-01-30 17:24:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:24:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:24:21 | INFO | fairseq.trainer | begin training epoch 66
2024-01-30 17:24:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:24:23 | INFO | train_inner | epoch 066:     32 / 1475 loss=2.411, nll_loss=0.778, ppl=1.71, wps=33799.2, ups=8.91, wpb=3792.6, bsz=195.8, num_updates=95900, lr=0.000102115, gnorm=0.91, loss_scale=16, train_wall=6, gb_free=29.7, wall=6662
2024-01-30 17:24:29 | INFO | train_inner | epoch 066:    132 / 1475 loss=2.379, nll_loss=0.74, ppl=1.67, wps=58786.9, ups=15.51, wpb=3789.7, bsz=187.5, num_updates=96000, lr=0.000102062, gnorm=0.896, loss_scale=16, train_wall=6, gb_free=30, wall=6668
2024-01-30 17:24:36 | INFO | train_inner | epoch 066:    232 / 1475 loss=2.384, nll_loss=0.747, ppl=1.68, wps=58249.3, ups=15.49, wpb=3759.6, bsz=194.2, num_updates=96100, lr=0.000102009, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.7, wall=6675
2024-01-30 17:24:42 | INFO | train_inner | epoch 066:    332 / 1475 loss=2.384, nll_loss=0.748, ppl=1.68, wps=59392.9, ups=15.65, wpb=3793.9, bsz=197.6, num_updates=96200, lr=0.000101956, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.7, wall=6681
2024-01-30 17:24:48 | INFO | train_inner | epoch 066:    432 / 1475 loss=2.372, nll_loss=0.733, ppl=1.66, wps=57531.1, ups=15.47, wpb=3719.5, bsz=217, num_updates=96300, lr=0.000101903, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=6688
2024-01-30 17:24:55 | INFO | train_inner | epoch 066:    532 / 1475 loss=2.405, nll_loss=0.771, ppl=1.71, wps=58578, ups=15.64, wpb=3744.6, bsz=190.6, num_updates=96400, lr=0.00010185, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.8, wall=6694
2024-01-30 17:25:01 | INFO | train_inner | epoch 066:    632 / 1475 loss=2.406, nll_loss=0.773, ppl=1.71, wps=59323.1, ups=15.66, wpb=3788, bsz=195.3, num_updates=96500, lr=0.000101797, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=6700
2024-01-30 17:25:08 | INFO | train_inner | epoch 066:    732 / 1475 loss=2.401, nll_loss=0.767, ppl=1.7, wps=57409.8, ups=15.6, wpb=3680.1, bsz=190.4, num_updates=96600, lr=0.000101745, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.8, wall=6707
2024-01-30 17:25:14 | INFO | train_inner | epoch 066:    832 / 1475 loss=2.43, nll_loss=0.799, ppl=1.74, wps=59217.8, ups=15.62, wpb=3790.5, bsz=180.6, num_updates=96700, lr=0.000101692, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.6, wall=6713
2024-01-30 17:25:20 | INFO | train_inner | epoch 066:    932 / 1475 loss=2.416, nll_loss=0.785, ppl=1.72, wps=58817.7, ups=15.7, wpb=3745.7, bsz=190.7, num_updates=96800, lr=0.000101639, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.8, wall=6719
2024-01-30 17:25:27 | INFO | train_inner | epoch 066:   1032 / 1475 loss=2.401, nll_loss=0.768, ppl=1.7, wps=59403.2, ups=15.52, wpb=3826.6, bsz=206.4, num_updates=96900, lr=0.000101587, gnorm=0.898, loss_scale=16, train_wall=6, gb_free=29.7, wall=6726
2024-01-30 17:25:33 | INFO | train_inner | epoch 066:   1132 / 1475 loss=2.434, nll_loss=0.804, ppl=1.75, wps=58337.1, ups=15.58, wpb=3743.4, bsz=189.9, num_updates=97000, lr=0.000101535, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.7, wall=6732
2024-01-30 17:25:40 | INFO | train_inner | epoch 066:   1232 / 1475 loss=2.406, nll_loss=0.773, ppl=1.71, wps=59421.4, ups=15.55, wpb=3821.7, bsz=205.5, num_updates=97100, lr=0.000101482, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=6739
2024-01-30 17:25:46 | INFO | train_inner | epoch 066:   1332 / 1475 loss=2.419, nll_loss=0.787, ppl=1.73, wps=58327.1, ups=15.64, wpb=3730.5, bsz=192.9, num_updates=97200, lr=0.00010143, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=6745
2024-01-30 17:25:52 | INFO | train_inner | epoch 066:   1432 / 1475 loss=2.425, nll_loss=0.795, ppl=1.74, wps=59728.2, ups=15.56, wpb=3839.2, bsz=201, num_updates=97300, lr=0.000101378, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.8, wall=6752
2024-01-30 17:25:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:25:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:25:58 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.436 | nll_loss 1.809 | ppl 3.5 | wps 133183 | wpb 3189.1 | bsz 163.4 | num_updates 97343 | best_loss 3.385
2024-01-30 17:25:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 97343 updates
2024-01-30 17:25:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint66.pt
2024-01-30 17:25:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint66.pt
2024-01-30 17:26:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint66.pt (epoch 66 @ 97343 updates, score 3.436) (writing took 2.31911332893651 seconds)
2024-01-30 17:26:00 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-01-30 17:26:00 | INFO | train | epoch 066 | loss 2.404 | nll_loss 0.77 | ppl 1.71 | wps 55867 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 97343 | lr 0.000101356 | gnorm 0.921 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 6759
2024-01-30 17:26:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:26:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:26:00 | INFO | fairseq.trainer | begin training epoch 67
2024-01-30 17:26:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:26:04 | INFO | train_inner | epoch 067:     57 / 1475 loss=2.384, nll_loss=0.748, ppl=1.68, wps=33369.3, ups=8.79, wpb=3795.7, bsz=201.4, num_updates=97400, lr=0.000101326, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=6763
2024-01-30 17:26:10 | INFO | train_inner | epoch 067:    157 / 1475 loss=2.391, nll_loss=0.754, ppl=1.69, wps=59611.8, ups=15.66, wpb=3807, bsz=188.5, num_updates=97500, lr=0.000101274, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=6769
2024-01-30 17:26:17 | INFO | train_inner | epoch 067:    257 / 1475 loss=2.365, nll_loss=0.726, ppl=1.65, wps=57369.3, ups=15.56, wpb=3687.6, bsz=203.8, num_updates=97600, lr=0.000101222, gnorm=0.889, loss_scale=16, train_wall=6, gb_free=29.7, wall=6776
2024-01-30 17:26:23 | INFO | train_inner | epoch 067:    357 / 1475 loss=2.4, nll_loss=0.764, ppl=1.7, wps=58829.6, ups=15.65, wpb=3757.9, bsz=187, num_updates=97700, lr=0.00010117, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=6782
2024-01-30 17:26:30 | INFO | train_inner | epoch 067:    457 / 1475 loss=2.371, nll_loss=0.732, ppl=1.66, wps=59110.4, ups=15.49, wpb=3815, bsz=211.8, num_updates=97800, lr=0.000101118, gnorm=0.882, loss_scale=16, train_wall=6, gb_free=29.7, wall=6789
2024-01-30 17:26:36 | INFO | train_inner | epoch 067:    557 / 1475 loss=2.398, nll_loss=0.764, ppl=1.7, wps=58657.5, ups=15.42, wpb=3804.2, bsz=196.6, num_updates=97900, lr=0.000101067, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.7, wall=6795
2024-01-30 17:26:42 | INFO | train_inner | epoch 067:    657 / 1475 loss=2.398, nll_loss=0.764, ppl=1.7, wps=58838.8, ups=15.66, wpb=3757.7, bsz=194.2, num_updates=98000, lr=0.000101015, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.7, wall=6801
2024-01-30 17:26:49 | INFO | train_inner | epoch 067:    757 / 1475 loss=2.398, nll_loss=0.763, ppl=1.7, wps=59069.9, ups=15.65, wpb=3774.6, bsz=195.4, num_updates=98100, lr=0.000100964, gnorm=0.917, loss_scale=32, train_wall=6, gb_free=29.6, wall=6808
2024-01-30 17:26:55 | INFO | train_inner | epoch 067:    857 / 1475 loss=2.401, nll_loss=0.767, ppl=1.7, wps=59023.5, ups=15.54, wpb=3799.2, bsz=198.2, num_updates=98200, lr=0.000100912, gnorm=0.903, loss_scale=32, train_wall=6, gb_free=29.7, wall=6814
2024-01-30 17:27:02 | INFO | train_inner | epoch 067:    957 / 1475 loss=2.404, nll_loss=0.771, ppl=1.71, wps=58193.8, ups=15.64, wpb=3721.7, bsz=195.4, num_updates=98300, lr=0.000100861, gnorm=0.932, loss_scale=32, train_wall=6, gb_free=29.7, wall=6821
2024-01-30 17:27:08 | INFO | train_inner | epoch 067:   1057 / 1475 loss=2.425, nll_loss=0.794, ppl=1.73, wps=58074.2, ups=15.64, wpb=3712.1, bsz=183.5, num_updates=98400, lr=0.00010081, gnorm=0.945, loss_scale=32, train_wall=6, gb_free=29.7, wall=6827
2024-01-30 17:27:14 | INFO | train_inner | epoch 067:   1157 / 1475 loss=2.413, nll_loss=0.78, ppl=1.72, wps=58942.8, ups=15.69, wpb=3757.9, bsz=197.2, num_updates=98500, lr=0.000100759, gnorm=0.935, loss_scale=32, train_wall=6, gb_free=29.7, wall=6833
2024-01-30 17:27:21 | INFO | train_inner | epoch 067:   1257 / 1475 loss=2.415, nll_loss=0.783, ppl=1.72, wps=58238.8, ups=15.6, wpb=3732.8, bsz=190.4, num_updates=98600, lr=0.000100707, gnorm=0.961, loss_scale=32, train_wall=6, gb_free=29.8, wall=6840
2024-01-30 17:27:27 | INFO | train_inner | epoch 067:   1357 / 1475 loss=2.413, nll_loss=0.78, ppl=1.72, wps=60022.3, ups=15.57, wpb=3856.1, bsz=202.1, num_updates=98700, lr=0.000100656, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.7, wall=6846
2024-01-30 17:27:34 | INFO | train_inner | epoch 067:   1457 / 1475 loss=2.413, nll_loss=0.782, ppl=1.72, wps=59088.4, ups=15.58, wpb=3791.5, bsz=195.8, num_updates=98800, lr=0.000100605, gnorm=0.926, loss_scale=32, train_wall=6, gb_free=29.7, wall=6853
2024-01-30 17:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:27:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:27:37 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.443 | nll_loss 1.813 | ppl 3.51 | wps 135092 | wpb 3189.1 | bsz 163.4 | num_updates 98818 | best_loss 3.385
2024-01-30 17:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 98818 updates
2024-01-30 17:27:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint67.pt
2024-01-30 17:27:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint67.pt
2024-01-30 17:27:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint67.pt (epoch 67 @ 98818 updates, score 3.443) (writing took 2.457642656052485 seconds)
2024-01-30 17:27:40 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-01-30 17:27:40 | INFO | train | epoch 067 | loss 2.399 | nll_loss 0.764 | ppl 1.7 | wps 55883.8 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 98818 | lr 0.000100596 | gnorm 0.919 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 6859
2024-01-30 17:27:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:27:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:27:40 | INFO | fairseq.trainer | begin training epoch 68
2024-01-30 17:27:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:27:45 | INFO | train_inner | epoch 068:     82 / 1475 loss=2.378, nll_loss=0.74, ppl=1.67, wps=33522.4, ups=8.81, wpb=3805.7, bsz=193.4, num_updates=98900, lr=0.000100555, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.7, wall=6864
2024-01-30 17:27:51 | INFO | train_inner | epoch 068:    182 / 1475 loss=2.35, nll_loss=0.709, ppl=1.63, wps=58412.1, ups=15.52, wpb=3763.4, bsz=216.6, num_updates=99000, lr=0.000100504, gnorm=0.884, loss_scale=32, train_wall=6, gb_free=29.7, wall=6871
2024-01-30 17:27:58 | INFO | train_inner | epoch 068:    282 / 1475 loss=2.378, nll_loss=0.741, ppl=1.67, wps=58766.1, ups=15.54, wpb=3781.6, bsz=194.5, num_updates=99100, lr=0.000100453, gnorm=0.913, loss_scale=32, train_wall=6, gb_free=29.7, wall=6877
2024-01-30 17:28:04 | INFO | train_inner | epoch 068:    382 / 1475 loss=2.399, nll_loss=0.763, ppl=1.7, wps=58898.9, ups=15.69, wpb=3753.7, bsz=185.6, num_updates=99200, lr=0.000100402, gnorm=0.938, loss_scale=32, train_wall=6, gb_free=29.7, wall=6883
2024-01-30 17:28:11 | INFO | train_inner | epoch 068:    482 / 1475 loss=2.396, nll_loss=0.761, ppl=1.69, wps=58599.9, ups=15.63, wpb=3750.2, bsz=195, num_updates=99300, lr=0.000100352, gnorm=0.935, loss_scale=32, train_wall=6, gb_free=29.8, wall=6890
2024-01-30 17:28:17 | INFO | train_inner | epoch 068:    582 / 1475 loss=2.38, nll_loss=0.744, ppl=1.67, wps=59801.4, ups=15.52, wpb=3853.1, bsz=206.3, num_updates=99400, lr=0.000100301, gnorm=0.886, loss_scale=32, train_wall=6, gb_free=29.7, wall=6896
2024-01-30 17:28:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 17:28:24 | INFO | train_inner | epoch 068:    683 / 1475 loss=2.388, nll_loss=0.753, ppl=1.68, wps=59003.6, ups=15.44, wpb=3822.2, bsz=202.2, num_updates=99500, lr=0.000100251, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.8, wall=6903
2024-01-30 17:28:30 | INFO | train_inner | epoch 068:    783 / 1475 loss=2.387, nll_loss=0.751, ppl=1.68, wps=57280.3, ups=15.63, wpb=3664.7, bsz=197.9, num_updates=99600, lr=0.000100201, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=6909
2024-01-30 17:28:36 | INFO | train_inner | epoch 068:    883 / 1475 loss=2.399, nll_loss=0.765, ppl=1.7, wps=57584.1, ups=15.6, wpb=3691.1, bsz=191.5, num_updates=99700, lr=0.00010015, gnorm=0.938, loss_scale=16, train_wall=6, gb_free=29.7, wall=6915
2024-01-30 17:28:43 | INFO | train_inner | epoch 068:    983 / 1475 loss=2.41, nll_loss=0.778, ppl=1.71, wps=58684.4, ups=15.64, wpb=3752, bsz=190.6, num_updates=99800, lr=0.0001001, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=6922
2024-01-30 17:28:49 | INFO | train_inner | epoch 068:   1083 / 1475 loss=2.403, nll_loss=0.77, ppl=1.7, wps=59807.1, ups=15.57, wpb=3840.6, bsz=192.2, num_updates=99900, lr=0.00010005, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.7, wall=6928
2024-01-30 17:28:56 | INFO | train_inner | epoch 068:   1183 / 1475 loss=2.4, nll_loss=0.767, ppl=1.7, wps=57999.1, ups=15.6, wpb=3719, bsz=197.4, num_updates=100000, lr=0.0001, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.8, wall=6935
2024-01-30 17:28:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:28:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:28:58 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.45 | nll_loss 1.822 | ppl 3.54 | wps 133706 | wpb 3189.1 | bsz 163.4 | num_updates 100000 | best_loss 3.385
2024-01-30 17:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 100000 updates
2024-01-30 17:28:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_68_100000.pt
2024-01-30 17:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_68_100000.pt
2024-01-30 17:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint_68_100000.pt (epoch 68 @ 100000 updates, score 3.45) (writing took 2.312113417079672 seconds)
2024-01-30 17:29:07 | INFO | train_inner | epoch 068:   1283 / 1475 loss=2.427, nll_loss=0.796, ppl=1.74, wps=34897.8, ups=9.05, wpb=3854.3, bsz=189.4, num_updates=100100, lr=9.995e-05, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.8, wall=6946
2024-01-30 17:29:13 | INFO | train_inner | epoch 068:   1383 / 1475 loss=2.42, nll_loss=0.789, ppl=1.73, wps=57776.9, ups=15.62, wpb=3698.5, bsz=187.5, num_updates=100200, lr=9.99001e-05, gnorm=0.978, loss_scale=16, train_wall=6, gb_free=29.7, wall=6952
2024-01-30 17:29:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:29:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:29:21 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.441 | nll_loss 1.81 | ppl 3.51 | wps 134195 | wpb 3189.1 | bsz 163.4 | num_updates 100292 | best_loss 3.385
2024-01-30 17:29:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 100292 updates
2024-01-30 17:29:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint68.pt
2024-01-30 17:29:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint68.pt
2024-01-30 17:29:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint68.pt (epoch 68 @ 100292 updates, score 3.441) (writing took 2.4811324619222432 seconds)
2024-01-30 17:29:24 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-01-30 17:29:24 | INFO | train | epoch 068 | loss 2.395 | nll_loss 0.76 | ppl 1.69 | wps 53357.6 | ups 14.15 | wpb 3771.8 | bsz 196 | num_updates 100292 | lr 9.98543e-05 | gnorm 0.924 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 6963
2024-01-30 17:29:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:29:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:29:24 | INFO | fairseq.trainer | begin training epoch 69
2024-01-30 17:29:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:29:24 | INFO | train_inner | epoch 069:      8 / 1475 loss=2.41, nll_loss=0.778, ppl=1.71, wps=33823.7, ups=8.79, wpb=3847.5, bsz=197.9, num_updates=100300, lr=9.98503e-05, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.6, wall=6964
2024-01-30 17:29:31 | INFO | train_inner | epoch 069:    108 / 1475 loss=2.381, nll_loss=0.742, ppl=1.67, wps=58507.5, ups=15.71, wpb=3725.1, bsz=188.4, num_updates=100400, lr=9.98006e-05, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=6970
2024-01-30 17:29:37 | INFO | train_inner | epoch 069:    208 / 1475 loss=2.364, nll_loss=0.724, ppl=1.65, wps=59300.1, ups=15.51, wpb=3824.4, bsz=205.1, num_updates=100500, lr=9.97509e-05, gnorm=0.882, loss_scale=16, train_wall=6, gb_free=29.7, wall=6976
2024-01-30 17:29:44 | INFO | train_inner | epoch 069:    308 / 1475 loss=2.373, nll_loss=0.735, ppl=1.66, wps=59379.8, ups=15.59, wpb=3809.9, bsz=197.8, num_updates=100600, lr=9.97013e-05, gnorm=0.906, loss_scale=16, train_wall=6, gb_free=29.6, wall=6983
2024-01-30 17:29:50 | INFO | train_inner | epoch 069:    408 / 1475 loss=2.401, nll_loss=0.766, ppl=1.7, wps=58781.1, ups=15.62, wpb=3763.4, bsz=191.8, num_updates=100700, lr=9.96518e-05, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.7, wall=6989
2024-01-30 17:29:57 | INFO | train_inner | epoch 069:    508 / 1475 loss=2.38, nll_loss=0.744, ppl=1.67, wps=59201.6, ups=15.49, wpb=3823, bsz=200.5, num_updates=100800, lr=9.96024e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=6996
2024-01-30 17:30:03 | INFO | train_inner | epoch 069:    608 / 1475 loss=2.379, nll_loss=0.743, ppl=1.67, wps=59438.4, ups=15.49, wpb=3836.9, bsz=207.4, num_updates=100900, lr=9.9553e-05, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.9, wall=7002
2024-01-30 17:30:09 | INFO | train_inner | epoch 069:    708 / 1475 loss=2.386, nll_loss=0.75, ppl=1.68, wps=58581.9, ups=15.56, wpb=3765.9, bsz=202.7, num_updates=101000, lr=9.95037e-05, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.8, wall=7009
2024-01-30 17:30:16 | INFO | train_inner | epoch 069:    808 / 1475 loss=2.394, nll_loss=0.759, ppl=1.69, wps=58425.8, ups=15.59, wpb=3746.5, bsz=188.7, num_updates=101100, lr=9.94545e-05, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.7, wall=7015
2024-01-30 17:30:22 | INFO | train_inner | epoch 069:    908 / 1475 loss=2.394, nll_loss=0.759, ppl=1.69, wps=58214.5, ups=15.57, wpb=3739.7, bsz=200.8, num_updates=101200, lr=9.94053e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.8, wall=7021
2024-01-30 17:30:29 | INFO | train_inner | epoch 069:   1008 / 1475 loss=2.398, nll_loss=0.763, ppl=1.7, wps=58246.5, ups=15.58, wpb=3737.4, bsz=188.3, num_updates=101300, lr=9.93563e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=7028
2024-01-30 17:30:35 | INFO | train_inner | epoch 069:   1108 / 1475 loss=2.421, nll_loss=0.789, ppl=1.73, wps=59803.6, ups=15.6, wpb=3834.4, bsz=182.3, num_updates=101400, lr=9.93073e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.7, wall=7034
2024-01-30 17:30:41 | INFO | train_inner | epoch 069:   1208 / 1475 loss=2.396, nll_loss=0.761, ppl=1.69, wps=58703, ups=15.72, wpb=3734.9, bsz=196.5, num_updates=101500, lr=9.92583e-05, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.8, wall=7041
2024-01-30 17:30:48 | INFO | train_inner | epoch 069:   1308 / 1475 loss=2.398, nll_loss=0.764, ppl=1.7, wps=57773.1, ups=15.6, wpb=3703, bsz=198, num_updates=101600, lr=9.92095e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=7047
2024-01-30 17:30:54 | INFO | train_inner | epoch 069:   1408 / 1475 loss=2.402, nll_loss=0.768, ppl=1.7, wps=58640.5, ups=15.58, wpb=3762.9, bsz=192.1, num_updates=101700, lr=9.91607e-05, gnorm=0.961, loss_scale=16, train_wall=6, gb_free=29.7, wall=7053
2024-01-30 17:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:30:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:31:01 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.444 | nll_loss 1.82 | ppl 3.53 | wps 134656 | wpb 3189.1 | bsz 163.4 | num_updates 101767 | best_loss 3.385
2024-01-30 17:31:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 101767 updates
2024-01-30 17:31:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint69.pt
2024-01-30 17:31:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint69.pt
2024-01-30 17:31:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint69.pt (epoch 69 @ 101767 updates, score 3.444) (writing took 2.3994963250588626 seconds)
2024-01-30 17:31:03 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-01-30 17:31:03 | INFO | train | epoch 069 | loss 2.39 | nll_loss 0.755 | ppl 1.69 | wps 55897.4 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 101767 | lr 9.9128e-05 | gnorm 0.926 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7062
2024-01-30 17:31:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:31:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:31:03 | INFO | fairseq.trainer | begin training epoch 70
2024-01-30 17:31:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:31:06 | INFO | train_inner | epoch 070:     33 / 1475 loss=2.377, nll_loss=0.74, ppl=1.67, wps=33256.6, ups=8.85, wpb=3756.5, bsz=204.8, num_updates=101800, lr=9.9112e-05, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.7, wall=7065
2024-01-30 17:31:12 | INFO | train_inner | epoch 070:    133 / 1475 loss=2.354, nll_loss=0.714, ppl=1.64, wps=59897.1, ups=15.58, wpb=3844.2, bsz=205.7, num_updates=101900, lr=9.90633e-05, gnorm=0.878, loss_scale=16, train_wall=6, gb_free=29.8, wall=7071
2024-01-30 17:31:18 | INFO | train_inner | epoch 070:    233 / 1475 loss=2.378, nll_loss=0.739, ppl=1.67, wps=59161, ups=15.59, wpb=3794.7, bsz=193.7, num_updates=102000, lr=9.90148e-05, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.6, wall=7077
2024-01-30 17:31:25 | INFO | train_inner | epoch 070:    333 / 1475 loss=2.387, nll_loss=0.75, ppl=1.68, wps=58914.3, ups=15.56, wpb=3785.2, bsz=189.8, num_updates=102100, lr=9.89663e-05, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.6, wall=7084
2024-01-30 17:31:31 | INFO | train_inner | epoch 070:    433 / 1475 loss=2.373, nll_loss=0.736, ppl=1.67, wps=57183.4, ups=15.6, wpb=3665.4, bsz=192.6, num_updates=102200, lr=9.89178e-05, gnorm=0.935, loss_scale=16, train_wall=6, gb_free=29.7, wall=7090
2024-01-30 17:31:38 | INFO | train_inner | epoch 070:    533 / 1475 loss=2.373, nll_loss=0.735, ppl=1.66, wps=57073.2, ups=15.58, wpb=3664.3, bsz=195.4, num_updates=102300, lr=9.88695e-05, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.7, wall=7097
2024-01-30 17:31:44 | INFO | train_inner | epoch 070:    633 / 1475 loss=2.402, nll_loss=0.767, ppl=1.7, wps=58454, ups=15.66, wpb=3732, bsz=181.5, num_updates=102400, lr=9.88212e-05, gnorm=0.954, loss_scale=16, train_wall=6, gb_free=29.8, wall=7103
2024-01-30 17:31:50 | INFO | train_inner | epoch 070:    733 / 1475 loss=2.375, nll_loss=0.738, ppl=1.67, wps=57892.9, ups=15.58, wpb=3715.2, bsz=197.8, num_updates=102500, lr=9.8773e-05, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.9, wall=7110
2024-01-30 17:31:57 | INFO | train_inner | epoch 070:    833 / 1475 loss=2.377, nll_loss=0.741, ppl=1.67, wps=59899.4, ups=15.49, wpb=3868, bsz=206.2, num_updates=102600, lr=9.87248e-05, gnorm=0.888, loss_scale=16, train_wall=6, gb_free=29.8, wall=7116
2024-01-30 17:32:03 | INFO | train_inner | epoch 070:    933 / 1475 loss=2.397, nll_loss=0.763, ppl=1.7, wps=59567, ups=15.55, wpb=3830, bsz=192.6, num_updates=102700, lr=9.86767e-05, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.7, wall=7122
2024-01-30 17:32:10 | INFO | train_inner | epoch 070:   1033 / 1475 loss=2.389, nll_loss=0.754, ppl=1.69, wps=59090.8, ups=15.61, wpb=3786, bsz=200.8, num_updates=102800, lr=9.86287e-05, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.6, wall=7129
2024-01-30 17:32:16 | INFO | train_inner | epoch 070:   1133 / 1475 loss=2.404, nll_loss=0.77, ppl=1.71, wps=59388.8, ups=15.58, wpb=3812.7, bsz=197.7, num_updates=102900, lr=9.85808e-05, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.7, wall=7135
2024-01-30 17:32:23 | INFO | train_inner | epoch 070:   1233 / 1475 loss=2.405, nll_loss=0.772, ppl=1.71, wps=58081.4, ups=15.62, wpb=3717.8, bsz=192.7, num_updates=103000, lr=9.85329e-05, gnorm=0.938, loss_scale=16, train_wall=6, gb_free=29.7, wall=7142
2024-01-30 17:32:29 | INFO | train_inner | epoch 070:   1333 / 1475 loss=2.407, nll_loss=0.775, ppl=1.71, wps=59710, ups=15.6, wpb=3827.6, bsz=197.6, num_updates=103100, lr=9.84851e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=7148
2024-01-30 17:32:35 | INFO | train_inner | epoch 070:   1433 / 1475 loss=2.4, nll_loss=0.767, ppl=1.7, wps=58232.4, ups=15.59, wpb=3736.1, bsz=192.2, num_updates=103200, lr=9.84374e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.8, wall=7154
2024-01-30 17:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:32:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:32:40 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.446 | nll_loss 1.82 | ppl 3.53 | wps 134810 | wpb 3189.1 | bsz 163.4 | num_updates 103242 | best_loss 3.385
2024-01-30 17:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 103242 updates
2024-01-30 17:32:40 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint70.pt
2024-01-30 17:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint70.pt
2024-01-30 17:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint70.pt (epoch 70 @ 103242 updates, score 3.446) (writing took 2.4381255459738895 seconds)
2024-01-30 17:32:43 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-01-30 17:32:43 | INFO | train | epoch 070 | loss 2.387 | nll_loss 0.751 | ppl 1.68 | wps 55883.7 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 103242 | lr 9.84174e-05 | gnorm 0.921 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7162
2024-01-30 17:32:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:32:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:32:43 | INFO | fairseq.trainer | begin training epoch 71
2024-01-30 17:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:32:47 | INFO | train_inner | epoch 071:     58 / 1475 loss=2.37, nll_loss=0.732, ppl=1.66, wps=33412.1, ups=8.84, wpb=3781.2, bsz=197, num_updates=103300, lr=9.83897e-05, gnorm=0.89, loss_scale=16, train_wall=6, gb_free=29.7, wall=7166
2024-01-30 17:32:53 | INFO | train_inner | epoch 071:    158 / 1475 loss=2.351, nll_loss=0.71, ppl=1.64, wps=58516.7, ups=15.5, wpb=3776.2, bsz=199.8, num_updates=103400, lr=9.83422e-05, gnorm=0.885, loss_scale=16, train_wall=6, gb_free=29.8, wall=7172
2024-01-30 17:33:00 | INFO | train_inner | epoch 071:    258 / 1475 loss=2.379, nll_loss=0.742, ppl=1.67, wps=60124.5, ups=15.59, wpb=3857, bsz=189.5, num_updates=103500, lr=9.82946e-05, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.8, wall=7179
2024-01-30 17:33:06 | INFO | train_inner | epoch 071:    358 / 1475 loss=2.38, nll_loss=0.743, ppl=1.67, wps=58916.2, ups=15.6, wpb=3776.5, bsz=197.8, num_updates=103600, lr=9.82472e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.6, wall=7185
2024-01-30 17:33:12 | INFO | train_inner | epoch 071:    458 / 1475 loss=2.372, nll_loss=0.734, ppl=1.66, wps=59559.5, ups=15.6, wpb=3817, bsz=195.5, num_updates=103700, lr=9.81998e-05, gnorm=0.903, loss_scale=16, train_wall=6, gb_free=29.7, wall=7191
2024-01-30 17:33:19 | INFO | train_inner | epoch 071:    558 / 1475 loss=2.371, nll_loss=0.733, ppl=1.66, wps=58971.1, ups=15.55, wpb=3791.5, bsz=201.7, num_updates=103800, lr=9.81525e-05, gnorm=0.935, loss_scale=16, train_wall=6, gb_free=29.7, wall=7198
2024-01-30 17:33:25 | INFO | train_inner | epoch 071:    658 / 1475 loss=2.375, nll_loss=0.738, ppl=1.67, wps=59612.2, ups=15.67, wpb=3804.9, bsz=198.3, num_updates=103900, lr=9.81052e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.8, wall=7204
2024-01-30 17:33:32 | INFO | train_inner | epoch 071:    758 / 1475 loss=2.382, nll_loss=0.745, ppl=1.68, wps=58884.2, ups=15.59, wpb=3777.8, bsz=193.9, num_updates=104000, lr=9.80581e-05, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=7211
2024-01-30 17:33:38 | INFO | train_inner | epoch 071:    858 / 1475 loss=2.396, nll_loss=0.761, ppl=1.7, wps=59496.2, ups=15.64, wpb=3804.5, bsz=192.8, num_updates=104100, lr=9.8011e-05, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.8, wall=7217
2024-01-30 17:33:44 | INFO | train_inner | epoch 071:    958 / 1475 loss=2.383, nll_loss=0.747, ppl=1.68, wps=58662.1, ups=15.64, wpb=3749.9, bsz=198.3, num_updates=104200, lr=9.79639e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.8, wall=7224
2024-01-30 17:33:51 | INFO | train_inner | epoch 071:   1058 / 1475 loss=2.396, nll_loss=0.762, ppl=1.7, wps=59092.9, ups=15.58, wpb=3792, bsz=194.1, num_updates=104300, lr=9.79169e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=7230
2024-01-30 17:33:57 | INFO | train_inner | epoch 071:   1158 / 1475 loss=2.413, nll_loss=0.78, ppl=1.72, wps=58295.7, ups=15.61, wpb=3735.4, bsz=188.2, num_updates=104400, lr=9.787e-05, gnorm=0.963, loss_scale=16, train_wall=6, gb_free=29.8, wall=7236
2024-01-30 17:34:04 | INFO | train_inner | epoch 071:   1258 / 1475 loss=2.369, nll_loss=0.732, ppl=1.66, wps=58138.9, ups=15.5, wpb=3750.6, bsz=211, num_updates=104500, lr=9.78232e-05, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.7, wall=7243
2024-01-30 17:34:10 | INFO | train_inner | epoch 071:   1358 / 1475 loss=2.382, nll_loss=0.746, ppl=1.68, wps=57354.5, ups=15.62, wpb=3672.3, bsz=193.5, num_updates=104600, lr=9.77764e-05, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.9, wall=7249
2024-01-30 17:34:16 | INFO | train_inner | epoch 071:   1458 / 1475 loss=2.398, nll_loss=0.766, ppl=1.7, wps=57565.3, ups=15.57, wpb=3696.4, bsz=196, num_updates=104700, lr=9.77297e-05, gnorm=0.974, loss_scale=16, train_wall=6, gb_free=29.8, wall=7256
2024-01-30 17:34:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:34:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:34:20 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.436 | nll_loss 1.812 | ppl 3.51 | wps 134285 | wpb 3189.1 | bsz 163.4 | num_updates 104717 | best_loss 3.385
2024-01-30 17:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 104717 updates
2024-01-30 17:34:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint71.pt
2024-01-30 17:34:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint71.pt
2024-01-30 17:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint71.pt (epoch 71 @ 104717 updates, score 3.436) (writing took 2.352192690013908 seconds)
2024-01-30 17:34:22 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-01-30 17:34:22 | INFO | train | epoch 071 | loss 2.382 | nll_loss 0.745 | ppl 1.68 | wps 55950.8 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 104717 | lr 9.77218e-05 | gnorm 0.926 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7261
2024-01-30 17:34:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:34:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:34:22 | INFO | fairseq.trainer | begin training epoch 72
2024-01-30 17:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:34:28 | INFO | train_inner | epoch 072:     83 / 1475 loss=2.374, nll_loss=0.736, ppl=1.67, wps=34053.8, ups=8.91, wpb=3823.9, bsz=191.4, num_updates=104800, lr=9.76831e-05, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=7267
2024-01-30 17:34:34 | INFO | train_inner | epoch 072:    183 / 1475 loss=2.355, nll_loss=0.714, ppl=1.64, wps=58118.7, ups=15.51, wpb=3747.7, bsz=198.7, num_updates=104900, lr=9.76365e-05, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.6, wall=7273
2024-01-30 17:34:41 | INFO | train_inner | epoch 072:    283 / 1475 loss=2.364, nll_loss=0.724, ppl=1.65, wps=58337.9, ups=15.65, wpb=3728.8, bsz=188.5, num_updates=105000, lr=9.759e-05, gnorm=0.927, loss_scale=16, train_wall=6, gb_free=29.7, wall=7280
2024-01-30 17:34:47 | INFO | train_inner | epoch 072:    383 / 1475 loss=2.363, nll_loss=0.725, ppl=1.65, wps=59682.4, ups=15.48, wpb=3854.9, bsz=208.8, num_updates=105100, lr=9.75436e-05, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.7, wall=7286
2024-01-30 17:34:53 | INFO | train_inner | epoch 072:    483 / 1475 loss=2.379, nll_loss=0.742, ppl=1.67, wps=58703.5, ups=15.63, wpb=3755.9, bsz=192.2, num_updates=105200, lr=9.74972e-05, gnorm=0.93, loss_scale=16, train_wall=6, gb_free=29.6, wall=7293
2024-01-30 17:35:00 | INFO | train_inner | epoch 072:    583 / 1475 loss=2.392, nll_loss=0.756, ppl=1.69, wps=58686.9, ups=15.62, wpb=3756.4, bsz=185.4, num_updates=105300, lr=9.74509e-05, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.7, wall=7299
2024-01-30 17:35:06 | INFO | train_inner | epoch 072:    683 / 1475 loss=2.39, nll_loss=0.755, ppl=1.69, wps=59118.1, ups=15.66, wpb=3775.3, bsz=190.1, num_updates=105400, lr=9.74047e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=7305
2024-01-30 17:35:13 | INFO | train_inner | epoch 072:    783 / 1475 loss=2.375, nll_loss=0.738, ppl=1.67, wps=59614.9, ups=15.67, wpb=3804.3, bsz=195.3, num_updates=105500, lr=9.73585e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=7312
2024-01-30 17:35:19 | INFO | train_inner | epoch 072:    883 / 1475 loss=2.394, nll_loss=0.76, ppl=1.69, wps=58998, ups=15.61, wpb=3779.6, bsz=192.2, num_updates=105600, lr=9.73124e-05, gnorm=0.972, loss_scale=16, train_wall=6, gb_free=29.7, wall=7318
2024-01-30 17:35:25 | INFO | train_inner | epoch 072:    983 / 1475 loss=2.356, nll_loss=0.717, ppl=1.64, wps=59190.6, ups=15.46, wpb=3828.4, bsz=217, num_updates=105700, lr=9.72663e-05, gnorm=0.891, loss_scale=16, train_wall=6, gb_free=29.8, wall=7325
2024-01-30 17:35:32 | INFO | train_inner | epoch 072:   1083 / 1475 loss=2.38, nll_loss=0.744, ppl=1.68, wps=58313.4, ups=15.53, wpb=3754.2, bsz=200.6, num_updates=105800, lr=9.72203e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.8, wall=7331
2024-01-30 17:35:38 | INFO | train_inner | epoch 072:   1183 / 1475 loss=2.375, nll_loss=0.738, ppl=1.67, wps=57928.1, ups=15.59, wpb=3715.7, bsz=205.2, num_updates=105900, lr=9.71744e-05, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=7337
2024-01-30 17:35:45 | INFO | train_inner | epoch 072:   1283 / 1475 loss=2.406, nll_loss=0.773, ppl=1.71, wps=57886.3, ups=15.68, wpb=3692.4, bsz=186.2, num_updates=106000, lr=9.71286e-05, gnorm=0.976, loss_scale=16, train_wall=6, gb_free=30, wall=7344
2024-01-30 17:35:51 | INFO | train_inner | epoch 072:   1383 / 1475 loss=2.4, nll_loss=0.767, ppl=1.7, wps=59400.7, ups=15.61, wpb=3806.1, bsz=189.9, num_updates=106100, lr=9.70828e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.8, wall=7350
2024-01-30 17:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:35:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:35:59 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.448 | nll_loss 1.821 | ppl 3.53 | wps 133870 | wpb 3189.1 | bsz 163.4 | num_updates 106192 | best_loss 3.385
2024-01-30 17:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 106192 updates
2024-01-30 17:35:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint72.pt
2024-01-30 17:36:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint72.pt
2024-01-30 17:36:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint72.pt (epoch 72 @ 106192 updates, score 3.448) (writing took 2.353664265014231 seconds)
2024-01-30 17:36:02 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-01-30 17:36:02 | INFO | train | epoch 072 | loss 2.378 | nll_loss 0.741 | ppl 1.67 | wps 55944.6 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 106192 | lr 9.70407e-05 | gnorm 0.929 | loss_scale 16 | train_wall 91 | gb_free 29.6 | wall 7361
2024-01-30 17:36:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:36:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:36:02 | INFO | fairseq.trainer | begin training epoch 73
2024-01-30 17:36:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:36:02 | INFO | train_inner | epoch 073:      8 / 1475 loss=2.387, nll_loss=0.751, ppl=1.68, wps=33573, ups=8.9, wpb=3773.5, bsz=192.2, num_updates=106200, lr=9.70371e-05, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=7361
2024-01-30 17:36:09 | INFO | train_inner | epoch 073:    108 / 1475 loss=2.36, nll_loss=0.719, ppl=1.65, wps=59337.2, ups=15.67, wpb=3787.1, bsz=190.5, num_updates=106300, lr=9.69914e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=7368
2024-01-30 17:36:15 | INFO | train_inner | epoch 073:    208 / 1475 loss=2.326, nll_loss=0.683, ppl=1.6, wps=59102.5, ups=15.52, wpb=3809, bsz=211.6, num_updates=106400, lr=9.69458e-05, gnorm=0.862, loss_scale=16, train_wall=6, gb_free=29.7, wall=7374
2024-01-30 17:36:22 | INFO | train_inner | epoch 073:    308 / 1475 loss=2.371, nll_loss=0.733, ppl=1.66, wps=59075.9, ups=15.58, wpb=3792, bsz=189.8, num_updates=106500, lr=9.69003e-05, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.9, wall=7381
2024-01-30 17:36:28 | INFO | train_inner | epoch 073:    408 / 1475 loss=2.37, nll_loss=0.732, ppl=1.66, wps=58802.8, ups=15.55, wpb=3782.6, bsz=195.3, num_updates=106600, lr=9.68549e-05, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=7387
2024-01-30 17:36:34 | INFO | train_inner | epoch 073:    508 / 1475 loss=2.37, nll_loss=0.732, ppl=1.66, wps=58359.8, ups=15.6, wpb=3741.3, bsz=195.3, num_updates=106700, lr=9.68095e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.8, wall=7394
2024-01-30 17:36:41 | INFO | train_inner | epoch 073:    608 / 1475 loss=2.379, nll_loss=0.742, ppl=1.67, wps=58596.2, ups=15.59, wpb=3758.2, bsz=194.6, num_updates=106800, lr=9.67641e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=7400
2024-01-30 17:36:47 | INFO | train_inner | epoch 073:    708 / 1475 loss=2.365, nll_loss=0.727, ppl=1.66, wps=58583.2, ups=15.5, wpb=3779.9, bsz=205.4, num_updates=106900, lr=9.67189e-05, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.8, wall=7406
2024-01-30 17:36:54 | INFO | train_inner | epoch 073:    808 / 1475 loss=2.361, nll_loss=0.723, ppl=1.65, wps=59210.7, ups=15.57, wpb=3802.6, bsz=203.5, num_updates=107000, lr=9.66736e-05, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.8, wall=7413
2024-01-30 17:37:00 | INFO | train_inner | epoch 073:    908 / 1475 loss=2.401, nll_loss=0.767, ppl=1.7, wps=58075.9, ups=15.68, wpb=3704.4, bsz=183.5, num_updates=107100, lr=9.66285e-05, gnorm=0.966, loss_scale=16, train_wall=6, gb_free=29.8, wall=7419
2024-01-30 17:37:07 | INFO | train_inner | epoch 073:   1008 / 1475 loss=2.367, nll_loss=0.73, ppl=1.66, wps=57943.1, ups=15.55, wpb=3725.3, bsz=199, num_updates=107200, lr=9.65834e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=7426
2024-01-30 17:37:13 | INFO | train_inner | epoch 073:   1108 / 1475 loss=2.367, nll_loss=0.73, ppl=1.66, wps=59021.4, ups=15.48, wpb=3812.7, bsz=211.6, num_updates=107300, lr=9.65384e-05, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=7432
2024-01-30 17:37:19 | INFO | train_inner | epoch 073:   1208 / 1475 loss=2.379, nll_loss=0.743, ppl=1.67, wps=57952, ups=15.56, wpb=3723.5, bsz=198.2, num_updates=107400, lr=9.64935e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=7439
2024-01-30 17:37:26 | INFO | train_inner | epoch 073:   1308 / 1475 loss=2.401, nll_loss=0.768, ppl=1.7, wps=58635.9, ups=15.58, wpb=3762.6, bsz=186.2, num_updates=107500, lr=9.64486e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.7, wall=7445
2024-01-30 17:37:32 | INFO | train_inner | epoch 073:   1408 / 1475 loss=2.405, nll_loss=0.772, ppl=1.71, wps=58877.2, ups=15.69, wpb=3752.5, bsz=182.6, num_updates=107600, lr=9.64037e-05, gnorm=0.956, loss_scale=16, train_wall=6, gb_free=29.8, wall=7451
2024-01-30 17:37:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:37:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:37:39 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.442 | nll_loss 1.816 | ppl 3.52 | wps 135184 | wpb 3189.1 | bsz 163.4 | num_updates 107667 | best_loss 3.385
2024-01-30 17:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 107667 updates
2024-01-30 17:37:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt
2024-01-30 17:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt
2024-01-30 17:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint73.pt (epoch 73 @ 107667 updates, score 3.442) (writing took 2.323722849949263 seconds)
2024-01-30 17:37:41 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-01-30 17:37:41 | INFO | train | epoch 073 | loss 2.375 | nll_loss 0.737 | ppl 1.67 | wps 55949.1 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 107667 | lr 9.63737e-05 | gnorm 0.926 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7460
2024-01-30 17:37:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:37:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:37:41 | INFO | fairseq.trainer | begin training epoch 74
2024-01-30 17:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:37:43 | INFO | train_inner | epoch 074:     33 / 1475 loss=2.391, nll_loss=0.757, ppl=1.69, wps=34640.7, ups=8.95, wpb=3868.8, bsz=189, num_updates=107700, lr=9.6359e-05, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.7, wall=7462
2024-01-30 17:37:50 | INFO | train_inner | epoch 074:    133 / 1475 loss=2.334, nll_loss=0.691, ppl=1.61, wps=58877.4, ups=15.46, wpb=3807.7, bsz=208.3, num_updates=107800, lr=9.63143e-05, gnorm=0.873, loss_scale=16, train_wall=6, gb_free=29.7, wall=7469
2024-01-30 17:37:56 | INFO | train_inner | epoch 074:    233 / 1475 loss=2.355, nll_loss=0.715, ppl=1.64, wps=57870.6, ups=15.57, wpb=3716.1, bsz=190.4, num_updates=107900, lr=9.62696e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=7475
2024-01-30 17:38:03 | INFO | train_inner | epoch 074:    333 / 1475 loss=2.362, nll_loss=0.724, ppl=1.65, wps=57495.2, ups=15.59, wpb=3687.2, bsz=190.1, num_updates=108000, lr=9.6225e-05, gnorm=0.938, loss_scale=16, train_wall=6, gb_free=29.8, wall=7482
2024-01-30 17:38:09 | INFO | train_inner | epoch 074:    433 / 1475 loss=2.362, nll_loss=0.723, ppl=1.65, wps=59224.9, ups=15.69, wpb=3775.4, bsz=192.5, num_updates=108100, lr=9.61805e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.8, wall=7488
2024-01-30 17:38:15 | INFO | train_inner | epoch 074:    533 / 1475 loss=2.371, nll_loss=0.734, ppl=1.66, wps=58641.2, ups=15.63, wpb=3751.7, bsz=188.7, num_updates=108200, lr=9.61361e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=7495
2024-01-30 17:38:22 | INFO | train_inner | epoch 074:    633 / 1475 loss=2.375, nll_loss=0.737, ppl=1.67, wps=58120.8, ups=15.57, wpb=3732.3, bsz=189, num_updates=108300, lr=9.60917e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=7501
2024-01-30 17:38:28 | INFO | train_inner | epoch 074:    733 / 1475 loss=2.376, nll_loss=0.739, ppl=1.67, wps=58863.4, ups=15.58, wpb=3777.9, bsz=192.7, num_updates=108400, lr=9.60473e-05, gnorm=0.935, loss_scale=16, train_wall=6, gb_free=29.7, wall=7507
2024-01-30 17:38:35 | INFO | train_inner | epoch 074:    833 / 1475 loss=2.385, nll_loss=0.748, ppl=1.68, wps=59131.5, ups=15.63, wpb=3782.1, bsz=190.8, num_updates=108500, lr=9.60031e-05, gnorm=0.938, loss_scale=16, train_wall=6, gb_free=29.8, wall=7514
2024-01-30 17:38:41 | INFO | train_inner | epoch 074:    933 / 1475 loss=2.365, nll_loss=0.728, ppl=1.66, wps=58672.4, ups=15.59, wpb=3762.7, bsz=205.8, num_updates=108600, lr=9.59589e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.8, wall=7520
2024-01-30 17:38:48 | INFO | train_inner | epoch 074:   1033 / 1475 loss=2.371, nll_loss=0.734, ppl=1.66, wps=58327.8, ups=15.49, wpb=3765.3, bsz=199.2, num_updates=108700, lr=9.59147e-05, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=7527
2024-01-30 17:38:54 | INFO | train_inner | epoch 074:   1133 / 1475 loss=2.38, nll_loss=0.744, ppl=1.68, wps=58566.5, ups=15.6, wpb=3754, bsz=197.2, num_updates=108800, lr=9.58706e-05, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=7533
2024-01-30 17:39:00 | INFO | train_inner | epoch 074:   1233 / 1475 loss=2.39, nll_loss=0.755, ppl=1.69, wps=59485.1, ups=15.6, wpb=3814.2, bsz=193.6, num_updates=108900, lr=9.58266e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.7, wall=7540
2024-01-30 17:39:07 | INFO | train_inner | epoch 074:   1333 / 1475 loss=2.387, nll_loss=0.752, ppl=1.68, wps=59514.6, ups=15.61, wpb=3812.3, bsz=200.5, num_updates=109000, lr=9.57826e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.9, wall=7546
2024-01-30 17:39:13 | INFO | train_inner | epoch 074:   1433 / 1475 loss=2.378, nll_loss=0.742, ppl=1.67, wps=58997, ups=15.51, wpb=3803.8, bsz=205.3, num_updates=109100, lr=9.57387e-05, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=7552
2024-01-30 17:39:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:39:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:39:18 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.451 | nll_loss 1.825 | ppl 3.54 | wps 129540 | wpb 3189.1 | bsz 163.4 | num_updates 109142 | best_loss 3.385
2024-01-30 17:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 109142 updates
2024-01-30 17:39:18 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt
2024-01-30 17:39:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt
2024-01-30 17:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint74.pt (epoch 74 @ 109142 updates, score 3.451) (writing took 2.3596827499568462 seconds)
2024-01-30 17:39:21 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-01-30 17:39:21 | INFO | train | epoch 074 | loss 2.371 | nll_loss 0.733 | ppl 1.66 | wps 55870.9 | ups 14.81 | wpb 3771.6 | bsz 195.9 | num_updates 109142 | lr 9.57203e-05 | gnorm 0.925 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7560
2024-01-30 17:39:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:39:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:39:21 | INFO | fairseq.trainer | begin training epoch 75
2024-01-30 17:39:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:39:25 | INFO | train_inner | epoch 075:     58 / 1475 loss=2.355, nll_loss=0.716, ppl=1.64, wps=33453.8, ups=8.81, wpb=3795.8, bsz=200.5, num_updates=109200, lr=9.56949e-05, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.7, wall=7564
2024-01-30 17:39:31 | INFO | train_inner | epoch 075:    158 / 1475 loss=2.353, nll_loss=0.713, ppl=1.64, wps=58426, ups=15.54, wpb=3760.8, bsz=199.8, num_updates=109300, lr=9.56511e-05, gnorm=0.9, loss_scale=16, train_wall=6, gb_free=29.7, wall=7570
2024-01-30 17:39:37 | INFO | train_inner | epoch 075:    258 / 1475 loss=2.349, nll_loss=0.708, ppl=1.63, wps=58696.3, ups=15.58, wpb=3767.9, bsz=200.6, num_updates=109400, lr=9.56074e-05, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.8, wall=7577
2024-01-30 17:39:44 | INFO | train_inner | epoch 075:    358 / 1475 loss=2.374, nll_loss=0.736, ppl=1.67, wps=58730.6, ups=15.64, wpb=3754, bsz=186.5, num_updates=109500, lr=9.55637e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.6, wall=7583
2024-01-30 17:39:50 | INFO | train_inner | epoch 075:    458 / 1475 loss=2.348, nll_loss=0.708, ppl=1.63, wps=58432.9, ups=15.5, wpb=3769, bsz=205.8, num_updates=109600, lr=9.55201e-05, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=30.1, wall=7589
2024-01-30 17:39:57 | INFO | train_inner | epoch 075:    558 / 1475 loss=2.38, nll_loss=0.743, ppl=1.67, wps=58625.3, ups=15.66, wpb=3742.6, bsz=185.6, num_updates=109700, lr=9.54765e-05, gnorm=0.957, loss_scale=16, train_wall=6, gb_free=29.7, wall=7596
2024-01-30 17:40:03 | INFO | train_inner | epoch 075:    658 / 1475 loss=2.38, nll_loss=0.743, ppl=1.67, wps=59507.4, ups=15.62, wpb=3810, bsz=187.8, num_updates=109800, lr=9.54331e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.7, wall=7602
2024-01-30 17:40:09 | INFO | train_inner | epoch 075:    758 / 1475 loss=2.362, nll_loss=0.724, ppl=1.65, wps=59041.4, ups=15.56, wpb=3795.6, bsz=200.7, num_updates=109900, lr=9.53896e-05, gnorm=0.955, loss_scale=16, train_wall=6, gb_free=29.7, wall=7609
2024-01-30 17:40:16 | INFO | train_inner | epoch 075:    858 / 1475 loss=2.38, nll_loss=0.744, ppl=1.67, wps=59488.4, ups=15.75, wpb=3776.7, bsz=187.4, num_updates=110000, lr=9.53463e-05, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=7615
2024-01-30 17:40:22 | INFO | train_inner | epoch 075:    958 / 1475 loss=2.36, nll_loss=0.721, ppl=1.65, wps=57756, ups=15.63, wpb=3696.4, bsz=197.4, num_updates=110100, lr=9.53029e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.7, wall=7621
2024-01-30 17:40:29 | INFO | train_inner | epoch 075:   1058 / 1475 loss=2.37, nll_loss=0.733, ppl=1.66, wps=58405.3, ups=15.57, wpb=3751.8, bsz=200.3, num_updates=110200, lr=9.52597e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=7628
2024-01-30 17:40:35 | INFO | train_inner | epoch 075:   1158 / 1475 loss=2.369, nll_loss=0.731, ppl=1.66, wps=58371.6, ups=15.67, wpb=3724.1, bsz=194.8, num_updates=110300, lr=9.52165e-05, gnorm=0.943, loss_scale=16, train_wall=6, gb_free=29.7, wall=7634
2024-01-30 17:40:42 | INFO | train_inner | epoch 075:   1258 / 1475 loss=2.368, nll_loss=0.732, ppl=1.66, wps=58631.1, ups=15.49, wpb=3786.2, bsz=202.2, num_updates=110400, lr=9.51734e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.8, wall=7641
2024-01-30 17:40:48 | INFO | train_inner | epoch 075:   1358 / 1475 loss=2.38, nll_loss=0.745, ppl=1.68, wps=59286.2, ups=15.47, wpb=3831.3, bsz=199.4, num_updates=110500, lr=9.51303e-05, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=7647
2024-01-30 17:40:54 | INFO | train_inner | epoch 075:   1458 / 1475 loss=2.382, nll_loss=0.747, ppl=1.68, wps=59423.9, ups=15.56, wpb=3817.9, bsz=196.9, num_updates=110600, lr=9.50873e-05, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.7, wall=7654
2024-01-30 17:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:40:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:40:58 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.451 | nll_loss 1.827 | ppl 3.55 | wps 133407 | wpb 3189.1 | bsz 163.4 | num_updates 110617 | best_loss 3.385
2024-01-30 17:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 110617 updates
2024-01-30 17:40:58 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt
2024-01-30 17:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt
2024-01-30 17:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint75.pt (epoch 75 @ 110617 updates, score 3.451) (writing took 2.356735553010367 seconds)
2024-01-30 17:41:00 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-01-30 17:41:00 | INFO | train | epoch 075 | loss 2.367 | nll_loss 0.73 | ppl 1.66 | wps 55920.4 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 110617 | lr 9.508e-05 | gnorm 0.928 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7659
2024-01-30 17:41:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:41:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:41:00 | INFO | fairseq.trainer | begin training epoch 76
2024-01-30 17:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:41:06 | INFO | train_inner | epoch 076:     83 / 1475 loss=2.343, nll_loss=0.701, ppl=1.63, wps=33571.5, ups=8.88, wpb=3782.5, bsz=199.5, num_updates=110700, lr=9.50443e-05, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.9, wall=7665
2024-01-30 17:41:12 | INFO | train_inner | epoch 076:    183 / 1475 loss=2.33, nll_loss=0.688, ppl=1.61, wps=58640.2, ups=15.55, wpb=3770.1, bsz=207.9, num_updates=110800, lr=9.50014e-05, gnorm=0.899, loss_scale=16, train_wall=6, gb_free=29.7, wall=7671
2024-01-30 17:41:18 | INFO | train_inner | epoch 076:    283 / 1475 loss=2.36, nll_loss=0.72, ppl=1.65, wps=58274.5, ups=15.65, wpb=3722.5, bsz=184, num_updates=110900, lr=9.49586e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=7678
2024-01-30 17:41:25 | INFO | train_inner | epoch 076:    383 / 1475 loss=2.329, nll_loss=0.686, ppl=1.61, wps=58642.9, ups=15.47, wpb=3789.8, bsz=215.3, num_updates=111000, lr=9.49158e-05, gnorm=0.887, loss_scale=16, train_wall=6, gb_free=29.7, wall=7684
2024-01-30 17:41:31 | INFO | train_inner | epoch 076:    483 / 1475 loss=2.364, nll_loss=0.725, ppl=1.65, wps=58645.7, ups=15.64, wpb=3748.6, bsz=189.9, num_updates=111100, lr=9.48731e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.7, wall=7690
2024-01-30 17:41:38 | INFO | train_inner | epoch 076:    583 / 1475 loss=2.339, nll_loss=0.698, ppl=1.62, wps=58815.5, ups=15.63, wpb=3763.1, bsz=207.7, num_updates=111200, lr=9.48304e-05, gnorm=0.913, loss_scale=16, train_wall=6, gb_free=29.7, wall=7697
2024-01-30 17:41:44 | INFO | train_inner | epoch 076:    683 / 1475 loss=2.36, nll_loss=0.722, ppl=1.65, wps=59398, ups=15.56, wpb=3816.5, bsz=193.4, num_updates=111300, lr=9.47878e-05, gnorm=0.918, loss_scale=16, train_wall=6, gb_free=29.7, wall=7703
2024-01-30 17:41:50 | INFO | train_inner | epoch 076:    783 / 1475 loss=2.39, nll_loss=0.754, ppl=1.69, wps=61232.1, ups=15.78, wpb=3879.8, bsz=183, num_updates=111400, lr=9.47452e-05, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.8, wall=7710
2024-01-30 17:42:20 | INFO | train_inner | epoch 076:    883 / 1475 loss=2.369, nll_loss=0.731, ppl=1.66, wps=12632.8, ups=3.36, wpb=3754.7, bsz=195.6, num_updates=111500, lr=9.47027e-05, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.9, wall=7739
2024-01-30 17:42:27 | INFO | train_inner | epoch 076:    983 / 1475 loss=2.381, nll_loss=0.745, ppl=1.68, wps=58166.6, ups=15.68, wpb=3710.5, bsz=198.4, num_updates=111600, lr=9.46603e-05, gnorm=0.956, loss_scale=16, train_wall=6, gb_free=29.6, wall=7746
2024-01-30 17:42:33 | INFO | train_inner | epoch 076:   1083 / 1475 loss=2.394, nll_loss=0.76, ppl=1.69, wps=58449, ups=15.58, wpb=3752.5, bsz=177.9, num_updates=111700, lr=9.46179e-05, gnorm=0.954, loss_scale=16, train_wall=6, gb_free=29.7, wall=7752
2024-01-30 17:42:39 | INFO | train_inner | epoch 076:   1183 / 1475 loss=2.37, nll_loss=0.734, ppl=1.66, wps=58851.5, ups=15.52, wpb=3791.2, bsz=195.8, num_updates=111800, lr=9.45756e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.7, wall=7759
2024-01-30 17:42:46 | INFO | train_inner | epoch 076:   1283 / 1475 loss=2.378, nll_loss=0.742, ppl=1.67, wps=59657.8, ups=15.64, wpb=3815.6, bsz=197.5, num_updates=111900, lr=9.45333e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=7765
2024-01-30 17:42:52 | INFO | train_inner | epoch 076:   1383 / 1475 loss=2.369, nll_loss=0.732, ppl=1.66, wps=58853.6, ups=15.51, wpb=3794.6, bsz=204, num_updates=112000, lr=9.44911e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=7771
2024-01-30 17:42:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:42:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:43:01 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.454 | nll_loss 1.83 | ppl 3.56 | wps 135387 | wpb 3189.1 | bsz 163.4 | num_updates 112092 | best_loss 3.385
2024-01-30 17:43:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 112092 updates
2024-01-30 17:43:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt
2024-01-30 17:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt
2024-01-30 17:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint76.pt (epoch 76 @ 112092 updates, score 3.454) (writing took 2.3753668719436973 seconds)
2024-01-30 17:43:03 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-01-30 17:43:03 | INFO | train | epoch 076 | loss 2.364 | nll_loss 0.726 | ppl 1.65 | wps 45160.2 | ups 11.97 | wpb 3771.6 | bsz 195.9 | num_updates 112092 | lr 9.44523e-05 | gnorm 0.93 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 7782
2024-01-30 17:43:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:43:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:43:03 | INFO | fairseq.trainer | begin training epoch 77
2024-01-30 17:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:43:04 | INFO | train_inner | epoch 077:      8 / 1475 loss=2.38, nll_loss=0.745, ppl=1.68, wps=31569.7, ups=8.53, wpb=3699.2, bsz=186.2, num_updates=112100, lr=9.4449e-05, gnorm=0.949, loss_scale=16, train_wall=6, gb_free=29.7, wall=7783
2024-01-30 17:43:10 | INFO | train_inner | epoch 077:    108 / 1475 loss=2.342, nll_loss=0.7, ppl=1.62, wps=58963.2, ups=15.51, wpb=3802.8, bsz=194.4, num_updates=112200, lr=9.44069e-05, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.7, wall=7790
2024-01-30 17:43:17 | INFO | train_inner | epoch 077:    208 / 1475 loss=2.33, nll_loss=0.687, ppl=1.61, wps=57657, ups=15.6, wpb=3696.8, bsz=196.1, num_updates=112300, lr=9.43648e-05, gnorm=0.905, loss_scale=16, train_wall=6, gb_free=29.8, wall=7796
2024-01-30 17:43:24 | INFO | train_inner | epoch 077:    308 / 1475 loss=2.351, nll_loss=0.71, ppl=1.64, wps=56611.1, ups=14.81, wpb=3823.2, bsz=197.4, num_updates=112400, lr=9.43228e-05, gnorm=0.903, loss_scale=16, train_wall=7, gb_free=29.7, wall=7803
2024-01-30 17:43:36 | INFO | train_inner | epoch 077:    408 / 1475 loss=2.35, nll_loss=0.709, ppl=1.63, wps=30938.3, ups=8.09, wpb=3822.2, bsz=197.8, num_updates=112500, lr=9.42809e-05, gnorm=0.912, loss_scale=16, train_wall=12, gb_free=29.7, wall=7815
2024-01-30 17:43:42 | INFO | train_inner | epoch 077:    508 / 1475 loss=2.353, nll_loss=0.713, ppl=1.64, wps=58796.8, ups=15.61, wpb=3766.1, bsz=197, num_updates=112600, lr=9.4239e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=7822
2024-01-30 17:43:49 | INFO | train_inner | epoch 077:    608 / 1475 loss=2.385, nll_loss=0.749, ppl=1.68, wps=59502.1, ups=15.73, wpb=3781.6, bsz=189.4, num_updates=112700, lr=9.41972e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.7, wall=7828
2024-01-30 17:43:55 | INFO | train_inner | epoch 077:    708 / 1475 loss=2.344, nll_loss=0.704, ppl=1.63, wps=59053.5, ups=15.41, wpb=3832.1, bsz=212, num_updates=112800, lr=9.41554e-05, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.8, wall=7834
2024-01-30 17:44:02 | INFO | train_inner | epoch 077:    808 / 1475 loss=2.371, nll_loss=0.733, ppl=1.66, wps=58351.3, ups=15.66, wpb=3725.1, bsz=185.8, num_updates=112900, lr=9.41137e-05, gnorm=0.957, loss_scale=16, train_wall=6, gb_free=29.7, wall=7841
2024-01-30 17:44:08 | INFO | train_inner | epoch 077:    908 / 1475 loss=2.371, nll_loss=0.734, ppl=1.66, wps=57758.6, ups=15.6, wpb=3702.9, bsz=184.6, num_updates=113000, lr=9.40721e-05, gnorm=0.96, loss_scale=16, train_wall=6, gb_free=29.7, wall=7847
2024-01-30 17:44:16 | INFO | train_inner | epoch 077:   1008 / 1475 loss=2.378, nll_loss=0.743, ppl=1.67, wps=48280.8, ups=12.51, wpb=3860.9, bsz=191.6, num_updates=113100, lr=9.40305e-05, gnorm=0.92, loss_scale=16, train_wall=8, gb_free=29.8, wall=7855
2024-01-30 17:44:22 | INFO | train_inner | epoch 077:   1108 / 1475 loss=2.37, nll_loss=0.733, ppl=1.66, wps=58451.8, ups=15.5, wpb=3771.2, bsz=195.7, num_updates=113200, lr=9.39889e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=7862
2024-01-30 17:44:29 | INFO | train_inner | epoch 077:   1208 / 1475 loss=2.371, nll_loss=0.734, ppl=1.66, wps=59008.6, ups=15.53, wpb=3799.1, bsz=200.8, num_updates=113300, lr=9.39475e-05, gnorm=0.93, loss_scale=16, train_wall=6, gb_free=29.7, wall=7868
2024-01-30 17:44:35 | INFO | train_inner | epoch 077:   1308 / 1475 loss=2.358, nll_loss=0.72, ppl=1.65, wps=56351.3, ups=15.58, wpb=3616.7, bsz=199.2, num_updates=113400, lr=9.3906e-05, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.7, wall=7874
2024-01-30 17:44:42 | INFO | train_inner | epoch 077:   1408 / 1475 loss=2.366, nll_loss=0.73, ppl=1.66, wps=60530.7, ups=15.57, wpb=3888.4, bsz=203.3, num_updates=113500, lr=9.38647e-05, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=7881
2024-01-30 17:44:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:44:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:44:48 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.464 | nll_loss 1.84 | ppl 3.58 | wps 135420 | wpb 3189.1 | bsz 163.4 | num_updates 113567 | best_loss 3.385
2024-01-30 17:44:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 113567 updates
2024-01-30 17:44:48 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt
2024-01-30 17:44:50 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt
2024-01-30 17:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint77.pt (epoch 77 @ 113567 updates, score 3.464) (writing took 2.881643370958045 seconds)
2024-01-30 17:44:51 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-01-30 17:44:51 | INFO | train | epoch 077 | loss 2.36 | nll_loss 0.721 | ppl 1.65 | wps 51570.4 | ups 13.67 | wpb 3771.6 | bsz 195.9 | num_updates 113567 | lr 9.3837e-05 | gnorm 0.931 | loss_scale 16 | train_wall 99 | gb_free 29.8 | wall 7890
2024-01-30 17:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:44:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:44:51 | INFO | fairseq.trainer | begin training epoch 78
2024-01-30 17:44:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:44:54 | INFO | train_inner | epoch 078:     33 / 1475 loss=2.34, nll_loss=0.7, ppl=1.62, wps=31486.1, ups=8.47, wpb=3715.9, bsz=203, num_updates=113600, lr=9.38233e-05, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=7893
2024-01-30 17:45:00 | INFO | train_inner | epoch 078:    133 / 1475 loss=2.339, nll_loss=0.697, ppl=1.62, wps=59614.5, ups=15.67, wpb=3804.4, bsz=193.8, num_updates=113700, lr=9.37821e-05, gnorm=0.894, loss_scale=16, train_wall=6, gb_free=29.7, wall=7899
2024-01-30 17:45:06 | INFO | train_inner | epoch 078:    233 / 1475 loss=2.35, nll_loss=0.709, ppl=1.63, wps=59073.2, ups=15.71, wpb=3760.4, bsz=189.3, num_updates=113800, lr=9.37408e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=7905
2024-01-30 17:45:13 | INFO | train_inner | epoch 078:    333 / 1475 loss=2.36, nll_loss=0.721, ppl=1.65, wps=55306.4, ups=14.79, wpb=3739.2, bsz=185.4, num_updates=113900, lr=9.36997e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=7912
2024-01-30 17:45:19 | INFO | train_inner | epoch 078:    433 / 1475 loss=2.347, nll_loss=0.706, ppl=1.63, wps=59990.7, ups=15.66, wpb=3829.7, bsz=198.2, num_updates=114000, lr=9.36586e-05, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.8, wall=7919
2024-01-30 17:45:26 | INFO | train_inner | epoch 078:    533 / 1475 loss=2.354, nll_loss=0.715, ppl=1.64, wps=59088, ups=15.54, wpb=3802.5, bsz=192.7, num_updates=114100, lr=9.36175e-05, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=7925
2024-01-30 17:45:32 | INFO | train_inner | epoch 078:    633 / 1475 loss=2.365, nll_loss=0.727, ppl=1.66, wps=58136.7, ups=15.61, wpb=3725.5, bsz=189, num_updates=114200, lr=9.35765e-05, gnorm=0.965, loss_scale=16, train_wall=6, gb_free=29.7, wall=7931
2024-01-30 17:45:39 | INFO | train_inner | epoch 078:    733 / 1475 loss=2.348, nll_loss=0.707, ppl=1.63, wps=58533.7, ups=15.58, wpb=3757.9, bsz=205.8, num_updates=114300, lr=9.35356e-05, gnorm=0.945, loss_scale=16, train_wall=6, gb_free=29.8, wall=7938
2024-01-30 17:45:45 | INFO | train_inner | epoch 078:    833 / 1475 loss=2.352, nll_loss=0.714, ppl=1.64, wps=59271.1, ups=15.55, wpb=3810.6, bsz=197.8, num_updates=114400, lr=9.34947e-05, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.8, wall=7944
2024-01-30 17:45:52 | INFO | train_inner | epoch 078:    933 / 1475 loss=2.363, nll_loss=0.725, ppl=1.65, wps=58218.1, ups=15.68, wpb=3713.5, bsz=191.6, num_updates=114500, lr=9.34539e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=30, wall=7951
2024-01-30 17:45:58 | INFO | train_inner | epoch 078:   1033 / 1475 loss=2.345, nll_loss=0.706, ppl=1.63, wps=58032.2, ups=15.54, wpb=3734, bsz=202.3, num_updates=114600, lr=9.34131e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.8, wall=7957
2024-01-30 17:46:04 | INFO | train_inner | epoch 078:   1133 / 1475 loss=2.368, nll_loss=0.73, ppl=1.66, wps=58427.9, ups=15.6, wpb=3745.9, bsz=200.6, num_updates=114700, lr=9.33724e-05, gnorm=0.938, loss_scale=16, train_wall=6, gb_free=29.7, wall=7964
2024-01-30 17:46:11 | INFO | train_inner | epoch 078:   1233 / 1475 loss=2.379, nll_loss=0.743, ppl=1.67, wps=59743.7, ups=15.7, wpb=3805.9, bsz=187.4, num_updates=114800, lr=9.33317e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.7, wall=7970
2024-01-30 17:46:17 | INFO | train_inner | epoch 078:   1333 / 1475 loss=2.36, nll_loss=0.722, ppl=1.65, wps=58962.8, ups=15.58, wpb=3783.4, bsz=204.4, num_updates=114900, lr=9.32911e-05, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.8, wall=7976
2024-01-30 17:46:24 | INFO | train_inner | epoch 078:   1433 / 1475 loss=2.363, nll_loss=0.727, ppl=1.65, wps=58035.9, ups=15.51, wpb=3741.3, bsz=199.9, num_updates=115000, lr=9.32505e-05, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.7, wall=7983
2024-01-30 17:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:46:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:46:29 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.461 | nll_loss 1.837 | ppl 3.57 | wps 134438 | wpb 3189.1 | bsz 163.4 | num_updates 115042 | best_loss 3.385
2024-01-30 17:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 115042 updates
2024-01-30 17:46:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint78.pt
2024-01-30 17:46:29 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint78.pt
2024-01-30 17:46:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint78.pt (epoch 78 @ 115042 updates, score 3.461) (writing took 2.327676499960944 seconds)
2024-01-30 17:46:31 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-01-30 17:46:31 | INFO | train | epoch 078 | loss 2.356 | nll_loss 0.717 | ppl 1.64 | wps 55792.5 | ups 14.79 | wpb 3771.6 | bsz 195.9 | num_updates 115042 | lr 9.32335e-05 | gnorm 0.929 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 7990
2024-01-30 17:46:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:46:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:46:31 | INFO | fairseq.trainer | begin training epoch 79
2024-01-30 17:46:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:46:35 | INFO | train_inner | epoch 079:     58 / 1475 loss=2.36, nll_loss=0.721, ppl=1.65, wps=33279.2, ups=8.96, wpb=3713.5, bsz=185.9, num_updates=115100, lr=9.321e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.8, wall=7994
2024-01-30 17:46:41 | INFO | train_inner | epoch 079:    158 / 1475 loss=2.345, nll_loss=0.703, ppl=1.63, wps=59337.7, ups=15.7, wpb=3779.2, bsz=182.2, num_updates=115200, lr=9.31695e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=8000
2024-01-30 17:46:48 | INFO | train_inner | epoch 079:    258 / 1475 loss=2.346, nll_loss=0.705, ppl=1.63, wps=60504.8, ups=15.61, wpb=3875.5, bsz=194.6, num_updates=115300, lr=9.31291e-05, gnorm=0.914, loss_scale=16, train_wall=6, gb_free=29.7, wall=8007
2024-01-30 17:46:54 | INFO | train_inner | epoch 079:    358 / 1475 loss=2.343, nll_loss=0.702, ppl=1.63, wps=58756.5, ups=15.64, wpb=3756.4, bsz=190.4, num_updates=115400, lr=9.30887e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=8013
2024-01-30 17:47:00 | INFO | train_inner | epoch 079:    458 / 1475 loss=2.35, nll_loss=0.709, ppl=1.64, wps=59782.2, ups=15.58, wpb=3835.9, bsz=200, num_updates=115500, lr=9.30484e-05, gnorm=0.912, loss_scale=16, train_wall=6, gb_free=29.7, wall=8019
2024-01-30 17:47:07 | INFO | train_inner | epoch 079:    558 / 1475 loss=2.339, nll_loss=0.698, ppl=1.62, wps=58855.5, ups=15.49, wpb=3799.5, bsz=207.1, num_updates=115600, lr=9.30082e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=8026
2024-01-30 17:47:13 | INFO | train_inner | epoch 079:    658 / 1475 loss=2.356, nll_loss=0.718, ppl=1.64, wps=59124.9, ups=15.61, wpb=3788.7, bsz=194.3, num_updates=115700, lr=9.2968e-05, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.6, wall=8032
2024-01-30 17:47:20 | INFO | train_inner | epoch 079:    758 / 1475 loss=2.344, nll_loss=0.704, ppl=1.63, wps=57575.2, ups=15.59, wpb=3692.1, bsz=200.4, num_updates=115800, lr=9.29278e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.7, wall=8039
2024-01-30 17:47:26 | INFO | train_inner | epoch 079:    858 / 1475 loss=2.368, nll_loss=0.731, ppl=1.66, wps=59023.5, ups=15.65, wpb=3772, bsz=186.5, num_updates=115900, lr=9.28877e-05, gnorm=0.943, loss_scale=32, train_wall=6, gb_free=29.7, wall=8045
2024-01-30 17:47:33 | INFO | train_inner | epoch 079:    958 / 1475 loss=2.356, nll_loss=0.718, ppl=1.64, wps=58135.8, ups=15.43, wpb=3768.2, bsz=200.9, num_updates=116000, lr=9.28477e-05, gnorm=0.937, loss_scale=32, train_wall=6, gb_free=29.7, wall=8052
2024-01-30 17:47:39 | INFO | train_inner | epoch 079:   1058 / 1475 loss=2.354, nll_loss=0.714, ppl=1.64, wps=59133.6, ups=15.61, wpb=3789.2, bsz=197.4, num_updates=116100, lr=9.28077e-05, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=8058
2024-01-30 17:47:45 | INFO | train_inner | epoch 079:   1158 / 1475 loss=2.358, nll_loss=0.72, ppl=1.65, wps=58742.3, ups=15.57, wpb=3773.1, bsz=198.7, num_updates=116200, lr=9.27677e-05, gnorm=0.946, loss_scale=32, train_wall=6, gb_free=29.8, wall=8064
2024-01-30 17:47:52 | INFO | train_inner | epoch 079:   1258 / 1475 loss=2.354, nll_loss=0.715, ppl=1.64, wps=58582.4, ups=15.56, wpb=3764.1, bsz=205.7, num_updates=116300, lr=9.27278e-05, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.7, wall=8071
2024-01-30 17:47:58 | INFO | train_inner | epoch 079:   1358 / 1475 loss=2.364, nll_loss=0.726, ppl=1.65, wps=58167.4, ups=15.64, wpb=3718.7, bsz=191.3, num_updates=116400, lr=9.2688e-05, gnorm=0.967, loss_scale=32, train_wall=6, gb_free=29.7, wall=8077
2024-01-30 17:48:05 | INFO | train_inner | epoch 079:   1458 / 1475 loss=2.367, nll_loss=0.731, ppl=1.66, wps=58607.3, ups=15.65, wpb=3745.5, bsz=196.2, num_updates=116500, lr=9.26482e-05, gnorm=0.937, loss_scale=32, train_wall=6, gb_free=29.8, wall=8084
2024-01-30 17:48:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:48:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:48:08 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 3.456 | nll_loss 1.833 | ppl 3.56 | wps 134364 | wpb 3189.1 | bsz 163.4 | num_updates 116517 | best_loss 3.385
2024-01-30 17:48:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 116517 updates
2024-01-30 17:48:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint79.pt
2024-01-30 17:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint79.pt
2024-01-30 17:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint79.pt (epoch 79 @ 116517 updates, score 3.456) (writing took 2.433108831057325 seconds)
2024-01-30 17:48:10 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-01-30 17:48:10 | INFO | train | epoch 079 | loss 2.353 | nll_loss 0.713 | ppl 1.64 | wps 55965.4 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 116517 | lr 9.26415e-05 | gnorm 0.934 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 8089
2024-01-30 17:48:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:48:10 | INFO | fairseq.trainer | begin training epoch 80
2024-01-30 17:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:48:16 | INFO | train_inner | epoch 080:     83 / 1475 loss=2.327, nll_loss=0.684, ppl=1.61, wps=33493.4, ups=8.87, wpb=3774.6, bsz=203.4, num_updates=116600, lr=9.26085e-05, gnorm=0.895, loss_scale=32, train_wall=6, gb_free=29.7, wall=8095
2024-01-30 17:48:22 | INFO | train_inner | epoch 080:    183 / 1475 loss=2.326, nll_loss=0.683, ppl=1.61, wps=58843.5, ups=15.57, wpb=3778.5, bsz=192.4, num_updates=116700, lr=9.25688e-05, gnorm=0.916, loss_scale=32, train_wall=6, gb_free=29.9, wall=8101
2024-01-30 17:48:29 | INFO | train_inner | epoch 080:    283 / 1475 loss=2.334, nll_loss=0.691, ppl=1.61, wps=57496.6, ups=15.63, wpb=3678.6, bsz=193.8, num_updates=116800, lr=9.25292e-05, gnorm=0.93, loss_scale=32, train_wall=6, gb_free=29.7, wall=8108
2024-01-30 17:48:35 | INFO | train_inner | epoch 080:    383 / 1475 loss=2.333, nll_loss=0.691, ppl=1.61, wps=59726.7, ups=15.59, wpb=3832.1, bsz=202.7, num_updates=116900, lr=9.24896e-05, gnorm=0.899, loss_scale=32, train_wall=6, gb_free=29.7, wall=8114
2024-01-30 17:48:41 | INFO | train_inner | epoch 080:    483 / 1475 loss=2.348, nll_loss=0.708, ppl=1.63, wps=58635, ups=15.59, wpb=3761.2, bsz=195.7, num_updates=117000, lr=9.245e-05, gnorm=0.93, loss_scale=32, train_wall=6, gb_free=29.7, wall=8121
2024-01-30 17:48:48 | INFO | train_inner | epoch 080:    583 / 1475 loss=2.329, nll_loss=0.686, ppl=1.61, wps=59902.2, ups=15.52, wpb=3859.7, bsz=207.1, num_updates=117100, lr=9.24105e-05, gnorm=0.875, loss_scale=32, train_wall=6, gb_free=29.7, wall=8127
2024-01-30 17:48:54 | INFO | train_inner | epoch 080:    683 / 1475 loss=2.359, nll_loss=0.72, ppl=1.65, wps=59271.4, ups=15.74, wpb=3765.7, bsz=189, num_updates=117200, lr=9.23711e-05, gnorm=0.957, loss_scale=32, train_wall=6, gb_free=29.7, wall=8133
2024-01-30 17:49:01 | INFO | train_inner | epoch 080:    783 / 1475 loss=2.341, nll_loss=0.702, ppl=1.63, wps=59331.6, ups=15.55, wpb=3816.1, bsz=203.4, num_updates=117300, lr=9.23317e-05, gnorm=0.906, loss_scale=32, train_wall=6, gb_free=29.7, wall=8140
2024-01-30 17:49:07 | INFO | train_inner | epoch 080:    883 / 1475 loss=2.355, nll_loss=0.716, ppl=1.64, wps=59061.4, ups=15.65, wpb=3774, bsz=190.7, num_updates=117400, lr=9.22924e-05, gnorm=0.945, loss_scale=32, train_wall=6, gb_free=29.8, wall=8146
2024-01-30 17:49:14 | INFO | train_inner | epoch 080:    983 / 1475 loss=2.367, nll_loss=0.729, ppl=1.66, wps=57878.6, ups=15.44, wpb=3748.8, bsz=192, num_updates=117500, lr=9.22531e-05, gnorm=0.936, loss_scale=32, train_wall=6, gb_free=29.8, wall=8153
2024-01-30 17:49:20 | INFO | train_inner | epoch 080:   1083 / 1475 loss=2.36, nll_loss=0.723, ppl=1.65, wps=57881.9, ups=15.55, wpb=3722.2, bsz=195.6, num_updates=117600, lr=9.22139e-05, gnorm=0.941, loss_scale=32, train_wall=6, gb_free=29.7, wall=8159
2024-01-30 17:49:26 | INFO | train_inner | epoch 080:   1183 / 1475 loss=2.363, nll_loss=0.726, ppl=1.65, wps=58008.2, ups=15.61, wpb=3715.3, bsz=188.6, num_updates=117700, lr=9.21747e-05, gnorm=0.941, loss_scale=32, train_wall=6, gb_free=29.7, wall=8166
2024-01-30 17:49:33 | INFO | train_inner | epoch 080:   1283 / 1475 loss=2.37, nll_loss=0.734, ppl=1.66, wps=60025.7, ups=15.54, wpb=3862.3, bsz=192.8, num_updates=117800, lr=9.21356e-05, gnorm=0.939, loss_scale=32, train_wall=6, gb_free=29.8, wall=8172
2024-01-30 17:49:39 | INFO | train_inner | epoch 080:   1383 / 1475 loss=2.366, nll_loss=0.729, ppl=1.66, wps=58890.4, ups=15.62, wpb=3769.4, bsz=201, num_updates=117900, lr=9.20965e-05, gnorm=0.94, loss_scale=32, train_wall=6, gb_free=29.7, wall=8178
2024-01-30 17:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:49:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:49:47 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 3.454 | nll_loss 1.833 | ppl 3.56 | wps 136349 | wpb 3189.1 | bsz 163.4 | num_updates 117992 | best_loss 3.385
2024-01-30 17:49:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 117992 updates
2024-01-30 17:49:47 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint80.pt
2024-01-30 17:49:48 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint80.pt
2024-01-30 17:49:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint80.pt (epoch 80 @ 117992 updates, score 3.454) (writing took 2.4658570119645447 seconds)
2024-01-30 17:49:50 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-01-30 17:49:50 | INFO | train | epoch 080 | loss 2.35 | nll_loss 0.71 | ppl 1.64 | wps 55891.6 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 117992 | lr 9.20606e-05 | gnorm 0.927 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 8189
2024-01-30 17:49:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:49:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:49:50 | INFO | fairseq.trainer | begin training epoch 81
2024-01-30 17:49:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:49:51 | INFO | train_inner | epoch 081:      8 / 1475 loss=2.366, nll_loss=0.729, ppl=1.66, wps=33131.6, ups=8.84, wpb=3745.9, bsz=195.7, num_updates=118000, lr=9.20575e-05, gnorm=0.944, loss_scale=32, train_wall=6, gb_free=29.7, wall=8190
2024-01-30 17:49:57 | INFO | train_inner | epoch 081:    108 / 1475 loss=2.32, nll_loss=0.676, ppl=1.6, wps=59233.4, ups=15.6, wpb=3797.5, bsz=202.7, num_updates=118100, lr=9.20185e-05, gnorm=0.908, loss_scale=32, train_wall=6, gb_free=29.9, wall=8196
2024-01-30 17:50:03 | INFO | train_inner | epoch 081:    208 / 1475 loss=2.324, nll_loss=0.681, ppl=1.6, wps=58451.1, ups=15.6, wpb=3746, bsz=194.9, num_updates=118200, lr=9.19795e-05, gnorm=0.907, loss_scale=32, train_wall=6, gb_free=29.7, wall=8202
2024-01-30 17:50:10 | INFO | train_inner | epoch 081:    308 / 1475 loss=2.346, nll_loss=0.705, ppl=1.63, wps=58253.7, ups=15.63, wpb=3725.9, bsz=188.8, num_updates=118300, lr=9.19407e-05, gnorm=0.934, loss_scale=32, train_wall=6, gb_free=29.6, wall=8209
2024-01-30 17:50:16 | INFO | train_inner | epoch 081:    408 / 1475 loss=2.344, nll_loss=0.704, ppl=1.63, wps=60432.2, ups=15.65, wpb=3862.1, bsz=190.4, num_updates=118400, lr=9.19018e-05, gnorm=0.919, loss_scale=32, train_wall=6, gb_free=29.8, wall=8215
2024-01-30 17:50:23 | INFO | train_inner | epoch 081:    508 / 1475 loss=2.348, nll_loss=0.707, ppl=1.63, wps=59041.6, ups=15.65, wpb=3771.5, bsz=189.4, num_updates=118500, lr=9.1863e-05, gnorm=0.953, loss_scale=32, train_wall=6, gb_free=29.7, wall=8222
2024-01-30 17:50:29 | INFO | train_inner | epoch 081:    608 / 1475 loss=2.338, nll_loss=0.696, ppl=1.62, wps=57664, ups=15.58, wpb=3701.8, bsz=196, num_updates=118600, lr=9.18243e-05, gnorm=0.958, loss_scale=32, train_wall=6, gb_free=29.7, wall=8228
2024-01-30 17:50:35 | INFO | train_inner | epoch 081:    708 / 1475 loss=2.342, nll_loss=0.701, ppl=1.63, wps=58901.3, ups=15.59, wpb=3778.2, bsz=207.4, num_updates=118700, lr=9.17856e-05, gnorm=0.937, loss_scale=32, train_wall=6, gb_free=29.8, wall=8234
2024-01-30 17:50:42 | INFO | train_inner | epoch 081:    808 / 1475 loss=2.35, nll_loss=0.71, ppl=1.64, wps=57071.1, ups=15.51, wpb=3680.4, bsz=193.4, num_updates=118800, lr=9.1747e-05, gnorm=0.951, loss_scale=32, train_wall=6, gb_free=29.7, wall=8241
2024-01-30 17:50:48 | INFO | train_inner | epoch 081:    908 / 1475 loss=2.336, nll_loss=0.695, ppl=1.62, wps=57750, ups=15.68, wpb=3682.5, bsz=194.8, num_updates=118900, lr=9.17084e-05, gnorm=0.941, loss_scale=32, train_wall=6, gb_free=29.8, wall=8247
2024-01-30 17:50:55 | INFO | train_inner | epoch 081:   1008 / 1475 loss=2.36, nll_loss=0.723, ppl=1.65, wps=59568.2, ups=15.59, wpb=3821, bsz=196.2, num_updates=119000, lr=9.16698e-05, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=8254
2024-01-30 17:51:01 | INFO | train_inner | epoch 081:   1108 / 1475 loss=2.354, nll_loss=0.715, ppl=1.64, wps=58542.9, ups=15.61, wpb=3751.4, bsz=197.7, num_updates=119100, lr=9.16314e-05, gnorm=0.939, loss_scale=32, train_wall=6, gb_free=29.7, wall=8260
2024-01-30 17:51:07 | INFO | train_inner | epoch 081:   1208 / 1475 loss=2.338, nll_loss=0.698, ppl=1.62, wps=58291.5, ups=15.46, wpb=3769.3, bsz=211.3, num_updates=119200, lr=9.15929e-05, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.6, wall=8267
2024-01-30 17:51:14 | INFO | train_inner | epoch 081:   1308 / 1475 loss=2.363, nll_loss=0.726, ppl=1.65, wps=59869.5, ups=15.57, wpb=3844.7, bsz=194.4, num_updates=119300, lr=9.15545e-05, gnorm=0.928, loss_scale=32, train_wall=6, gb_free=29.7, wall=8273
2024-01-30 17:51:20 | INFO | train_inner | epoch 081:   1408 / 1475 loss=2.367, nll_loss=0.731, ppl=1.66, wps=59294.5, ups=15.58, wpb=3806.8, bsz=189, num_updates=119400, lr=9.15162e-05, gnorm=0.935, loss_scale=32, train_wall=6, gb_free=29.7, wall=8279
2024-01-30 17:51:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:51:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:51:27 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 3.46 | nll_loss 1.836 | ppl 3.57 | wps 135029 | wpb 3189.1 | bsz 163.4 | num_updates 119467 | best_loss 3.385
2024-01-30 17:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 119467 updates
2024-01-30 17:51:27 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint81.pt
2024-01-30 17:51:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint81.pt
2024-01-30 17:51:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint81.pt (epoch 81 @ 119467 updates, score 3.46) (writing took 2.4812315420713276 seconds)
2024-01-30 17:51:29 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-01-30 17:51:29 | INFO | train | epoch 081 | loss 2.346 | nll_loss 0.706 | ppl 1.63 | wps 55892.4 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 119467 | lr 9.14905e-05 | gnorm 0.933 | loss_scale 32 | train_wall 91 | gb_free 29.7 | wall 8289
2024-01-30 17:51:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:51:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:51:29 | INFO | fairseq.trainer | begin training epoch 82
2024-01-30 17:51:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:51:32 | INFO | train_inner | epoch 082:     33 / 1475 loss=2.359, nll_loss=0.721, ppl=1.65, wps=33688.7, ups=8.81, wpb=3824.9, bsz=191.4, num_updates=119500, lr=9.14779e-05, gnorm=0.954, loss_scale=32, train_wall=6, gb_free=29.7, wall=8291
2024-01-30 17:51:38 | INFO | train_inner | epoch 082:    133 / 1475 loss=2.331, nll_loss=0.687, ppl=1.61, wps=59468.2, ups=15.64, wpb=3802.6, bsz=193.5, num_updates=119600, lr=9.14396e-05, gnorm=0.929, loss_scale=32, train_wall=6, gb_free=29.7, wall=8297
2024-01-30 17:51:45 | INFO | train_inner | epoch 082:    233 / 1475 loss=2.335, nll_loss=0.693, ppl=1.62, wps=59415.6, ups=15.57, wpb=3816.9, bsz=191, num_updates=119700, lr=9.14014e-05, gnorm=0.953, loss_scale=32, train_wall=6, gb_free=29.7, wall=8304
2024-01-30 17:51:51 | INFO | train_inner | epoch 082:    333 / 1475 loss=2.327, nll_loss=0.684, ppl=1.61, wps=57320.4, ups=15.67, wpb=3657.3, bsz=187.4, num_updates=119800, lr=9.13633e-05, gnorm=0.952, loss_scale=32, train_wall=6, gb_free=29.7, wall=8310
2024-01-30 17:51:57 | INFO | train_inner | epoch 082:    433 / 1475 loss=2.33, nll_loss=0.689, ppl=1.61, wps=59347.3, ups=15.59, wpb=3806.1, bsz=197.7, num_updates=119900, lr=9.13252e-05, gnorm=0.918, loss_scale=32, train_wall=6, gb_free=29.7, wall=8316
2024-01-30 17:52:04 | INFO | train_inner | epoch 082:    533 / 1475 loss=2.328, nll_loss=0.686, ppl=1.61, wps=58143.9, ups=15.57, wpb=3734.1, bsz=197.2, num_updates=120000, lr=9.12871e-05, gnorm=0.919, loss_scale=32, train_wall=6, gb_free=29.7, wall=8323
2024-01-30 17:52:10 | INFO | train_inner | epoch 082:    633 / 1475 loss=2.343, nll_loss=0.703, ppl=1.63, wps=59334.4, ups=15.6, wpb=3803.8, bsz=197.4, num_updates=120100, lr=9.12491e-05, gnorm=0.932, loss_scale=32, train_wall=6, gb_free=29.7, wall=8329
2024-01-30 17:52:17 | INFO | train_inner | epoch 082:    733 / 1475 loss=2.347, nll_loss=0.707, ppl=1.63, wps=59235.2, ups=15.64, wpb=3786.4, bsz=193.9, num_updates=120200, lr=9.12111e-05, gnorm=0.933, loss_scale=32, train_wall=6, gb_free=29.9, wall=8336
2024-01-30 17:52:23 | INFO | train_inner | epoch 082:    833 / 1475 loss=2.35, nll_loss=0.71, ppl=1.64, wps=58640, ups=15.68, wpb=3740.9, bsz=195.8, num_updates=120300, lr=9.11732e-05, gnorm=0.944, loss_scale=32, train_wall=6, gb_free=29.8, wall=8342
2024-01-30 17:52:29 | INFO | train_inner | epoch 082:    933 / 1475 loss=2.36, nll_loss=0.721, ppl=1.65, wps=59260, ups=15.66, wpb=3783.9, bsz=185.9, num_updates=120400, lr=9.11353e-05, gnorm=0.946, loss_scale=32, train_wall=6, gb_free=29.7, wall=8348
2024-01-30 17:52:36 | INFO | train_inner | epoch 082:   1033 / 1475 loss=2.347, nll_loss=0.707, ppl=1.63, wps=58372.7, ups=15.64, wpb=3733.1, bsz=195.7, num_updates=120500, lr=9.10975e-05, gnorm=0.94, loss_scale=32, train_wall=6, gb_free=29.7, wall=8355
2024-01-30 17:52:42 | INFO | train_inner | epoch 082:   1133 / 1475 loss=2.348, nll_loss=0.709, ppl=1.63, wps=58771.4, ups=15.6, wpb=3766.7, bsz=197, num_updates=120600, lr=9.10597e-05, gnorm=0.949, loss_scale=32, train_wall=6, gb_free=29.7, wall=8361
2024-01-30 17:52:48 | INFO | train_inner | epoch 082:   1233 / 1475 loss=2.342, nll_loss=0.702, ppl=1.63, wps=58072.2, ups=15.72, wpb=3695.2, bsz=203.9, num_updates=120700, lr=9.1022e-05, gnorm=0.925, loss_scale=32, train_wall=6, gb_free=29.7, wall=8368
2024-01-30 17:52:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2024-01-30 17:52:55 | INFO | train_inner | epoch 082:   1334 / 1475 loss=2.344, nll_loss=0.705, ppl=1.63, wps=59324.9, ups=15.4, wpb=3851.2, bsz=210.6, num_updates=120800, lr=9.09843e-05, gnorm=0.917, loss_scale=16, train_wall=6, gb_free=29.7, wall=8374
2024-01-30 17:53:01 | INFO | train_inner | epoch 082:   1434 / 1475 loss=2.361, nll_loss=0.724, ppl=1.65, wps=59354, ups=15.53, wpb=3822.9, bsz=199, num_updates=120900, lr=9.09467e-05, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=8381
2024-01-30 17:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:53:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:53:06 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 3.459 | nll_loss 1.839 | ppl 3.58 | wps 135363 | wpb 3189.1 | bsz 163.4 | num_updates 120941 | best_loss 3.385
2024-01-30 17:53:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 120941 updates
2024-01-30 17:53:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint82.pt
2024-01-30 17:53:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint82.pt
2024-01-30 17:53:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint82.pt (epoch 82 @ 120941 updates, score 3.459) (writing took 2.463479249039665 seconds)
2024-01-30 17:53:09 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-01-30 17:53:09 | INFO | train | epoch 082 | loss 2.343 | nll_loss 0.702 | ppl 1.63 | wps 55950.8 | ups 14.83 | wpb 3771.8 | bsz 196 | num_updates 120941 | lr 9.09313e-05 | gnorm 0.935 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 8388
2024-01-30 17:53:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:53:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:53:09 | INFO | fairseq.trainer | begin training epoch 83
2024-01-30 17:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:53:13 | INFO | train_inner | epoch 083:     59 / 1475 loss=2.327, nll_loss=0.684, ppl=1.61, wps=33170.8, ups=8.84, wpb=3751, bsz=199.7, num_updates=121000, lr=9.09091e-05, gnorm=0.911, loss_scale=16, train_wall=6, gb_free=29.7, wall=8392
2024-01-30 17:53:19 | INFO | train_inner | epoch 083:    159 / 1475 loss=2.325, nll_loss=0.681, ppl=1.6, wps=59305.1, ups=15.54, wpb=3817.4, bsz=192.6, num_updates=121100, lr=9.08715e-05, gnorm=0.907, loss_scale=16, train_wall=6, gb_free=29.6, wall=8398
2024-01-30 17:53:26 | INFO | train_inner | epoch 083:    259 / 1475 loss=2.317, nll_loss=0.674, ppl=1.6, wps=58749, ups=15.47, wpb=3797.7, bsz=206.7, num_updates=121200, lr=9.08341e-05, gnorm=0.901, loss_scale=16, train_wall=6, gb_free=29.7, wall=8405
2024-01-30 17:53:32 | INFO | train_inner | epoch 083:    359 / 1475 loss=2.316, nll_loss=0.672, ppl=1.59, wps=58335.6, ups=15.57, wpb=3746.7, bsz=199.2, num_updates=121300, lr=9.07966e-05, gnorm=0.903, loss_scale=16, train_wall=6, gb_free=29.6, wall=8411
2024-01-30 17:53:38 | INFO | train_inner | epoch 083:    459 / 1475 loss=2.36, nll_loss=0.721, ppl=1.65, wps=59385.6, ups=15.72, wpb=3778.2, bsz=175.1, num_updates=121400, lr=9.07592e-05, gnorm=0.954, loss_scale=16, train_wall=6, gb_free=29.7, wall=8418
2024-01-30 17:53:45 | INFO | train_inner | epoch 083:    559 / 1475 loss=2.323, nll_loss=0.68, ppl=1.6, wps=58888, ups=15.55, wpb=3785.8, bsz=205.1, num_updates=121500, lr=9.07218e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=8424
2024-01-30 17:53:51 | INFO | train_inner | epoch 083:    659 / 1475 loss=2.336, nll_loss=0.695, ppl=1.62, wps=58870.1, ups=15.75, wpb=3736.6, bsz=192.1, num_updates=121600, lr=9.06845e-05, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.8, wall=8430
2024-01-30 17:53:58 | INFO | train_inner | epoch 083:    759 / 1475 loss=2.355, nll_loss=0.716, ppl=1.64, wps=58975.6, ups=15.65, wpb=3769.4, bsz=187, num_updates=121700, lr=9.06473e-05, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.8, wall=8437
2024-01-30 17:54:04 | INFO | train_inner | epoch 083:    859 / 1475 loss=2.347, nll_loss=0.706, ppl=1.63, wps=59695.6, ups=15.63, wpb=3819.9, bsz=200.7, num_updates=121800, lr=9.061e-05, gnorm=0.947, loss_scale=16, train_wall=6, gb_free=29.7, wall=8443
2024-01-30 17:54:10 | INFO | train_inner | epoch 083:    959 / 1475 loss=2.331, nll_loss=0.69, ppl=1.61, wps=59012.6, ups=15.6, wpb=3781.7, bsz=201.6, num_updates=121900, lr=9.05729e-05, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.8, wall=8449
2024-01-30 17:54:17 | INFO | train_inner | epoch 083:   1059 / 1475 loss=2.35, nll_loss=0.711, ppl=1.64, wps=57601.2, ups=15.64, wpb=3682.5, bsz=197.8, num_updates=122000, lr=9.05357e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.6, wall=8456
2024-01-30 17:54:23 | INFO | train_inner | epoch 083:   1159 / 1475 loss=2.35, nll_loss=0.712, ppl=1.64, wps=59380.8, ups=15.57, wpb=3814.2, bsz=194.2, num_updates=122100, lr=9.04987e-05, gnorm=0.941, loss_scale=16, train_wall=6, gb_free=29.7, wall=8462
2024-01-30 17:54:30 | INFO | train_inner | epoch 083:   1259 / 1475 loss=2.343, nll_loss=0.704, ppl=1.63, wps=58185.9, ups=15.57, wpb=3737.4, bsz=203.4, num_updates=122200, lr=9.04616e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=8469
2024-01-30 17:54:36 | INFO | train_inner | epoch 083:   1359 / 1475 loss=2.37, nll_loss=0.733, ppl=1.66, wps=58623.5, ups=15.71, wpb=3731.6, bsz=184.6, num_updates=122300, lr=9.04246e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.7, wall=8475
2024-01-30 17:54:42 | INFO | train_inner | epoch 083:   1459 / 1475 loss=2.356, nll_loss=0.718, ppl=1.64, wps=59233.9, ups=15.54, wpb=3812.1, bsz=200.9, num_updates=122400, lr=9.03877e-05, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.7, wall=8482
2024-01-30 17:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:54:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:54:46 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 3.463 | nll_loss 1.843 | ppl 3.59 | wps 134767 | wpb 3189.1 | bsz 163.4 | num_updates 122416 | best_loss 3.385
2024-01-30 17:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 122416 updates
2024-01-30 17:54:46 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint83.pt
2024-01-30 17:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint83.pt
2024-01-30 17:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint83.pt (epoch 83 @ 122416 updates, score 3.463) (writing took 2.457986664958298 seconds)
2024-01-30 17:54:48 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-01-30 17:54:48 | INFO | train | epoch 083 | loss 2.34 | nll_loss 0.7 | ppl 1.62 | wps 55952 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 122416 | lr 9.03818e-05 | gnorm 0.935 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 8487
2024-01-30 17:54:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:54:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:54:48 | INFO | fairseq.trainer | begin training epoch 84
2024-01-30 17:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:54:54 | INFO | train_inner | epoch 084:     84 / 1475 loss=2.329, nll_loss=0.686, ppl=1.61, wps=33252.1, ups=8.83, wpb=3764.9, bsz=189.2, num_updates=122500, lr=9.03508e-05, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.7, wall=8493
2024-01-30 17:55:00 | INFO | train_inner | epoch 084:    184 / 1475 loss=2.337, nll_loss=0.695, ppl=1.62, wps=59344.7, ups=15.68, wpb=3784.7, bsz=185.8, num_updates=122600, lr=9.03139e-05, gnorm=0.922, loss_scale=16, train_wall=6, gb_free=29.7, wall=8499
2024-01-30 17:55:07 | INFO | train_inner | epoch 084:    284 / 1475 loss=2.321, nll_loss=0.679, ppl=1.6, wps=59329.8, ups=15.47, wpb=3834.7, bsz=192.1, num_updates=122700, lr=9.02771e-05, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.7, wall=8506
2024-01-30 17:55:13 | INFO | train_inner | epoch 084:    384 / 1475 loss=2.324, nll_loss=0.681, ppl=1.6, wps=57500.8, ups=15.66, wpb=3671.8, bsz=186.7, num_updates=122800, lr=9.02404e-05, gnorm=0.943, loss_scale=16, train_wall=6, gb_free=29.8, wall=8512
2024-01-30 17:55:19 | INFO | train_inner | epoch 084:    484 / 1475 loss=2.325, nll_loss=0.682, ppl=1.6, wps=60064.2, ups=15.64, wpb=3840.5, bsz=202.1, num_updates=122900, lr=9.02036e-05, gnorm=0.915, loss_scale=16, train_wall=6, gb_free=29.7, wall=8518
2024-01-30 17:55:26 | INFO | train_inner | epoch 084:    584 / 1475 loss=2.32, nll_loss=0.677, ppl=1.6, wps=57897.9, ups=15.56, wpb=3721.5, bsz=201, num_updates=123000, lr=9.0167e-05, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.8, wall=8525
2024-01-30 17:55:32 | INFO | train_inner | epoch 084:    684 / 1475 loss=2.346, nll_loss=0.707, ppl=1.63, wps=58722.3, ups=15.53, wpb=3780.4, bsz=199.6, num_updates=123100, lr=9.01303e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.7, wall=8531
2024-01-30 17:55:39 | INFO | train_inner | epoch 084:    784 / 1475 loss=2.343, nll_loss=0.702, ppl=1.63, wps=59655, ups=15.61, wpb=3821.8, bsz=198.6, num_updates=123200, lr=9.00937e-05, gnorm=0.94, loss_scale=16, train_wall=6, gb_free=29.7, wall=8538
2024-01-30 17:55:45 | INFO | train_inner | epoch 084:    884 / 1475 loss=2.323, nll_loss=0.68, ppl=1.6, wps=57516.5, ups=15.63, wpb=3679.6, bsz=201.9, num_updates=123300, lr=9.00572e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.6, wall=8544
2024-01-30 17:55:51 | INFO | train_inner | epoch 084:    984 / 1475 loss=2.335, nll_loss=0.695, ppl=1.62, wps=58992.7, ups=15.6, wpb=3781.6, bsz=205.1, num_updates=123400, lr=9.00207e-05, gnorm=0.923, loss_scale=16, train_wall=6, gb_free=29.7, wall=8551
2024-01-30 17:55:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2024-01-30 17:55:58 | INFO | train_inner | epoch 084:   1085 / 1475 loss=2.342, nll_loss=0.702, ppl=1.63, wps=58514.7, ups=15.53, wpb=3767.3, bsz=196.8, num_updates=123500, lr=8.99843e-05, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.6, wall=8557
2024-01-30 17:56:04 | INFO | train_inner | epoch 084:   1185 / 1475 loss=2.364, nll_loss=0.727, ppl=1.65, wps=59111.1, ups=15.6, wpb=3789.6, bsz=189.7, num_updates=123600, lr=8.99478e-05, gnorm=0.963, loss_scale=8, train_wall=6, gb_free=29.7, wall=8563
2024-01-30 17:56:11 | INFO | train_inner | epoch 084:   1285 / 1475 loss=2.359, nll_loss=0.722, ppl=1.65, wps=59886.3, ups=15.58, wpb=3844.2, bsz=192.4, num_updates=123700, lr=8.99115e-05, gnorm=0.931, loss_scale=8, train_wall=6, gb_free=29.8, wall=8570
2024-01-30 17:56:17 | INFO | train_inner | epoch 084:   1385 / 1475 loss=2.341, nll_loss=0.701, ppl=1.63, wps=58822.6, ups=15.58, wpb=3774.7, bsz=203.4, num_updates=123800, lr=8.98752e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.6, wall=8576
2024-01-30 17:56:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:56:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:56:25 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 3.464 | nll_loss 1.845 | ppl 3.59 | wps 134966 | wpb 3189.1 | bsz 163.4 | num_updates 123890 | best_loss 3.385
2024-01-30 17:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 123890 updates
2024-01-30 17:56:25 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint84.pt
2024-01-30 17:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint84.pt
2024-01-30 17:56:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint84.pt (epoch 84 @ 123890 updates, score 3.464) (writing took 2.3768509400542825 seconds)
2024-01-30 17:56:28 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-01-30 17:56:28 | INFO | train | epoch 084 | loss 2.337 | nll_loss 0.696 | ppl 1.62 | wps 55966 | ups 14.84 | wpb 3771.4 | bsz 195.9 | num_updates 123890 | lr 8.98425e-05 | gnorm 0.935 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 8587
2024-01-30 17:56:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:56:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:56:28 | INFO | fairseq.trainer | begin training epoch 85
2024-01-30 17:56:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:56:28 | INFO | train_inner | epoch 085:     10 / 1475 loss=2.343, nll_loss=0.703, ppl=1.63, wps=33174.4, ups=8.9, wpb=3726.6, bsz=192.2, num_updates=123900, lr=8.98389e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=8587
2024-01-30 17:56:35 | INFO | train_inner | epoch 085:    110 / 1475 loss=2.303, nll_loss=0.657, ppl=1.58, wps=59014.7, ups=15.64, wpb=3774.1, bsz=201.3, num_updates=124000, lr=8.98027e-05, gnorm=0.897, loss_scale=8, train_wall=6, gb_free=29.7, wall=8594
2024-01-30 17:56:41 | INFO | train_inner | epoch 085:    210 / 1475 loss=2.325, nll_loss=0.682, ppl=1.6, wps=58575.1, ups=15.69, wpb=3732.8, bsz=190.3, num_updates=124100, lr=8.97665e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.9, wall=8600
2024-01-30 17:56:48 | INFO | train_inner | epoch 085:    310 / 1475 loss=2.317, nll_loss=0.674, ppl=1.6, wps=59290.1, ups=15.56, wpb=3810.3, bsz=199, num_updates=124200, lr=8.97303e-05, gnorm=0.908, loss_scale=8, train_wall=6, gb_free=29.7, wall=8607
2024-01-30 17:56:54 | INFO | train_inner | epoch 085:    410 / 1475 loss=2.328, nll_loss=0.685, ppl=1.61, wps=58629.3, ups=15.61, wpb=3755.1, bsz=189.8, num_updates=124300, lr=8.96942e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.8, wall=8613
2024-01-30 17:57:00 | INFO | train_inner | epoch 085:    510 / 1475 loss=2.327, nll_loss=0.685, ppl=1.61, wps=59172.4, ups=15.63, wpb=3784.9, bsz=200.6, num_updates=124400, lr=8.96582e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.9, wall=8619
2024-01-30 17:57:07 | INFO | train_inner | epoch 085:    610 / 1475 loss=2.308, nll_loss=0.664, ppl=1.58, wps=58577.7, ups=15.53, wpb=3771, bsz=210.6, num_updates=124500, lr=8.96221e-05, gnorm=1.053, loss_scale=8, train_wall=6, gb_free=29.7, wall=8626
2024-01-30 17:57:13 | INFO | train_inner | epoch 085:    710 / 1475 loss=2.358, nll_loss=0.718, ppl=1.65, wps=59289.2, ups=15.74, wpb=3766.9, bsz=181.4, num_updates=124600, lr=8.95862e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.8, wall=8632
2024-01-30 17:57:20 | INFO | train_inner | epoch 085:    810 / 1475 loss=2.334, nll_loss=0.693, ppl=1.62, wps=58917.2, ups=15.71, wpb=3749.5, bsz=194.2, num_updates=124700, lr=8.95502e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=8639
2024-01-30 17:57:26 | INFO | train_inner | epoch 085:    910 / 1475 loss=2.338, nll_loss=0.697, ppl=1.62, wps=59165.3, ups=15.62, wpb=3787, bsz=196.9, num_updates=124800, lr=8.95144e-05, gnorm=0.935, loss_scale=8, train_wall=6, gb_free=29.7, wall=8645
2024-01-30 17:57:32 | INFO | train_inner | epoch 085:   1010 / 1475 loss=2.322, nll_loss=0.68, ppl=1.6, wps=58617.1, ups=15.43, wpb=3799.8, bsz=212.7, num_updates=124900, lr=8.94785e-05, gnorm=0.907, loss_scale=8, train_wall=6, gb_free=29.7, wall=8652
2024-01-30 17:57:39 | INFO | train_inner | epoch 085:   1110 / 1475 loss=2.347, nll_loss=0.708, ppl=1.63, wps=58452.7, ups=15.63, wpb=3740.5, bsz=191.4, num_updates=125000, lr=8.94427e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=8658
2024-01-30 17:57:45 | INFO | train_inner | epoch 085:   1210 / 1475 loss=2.348, nll_loss=0.709, ppl=1.63, wps=58682.8, ups=15.66, wpb=3747.6, bsz=190.8, num_updates=125100, lr=8.9407e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.9, wall=8664
2024-01-30 17:57:52 | INFO | train_inner | epoch 085:   1310 / 1475 loss=2.343, nll_loss=0.704, ppl=1.63, wps=58252.9, ups=15.59, wpb=3736.1, bsz=191.7, num_updates=125200, lr=8.93713e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.7, wall=8671
2024-01-30 17:57:58 | INFO | train_inner | epoch 085:   1410 / 1475 loss=2.346, nll_loss=0.707, ppl=1.63, wps=59689.4, ups=15.54, wpb=3840.7, bsz=197.4, num_updates=125300, lr=8.93356e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.7, wall=8677
2024-01-30 17:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:58:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:58:04 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 3.468 | nll_loss 1.848 | ppl 3.6 | wps 135289 | wpb 3189.1 | bsz 163.4 | num_updates 125365 | best_loss 3.385
2024-01-30 17:58:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 125365 updates
2024-01-30 17:58:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint85.pt
2024-01-30 17:58:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint85.pt
2024-01-30 17:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint85.pt (epoch 85 @ 125365 updates, score 3.468) (writing took 2.3463019009213895 seconds)
2024-01-30 17:58:07 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2024-01-30 17:58:07 | INFO | train | epoch 085 | loss 2.333 | nll_loss 0.692 | ppl 1.62 | wps 56016.1 | ups 14.85 | wpb 3771.6 | bsz 195.9 | num_updates 125365 | lr 8.93124e-05 | gnorm 0.946 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 8686
2024-01-30 17:58:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:58:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:58:07 | INFO | fairseq.trainer | begin training epoch 86
2024-01-30 17:58:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:58:09 | INFO | train_inner | epoch 086:     35 / 1475 loss=2.336, nll_loss=0.695, ppl=1.62, wps=33456.4, ups=8.92, wpb=3749.8, bsz=195.5, num_updates=125400, lr=8.93e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.7, wall=8688
2024-01-30 17:58:16 | INFO | train_inner | epoch 086:    135 / 1475 loss=2.334, nll_loss=0.692, ppl=1.62, wps=59204.3, ups=15.67, wpb=3778.4, bsz=186.8, num_updates=125500, lr=8.92644e-05, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.7, wall=8695
2024-01-30 17:58:22 | INFO | train_inner | epoch 086:    235 / 1475 loss=2.315, nll_loss=0.671, ppl=1.59, wps=59113, ups=15.61, wpb=3788.1, bsz=197.1, num_updates=125600, lr=8.92288e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.7, wall=8701
2024-01-30 17:58:28 | INFO | train_inner | epoch 086:    335 / 1475 loss=2.327, nll_loss=0.684, ppl=1.61, wps=59837.2, ups=15.59, wpb=3838.5, bsz=187.2, num_updates=125700, lr=8.91933e-05, gnorm=0.923, loss_scale=8, train_wall=6, gb_free=29.7, wall=8708
2024-01-30 17:58:35 | INFO | train_inner | epoch 086:    435 / 1475 loss=2.313, nll_loss=0.67, ppl=1.59, wps=56976.2, ups=15.61, wpb=3650.6, bsz=193, num_updates=125800, lr=8.91579e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=8714
2024-01-30 17:58:41 | INFO | train_inner | epoch 086:    535 / 1475 loss=2.326, nll_loss=0.684, ppl=1.61, wps=60150, ups=15.6, wpb=3854.8, bsz=203.3, num_updates=125900, lr=8.91225e-05, gnorm=0.904, loss_scale=8, train_wall=6, gb_free=29.7, wall=8720
2024-01-30 17:58:48 | INFO | train_inner | epoch 086:    635 / 1475 loss=2.329, nll_loss=0.687, ppl=1.61, wps=59863.9, ups=15.61, wpb=3835.4, bsz=200.6, num_updates=126000, lr=8.90871e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=8727
2024-01-30 17:58:54 | INFO | train_inner | epoch 086:    735 / 1475 loss=2.334, nll_loss=0.692, ppl=1.62, wps=59157.9, ups=15.55, wpb=3805.5, bsz=190.5, num_updates=126100, lr=8.90517e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.8, wall=8733
2024-01-30 17:59:00 | INFO | train_inner | epoch 086:    835 / 1475 loss=2.346, nll_loss=0.706, ppl=1.63, wps=58711.5, ups=15.64, wpb=3754.6, bsz=188.7, num_updates=126200, lr=8.90165e-05, gnorm=0.976, loss_scale=8, train_wall=6, gb_free=29.8, wall=8740
2024-01-30 17:59:07 | INFO | train_inner | epoch 086:    935 / 1475 loss=2.332, nll_loss=0.69, ppl=1.61, wps=59498.4, ups=15.59, wpb=3815.7, bsz=199.5, num_updates=126300, lr=8.89812e-05, gnorm=0.924, loss_scale=8, train_wall=6, gb_free=29.7, wall=8746
2024-01-30 17:59:13 | INFO | train_inner | epoch 086:   1035 / 1475 loss=2.328, nll_loss=0.686, ppl=1.61, wps=58640.9, ups=15.63, wpb=3751.9, bsz=197.2, num_updates=126400, lr=8.8946e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=8752
2024-01-30 17:59:20 | INFO | train_inner | epoch 086:   1135 / 1475 loss=2.337, nll_loss=0.696, ppl=1.62, wps=57914, ups=15.59, wpb=3714.3, bsz=199, num_updates=126500, lr=8.89108e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=8759
2024-01-30 17:59:26 | INFO | train_inner | epoch 086:   1235 / 1475 loss=2.335, nll_loss=0.695, ppl=1.62, wps=57498.1, ups=15.65, wpb=3673.2, bsz=203, num_updates=126600, lr=8.88757e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.8, wall=8765
2024-01-30 17:59:33 | INFO | train_inner | epoch 086:   1335 / 1475 loss=2.342, nll_loss=0.702, ppl=1.63, wps=58514.5, ups=15.54, wpb=3764.9, bsz=201.6, num_updates=126700, lr=8.88406e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.8, wall=8772
2024-01-30 17:59:39 | INFO | train_inner | epoch 086:   1435 / 1475 loss=2.34, nll_loss=0.7, ppl=1.62, wps=59260.6, ups=15.66, wpb=3784.3, bsz=197, num_updates=126800, lr=8.88056e-05, gnorm=0.936, loss_scale=8, train_wall=6, gb_free=29.7, wall=8778
2024-01-30 17:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 17:59:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:59:44 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 3.46 | nll_loss 1.84 | ppl 3.58 | wps 134597 | wpb 3189.1 | bsz 163.4 | num_updates 126840 | best_loss 3.385
2024-01-30 17:59:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 126840 updates
2024-01-30 17:59:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint86.pt
2024-01-30 17:59:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint86.pt
2024-01-30 17:59:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint86.pt (epoch 86 @ 126840 updates, score 3.46) (writing took 2.3675301539478824 seconds)
2024-01-30 17:59:46 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2024-01-30 17:59:46 | INFO | train | epoch 086 | loss 2.331 | nll_loss 0.689 | ppl 1.61 | wps 56016.5 | ups 14.85 | wpb 3771.6 | bsz 195.9 | num_updates 126840 | lr 8.87916e-05 | gnorm 0.942 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 8785
2024-01-30 17:59:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 17:59:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 17:59:46 | INFO | fairseq.trainer | begin training epoch 87
2024-01-30 17:59:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 17:59:50 | INFO | train_inner | epoch 087:     60 / 1475 loss=2.317, nll_loss=0.674, ppl=1.6, wps=33900.4, ups=8.87, wpb=3821.8, bsz=200.5, num_updates=126900, lr=8.87706e-05, gnorm=0.911, loss_scale=8, train_wall=6, gb_free=29.7, wall=8789
2024-01-30 17:59:57 | INFO | train_inner | epoch 087:    160 / 1475 loss=2.312, nll_loss=0.667, ppl=1.59, wps=59050.4, ups=15.67, wpb=3768.6, bsz=201, num_updates=127000, lr=8.87357e-05, gnorm=0.922, loss_scale=8, train_wall=6, gb_free=29.7, wall=8796
2024-01-30 18:00:03 | INFO | train_inner | epoch 087:    260 / 1475 loss=2.301, nll_loss=0.656, ppl=1.58, wps=58606.3, ups=15.57, wpb=3764.6, bsz=198.2, num_updates=127100, lr=8.87007e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.7, wall=8802
2024-01-30 18:00:09 | INFO | train_inner | epoch 087:    360 / 1475 loss=2.305, nll_loss=0.66, ppl=1.58, wps=57738.1, ups=15.57, wpb=3708.8, bsz=204.7, num_updates=127200, lr=8.86659e-05, gnorm=0.922, loss_scale=8, train_wall=6, gb_free=29.7, wall=8809
2024-01-30 18:00:16 | INFO | train_inner | epoch 087:    460 / 1475 loss=2.329, nll_loss=0.687, ppl=1.61, wps=58614.7, ups=15.62, wpb=3752.1, bsz=190.7, num_updates=127300, lr=8.8631e-05, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.7, wall=8815
2024-01-30 18:00:22 | INFO | train_inner | epoch 087:    560 / 1475 loss=2.328, nll_loss=0.686, ppl=1.61, wps=59320.5, ups=15.66, wpb=3787.4, bsz=185.4, num_updates=127400, lr=8.85962e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=8821
2024-01-30 18:00:29 | INFO | train_inner | epoch 087:    660 / 1475 loss=2.342, nll_loss=0.702, ppl=1.63, wps=59408.5, ups=15.73, wpb=3777.8, bsz=179.9, num_updates=127500, lr=8.85615e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.7, wall=8828
2024-01-30 18:00:35 | INFO | train_inner | epoch 087:    760 / 1475 loss=2.322, nll_loss=0.68, ppl=1.6, wps=58079.2, ups=15.56, wpb=3732.2, bsz=193.7, num_updates=127600, lr=8.85268e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=8834
2024-01-30 18:00:41 | INFO | train_inner | epoch 087:    860 / 1475 loss=2.33, nll_loss=0.689, ppl=1.61, wps=58725.1, ups=15.57, wpb=3770.8, bsz=200.6, num_updates=127700, lr=8.84921e-05, gnorm=0.935, loss_scale=8, train_wall=6, gb_free=29.7, wall=8841
2024-01-30 18:00:48 | INFO | train_inner | epoch 087:    960 / 1475 loss=2.332, nll_loss=0.691, ppl=1.61, wps=58831.9, ups=15.61, wpb=3768, bsz=197.3, num_updates=127800, lr=8.84575e-05, gnorm=0.94, loss_scale=8, train_wall=6, gb_free=29.7, wall=8847
2024-01-30 18:00:54 | INFO | train_inner | epoch 087:   1060 / 1475 loss=2.331, nll_loss=0.69, ppl=1.61, wps=57273.9, ups=15.64, wpb=3662.9, bsz=192.3, num_updates=127900, lr=8.84229e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=8853
2024-01-30 18:01:01 | INFO | train_inner | epoch 087:   1160 / 1475 loss=2.352, nll_loss=0.714, ppl=1.64, wps=59831.2, ups=15.6, wpb=3836.3, bsz=192.2, num_updates=128000, lr=8.83883e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.8, wall=8860
2024-01-30 18:01:07 | INFO | train_inner | epoch 087:   1260 / 1475 loss=2.331, nll_loss=0.691, ppl=1.61, wps=59453.8, ups=15.58, wpb=3815.6, bsz=201.6, num_updates=128100, lr=8.83538e-05, gnorm=0.936, loss_scale=8, train_wall=6, gb_free=29.7, wall=8866
2024-01-30 18:01:13 | INFO | train_inner | epoch 087:   1360 / 1475 loss=2.344, nll_loss=0.705, ppl=1.63, wps=60575.4, ups=15.7, wpb=3858.4, bsz=198.3, num_updates=128200, lr=8.83194e-05, gnorm=0.931, loss_scale=8, train_wall=6, gb_free=29.8, wall=8873
2024-01-30 18:01:20 | INFO | train_inner | epoch 087:   1460 / 1475 loss=2.344, nll_loss=0.705, ppl=1.63, wps=58847.9, ups=15.59, wpb=3775, bsz=195.9, num_updates=128300, lr=8.82849e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.7, wall=8879
2024-01-30 18:01:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:01:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:01:23 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 3.471 | nll_loss 1.851 | ppl 3.61 | wps 134799 | wpb 3189.1 | bsz 163.4 | num_updates 128315 | best_loss 3.385
2024-01-30 18:01:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 128315 updates
2024-01-30 18:01:23 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint87.pt
2024-01-30 18:01:24 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint87.pt
2024-01-30 18:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint87.pt (epoch 87 @ 128315 updates, score 3.471) (writing took 2.357626280048862 seconds)
2024-01-30 18:01:25 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2024-01-30 18:01:25 | INFO | train | epoch 087 | loss 2.328 | nll_loss 0.686 | ppl 1.61 | wps 56019.1 | ups 14.85 | wpb 3771.6 | bsz 195.9 | num_updates 128315 | lr 8.82798e-05 | gnorm 0.937 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 8885
2024-01-30 18:01:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:01:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:01:26 | INFO | fairseq.trainer | begin training epoch 88
2024-01-30 18:01:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:01:31 | INFO | train_inner | epoch 088:     85 / 1475 loss=2.311, nll_loss=0.666, ppl=1.59, wps=34306.4, ups=8.91, wpb=3849.8, bsz=194.2, num_updates=128400, lr=8.82506e-05, gnorm=0.906, loss_scale=8, train_wall=6, gb_free=29.7, wall=8890
2024-01-30 18:01:37 | INFO | train_inner | epoch 088:    185 / 1475 loss=2.32, nll_loss=0.676, ppl=1.6, wps=58674, ups=15.58, wpb=3765.6, bsz=184.7, num_updates=128500, lr=8.82162e-05, gnorm=0.936, loss_scale=8, train_wall=6, gb_free=29.7, wall=8897
2024-01-30 18:01:44 | INFO | train_inner | epoch 088:    285 / 1475 loss=2.321, nll_loss=0.678, ppl=1.6, wps=59087, ups=15.61, wpb=3784.3, bsz=194.5, num_updates=128600, lr=8.81819e-05, gnorm=0.934, loss_scale=8, train_wall=6, gb_free=29.7, wall=8903
2024-01-30 18:01:50 | INFO | train_inner | epoch 088:    385 / 1475 loss=2.317, nll_loss=0.673, ppl=1.59, wps=59009.8, ups=15.63, wpb=3774.7, bsz=199.2, num_updates=128700, lr=8.81476e-05, gnorm=0.924, loss_scale=8, train_wall=6, gb_free=29.8, wall=8909
2024-01-30 18:01:57 | INFO | train_inner | epoch 088:    485 / 1475 loss=2.326, nll_loss=0.684, ppl=1.61, wps=58759.3, ups=15.61, wpb=3764.6, bsz=192.2, num_updates=128800, lr=8.81134e-05, gnorm=0.929, loss_scale=8, train_wall=6, gb_free=29.7, wall=8916
2024-01-30 18:02:03 | INFO | train_inner | epoch 088:    585 / 1475 loss=2.296, nll_loss=0.65, ppl=1.57, wps=59513.3, ups=15.39, wpb=3866.2, bsz=224, num_updates=128900, lr=8.80792e-05, gnorm=0.88, loss_scale=8, train_wall=6, gb_free=29.7, wall=8922
2024-01-30 18:02:10 | INFO | train_inner | epoch 088:    685 / 1475 loss=2.319, nll_loss=0.676, ppl=1.6, wps=58615.9, ups=15.66, wpb=3743.7, bsz=196.6, num_updates=129000, lr=8.80451e-05, gnorm=0.932, loss_scale=8, train_wall=6, gb_free=29.7, wall=8929
2024-01-30 18:02:16 | INFO | train_inner | epoch 088:    785 / 1475 loss=2.332, nll_loss=0.69, ppl=1.61, wps=58255, ups=15.7, wpb=3709.9, bsz=192.1, num_updates=129100, lr=8.8011e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.7, wall=8935
2024-01-30 18:02:22 | INFO | train_inner | epoch 088:    885 / 1475 loss=2.316, nll_loss=0.673, ppl=1.59, wps=58764.8, ups=15.6, wpb=3765.9, bsz=200.8, num_updates=129200, lr=8.79769e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=8941
2024-01-30 18:02:29 | INFO | train_inner | epoch 088:    985 / 1475 loss=2.35, nll_loss=0.711, ppl=1.64, wps=58361.1, ups=15.65, wpb=3728.7, bsz=181.8, num_updates=129300, lr=8.79429e-05, gnorm=0.988, loss_scale=8, train_wall=6, gb_free=29.9, wall=8948
2024-01-30 18:02:35 | INFO | train_inner | epoch 088:   1085 / 1475 loss=2.341, nll_loss=0.701, ppl=1.63, wps=56742.9, ups=14.99, wpb=3785.4, bsz=186.6, num_updates=129400, lr=8.79089e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=8955
2024-01-30 18:02:42 | INFO | train_inner | epoch 088:   1185 / 1475 loss=2.329, nll_loss=0.689, ppl=1.61, wps=58296.2, ups=15.61, wpb=3735.4, bsz=200.7, num_updates=129500, lr=8.7875e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.8, wall=8961
2024-01-30 18:02:48 | INFO | train_inner | epoch 088:   1285 / 1475 loss=2.331, nll_loss=0.692, ppl=1.62, wps=59351.5, ups=15.55, wpb=3816.5, bsz=198.6, num_updates=129600, lr=8.7841e-05, gnorm=0.948, loss_scale=8, train_wall=6, gb_free=29.7, wall=8967
2024-01-30 18:02:55 | INFO | train_inner | epoch 088:   1385 / 1475 loss=2.342, nll_loss=0.703, ppl=1.63, wps=57865.7, ups=15.64, wpb=3700.1, bsz=191.4, num_updates=129700, lr=8.78072e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.7, wall=8974
2024-01-30 18:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:03:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:03:03 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 3.474 | nll_loss 1.858 | ppl 3.62 | wps 134860 | wpb 3189.1 | bsz 163.4 | num_updates 129790 | best_loss 3.385
2024-01-30 18:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 129790 updates
2024-01-30 18:03:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint88.pt
2024-01-30 18:03:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint88.pt
2024-01-30 18:03:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint88.pt (epoch 88 @ 129790 updates, score 3.474) (writing took 2.486113522085361 seconds)
2024-01-30 18:03:05 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2024-01-30 18:03:05 | INFO | train | epoch 088 | loss 2.325 | nll_loss 0.683 | ppl 1.6 | wps 55761.3 | ups 14.78 | wpb 3771.6 | bsz 195.9 | num_updates 129790 | lr 8.77767e-05 | gnorm 0.941 | loss_scale 8 | train_wall 92 | gb_free 29.9 | wall 8984
2024-01-30 18:03:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:03:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:03:05 | INFO | fairseq.trainer | begin training epoch 89
2024-01-30 18:03:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:03:06 | INFO | train_inner | epoch 089:     10 / 1475 loss=2.317, nll_loss=0.675, ppl=1.6, wps=33188.5, ups=8.77, wpb=3783.5, bsz=206.5, num_updates=129800, lr=8.77733e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.8, wall=8985
2024-01-30 18:03:12 | INFO | train_inner | epoch 089:    110 / 1475 loss=2.303, nll_loss=0.657, ppl=1.58, wps=59297.9, ups=15.61, wpb=3799.4, bsz=195.4, num_updates=129900, lr=8.77396e-05, gnorm=0.909, loss_scale=8, train_wall=6, gb_free=29.7, wall=8992
2024-01-30 18:03:20 | INFO | train_inner | epoch 089:    210 / 1475 loss=2.314, nll_loss=0.67, ppl=1.59, wps=49023.3, ups=13.05, wpb=3756.8, bsz=193.3, num_updates=130000, lr=8.77058e-05, gnorm=0.954, loss_scale=8, train_wall=7, gb_free=29.8, wall=8999
2024-01-30 18:03:27 | INFO | train_inner | epoch 089:    310 / 1475 loss=2.319, nll_loss=0.675, ppl=1.6, wps=58707.8, ups=15.75, wpb=3727.6, bsz=187.5, num_updates=130100, lr=8.76721e-05, gnorm=0.954, loss_scale=8, train_wall=6, gb_free=29.7, wall=9006
2024-01-30 18:03:33 | INFO | train_inner | epoch 089:    410 / 1475 loss=2.326, nll_loss=0.684, ppl=1.61, wps=58778.6, ups=15.55, wpb=3779.2, bsz=190.9, num_updates=130200, lr=8.76384e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.7, wall=9012
2024-01-30 18:03:39 | INFO | train_inner | epoch 089:    510 / 1475 loss=2.31, nll_loss=0.666, ppl=1.59, wps=58333.7, ups=15.63, wpb=3733.2, bsz=198.1, num_updates=130300, lr=8.76048e-05, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.7, wall=9019
2024-01-30 18:03:46 | INFO | train_inner | epoch 089:    610 / 1475 loss=2.328, nll_loss=0.687, ppl=1.61, wps=58579.2, ups=15.57, wpb=3762.4, bsz=184.7, num_updates=130400, lr=8.75712e-05, gnorm=0.928, loss_scale=8, train_wall=6, gb_free=29.8, wall=9025
2024-01-30 18:03:52 | INFO | train_inner | epoch 089:    710 / 1475 loss=2.286, nll_loss=0.64, ppl=1.56, wps=58857.8, ups=15.38, wpb=3828.1, bsz=223.4, num_updates=130500, lr=8.75376e-05, gnorm=0.886, loss_scale=8, train_wall=6, gb_free=29.7, wall=9031
2024-01-30 18:03:59 | INFO | train_inner | epoch 089:    810 / 1475 loss=2.314, nll_loss=0.671, ppl=1.59, wps=58033.2, ups=15.55, wpb=3732.9, bsz=193.2, num_updates=130600, lr=8.75041e-05, gnorm=0.939, loss_scale=8, train_wall=6, gb_free=29.7, wall=9038
2024-01-30 18:04:05 | INFO | train_inner | epoch 089:    910 / 1475 loss=2.334, nll_loss=0.693, ppl=1.62, wps=58488.7, ups=15.65, wpb=3738.2, bsz=189.3, num_updates=130700, lr=8.74706e-05, gnorm=0.962, loss_scale=8, train_wall=6, gb_free=29.8, wall=9044
2024-01-30 18:04:12 | INFO | train_inner | epoch 089:   1010 / 1475 loss=2.344, nll_loss=0.704, ppl=1.63, wps=60255.2, ups=15.67, wpb=3845.2, bsz=187, num_updates=130800, lr=8.74372e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=9051
2024-01-30 18:04:18 | INFO | train_inner | epoch 089:   1110 / 1475 loss=2.32, nll_loss=0.678, ppl=1.6, wps=59104.6, ups=15.49, wpb=3815.8, bsz=208.2, num_updates=130900, lr=8.74038e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.7, wall=9057
2024-01-30 18:04:24 | INFO | train_inner | epoch 089:   1210 / 1475 loss=2.352, nll_loss=0.713, ppl=1.64, wps=59140.1, ups=15.64, wpb=3782.1, bsz=186.2, num_updates=131000, lr=8.73704e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=9064
2024-01-30 18:04:31 | INFO | train_inner | epoch 089:   1310 / 1475 loss=2.329, nll_loss=0.688, ppl=1.61, wps=58633.1, ups=15.68, wpb=3738.3, bsz=195.5, num_updates=131100, lr=8.73371e-05, gnorm=0.956, loss_scale=8, train_wall=6, gb_free=29.7, wall=9070
2024-01-30 18:04:37 | INFO | train_inner | epoch 089:   1410 / 1475 loss=2.336, nll_loss=0.697, ppl=1.62, wps=58758.1, ups=15.6, wpb=3766.8, bsz=199.3, num_updates=131200, lr=8.73038e-05, gnorm=0.969, loss_scale=8, train_wall=6, gb_free=29.7, wall=9076
2024-01-30 18:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:04:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:04:44 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 3.472 | nll_loss 1.858 | ppl 3.62 | wps 134843 | wpb 3189.1 | bsz 163.4 | num_updates 131265 | best_loss 3.385
2024-01-30 18:04:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 131265 updates
2024-01-30 18:04:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint89.pt
2024-01-30 18:04:45 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint89.pt
2024-01-30 18:04:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint89.pt (epoch 89 @ 131265 updates, score 3.472) (writing took 2.881406973931007 seconds)
2024-01-30 18:04:47 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2024-01-30 18:04:47 | INFO | train | epoch 089 | loss 2.322 | nll_loss 0.679 | ppl 1.6 | wps 54832.7 | ups 14.54 | wpb 3771.6 | bsz 195.9 | num_updates 131265 | lr 8.72822e-05 | gnorm 0.941 | loss_scale 8 | train_wall 92 | gb_free 29.7 | wall 9086
2024-01-30 18:04:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:04:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:04:47 | INFO | fairseq.trainer | begin training epoch 90
2024-01-30 18:04:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:04:49 | INFO | train_inner | epoch 090:     35 / 1475 loss=2.306, nll_loss=0.662, ppl=1.58, wps=32101.1, ups=8.41, wpb=3818.2, bsz=211.7, num_updates=131300, lr=8.72705e-05, gnorm=0.904, loss_scale=8, train_wall=6, gb_free=29.7, wall=9088
2024-01-30 18:04:56 | INFO | train_inner | epoch 090:    135 / 1475 loss=2.285, nll_loss=0.638, ppl=1.56, wps=58483.2, ups=15.55, wpb=3761.6, bsz=202.6, num_updates=131400, lr=8.72373e-05, gnorm=0.916, loss_scale=8, train_wall=6, gb_free=29.7, wall=9095
2024-01-30 18:05:02 | INFO | train_inner | epoch 090:    235 / 1475 loss=2.312, nll_loss=0.667, ppl=1.59, wps=57441.4, ups=15.69, wpb=3662, bsz=181.3, num_updates=131500, lr=8.72041e-05, gnorm=0.977, loss_scale=8, train_wall=6, gb_free=29.7, wall=9101
2024-01-30 18:05:08 | INFO | train_inner | epoch 090:    335 / 1475 loss=2.317, nll_loss=0.673, ppl=1.59, wps=57918, ups=15.66, wpb=3698.5, bsz=181.1, num_updates=131600, lr=8.7171e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=9107
2024-01-30 18:05:15 | INFO | train_inner | epoch 090:    435 / 1475 loss=2.322, nll_loss=0.679, ppl=1.6, wps=59921.3, ups=15.54, wpb=3855.7, bsz=195.9, num_updates=131700, lr=8.71379e-05, gnorm=0.924, loss_scale=8, train_wall=6, gb_free=29.8, wall=9114
2024-01-30 18:05:21 | INFO | train_inner | epoch 090:    535 / 1475 loss=2.32, nll_loss=0.677, ppl=1.6, wps=58465.2, ups=15.7, wpb=3724.1, bsz=189, num_updates=131800, lr=8.71048e-05, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.8, wall=9120
2024-01-30 18:05:28 | INFO | train_inner | epoch 090:    635 / 1475 loss=2.309, nll_loss=0.665, ppl=1.59, wps=59381.4, ups=15.53, wpb=3823.6, bsz=203.3, num_updates=131900, lr=8.70718e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.7, wall=9127
2024-01-30 18:05:34 | INFO | train_inner | epoch 090:    735 / 1475 loss=2.31, nll_loss=0.667, ppl=1.59, wps=59653.1, ups=15.5, wpb=3847.9, bsz=203.8, num_updates=132000, lr=8.70388e-05, gnorm=0.934, loss_scale=8, train_wall=6, gb_free=29.7, wall=9133
2024-01-30 18:05:40 | INFO | train_inner | epoch 090:    835 / 1475 loss=2.313, nll_loss=0.671, ppl=1.59, wps=57927.8, ups=15.6, wpb=3713.7, bsz=199.4, num_updates=132100, lr=8.70059e-05, gnorm=0.97, loss_scale=8, train_wall=6, gb_free=29.8, wall=9140
2024-01-30 18:05:47 | INFO | train_inner | epoch 090:    935 / 1475 loss=2.329, nll_loss=0.689, ppl=1.61, wps=58993, ups=15.53, wpb=3797.7, bsz=194.4, num_updates=132200, lr=8.6973e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.7, wall=9146
2024-01-30 18:05:53 | INFO | train_inner | epoch 090:   1035 / 1475 loss=2.327, nll_loss=0.686, ppl=1.61, wps=58550.3, ups=15.56, wpb=3763.4, bsz=197, num_updates=132300, lr=8.69401e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.6, wall=9152
2024-01-30 18:06:00 | INFO | train_inner | epoch 090:   1135 / 1475 loss=2.33, nll_loss=0.689, ppl=1.61, wps=58851.4, ups=15.69, wpb=3750.8, bsz=195.3, num_updates=132400, lr=8.69072e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=9159
2024-01-30 18:06:06 | INFO | train_inner | epoch 090:   1235 / 1475 loss=2.331, nll_loss=0.69, ppl=1.61, wps=59314.2, ups=15.68, wpb=3782.6, bsz=194.3, num_updates=132500, lr=8.68744e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=9165
2024-01-30 18:06:12 | INFO | train_inner | epoch 090:   1335 / 1475 loss=2.334, nll_loss=0.693, ppl=1.62, wps=59227.9, ups=15.73, wpb=3765.5, bsz=189.1, num_updates=132600, lr=8.68417e-05, gnorm=0.959, loss_scale=8, train_wall=6, gb_free=29.7, wall=9172
2024-01-30 18:06:19 | INFO | train_inner | epoch 090:   1435 / 1475 loss=2.338, nll_loss=0.699, ppl=1.62, wps=58775.2, ups=15.57, wpb=3774.9, bsz=202.9, num_updates=132700, lr=8.6809e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=9178
2024-01-30 18:06:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:06:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:06:24 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 3.468 | nll_loss 1.848 | ppl 3.6 | wps 135472 | wpb 3189.1 | bsz 163.4 | num_updates 132740 | best_loss 3.385
2024-01-30 18:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 132740 updates
2024-01-30 18:06:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint90.pt
2024-01-30 18:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint90.pt
2024-01-30 18:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint90.pt (epoch 90 @ 132740 updates, score 3.468) (writing took 2.521815693937242 seconds)
2024-01-30 18:06:26 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2024-01-30 18:06:26 | INFO | train | epoch 090 | loss 2.319 | nll_loss 0.677 | ppl 1.6 | wps 55893.2 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 132740 | lr 8.67959e-05 | gnorm 0.944 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 9185
2024-01-30 18:06:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:06:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:06:26 | INFO | fairseq.trainer | begin training epoch 91
2024-01-30 18:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:06:30 | INFO | train_inner | epoch 091:     60 / 1475 loss=2.309, nll_loss=0.665, ppl=1.59, wps=33161.7, ups=8.75, wpb=3788.6, bsz=198.7, num_updates=132800, lr=8.67763e-05, gnorm=0.924, loss_scale=8, train_wall=6, gb_free=29.7, wall=9189
2024-01-30 18:06:37 | INFO | train_inner | epoch 091:    160 / 1475 loss=2.299, nll_loss=0.653, ppl=1.57, wps=59008.9, ups=15.65, wpb=3769.9, bsz=203.8, num_updates=132900, lr=8.67436e-05, gnorm=0.92, loss_scale=8, train_wall=6, gb_free=29.7, wall=9196
2024-01-30 18:06:43 | INFO | train_inner | epoch 091:    260 / 1475 loss=2.303, nll_loss=0.658, ppl=1.58, wps=58617.5, ups=15.61, wpb=3754.2, bsz=194.6, num_updates=133000, lr=8.6711e-05, gnorm=0.928, loss_scale=8, train_wall=6, gb_free=29.7, wall=9202
2024-01-30 18:06:49 | INFO | train_inner | epoch 091:    360 / 1475 loss=2.311, nll_loss=0.666, ppl=1.59, wps=59231.8, ups=15.67, wpb=3780.5, bsz=193.3, num_updates=133100, lr=8.66784e-05, gnorm=0.934, loss_scale=8, train_wall=6, gb_free=29.6, wall=9209
2024-01-30 18:06:56 | INFO | train_inner | epoch 091:    460 / 1475 loss=2.318, nll_loss=0.675, ppl=1.6, wps=59284.4, ups=15.52, wpb=3818.7, bsz=191.3, num_updates=133200, lr=8.66459e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.7, wall=9215
2024-01-30 18:07:02 | INFO | train_inner | epoch 091:    560 / 1475 loss=2.3, nll_loss=0.655, ppl=1.57, wps=58858.6, ups=15.53, wpb=3788.9, bsz=199.4, num_updates=133300, lr=8.66134e-05, gnorm=0.916, loss_scale=8, train_wall=6, gb_free=29.7, wall=9221
2024-01-30 18:07:09 | INFO | train_inner | epoch 091:    660 / 1475 loss=2.315, nll_loss=0.671, ppl=1.59, wps=59056.4, ups=15.51, wpb=3807.1, bsz=198, num_updates=133400, lr=8.65809e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.7, wall=9228
2024-01-30 18:07:15 | INFO | train_inner | epoch 091:    760 / 1475 loss=2.311, nll_loss=0.667, ppl=1.59, wps=57888, ups=15.57, wpb=3716.8, bsz=193.7, num_updates=133500, lr=8.65485e-05, gnorm=0.956, loss_scale=8, train_wall=6, gb_free=29.8, wall=9234
2024-01-30 18:07:22 | INFO | train_inner | epoch 091:    860 / 1475 loss=2.301, nll_loss=0.657, ppl=1.58, wps=58044.5, ups=15.46, wpb=3753.3, bsz=212.3, num_updates=133600, lr=8.65161e-05, gnorm=0.926, loss_scale=8, train_wall=6, gb_free=29.8, wall=9241
2024-01-30 18:07:28 | INFO | train_inner | epoch 091:    960 / 1475 loss=2.315, nll_loss=0.672, ppl=1.59, wps=58370.6, ups=15.63, wpb=3733.8, bsz=195.5, num_updates=133700, lr=8.64837e-05, gnorm=0.964, loss_scale=8, train_wall=6, gb_free=29.7, wall=9247
2024-01-30 18:07:34 | INFO | train_inner | epoch 091:   1060 / 1475 loss=2.326, nll_loss=0.685, ppl=1.61, wps=57044.1, ups=15.59, wpb=3658.3, bsz=196, num_updates=133800, lr=8.64514e-05, gnorm=0.957, loss_scale=8, train_wall=6, gb_free=29.7, wall=9254
2024-01-30 18:07:41 | INFO | train_inner | epoch 091:   1160 / 1475 loss=2.342, nll_loss=0.702, ppl=1.63, wps=60210.4, ups=15.67, wpb=3842.7, bsz=183, num_updates=133900, lr=8.64191e-05, gnorm=0.95, loss_scale=8, train_wall=6, gb_free=29.7, wall=9260
2024-01-30 18:07:47 | INFO | train_inner | epoch 091:   1260 / 1475 loss=2.329, nll_loss=0.688, ppl=1.61, wps=58933.4, ups=15.62, wpb=3771.8, bsz=193.8, num_updates=134000, lr=8.63868e-05, gnorm=0.96, loss_scale=8, train_wall=6, gb_free=29.9, wall=9266
2024-01-30 18:07:54 | INFO | train_inner | epoch 091:   1360 / 1475 loss=2.327, nll_loss=0.686, ppl=1.61, wps=59134.6, ups=15.56, wpb=3799.8, bsz=199, num_updates=134100, lr=8.63546e-05, gnorm=0.955, loss_scale=8, train_wall=6, gb_free=29.8, wall=9273
2024-01-30 18:08:00 | INFO | train_inner | epoch 091:   1460 / 1475 loss=2.339, nll_loss=0.699, ppl=1.62, wps=59288, ups=15.64, wpb=3790.9, bsz=190.1, num_updates=134200, lr=8.63224e-05, gnorm=0.965, loss_scale=8, train_wall=6, gb_free=29.7, wall=9279
2024-01-30 18:08:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:08:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:08:03 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 3.469 | nll_loss 1.851 | ppl 3.61 | wps 133417 | wpb 3189.1 | bsz 163.4 | num_updates 134215 | best_loss 3.385
2024-01-30 18:08:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 134215 updates
2024-01-30 18:08:03 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint91.pt
2024-01-30 18:08:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint91.pt
2024-01-30 18:08:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint91.pt (epoch 91 @ 134215 updates, score 3.469) (writing took 2.4069842140888795 seconds)
2024-01-30 18:08:06 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2024-01-30 18:08:06 | INFO | train | epoch 091 | loss 2.316 | nll_loss 0.673 | ppl 1.59 | wps 55899.2 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 134215 | lr 8.63176e-05 | gnorm 0.943 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 9285
2024-01-30 18:08:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:08:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:08:06 | INFO | fairseq.trainer | begin training epoch 92
2024-01-30 18:08:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:08:11 | INFO | train_inner | epoch 092:     85 / 1475 loss=2.306, nll_loss=0.661, ppl=1.58, wps=33343, ups=8.87, wpb=3759.1, bsz=192.9, num_updates=134300, lr=8.62903e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.7, wall=9290
2024-01-30 18:08:18 | INFO | train_inner | epoch 092:    185 / 1475 loss=2.291, nll_loss=0.645, ppl=1.56, wps=59349.1, ups=15.52, wpb=3824.5, bsz=196.7, num_updates=134400, lr=8.62582e-05, gnorm=0.895, loss_scale=8, train_wall=6, gb_free=29.7, wall=9297
2024-01-30 18:08:24 | INFO | train_inner | epoch 092:    285 / 1475 loss=2.296, nll_loss=0.65, ppl=1.57, wps=58564.7, ups=15.62, wpb=3748.6, bsz=199.6, num_updates=134500, lr=8.62261e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.6, wall=9303
2024-01-30 18:08:31 | INFO | train_inner | epoch 092:    385 / 1475 loss=2.312, nll_loss=0.667, ppl=1.59, wps=58472.5, ups=15.6, wpb=3748.9, bsz=185.8, num_updates=134600, lr=8.61941e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.8, wall=9310
2024-01-30 18:08:37 | INFO | train_inner | epoch 092:    485 / 1475 loss=2.311, nll_loss=0.668, ppl=1.59, wps=59154.1, ups=15.61, wpb=3789.9, bsz=186.3, num_updates=134700, lr=8.61621e-05, gnorm=0.942, loss_scale=8, train_wall=6, gb_free=29.7, wall=9316
2024-01-30 18:08:43 | INFO | train_inner | epoch 092:    585 / 1475 loss=2.306, nll_loss=0.661, ppl=1.58, wps=57363.8, ups=15.58, wpb=3681.4, bsz=194.5, num_updates=134800, lr=8.61301e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.7, wall=9323
2024-01-30 18:08:50 | INFO | train_inner | epoch 092:    685 / 1475 loss=2.321, nll_loss=0.679, ppl=1.6, wps=58877.2, ups=15.52, wpb=3793.8, bsz=192.2, num_updates=134900, lr=8.60982e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.7, wall=9329
2024-01-30 18:08:56 | INFO | train_inner | epoch 092:    785 / 1475 loss=2.3, nll_loss=0.656, ppl=1.58, wps=57764.9, ups=15.58, wpb=3707.4, bsz=202.6, num_updates=135000, lr=8.60663e-05, gnorm=0.941, loss_scale=8, train_wall=6, gb_free=29.8, wall=9335
2024-01-30 18:09:03 | INFO | train_inner | epoch 092:    885 / 1475 loss=2.306, nll_loss=0.662, ppl=1.58, wps=59966.6, ups=15.58, wpb=3847.9, bsz=207.8, num_updates=135100, lr=8.60344e-05, gnorm=0.914, loss_scale=8, train_wall=6, gb_free=29.7, wall=9342
2024-01-30 18:09:09 | INFO | train_inner | epoch 092:    985 / 1475 loss=2.322, nll_loss=0.681, ppl=1.6, wps=60311.8, ups=15.57, wpb=3873.5, bsz=198.3, num_updates=135200, lr=8.60026e-05, gnorm=0.92, loss_scale=8, train_wall=6, gb_free=29.8, wall=9348
2024-01-30 18:09:15 | INFO | train_inner | epoch 092:   1085 / 1475 loss=2.329, nll_loss=0.689, ppl=1.61, wps=59268.1, ups=15.72, wpb=3770.2, bsz=195.7, num_updates=135300, lr=8.59708e-05, gnorm=0.968, loss_scale=8, train_wall=6, gb_free=29.7, wall=9355
2024-01-30 18:09:22 | INFO | train_inner | epoch 092:   1185 / 1475 loss=2.33, nll_loss=0.689, ppl=1.61, wps=58018.1, ups=15.64, wpb=3709.6, bsz=188.7, num_updates=135400, lr=8.59391e-05, gnorm=0.971, loss_scale=8, train_wall=6, gb_free=29.7, wall=9361
2024-01-30 18:09:28 | INFO | train_inner | epoch 092:   1285 / 1475 loss=2.313, nll_loss=0.67, ppl=1.59, wps=59398.7, ups=15.51, wpb=3830.2, bsz=214, num_updates=135500, lr=8.59074e-05, gnorm=0.922, loss_scale=8, train_wall=6, gb_free=29.8, wall=9367
2024-01-30 18:09:35 | INFO | train_inner | epoch 092:   1385 / 1475 loss=2.331, nll_loss=0.691, ppl=1.61, wps=58455.2, ups=15.54, wpb=3762.2, bsz=188.9, num_updates=135600, lr=8.58757e-05, gnorm=0.98, loss_scale=8, train_wall=6, gb_free=29.7, wall=9374
2024-01-30 18:09:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:09:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:09:43 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 3.47 | nll_loss 1.852 | ppl 3.61 | wps 134738 | wpb 3189.1 | bsz 163.4 | num_updates 135690 | best_loss 3.385
2024-01-30 18:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 135690 updates
2024-01-30 18:09:43 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint92.pt
2024-01-30 18:09:44 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint92.pt
2024-01-30 18:09:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint92.pt (epoch 92 @ 135690 updates, score 3.47) (writing took 2.3721915129572153 seconds)
2024-01-30 18:09:45 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2024-01-30 18:09:45 | INFO | train | epoch 092 | loss 2.314 | nll_loss 0.671 | ppl 1.59 | wps 55963.5 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 135690 | lr 8.58472e-05 | gnorm 0.942 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 9384
2024-01-30 18:09:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:09:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:09:45 | INFO | fairseq.trainer | begin training epoch 93
2024-01-30 18:09:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:09:46 | INFO | train_inner | epoch 093:     10 / 1475 loss=2.326, nll_loss=0.685, ppl=1.61, wps=33318.3, ups=8.93, wpb=3730.5, bsz=199.3, num_updates=135700, lr=8.5844e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=9385
2024-01-30 18:09:52 | INFO | train_inner | epoch 093:    110 / 1475 loss=2.286, nll_loss=0.637, ppl=1.56, wps=57200.7, ups=15.62, wpb=3661.1, bsz=193.4, num_updates=135800, lr=8.58124e-05, gnorm=0.951, loss_scale=8, train_wall=6, gb_free=29.6, wall=9391
2024-01-30 18:09:59 | INFO | train_inner | epoch 093:    210 / 1475 loss=2.304, nll_loss=0.659, ppl=1.58, wps=59540.4, ups=15.68, wpb=3796.6, bsz=192.7, num_updates=135900, lr=8.57808e-05, gnorm=0.922, loss_scale=8, train_wall=6, gb_free=29.8, wall=9398
2024-01-30 18:10:05 | INFO | train_inner | epoch 093:    310 / 1475 loss=2.285, nll_loss=0.638, ppl=1.56, wps=58510.3, ups=15.55, wpb=3763.2, bsz=202.6, num_updates=136000, lr=8.57493e-05, gnorm=0.927, loss_scale=8, train_wall=6, gb_free=29.7, wall=9404
2024-01-30 18:10:12 | INFO | train_inner | epoch 093:    410 / 1475 loss=2.313, nll_loss=0.67, ppl=1.59, wps=59287, ups=15.61, wpb=3798.9, bsz=191.9, num_updates=136100, lr=8.57178e-05, gnorm=0.949, loss_scale=8, train_wall=6, gb_free=29.8, wall=9411
2024-01-30 18:10:18 | INFO | train_inner | epoch 093:    510 / 1475 loss=2.317, nll_loss=0.674, ppl=1.6, wps=59425.9, ups=15.63, wpb=3802.6, bsz=188.4, num_updates=136200, lr=8.56863e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=9417
2024-01-30 18:10:24 | INFO | train_inner | epoch 093:    610 / 1475 loss=2.296, nll_loss=0.65, ppl=1.57, wps=58067.4, ups=15.56, wpb=3732.4, bsz=208.2, num_updates=136300, lr=8.56549e-05, gnorm=0.932, loss_scale=8, train_wall=6, gb_free=29.8, wall=9423
2024-01-30 18:10:31 | INFO | train_inner | epoch 093:    710 / 1475 loss=2.307, nll_loss=0.663, ppl=1.58, wps=59341, ups=15.54, wpb=3819.2, bsz=189.8, num_updates=136400, lr=8.56235e-05, gnorm=0.928, loss_scale=8, train_wall=6, gb_free=29.7, wall=9430
2024-01-30 18:10:37 | INFO | train_inner | epoch 093:    810 / 1475 loss=2.309, nll_loss=0.666, ppl=1.59, wps=58669.4, ups=15.57, wpb=3769.3, bsz=198.8, num_updates=136500, lr=8.55921e-05, gnorm=0.958, loss_scale=8, train_wall=6, gb_free=29.7, wall=9436
2024-01-30 18:10:44 | INFO | train_inner | epoch 093:    910 / 1475 loss=2.303, nll_loss=0.66, ppl=1.58, wps=57734.2, ups=15.59, wpb=3703.3, bsz=203.5, num_updates=136600, lr=8.55608e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=9443
2024-01-30 18:10:50 | INFO | train_inner | epoch 093:   1010 / 1475 loss=2.322, nll_loss=0.68, ppl=1.6, wps=59689.6, ups=15.64, wpb=3816.5, bsz=188.8, num_updates=136700, lr=8.55295e-05, gnorm=0.945, loss_scale=8, train_wall=6, gb_free=29.8, wall=9449
2024-01-30 18:10:56 | INFO | train_inner | epoch 093:   1110 / 1475 loss=2.333, nll_loss=0.692, ppl=1.62, wps=58553.3, ups=15.53, wpb=3771.4, bsz=188.6, num_updates=136800, lr=8.54982e-05, gnorm=0.967, loss_scale=8, train_wall=6, gb_free=29.8, wall=9456
2024-01-30 18:11:03 | INFO | train_inner | epoch 093:   1210 / 1475 loss=2.324, nll_loss=0.683, ppl=1.61, wps=58752.5, ups=15.52, wpb=3784.8, bsz=193.5, num_updates=136900, lr=8.5467e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=9462
2024-01-30 18:11:09 | INFO | train_inner | epoch 093:   1310 / 1475 loss=2.321, nll_loss=0.679, ppl=1.6, wps=59722, ups=15.62, wpb=3822.4, bsz=203, num_updates=137000, lr=8.54358e-05, gnorm=0.947, loss_scale=8, train_wall=6, gb_free=29.7, wall=9468
2024-01-30 18:11:16 | INFO | train_inner | epoch 093:   1410 / 1475 loss=2.313, nll_loss=0.671, ppl=1.59, wps=58791.6, ups=15.61, wpb=3767.3, bsz=205.3, num_updates=137100, lr=8.54046e-05, gnorm=0.93, loss_scale=8, train_wall=6, gb_free=29.8, wall=9475
2024-01-30 18:11:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:11:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:11:22 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 3.474 | nll_loss 1.859 | ppl 3.63 | wps 134485 | wpb 3189.1 | bsz 163.4 | num_updates 137165 | best_loss 3.385
2024-01-30 18:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 137165 updates
2024-01-30 18:11:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint93.pt
2024-01-30 18:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint93.pt
2024-01-30 18:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint93.pt (epoch 93 @ 137165 updates, score 3.474) (writing took 2.4761313729686663 seconds)
2024-01-30 18:11:25 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2024-01-30 18:11:25 | INFO | train | epoch 093 | loss 2.311 | nll_loss 0.668 | ppl 1.59 | wps 55890.4 | ups 14.82 | wpb 3771.6 | bsz 195.9 | num_updates 137165 | lr 8.53844e-05 | gnorm 0.943 | loss_scale 8 | train_wall 91 | gb_free 29.8 | wall 9484
2024-01-30 18:11:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:11:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:11:25 | INFO | fairseq.trainer | begin training epoch 94
2024-01-30 18:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:11:27 | INFO | train_inner | epoch 094:     35 / 1475 loss=2.34, nll_loss=0.7, ppl=1.62, wps=33400.1, ups=8.85, wpb=3773.4, bsz=182.7, num_updates=137200, lr=8.53735e-05, gnorm=0.966, loss_scale=8, train_wall=6, gb_free=29.7, wall=9486
2024-01-30 18:11:33 | INFO | train_inner | epoch 094:    135 / 1475 loss=2.285, nll_loss=0.637, ppl=1.56, wps=57239.2, ups=15.65, wpb=3657.9, bsz=185.4, num_updates=137300, lr=8.53424e-05, gnorm=0.938, loss_scale=8, train_wall=6, gb_free=29.7, wall=9493
2024-01-30 18:11:40 | INFO | train_inner | epoch 094:    235 / 1475 loss=2.298, nll_loss=0.653, ppl=1.57, wps=58626, ups=15.63, wpb=3751, bsz=189.8, num_updates=137400, lr=8.53113e-05, gnorm=0.934, loss_scale=8, train_wall=6, gb_free=29.8, wall=9499
2024-01-30 18:11:46 | INFO | train_inner | epoch 094:    335 / 1475 loss=2.292, nll_loss=0.646, ppl=1.56, wps=59441.3, ups=15.51, wpb=3833.6, bsz=202.6, num_updates=137500, lr=8.52803e-05, gnorm=0.915, loss_scale=8, train_wall=6, gb_free=29.7, wall=9505
2024-01-30 18:11:53 | INFO | train_inner | epoch 094:    435 / 1475 loss=2.309, nll_loss=0.666, ppl=1.59, wps=59518.9, ups=15.62, wpb=3811.1, bsz=193, num_updates=137600, lr=8.52493e-05, gnorm=0.931, loss_scale=8, train_wall=6, gb_free=29.7, wall=9512
2024-01-30 18:11:59 | INFO | train_inner | epoch 094:    535 / 1475 loss=2.282, nll_loss=0.634, ppl=1.55, wps=57081.3, ups=15.44, wpb=3695.9, bsz=206.6, num_updates=137700, lr=8.52183e-05, gnorm=0.934, loss_scale=8, train_wall=6, gb_free=29.7, wall=9518
2024-01-30 18:12:06 | INFO | train_inner | epoch 094:    635 / 1475 loss=2.302, nll_loss=0.657, ppl=1.58, wps=58551.1, ups=15.69, wpb=3731.3, bsz=193.4, num_updates=137800, lr=8.51874e-05, gnorm=0.946, loss_scale=8, train_wall=6, gb_free=29.7, wall=9525
2024-01-30 18:12:12 | INFO | train_inner | epoch 094:    735 / 1475 loss=2.316, nll_loss=0.673, ppl=1.59, wps=59454.7, ups=15.6, wpb=3811.2, bsz=192.7, num_updates=137900, lr=8.51565e-05, gnorm=0.94, loss_scale=8, train_wall=6, gb_free=29.7, wall=9531
2024-01-30 18:12:18 | INFO | train_inner | epoch 094:    835 / 1475 loss=2.325, nll_loss=0.683, ppl=1.61, wps=59360, ups=15.69, wpb=3784.3, bsz=186, num_updates=138000, lr=8.51257e-05, gnorm=0.961, loss_scale=8, train_wall=6, gb_free=29.7, wall=9537
2024-01-30 18:12:25 | INFO | train_inner | epoch 094:    935 / 1475 loss=2.312, nll_loss=0.669, ppl=1.59, wps=59705.5, ups=15.54, wpb=3843.1, bsz=208.1, num_updates=138100, lr=8.50948e-05, gnorm=0.923, loss_scale=8, train_wall=6, gb_free=29.7, wall=9544
2024-01-30 18:12:31 | INFO | train_inner | epoch 094:   1035 / 1475 loss=2.321, nll_loss=0.679, ppl=1.6, wps=59786.7, ups=15.61, wpb=3829, bsz=189.3, num_updates=138200, lr=8.5064e-05, gnorm=0.944, loss_scale=8, train_wall=6, gb_free=29.7, wall=9550
2024-01-30 18:12:38 | INFO | train_inner | epoch 094:   1135 / 1475 loss=2.308, nll_loss=0.666, ppl=1.59, wps=57226.8, ups=15.56, wpb=3677.2, bsz=195, num_updates=138300, lr=8.50333e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=9557
2024-01-30 18:12:44 | INFO | train_inner | epoch 094:   1235 / 1475 loss=2.307, nll_loss=0.664, ppl=1.58, wps=59510.2, ups=15.6, wpb=3814.7, bsz=213.4, num_updates=138400, lr=8.50026e-05, gnorm=0.927, loss_scale=8, train_wall=6, gb_free=29.7, wall=9563
2024-01-30 18:12:50 | INFO | train_inner | epoch 094:   1335 / 1475 loss=2.324, nll_loss=0.682, ppl=1.6, wps=59931.9, ups=15.63, wpb=3834.6, bsz=193.4, num_updates=138500, lr=8.49719e-05, gnorm=0.974, loss_scale=8, train_wall=6, gb_free=29.7, wall=9570
2024-01-30 18:12:57 | INFO | train_inner | epoch 094:   1435 / 1475 loss=2.32, nll_loss=0.679, ppl=1.6, wps=58494, ups=15.64, wpb=3740.4, bsz=201.1, num_updates=138600, lr=8.49412e-05, gnorm=0.952, loss_scale=8, train_wall=6, gb_free=29.7, wall=9576
2024-01-30 18:12:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:12:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:13:02 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 3.483 | nll_loss 1.865 | ppl 3.64 | wps 134318 | wpb 3189.1 | bsz 163.4 | num_updates 138640 | best_loss 3.385
2024-01-30 18:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 138640 updates
2024-01-30 18:13:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint94.pt
2024-01-30 18:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint94.pt
2024-01-30 18:13:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint94.pt (epoch 94 @ 138640 updates, score 3.483) (writing took 2.417466612998396 seconds)
2024-01-30 18:13:04 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2024-01-30 18:13:04 | INFO | train | epoch 094 | loss 2.308 | nll_loss 0.665 | ppl 1.59 | wps 55956.3 | ups 14.84 | wpb 3771.6 | bsz 195.9 | num_updates 138640 | lr 8.49289e-05 | gnorm 0.942 | loss_scale 8 | train_wall 91 | gb_free 29.7 | wall 9583
2024-01-30 18:13:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:13:04 | INFO | fairseq.trainer | begin training epoch 95
2024-01-30 18:13:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:13:08 | INFO | train_inner | epoch 095:     60 / 1475 loss=2.302, nll_loss=0.657, ppl=1.58, wps=33368.6, ups=8.84, wpb=3773.3, bsz=196.5, num_updates=138700, lr=8.49106e-05, gnorm=0.931, loss_scale=8, train_wall=6, gb_free=29.8, wall=9587
2024-01-30 18:13:15 | INFO | train_inner | epoch 095:    160 / 1475 loss=2.296, nll_loss=0.65, ppl=1.57, wps=51880.3, ups=13.65, wpb=3800, bsz=189.4, num_updates=138800, lr=8.488e-05, gnorm=0.926, loss_scale=8, train_wall=7, gb_free=29.7, wall=9595
2024-01-30 18:13:22 | INFO | train_inner | epoch 095:    260 / 1475 loss=2.294, nll_loss=0.648, ppl=1.57, wps=58539.7, ups=15.61, wpb=3749.9, bsz=194.4, num_updates=138900, lr=8.48494e-05, gnorm=0.938, loss_scale=8, train_wall=6, gb_free=29.7, wall=9601
2024-01-30 18:13:28 | INFO | train_inner | epoch 095:    360 / 1475 loss=2.294, nll_loss=0.649, ppl=1.57, wps=58500.2, ups=15.59, wpb=3752.1, bsz=200, num_updates=139000, lr=8.48189e-05, gnorm=0.934, loss_scale=8, train_wall=6, gb_free=29.7, wall=9607
2024-01-30 18:13:35 | INFO | train_inner | epoch 095:    460 / 1475 loss=2.311, nll_loss=0.667, ppl=1.59, wps=58878.2, ups=15.68, wpb=3754, bsz=191, num_updates=139100, lr=8.47884e-05, gnorm=0.979, loss_scale=8, train_wall=6, gb_free=29.9, wall=9614
2024-01-30 18:13:41 | INFO | train_inner | epoch 095:    560 / 1475 loss=2.311, nll_loss=0.667, ppl=1.59, wps=56606.2, ups=15.69, wpb=3607.2, bsz=178.6, num_updates=139200, lr=8.47579e-05, gnorm=1.014, loss_scale=8, train_wall=6, gb_free=29.6, wall=9620
2024-01-30 18:13:47 | INFO | train_inner | epoch 095:    660 / 1475 loss=2.311, nll_loss=0.669, ppl=1.59, wps=58819.4, ups=15.54, wpb=3785.8, bsz=199.8, num_updates=139300, lr=8.47275e-05, gnorm=0.943, loss_scale=8, train_wall=6, gb_free=29.7, wall=9627
2024-01-30 18:13:54 | INFO | train_inner | epoch 095:    760 / 1475 loss=2.3, nll_loss=0.657, ppl=1.58, wps=59255.1, ups=15.56, wpb=3808.2, bsz=202.5, num_updates=139400, lr=8.46971e-05, gnorm=0.929, loss_scale=8, train_wall=6, gb_free=29.7, wall=9633
2024-01-30 18:14:00 | INFO | train_inner | epoch 095:    860 / 1475 loss=2.303, nll_loss=0.659, ppl=1.58, wps=59110.9, ups=15.59, wpb=3792.6, bsz=197.7, num_updates=139500, lr=8.46668e-05, gnorm=0.937, loss_scale=8, train_wall=6, gb_free=29.7, wall=9639
2024-01-30 18:14:07 | INFO | train_inner | epoch 095:    960 / 1475 loss=2.312, nll_loss=0.669, ppl=1.59, wps=59575.5, ups=15.64, wpb=3810.2, bsz=198.2, num_updates=139600, lr=8.46364e-05, gnorm=0.94, loss_scale=8, train_wall=6, gb_free=29.7, wall=9646
2024-01-30 18:14:13 | INFO | train_inner | epoch 095:   1060 / 1475 loss=2.298, nll_loss=0.654, ppl=1.57, wps=58174.5, ups=15.62, wpb=3723.2, bsz=196, num_updates=139700, lr=8.46061e-05, gnorm=0.953, loss_scale=8, train_wall=6, gb_free=29.7, wall=9652
2024-01-30 18:14:19 | INFO | train_inner | epoch 095:   1160 / 1475 loss=2.306, nll_loss=0.663, ppl=1.58, wps=60626.8, ups=15.6, wpb=3886.8, bsz=207.8, num_updates=139800, lr=8.45759e-05, gnorm=0.916, loss_scale=8, train_wall=6, gb_free=29.7, wall=9659
2024-01-30 18:14:26 | INFO | train_inner | epoch 095:   1260 / 1475 loss=2.32, nll_loss=0.678, ppl=1.6, wps=58598.8, ups=15.66, wpb=3742.6, bsz=191.1, num_updates=139900, lr=8.45456e-05, gnorm=0.97, loss_scale=16, train_wall=6, gb_free=29.7, wall=9665
2024-01-30 18:14:32 | INFO | train_inner | epoch 095:   1360 / 1475 loss=2.313, nll_loss=0.671, ppl=1.59, wps=59043.8, ups=15.53, wpb=3801.1, bsz=198.6, num_updates=140000, lr=8.45154e-05, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.6, wall=9671
2024-01-30 18:14:39 | INFO | train_inner | epoch 095:   1460 / 1475 loss=2.315, nll_loss=0.673, ppl=1.59, wps=59640.7, ups=15.7, wpb=3798.5, bsz=197.4, num_updates=140100, lr=8.44853e-05, gnorm=0.937, loss_scale=16, train_wall=6, gb_free=29.7, wall=9678
2024-01-30 18:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:14:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:14:42 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 3.486 | nll_loss 1.873 | ppl 3.66 | wps 134263 | wpb 3189.1 | bsz 163.4 | num_updates 140115 | best_loss 3.385
2024-01-30 18:14:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 140115 updates
2024-01-30 18:14:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint95.pt
2024-01-30 18:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint95.pt
2024-01-30 18:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint95.pt (epoch 95 @ 140115 updates, score 3.486) (writing took 2.6401774289552122 seconds)
2024-01-30 18:14:45 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2024-01-30 18:14:45 | INFO | train | epoch 095 | loss 2.305 | nll_loss 0.661 | ppl 1.58 | wps 55355.7 | ups 14.68 | wpb 3771.6 | bsz 195.9 | num_updates 140115 | lr 8.44807e-05 | gnorm 0.947 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 9684
2024-01-30 18:14:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:14:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:14:45 | INFO | fairseq.trainer | begin training epoch 96
2024-01-30 18:14:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:14:50 | INFO | train_inner | epoch 096:     85 / 1475 loss=2.284, nll_loss=0.636, ppl=1.55, wps=32482.9, ups=8.71, wpb=3730.1, bsz=194, num_updates=140200, lr=8.44551e-05, gnorm=0.916, loss_scale=16, train_wall=6, gb_free=29.7, wall=9689
2024-01-30 18:14:57 | INFO | train_inner | epoch 096:    185 / 1475 loss=2.285, nll_loss=0.638, ppl=1.56, wps=59150.4, ups=15.55, wpb=3804.3, bsz=201, num_updates=140300, lr=8.4425e-05, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=9696
2024-01-30 18:15:03 | INFO | train_inner | epoch 096:    285 / 1475 loss=2.284, nll_loss=0.637, ppl=1.56, wps=58069.9, ups=15.65, wpb=3710.7, bsz=197.2, num_updates=140400, lr=8.43949e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.8, wall=9702
2024-01-30 18:15:09 | INFO | train_inner | epoch 096:    385 / 1475 loss=2.302, nll_loss=0.657, ppl=1.58, wps=58838, ups=15.63, wpb=3764.7, bsz=192.3, num_updates=140500, lr=8.43649e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.8, wall=9708
2024-01-30 18:15:16 | INFO | train_inner | epoch 096:    485 / 1475 loss=2.297, nll_loss=0.652, ppl=1.57, wps=58189.7, ups=15.64, wpb=3720.9, bsz=193.8, num_updates=140600, lr=8.43349e-05, gnorm=0.949, loss_scale=16, train_wall=6, gb_free=29.7, wall=9715
2024-01-30 18:15:22 | INFO | train_inner | epoch 096:    585 / 1475 loss=2.3, nll_loss=0.656, ppl=1.58, wps=58602.3, ups=15.6, wpb=3757.2, bsz=198.2, num_updates=140700, lr=8.43049e-05, gnorm=0.938, loss_scale=16, train_wall=6, gb_free=29.8, wall=9721
2024-01-30 18:15:29 | INFO | train_inner | epoch 096:    685 / 1475 loss=2.305, nll_loss=0.662, ppl=1.58, wps=59322.3, ups=15.58, wpb=3808.5, bsz=194.2, num_updates=140800, lr=8.4275e-05, gnorm=0.926, loss_scale=16, train_wall=6, gb_free=29.6, wall=9728
2024-01-30 18:15:35 | INFO | train_inner | epoch 096:    785 / 1475 loss=2.301, nll_loss=0.657, ppl=1.58, wps=58776.7, ups=15.59, wpb=3769.5, bsz=198.9, num_updates=140900, lr=8.42451e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.8, wall=9734
2024-01-30 18:15:41 | INFO | train_inner | epoch 096:    885 / 1475 loss=2.306, nll_loss=0.663, ppl=1.58, wps=58702.5, ups=15.6, wpb=3762.2, bsz=194.9, num_updates=141000, lr=8.42152e-05, gnorm=0.963, loss_scale=16, train_wall=6, gb_free=29.7, wall=9741
2024-01-30 18:15:48 | INFO | train_inner | epoch 096:    985 / 1475 loss=2.315, nll_loss=0.673, ppl=1.59, wps=58876.3, ups=15.66, wpb=3759.7, bsz=192, num_updates=141100, lr=8.41853e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.7, wall=9747
2024-01-30 18:15:54 | INFO | train_inner | epoch 096:   1085 / 1475 loss=2.313, nll_loss=0.671, ppl=1.59, wps=59837.3, ups=15.55, wpb=3848.7, bsz=193.8, num_updates=141200, lr=8.41555e-05, gnorm=0.953, loss_scale=16, train_wall=6, gb_free=29.6, wall=9753
2024-01-30 18:16:01 | INFO | train_inner | epoch 096:   1185 / 1475 loss=2.319, nll_loss=0.677, ppl=1.6, wps=57897.4, ups=15.66, wpb=3697.3, bsz=187.1, num_updates=141300, lr=8.41257e-05, gnorm=0.962, loss_scale=16, train_wall=6, gb_free=29.7, wall=9760
2024-01-30 18:16:07 | INFO | train_inner | epoch 096:   1285 / 1475 loss=2.327, nll_loss=0.686, ppl=1.61, wps=59024.1, ups=15.58, wpb=3787.6, bsz=193.8, num_updates=141400, lr=8.4096e-05, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=29.7, wall=9766
2024-01-30 18:16:13 | INFO | train_inner | epoch 096:   1385 / 1475 loss=2.31, nll_loss=0.667, ppl=1.59, wps=59593.3, ups=15.57, wpb=3828.7, bsz=202.9, num_updates=141500, lr=8.40663e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.7, wall=9773
2024-01-30 18:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:16:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:16:22 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 3.47 | nll_loss 1.856 | ppl 3.62 | wps 134432 | wpb 3189.1 | bsz 163.4 | num_updates 141590 | best_loss 3.385
2024-01-30 18:16:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 141590 updates
2024-01-30 18:16:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint96.pt
2024-01-30 18:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint96.pt
2024-01-30 18:16:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint96.pt (epoch 96 @ 141590 updates, score 3.47) (writing took 2.4216454430716112 seconds)
2024-01-30 18:16:24 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2024-01-30 18:16:24 | INFO | train | epoch 096 | loss 2.304 | nll_loss 0.66 | ppl 1.58 | wps 55951.3 | ups 14.83 | wpb 3771.6 | bsz 195.9 | num_updates 141590 | lr 8.40395e-05 | gnorm 0.945 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 9783
2024-01-30 18:16:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:16:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:16:24 | INFO | fairseq.trainer | begin training epoch 97
2024-01-30 18:16:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:16:25 | INFO | train_inner | epoch 097:     10 / 1475 loss=2.303, nll_loss=0.66, ppl=1.58, wps=33614.5, ups=8.81, wpb=3816.3, bsz=205.2, num_updates=141600, lr=8.40366e-05, gnorm=0.928, loss_scale=16, train_wall=6, gb_free=29.8, wall=9784
2024-01-30 18:16:31 | INFO | train_inner | epoch 097:    110 / 1475 loss=2.282, nll_loss=0.634, ppl=1.55, wps=58307.6, ups=15.73, wpb=3706.5, bsz=190.4, num_updates=141700, lr=8.40069e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.8, wall=9790
2024-01-30 18:16:38 | INFO | train_inner | epoch 097:    210 / 1475 loss=2.288, nll_loss=0.642, ppl=1.56, wps=59252.2, ups=15.68, wpb=3779.9, bsz=200.2, num_updates=141800, lr=8.39773e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=9797
2024-01-30 18:16:44 | INFO | train_inner | epoch 097:    310 / 1475 loss=2.266, nll_loss=0.618, ppl=1.53, wps=58263.7, ups=15.47, wpb=3766.5, bsz=216, num_updates=141900, lr=8.39477e-05, gnorm=0.909, loss_scale=16, train_wall=6, gb_free=29.7, wall=9803
2024-01-30 18:16:50 | INFO | train_inner | epoch 097:    410 / 1475 loss=2.297, nll_loss=0.652, ppl=1.57, wps=58644.8, ups=15.59, wpb=3762.1, bsz=191.3, num_updates=142000, lr=8.39181e-05, gnorm=0.949, loss_scale=16, train_wall=6, gb_free=29.9, wall=9810
2024-01-30 18:16:57 | INFO | train_inner | epoch 097:    510 / 1475 loss=2.295, nll_loss=0.65, ppl=1.57, wps=59741.7, ups=15.44, wpb=3868.9, bsz=197.1, num_updates=142100, lr=8.38886e-05, gnorm=0.93, loss_scale=16, train_wall=6, gb_free=29.7, wall=9816
2024-01-30 18:17:03 | INFO | train_inner | epoch 097:    610 / 1475 loss=2.296, nll_loss=0.651, ppl=1.57, wps=57400, ups=15.38, wpb=3732.9, bsz=192.8, num_updates=142200, lr=8.38591e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.7, wall=9823
2024-01-30 18:17:10 | INFO | train_inner | epoch 097:    710 / 1475 loss=2.293, nll_loss=0.648, ppl=1.57, wps=59111.4, ups=15.5, wpb=3812.7, bsz=207.3, num_updates=142300, lr=8.38296e-05, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.8, wall=9829
2024-01-30 18:17:16 | INFO | train_inner | epoch 097:    810 / 1475 loss=2.314, nll_loss=0.671, ppl=1.59, wps=57445.2, ups=15.54, wpb=3696.7, bsz=185.2, num_updates=142400, lr=8.38002e-05, gnorm=0.979, loss_scale=16, train_wall=6, gb_free=29.7, wall=9835
2024-01-30 18:17:23 | INFO | train_inner | epoch 097:    910 / 1475 loss=2.312, nll_loss=0.67, ppl=1.59, wps=58382.1, ups=15.4, wpb=3791.5, bsz=191.3, num_updates=142500, lr=8.37708e-05, gnorm=0.961, loss_scale=16, train_wall=6, gb_free=29.6, wall=9842
2024-01-30 18:17:29 | INFO | train_inner | epoch 097:   1010 / 1475 loss=2.303, nll_loss=0.66, ppl=1.58, wps=58579.3, ups=15.32, wpb=3824, bsz=196.7, num_updates=142600, lr=8.37414e-05, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.7, wall=9848
2024-01-30 18:17:36 | INFO | train_inner | epoch 097:   1110 / 1475 loss=2.308, nll_loss=0.666, ppl=1.59, wps=59461, ups=15.66, wpb=3797.8, bsz=193.6, num_updates=142700, lr=8.37121e-05, gnorm=0.949, loss_scale=16, train_wall=6, gb_free=29.7, wall=9855
2024-01-30 18:17:42 | INFO | train_inner | epoch 097:   1210 / 1475 loss=2.319, nll_loss=0.678, ppl=1.6, wps=58191.1, ups=15.58, wpb=3734.4, bsz=191, num_updates=142800, lr=8.36827e-05, gnorm=0.983, loss_scale=16, train_wall=6, gb_free=29.7, wall=9861
2024-01-30 18:17:49 | INFO | train_inner | epoch 097:   1310 / 1475 loss=2.31, nll_loss=0.668, ppl=1.59, wps=57990.1, ups=15.45, wpb=3753.6, bsz=194.3, num_updates=142900, lr=8.36535e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.7, wall=9868
2024-01-30 18:17:55 | INFO | train_inner | epoch 097:   1410 / 1475 loss=2.313, nll_loss=0.671, ppl=1.59, wps=57885.6, ups=15.38, wpb=3763.9, bsz=194, num_updates=143000, lr=8.36242e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.7, wall=9874
2024-01-30 18:17:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:17:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:18:02 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 3.481 | nll_loss 1.864 | ppl 3.64 | wps 131269 | wpb 3189.1 | bsz 163.4 | num_updates 143065 | best_loss 3.385
2024-01-30 18:18:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 143065 updates
2024-01-30 18:18:02 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint97.pt
2024-01-30 18:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint97.pt
2024-01-30 18:18:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint97.pt (epoch 97 @ 143065 updates, score 3.481) (writing took 2.486216555000283 seconds)
2024-01-30 18:18:04 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2024-01-30 18:18:04 | INFO | train | epoch 097 | loss 2.3 | nll_loss 0.656 | ppl 1.58 | wps 55535.4 | ups 14.72 | wpb 3771.6 | bsz 195.9 | num_updates 143065 | lr 8.36052e-05 | gnorm 0.949 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 9883
2024-01-30 18:18:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:18:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:18:04 | INFO | fairseq.trainer | begin training epoch 98
2024-01-30 18:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:18:07 | INFO | train_inner | epoch 098:     35 / 1475 loss=2.3, nll_loss=0.655, ppl=1.57, wps=32924.3, ups=8.69, wpb=3787.1, bsz=197.7, num_updates=143100, lr=8.3595e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.8, wall=9886
2024-01-30 18:18:13 | INFO | train_inner | epoch 098:    135 / 1475 loss=2.287, nll_loss=0.641, ppl=1.56, wps=57074.1, ups=15.16, wpb=3763.8, bsz=192.2, num_updates=143200, lr=8.35658e-05, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.7, wall=9892
2024-01-30 18:18:20 | INFO | train_inner | epoch 098:    235 / 1475 loss=2.272, nll_loss=0.623, ppl=1.54, wps=57741.1, ups=15.38, wpb=3754.8, bsz=205, num_updates=143300, lr=8.35366e-05, gnorm=0.921, loss_scale=16, train_wall=6, gb_free=29.7, wall=9899
2024-01-30 18:18:26 | INFO | train_inner | epoch 098:    335 / 1475 loss=2.267, nll_loss=0.619, ppl=1.54, wps=56286.3, ups=15.32, wpb=3674.9, bsz=201.9, num_updates=143400, lr=8.35075e-05, gnorm=0.934, loss_scale=16, train_wall=6, gb_free=29.8, wall=9905
2024-01-30 18:18:33 | INFO | train_inner | epoch 098:    435 / 1475 loss=2.284, nll_loss=0.638, ppl=1.56, wps=58201.3, ups=15.31, wpb=3801.3, bsz=203.3, num_updates=143500, lr=8.34784e-05, gnorm=0.919, loss_scale=16, train_wall=6, gb_free=29.6, wall=9912
2024-01-30 18:18:39 | INFO | train_inner | epoch 098:    535 / 1475 loss=2.289, nll_loss=0.643, ppl=1.56, wps=57417.1, ups=15.43, wpb=3720.4, bsz=194.1, num_updates=143600, lr=8.34493e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.8, wall=9918
2024-01-30 18:18:46 | INFO | train_inner | epoch 098:    635 / 1475 loss=2.296, nll_loss=0.652, ppl=1.57, wps=57880.5, ups=15.3, wpb=3783.1, bsz=199.7, num_updates=143700, lr=8.34203e-05, gnorm=0.951, loss_scale=16, train_wall=6, gb_free=29.7, wall=9925
2024-01-30 18:18:52 | INFO | train_inner | epoch 098:    735 / 1475 loss=2.315, nll_loss=0.673, ppl=1.59, wps=59042.8, ups=15.47, wpb=3817.3, bsz=186.5, num_updates=143800, lr=8.33913e-05, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.7, wall=9931
2024-01-30 18:18:59 | INFO | train_inner | epoch 098:    835 / 1475 loss=2.29, nll_loss=0.646, ppl=1.56, wps=59009.4, ups=15.49, wpb=3808.8, bsz=203.5, num_updates=143900, lr=8.33623e-05, gnorm=0.931, loss_scale=16, train_wall=6, gb_free=29.7, wall=9938
2024-01-30 18:19:05 | INFO | train_inner | epoch 098:    935 / 1475 loss=2.302, nll_loss=0.657, ppl=1.58, wps=57221.2, ups=15.45, wpb=3704, bsz=194.6, num_updates=144000, lr=8.33333e-05, gnorm=0.963, loss_scale=16, train_wall=6, gb_free=30, wall=9944
2024-01-30 18:19:12 | INFO | train_inner | epoch 098:   1035 / 1475 loss=2.299, nll_loss=0.654, ppl=1.57, wps=57786.9, ups=15.32, wpb=3773.1, bsz=201.9, num_updates=144100, lr=8.33044e-05, gnorm=0.948, loss_scale=16, train_wall=6, gb_free=29.7, wall=9951
2024-01-30 18:19:18 | INFO | train_inner | epoch 098:   1135 / 1475 loss=2.311, nll_loss=0.669, ppl=1.59, wps=59269.4, ups=15.46, wpb=3834.4, bsz=186.7, num_updates=144200, lr=8.32755e-05, gnorm=0.98, loss_scale=16, train_wall=6, gb_free=29.7, wall=9957
2024-01-30 18:19:25 | INFO | train_inner | epoch 098:   1235 / 1475 loss=2.309, nll_loss=0.666, ppl=1.59, wps=58862.6, ups=15.56, wpb=3782, bsz=190.6, num_updates=144300, lr=8.32467e-05, gnorm=0.976, loss_scale=16, train_wall=6, gb_free=29.7, wall=9964
2024-01-30 18:19:31 | INFO | train_inner | epoch 098:   1335 / 1475 loss=2.324, nll_loss=0.684, ppl=1.61, wps=59784.3, ups=15.57, wpb=3839.8, bsz=196, num_updates=144400, lr=8.32178e-05, gnorm=0.965, loss_scale=16, train_wall=6, gb_free=29.7, wall=9970
2024-01-30 18:19:37 | INFO | train_inner | epoch 098:   1435 / 1475 loss=2.318, nll_loss=0.676, ppl=1.6, wps=58815.3, ups=15.72, wpb=3742.5, bsz=190.2, num_updates=144500, lr=8.3189e-05, gnorm=0.97, loss_scale=16, train_wall=6, gb_free=29.8, wall=9977
2024-01-30 18:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:19:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:19:42 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 3.48 | nll_loss 1.867 | ppl 3.65 | wps 134344 | wpb 3189.1 | bsz 163.4 | num_updates 144540 | best_loss 3.385
2024-01-30 18:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 144540 updates
2024-01-30 18:19:42 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint98.pt
2024-01-30 18:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint98.pt
2024-01-30 18:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint98.pt (epoch 98 @ 144540 updates, score 3.48) (writing took 2.3503406189847738 seconds)
2024-01-30 18:19:45 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2024-01-30 18:19:45 | INFO | train | epoch 098 | loss 2.298 | nll_loss 0.653 | ppl 1.57 | wps 55386.7 | ups 14.69 | wpb 3771.6 | bsz 195.9 | num_updates 144540 | lr 8.31775e-05 | gnorm 0.948 | loss_scale 16 | train_wall 92 | gb_free 29.7 | wall 9984
2024-01-30 18:19:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:19:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:19:45 | INFO | fairseq.trainer | begin training epoch 99
2024-01-30 18:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:19:49 | INFO | train_inner | epoch 099:     60 / 1475 loss=2.303, nll_loss=0.659, ppl=1.58, wps=33893.1, ups=8.91, wpb=3802, bsz=184.3, num_updates=144600, lr=8.31603e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.7, wall=9988
2024-01-30 18:19:55 | INFO | train_inner | epoch 099:    160 / 1475 loss=2.273, nll_loss=0.625, ppl=1.54, wps=57852.5, ups=15.69, wpb=3687.9, bsz=191.9, num_updates=144700, lr=8.31315e-05, gnorm=0.929, loss_scale=16, train_wall=6, gb_free=29.8, wall=9994
2024-01-30 18:20:01 | INFO | train_inner | epoch 099:    260 / 1475 loss=2.274, nll_loss=0.626, ppl=1.54, wps=58933.6, ups=15.54, wpb=3793.4, bsz=200.2, num_updates=144800, lr=8.31028e-05, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.7, wall=10001
2024-01-30 18:20:08 | INFO | train_inner | epoch 099:    360 / 1475 loss=2.277, nll_loss=0.63, ppl=1.55, wps=58671.1, ups=15.63, wpb=3754.7, bsz=199.8, num_updates=144900, lr=8.30741e-05, gnorm=0.92, loss_scale=16, train_wall=6, gb_free=29.7, wall=10007
2024-01-30 18:20:14 | INFO | train_inner | epoch 099:    460 / 1475 loss=2.272, nll_loss=0.625, ppl=1.54, wps=58757.8, ups=15.54, wpb=3782, bsz=207, num_updates=145000, lr=8.30455e-05, gnorm=0.902, loss_scale=16, train_wall=6, gb_free=29.7, wall=10013
2024-01-30 18:20:21 | INFO | train_inner | epoch 099:    560 / 1475 loss=2.298, nll_loss=0.653, ppl=1.57, wps=60175.9, ups=15.65, wpb=3846.2, bsz=192.2, num_updates=145100, lr=8.30169e-05, gnorm=0.964, loss_scale=16, train_wall=6, gb_free=29.6, wall=10020
2024-01-30 18:20:27 | INFO | train_inner | epoch 099:    660 / 1475 loss=2.299, nll_loss=0.655, ppl=1.57, wps=57976.6, ups=15.62, wpb=3711.1, bsz=191.6, num_updates=145200, lr=8.29883e-05, gnorm=0.957, loss_scale=16, train_wall=6, gb_free=30, wall=10026
2024-01-30 18:20:33 | INFO | train_inner | epoch 099:    760 / 1475 loss=2.304, nll_loss=0.66, ppl=1.58, wps=59814.5, ups=15.64, wpb=3823.4, bsz=193.8, num_updates=145300, lr=8.29597e-05, gnorm=0.95, loss_scale=16, train_wall=6, gb_free=29.7, wall=10033
2024-01-30 18:20:40 | INFO | train_inner | epoch 099:    860 / 1475 loss=2.302, nll_loss=0.659, ppl=1.58, wps=59458.7, ups=15.64, wpb=3802.8, bsz=196, num_updates=145400, lr=8.29312e-05, gnorm=0.956, loss_scale=16, train_wall=6, gb_free=29.6, wall=10039
2024-01-30 18:20:46 | INFO | train_inner | epoch 099:    960 / 1475 loss=2.314, nll_loss=0.671, ppl=1.59, wps=58110.9, ups=15.65, wpb=3712.3, bsz=183.9, num_updates=145500, lr=8.29027e-05, gnorm=0.978, loss_scale=16, train_wall=6, gb_free=29.6, wall=10045
2024-01-30 18:20:53 | INFO | train_inner | epoch 099:   1060 / 1475 loss=2.297, nll_loss=0.654, ppl=1.57, wps=58177.8, ups=15.62, wpb=3724.7, bsz=195.1, num_updates=145600, lr=8.28742e-05, gnorm=0.974, loss_scale=16, train_wall=6, gb_free=29.7, wall=10052
2024-01-30 18:20:59 | INFO | train_inner | epoch 099:   1160 / 1475 loss=2.306, nll_loss=0.663, ppl=1.58, wps=58269.5, ups=15.54, wpb=3748.7, bsz=199.6, num_updates=145700, lr=8.28457e-05, gnorm=0.975, loss_scale=16, train_wall=6, gb_free=29.8, wall=10058
2024-01-30 18:21:06 | INFO | train_inner | epoch 099:   1260 / 1475 loss=2.29, nll_loss=0.645, ppl=1.56, wps=59516.7, ups=15.52, wpb=3835.7, bsz=207.2, num_updates=145800, lr=8.28173e-05, gnorm=0.936, loss_scale=16, train_wall=6, gb_free=29.7, wall=10065
2024-01-30 18:21:12 | INFO | train_inner | epoch 099:   1360 / 1475 loss=2.32, nll_loss=0.679, ppl=1.6, wps=59554.9, ups=15.67, wpb=3800.6, bsz=189.9, num_updates=145900, lr=8.27889e-05, gnorm=0.973, loss_scale=16, train_wall=6, gb_free=29.8, wall=10071
2024-01-30 18:21:18 | INFO | train_inner | epoch 099:   1460 / 1475 loss=2.301, nll_loss=0.658, ppl=1.58, wps=58049.5, ups=15.55, wpb=3732.5, bsz=201.7, num_updates=146000, lr=8.27606e-05, gnorm=0.946, loss_scale=16, train_wall=6, gb_free=29.7, wall=10077
2024-01-30 18:21:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:21:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:21:22 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 3.486 | nll_loss 1.87 | ppl 3.66 | wps 134645 | wpb 3189.1 | bsz 163.4 | num_updates 146015 | best_loss 3.385
2024-01-30 18:21:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 146015 updates
2024-01-30 18:21:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint99.pt
2024-01-30 18:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint99.pt
2024-01-30 18:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint99.pt (epoch 99 @ 146015 updates, score 3.486) (writing took 2.3835979769937694 seconds)
2024-01-30 18:21:24 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2024-01-30 18:21:24 | INFO | train | epoch 099 | loss 2.295 | nll_loss 0.65 | ppl 1.57 | wps 55998.7 | ups 14.85 | wpb 3771.6 | bsz 195.9 | num_updates 146015 | lr 8.27563e-05 | gnorm 0.948 | loss_scale 16 | train_wall 91 | gb_free 29.8 | wall 10083
2024-01-30 18:21:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1475
2024-01-30 18:21:24 | INFO | fairseq.trainer | begin training epoch 100
2024-01-30 18:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-30 18:21:30 | INFO | train_inner | epoch 100:     85 / 1475 loss=2.262, nll_loss=0.613, ppl=1.53, wps=33418.5, ups=8.84, wpb=3782.3, bsz=209.2, num_updates=146100, lr=8.27323e-05, gnorm=0.908, loss_scale=16, train_wall=6, gb_free=29.6, wall=10089
2024-01-30 18:21:36 | INFO | train_inner | epoch 100:    185 / 1475 loss=2.279, nll_loss=0.631, ppl=1.55, wps=58897.1, ups=15.6, wpb=3775.7, bsz=199.2, num_updates=146200, lr=8.2704e-05, gnorm=0.924, loss_scale=16, train_wall=6, gb_free=29.7, wall=10095
2024-01-30 18:21:42 | INFO | train_inner | epoch 100:    285 / 1475 loss=2.283, nll_loss=0.636, ppl=1.55, wps=59594.6, ups=15.64, wpb=3809.7, bsz=199, num_updates=146300, lr=8.26757e-05, gnorm=0.932, loss_scale=16, train_wall=6, gb_free=29.7, wall=10102
2024-01-30 18:21:49 | INFO | train_inner | epoch 100:    385 / 1475 loss=2.274, nll_loss=0.628, ppl=1.55, wps=59504.4, ups=15.55, wpb=3825.6, bsz=207.4, num_updates=146400, lr=8.26475e-05, gnorm=0.925, loss_scale=16, train_wall=6, gb_free=29.8, wall=10108
2024-01-30 18:21:55 | INFO | train_inner | epoch 100:    485 / 1475 loss=2.292, nll_loss=0.647, ppl=1.57, wps=59296.8, ups=15.47, wpb=3832.3, bsz=197.1, num_updates=146500, lr=8.26192e-05, gnorm=0.933, loss_scale=16, train_wall=6, gb_free=29.8, wall=10114
2024-01-30 18:22:02 | INFO | train_inner | epoch 100:    585 / 1475 loss=2.292, nll_loss=0.647, ppl=1.57, wps=59439.8, ups=15.6, wpb=3809.7, bsz=188, num_updates=146600, lr=8.25911e-05, gnorm=0.939, loss_scale=16, train_wall=6, gb_free=29.7, wall=10121
2024-01-30 18:22:08 | INFO | train_inner | epoch 100:    685 / 1475 loss=2.296, nll_loss=0.65, ppl=1.57, wps=59188.7, ups=15.61, wpb=3790.8, bsz=194.8, num_updates=146700, lr=8.25629e-05, gnorm=0.958, loss_scale=16, train_wall=6, gb_free=29.8, wall=10127
2024-01-30 18:22:15 | INFO | train_inner | epoch 100:    785 / 1475 loss=2.309, nll_loss=0.665, ppl=1.59, wps=59442.9, ups=15.7, wpb=3786, bsz=185.8, num_updates=146800, lr=8.25348e-05, gnorm=0.976, loss_scale=16, train_wall=6, gb_free=29.7, wall=10134
2024-01-30 18:22:21 | INFO | train_inner | epoch 100:    885 / 1475 loss=2.297, nll_loss=0.653, ppl=1.57, wps=57906.3, ups=15.66, wpb=3698.4, bsz=189.3, num_updates=146900, lr=8.25067e-05, gnorm=0.984, loss_scale=16, train_wall=6, gb_free=29.8, wall=10140
2024-01-30 18:22:27 | INFO | train_inner | epoch 100:    985 / 1475 loss=2.283, nll_loss=0.638, ppl=1.56, wps=57545.4, ups=15.53, wpb=3706.2, bsz=200.7, num_updates=147000, lr=8.24786e-05, gnorm=0.944, loss_scale=16, train_wall=6, gb_free=29.6, wall=10146
2024-01-30 18:22:34 | INFO | train_inner | epoch 100:   1085 / 1475 loss=2.309, nll_loss=0.666, ppl=1.59, wps=59820.9, ups=15.67, wpb=3817, bsz=193.3, num_updates=147100, lr=8.24506e-05, gnorm=0.959, loss_scale=16, train_wall=6, gb_free=29.7, wall=10153
2024-01-30 18:22:40 | INFO | train_inner | epoch 100:   1185 / 1475 loss=2.312, nll_loss=0.67, ppl=1.59, wps=58636.6, ups=15.66, wpb=3743.6, bsz=188.2, num_updates=147200, lr=8.24226e-05, gnorm=0.976, loss_scale=16, train_wall=6, gb_free=29.8, wall=10159
2024-01-30 18:22:47 | INFO | train_inner | epoch 100:   1285 / 1475 loss=2.303, nll_loss=0.659, ppl=1.58, wps=57725, ups=15.66, wpb=3686.2, bsz=190.5, num_updates=147300, lr=8.23946e-05, gnorm=0.99, loss_scale=16, train_wall=6, gb_free=29.7, wall=10166
2024-01-30 18:22:53 | INFO | train_inner | epoch 100:   1385 / 1475 loss=2.303, nll_loss=0.661, ppl=1.58, wps=59086.2, ups=15.64, wpb=3779.1, bsz=206.6, num_updates=147400, lr=8.23666e-05, gnorm=0.942, loss_scale=16, train_wall=6, gb_free=29.9, wall=10172
2024-01-30 18:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-30 18:22:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:23:01 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 3.484 | nll_loss 1.871 | ppl 3.66 | wps 133897 | wpb 3189.1 | bsz 163.4 | num_updates 147490 | best_loss 3.385
2024-01-30 18:23:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 147490 updates
2024-01-30 18:23:01 | INFO | fairseq.trainer | Saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint100.pt
2024-01-30 18:23:02 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint100.pt
2024-01-30 18:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/checkpoint100.pt (epoch 100 @ 147490 updates, score 3.484) (writing took 2.371152766980231 seconds)
2024-01-30 18:23:03 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2024-01-30 18:23:03 | INFO | train | epoch 100 | loss 2.293 | nll_loss 0.648 | ppl 1.57 | wps 55991.1 | ups 14.85 | wpb 3771.6 | bsz 195.9 | num_updates 147490 | lr 8.23415e-05 | gnorm 0.95 | loss_scale 16 | train_wall 91 | gb_free 29.7 | wall 10182
2024-01-30 18:23:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-30 18:23:03 | INFO | fairseq_cli.train | done training in 10176.8 seconds
Training complete.
Finetuning complete.
----------------------------------------------------------
Translating the test set...
Starting translation...
Preprocessing data...
Source dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt
Target dictionary: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt
Test prefix: /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref=None, validpref=None, testpref='/pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test', align_suffix=None, destdir='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', thresholdtgt=0, thresholdsrc=0, tgtdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.de.txt', srcdict='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)
INFO:fairseq_cli.preprocess:[en] Dictionary: 6520 types
INFO:fairseq_cli.preprocess:[en] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.en: 15516 sents, 256102 tokens, 0.0215% replaced (by <unk>)
INFO:fairseq_cli.preprocess:[de] Dictionary: 7560 types
INFO:fairseq_cli.preprocess:[de] /pfs/work7/workspace/scratch/uxude-MT/dataset/covost/spm/test.de: 15516 sents, 269168 tokens, 0.0241% replaced (by <unk>)
INFO:fairseq_cli.preprocess:Wrote preprocessed data to /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Preprocessing done
Average checkpoints...
Checkpoints folder: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models
Checkpoint path: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Namespace(inputs=['/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models'], output='/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, num_best_checkpoints=0, checkpoint_upper_bound=None)
Traceback (most recent call last):
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 176, in <module>
    main()
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 149, in main
    args.inputs = last_n_checkpoints(
  File "/home/kit/stud/uxude/fairseq/scripts/average_checkpoints.py", line 94, in last_n_checkpoints
    raise Exception(
Exception: ('Found {} checkpoint files but need at least {}', 1, 5)
Checkpoints averaged
Generating translations...
Binary data directory: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset
Prediction output directory: /home/kit/stud/uxude/predictions/finetune_mt_covost
DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=utils
INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/binarized_dataset', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
INFO:fairseq.tasks.translation:[en] dictionary: 6520 types
INFO:fairseq.tasks.translation:[de] dictionary: 7560 types
INFO:fairseq_cli.generate:loading model(s) from /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Traceback (most recent call last):
  File "/home/kit/stud/uxude/miniconda3/envs/nmt/bin/fairseq-generate", line 8, in <module>
    sys.exit(cli_main())
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 413, in cli_main
    main(args)
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 50, in main
    return _main(cfg, sys.stdout)
  File "/home/kit/stud/uxude/fairseq/fairseq_cli/generate.py", line 96, in _main
    models, saved_cfg = checkpoint_utils.load_model_ensemble(
  File "/home/kit/stud/uxude/fairseq/fairseq/checkpoint_utils.py", line 392, in load_model_ensemble
    ensemble, args, _task = load_model_ensemble_and_task(
  File "/home/kit/stud/uxude/fairseq/fairseq/checkpoint_utils.py", line 448, in load_model_ensemble_and_task
    raise IOError("Model file not found: {}".format(filename))
OSError: Model file not found: /pfs/work7/workspace/scratch/uxude-MT/train/finetune_mt_covost/models/avg_last_5_checkpoint.pt
Translations done
Prediction files written for /home/kit/stud/uxude/predictions/finetune_mt_covost/hyp.txt and /home/kit/stud/uxude/predictions/finetune_mt_covost/ref.txt
Sample predictions:
Sample:
Reference:
Sample:
Reference:
WER:
BLEU:
sacreBLEU: System and reference streams have different lengths.
Translation complete.